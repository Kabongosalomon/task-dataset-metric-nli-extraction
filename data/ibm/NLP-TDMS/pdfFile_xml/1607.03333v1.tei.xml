<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RGBD Salient Object Detection via Deep Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">RGBD Salient Object Detection via Deep Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-RGBD saliency detection</term>
					<term>Convolutional neural network</term>
					<term>Laplacian propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem. In this paper, we design a new convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem. In this paper, we design a new convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods.</p><p>Index Terms-RGBD saliency detection, Convolutional neural network, Laplacian propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S ALIENCY detection, which is to predict where human looks in the image, has attracted a lot of research interests in recent years. It serves as an important pre-processing step in many problems such as image classification, image retargeting and object recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Unlike RGB saliency detection which receives much research attention, there are not many exploration on RGBD cases. The recently emerged sensing technologies, such as Time-of-flight sensor and Microsoft Kinect, provides excellent ability and flexibility to capture RGBD image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Detecting RGBD saliency becomes essential for many applications such as 3D content surveillance, retrieval, and image recognition <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In this paper, we focus on how to integrate RGB and the additional depth information for RGBD saliency detection.</p><p>According to how saliency is defined, saliency detection methods can be classified into two categories: top-down approach and bottom-up approach <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Top-down saliency detection is task-dependent that incorporates high level features to locate the salient object. On the other hand, bottomup approach is task-free, and it utilizes low level features that are biologically motivated to estimate salient regions. Most of the existing bottom-up saliency detection methods focus on designing different low-level cues to represent salient objects. The saliency maps of these low-level features are then fused to become a master saliency map. As human attention are preferentially attracted by the high contrast regions with their surrounding, contrast-based features (like the color, edge orientation or texture contrasts) make a crucial role to derive the salient objects. Background <ref type="bibr" target="#b15">[16]</ref> and color compactness priors <ref type="bibr" target="#b16">[17]</ref> consider salient object in different perspectives. The first one leverages the fact that most of the salient objects are far from image boundaries, the latter one utilizes the color compactness of the salient object. In addition to RGB information, depth has been shown to be one of the practical cue to extract saliency <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Most existing approaches for 3D saliency detection either treat the depth map as an indicator to weight the RGB saliency map <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> or consider depth cues as an independent image channel <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Notwithstanding the demonstrated success of these features, whether these features complement to each other remains a question. The interaction mechanism of different saliency features is not well explored, and it is not clear how to integrate 2D saliency features with depth-induced saliency feature in a better way. Linearly combining the saliency maps produced by these features cannot guarantee better result (as shown in <ref type="figure" target="#fig_0">Figure 1g</ref>). Some other more complex combination algorithms have been proposed in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Qin et al. <ref type="bibr" target="#b12">[13]</ref> propose a Multi-layer Cellular Automata (MCA, a Bayesian framework) to merge different saliency maps by taking advantage of the superiority of each saliency detection methods. Recently, several heuristic algorithms are designed to combine the 2D related saliency maps and depthinduced saliency map <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, as restricted by the computed saliency values, these saliency map combination methods are not able to correct wrongly estimated salient regions. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, heuristic based algorithms <ref type="figure" target="#fig_0">(Figure 1d</ref> to 1f) cannot detect the salient object correctly. Adopting these saliency maps for further fusion, neither simple linear fusion ( <ref type="figure" target="#fig_0">Figure 1g</ref>) nor MCA integration ( <ref type="figure" target="#fig_0">Figure 1h</ref>) are able to recover the salient object. We wonder whether a good integration can address this problem by further adopting Convolutional Neural Network technique to train a saliency map integration model. The resulted image shown in <ref type="figure" target="#fig_0">Figure  1i</ref> indicates that saliency map integration is hugely influenced by the quality of the input saliency maps. Based on the these observations, we take one step back to handle more raw and flexible saliency features.</p><p>In this paper, we propose a deep fusion framework for RGBD saliency detection. The proposed method takes advan-arXiv:1607.03333v1 [cs.CV] 12 Jul 2016 (c) Ground truth saliency map. (d) Saliency map by LMH <ref type="bibr" target="#b9">[10]</ref>. (e) Saliency map by ACSD <ref type="bibr" target="#b10">[11]</ref>. (f) Saliency map by GP <ref type="bibr" target="#b11">[12]</ref>. (g) to (i) are the saliency map integration results of (d), (e), and (f). (g) Linear combination (i.e., averaging). (h) MCA integration <ref type="bibr" target="#b12">[13]</ref>. (i) CNN based fusion. (j) Saliency map by the proposed hyper-feature fusion.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><p>tage of the representation learning power of CNN to extract the hyper-feature by fusing different hand-designed saliency features to detect salient object (as shown in <ref type="figure" target="#fig_0">Figure 1j</ref>). We first compute several feature vectors from original RGBD image, which include local and global contrast, background prior, and color compactness. We then propose a CNN architecture to incorporate these regional feature vectors into a more representative and unified features. Compared with feeding raw image pixels, these extracted saliency features are well-designed and they can guide the learning of CNN towards saliency-optimized more effectively. As the resulted saliency map may suffer from local inconsistency and noisy false positive, we further integrate a Laplacian propagation framework with the proposed CNN. This approach propagates high confidence saliency to the other regions by taking account of the color and depth consistency and the intrinsic structure of the input image <ref type="bibr" target="#b24">[25]</ref>, which is able to remove noisy values and produce smooth saliency map. The Laplacian propagation is solved with fast convergence by the adoption of Conjugate gradient and preconditioner. Experimental evaluations demonstrate that, once our deep fusion framework are properly trained, it generalizes well to different datasets without any additional training and outperforms the state-ofthe-art approaches.</p><p>The main contributions of this paper are summarized as follows.</p><p>1. We propose a simple yet effective deep learning model to explore the interaction mechanism of RGB and depthinduced saliency features for RGBD saliency detection. This deep model is able to generate representative and discriminative hyper-features automatically rather than hand-designing heuristical features for saliency.</p><p>2. We adopt Laplacian Propagation to refine the resulted saliency map and solve it with fast convergence. Different from CRF model, our Laplacian Propagation not only considers the spatial consistency but also exploits the intrinsic structure of the input image <ref type="bibr" target="#b24">[25]</ref>. Extensive experiments further demonstrate that this proposed Laplacian Propagation is able to refine the saliency maps of existing approaches, which can be widely adopted as a post processing step.</p><p>3. We investigate the limitations of saliency map integration, and demonstrate that simple features fusion are able to obtain superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we give a brief survey and review of RGB and RGBD saliency detection methods, respectively. Comprehensive literature reviews on these saliency detection methods can be found in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>RGB saliency detection: As suggested by the studies of cognitive science <ref type="bibr" target="#b13">[14]</ref>, bottom-up saliency is driven by low-level stimulus features. This concept is also adopted in computer vision to model saliency. Contrast-based cues, especially color contrast, are the most widely adopted features in previous works. These contrast-based methods can be roughly classified into two categories: local and global approaches. Local method calculates color, edge orientation or texture contrast of a pixel/region with respect to a local window to measure saliency <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, they develop an early local based visual saliency detection method by computing center surrounding differences across multi-scale image features to estimate saliency. Bruce et al. <ref type="bibr" target="#b26">[27]</ref> propose to apply sparse representation on local image patches. However, based only on local contrast, these methods may highlight the boundaries of salient object <ref type="bibr" target="#b16">[17]</ref> and be sensitive to high frequency content <ref type="bibr" target="#b27">[28]</ref>. In contrast to local approach, the global approach measures salient region by estimating the contrast over the entire image. Achanta et al. <ref type="bibr" target="#b28">[29]</ref> model saliency by computing color difference to the mean image color. Cheng et al. <ref type="bibr" target="#b29">[30]</ref> propose a histogram-based global contrast saliency method by considering the spatial weighted coherence. Although these global methods achieve superior performances, they may suffer from distractions when background shares similar color to the salient object. Background and color compactness priors are proposed as a complement to contrast-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b16">[17]</ref>. These methods are built on strong assumptions, which may invalid in some scenarios.</p><p>As each feature has different strengths, some works focus on designing the integration mechanism for different saliency features <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Liu et al. <ref type="bibr" target="#b21">[22]</ref> use CRF to integrate three different features from both local and global point of views. Yan et al. <ref type="bibr" target="#b22">[23]</ref> propose a hierarchical framework to integrate saliency maps in different scales, which can handle small high contrast regions well. Unlike these methods that directly combine the saliency maps obtained from different saliency cues, the proposed method records lowlevel saliency feature in vector forms and jointly learns the interaction mechanism to become a hyper-feature with CNN.</p><p>Similar to the proposed method, CNN has been adopted in <ref type="figure">Fig. 2</ref>: The pipeline of the proposed method. Our method composes of three modules. First, it generates different RGB and depth based saliency feature vectors from the RGBD input image. These generated saliency feature vectors are then fed to the CNN. The CNN takes an input of size 32 × 32 × 6 and generates the saliency confidence value (the probability of this patch belonging to salient). Finally, a Laplacian propagation is performed on the resulted probabilities to extract the final spatially consistent saliency map.</p><p>some other works to extract hierarchical feature representations for detecting salient regions <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. In contrast to most of these deep networks that take raw image pixels as input, the proposed method aims at designing a unified CNN framework to learn the interaction mechanism of different saliency cues. RGBD saliency detection: Unlike RGB saliency detection, RGBD saliency receives less research attention <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Maki et al. <ref type="bibr" target="#b17">[18]</ref> propose an early computational model on depth-based attention by measuring disparity, flow and motion. Similar to color contrast, Zhang et al. design a stereoscopic visual attention algorithm based on depth and motion contrast for 3D video <ref type="bibr" target="#b20">[21]</ref>. Desingh et al. <ref type="bibr" target="#b19">[20]</ref> estimate saliency regions by fusing the saliency maps produced by appearance and depth cues independently. These methods either treat the depth map as an indicator to weight the RGB saliency map <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> or consider depth map as an independent image channel for saliency detection <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. On the other hand, Peng et al. <ref type="bibr" target="#b9">[10]</ref> propose a multi-stage RGBD model to combine both depth and appearance cues to detect saliency. Ren et al. <ref type="bibr" target="#b11">[12]</ref> integrate the normalized depth prior and the surface orientation prior with RGB saliency cues directly for the RGBD saliency detection. These methods combine the depth-induced saliency map with RGB saliency map either directly <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> or in a hierarchy way to calculate the final RGBD saliency map <ref type="bibr" target="#b9">[10]</ref>. However, these saliency map level integration is not optimal as it is restricted by the determined saliency values. On the contrary, we incorporate different saliency cues and fuse them with CNN in feature level. <ref type="figure">Figure 2</ref>, the proposed deep fusion framework for RGBD salient object detection composes of three modules. The first module generates various saliency feature vectors for each superpixel region. The second module is to extract hyper-feature representation from the obtained saliency feature vectors. The third module is the Laplacian propagation framework which helps to detect a spatially consistent saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Saliency feature vectors extraction</head><p>Given an image, we aim to represent saliency by some demonstrated effective saliency features. <ref type="figure" target="#fig_2">Figure 3</ref> gives an illustration on the proposed saliency feature extraction. We first segment the image into N superpixels using SLIC method <ref type="bibr" target="#b39">[40]</ref>. Given a RGB image I, we denote the segmented N regions as P = {P 1 , P 2 , ..., P i , ...P N }. For each superpixel P i , we denote the calculated saliency features as a vector Γ Pi . In the following, we will take region P i (the region that marked in orange in <ref type="figure" target="#fig_2">Figure 3</ref>) as an example to show how we calculate different saliency feature vectors.</p><p>Different from the classical saliency detection methods that directly calculate the saliency values for each superpixel, we record the saliency features for each image region and no further operation is performed to make saliency features as raw as possible. For region P i , there are seven types of feature vectors:</p><formula xml:id="formula_1">Γ Pi = Θ CL i , Θ CG i , Θ DL i , Θ DG i , Θ CB i , Θ DB i , Θ CS i</formula><p>, where C and D represent color and depth information respectively, L indicates that saliency is determined in the local scope and G indicates the global scope, B and S represent the background and color compactness priors respectively. More specifically, the color based feature vectors are recorded in the following formula,</p><formula xml:id="formula_2">       Θ CL i = {P CL i,1 , ..., P CL i,j , ..., P CL i,N } Θ CG i = {P CG i,1 , ..., P CG i,j , ..., P CG i,N } Θ CB i = {P CB i,1 , ..., P CB i,j , ..., P CB i,N b } Θ CS i = {P CS i,1 , ..., P CS i,j , ..., P CS i,N } ,<label>(1)</label></formula><p>and the depth based feature vectors are defined similarly. We compute the color-based features in Lab color space. The local color contrast P CL i,j is calculated as:</p><formula xml:id="formula_3">P CL i,j = t(j)φ L (i, j) c i − c j 2 ,<label>(2)</label></formula><p>where t(j) is the total number of pixels in region P j , and a larger superpixel contributes more to the saliency. c i and c j are the mean color values of the region P i and P j . φ L (i, j) is used to control the spatial influential distance. This weight is defined as exp(− , and x i and x j are the centers of corresponding regions. In our experiment, the parameter σ Lr = 0.15 is set to make the neighbors have higher influence on the calculated contrast values, while the influence of other regions are negligible. Similar to the local color contrast vector, the global color contrast vector is defined as, The difference between the global contrast and local contrast lies in the spatial weight φ G (i, j), where in the global contrast the parameter σ Gr is set to 0.45 to cover the entire image. Likewise, the depth contrast between region P j and region P i can be calculated as in Eq. 4 and Eq. 5.</p><formula xml:id="formula_4">P CG i,j = t(j)φ G (i, j) c i − c j 2 .<label>(3)</label></formula><formula xml:id="formula_5">P DL i,j = t(j)φ L (i, j) |d i − d j | ,<label>(4)</label></formula><formula xml:id="formula_6">P DG i,j = t(j)φ G (i, j) |d i − d j | ,<label>(5)</label></formula><p>where d i and d j are the mean depth values of the region P i and P j respectively. Generally speaking, the colors of an object are compacted together whereas the colors belong to the background are widely distributed in the entire image. The element P CS i,j in the color compactness based feature vector is calculated as following.</p><formula xml:id="formula_7">P CS i,j = φ(c i , c j ) x j − u cs i 2 ,<label>(6)</label></formula><p>where the function φ(c i , c j ) is used to calculate the similarity of two colors c i and c j , and is defined as</p><formula xml:id="formula_8">exp(− ci−cj 2 2 2δ 2 c ). u cs i = M j=1</formula><p>φ(c i , c j )x j defines the weighted mean position of color c i . The parameter δ c is set to 20 in our implementation. We omit the depth compactness prior in our method since the depth map contains only dozens of depth levels and their spatial distributions can be very random. The experiment results also show that whether adding the depth compactness or not does not affect the final results too much. Beside color compactness prior, we further introduce the background prior, which leverages the fact that salient object is less possible to be arranged to close to the image boundaries. We first extract N b regions along the image boundary as pseudo-background regions. Then the color or depth contrast to the pseudo-background regions will be calculated similar to Eq. 3 and Eq. 5. In our experiment, the number of superpixels N is set to 1024 and N b is set to 160.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-feature extraction with CNN</head><p>Given the obtained saliency feature vectors, we then propose a CNN architecture to automatically incorporate them into unified and representative features. We formulate saliency detection as a binary logistic regression problem, which takes a patch as input and output the probabilities of two classes. Our CNN takes an input of size 32 × 32 × 6, and generates a prediction as saliency output. For each superpixel P i , all the seven saliency feature vectors are integrated into a multiple channel image as follows:</p><p>(1) Reshape the N length vector (Θ CL , Θ CG , Θ DL , Θ DG and Θ CS ) to size 32 × 32 to form the first five channels, respectively;</p><p>(2) Perform zero padding to the N b length vector Θ CB and Θ DB to length N/2 and then concatenate and reshape them into size 32 × 32 to form the sixth channel.</p><p>As shown in <ref type="figure">Figure 2</ref>, our network consists of three convolutional layers followed by a fully connected layer and a logistic regression output layer with sigmoid nonlinear function. Following the first and second convolutional layers, we add an average pooling layer for translation invariance. We adopt the sigmoid function as the nonlinear mapping function for the three convolutional layers, while Rectified Linear Unites (ReLUs) is applied in the last two layers. Dropout procedure is applied after the first fully connected layers to avoid overfitting.</p><p>For simplification, we use conv(N, K) and f c(N ) to indicate the convolutional layer and the fully connected layer with N output and kernel size K. pool(T, K) indicates the pooling layer with type T and kernel size K. sig and relu represent the sigmoid function and ReLUs. Then the architecture of our CNN can be described as conv1 <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b4">5)</ref>  <ref type="formula" target="#formula_3">(2)</ref>. This proposed CNN was trained with back-propagation using stochastic gradient descent (SGD).</p><formula xml:id="formula_9">− sig1 − pool1(M EAN, 2)−conv2(12, 5)−sig2−pool2(M EAN, 2)− conv3(24, 3) − sig3 − f c4(200) − relu4 − dropout4 − f c5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Laplacian propagation</head><p>As saliency values are estimated for each superpixel individually, the proposed CNN in Section III-B may fail to retain the spatial consistency and lead to noisy output. <ref type="figure" target="#fig_3">Figure 4c</ref> shows two examples of the saliency maps produced by our CNN for RGBD image. It indicates that our CNN omits some salient regions and wrongly detects some background regions as salient. Despite these misdetected regions, most of the regions with high probability to be salient are correct, robust, and reliable. The same situation also occurs for non-salient probability in the background <ref type="figure" target="#fig_3">(Figure 4d</ref>). As a consequence, these high confident regions are used as guidance, and they are employed in a Laplacian propagation framework <ref type="bibr" target="#b24">[25]</ref> to obtain a more spatially consistent saliency map. The key of the Laplacian propagation lies in propagating the saliency from the regions with high probability to those ambiguous regions by considering two criteria: (1) neighboring regions are more likely to have similar saliency values; and (2) regions within the same manifold are more likely to have similar saliency values. Given a set of superpixels P = {P 1 , P 2 , ..., P N } of an input image I and a label set L = {1, 2}, we denote the salient and non-salient probability generated by the proposed CNN as w sal and w non sal . The superpixels in P are labeled as 1 if w sal &gt; τ 1 , or as 2 if w non sal &gt; τ 2 . The goal of Laplacian propagation is to predict the labels of the remaining regions.</p><p>Let</p><formula xml:id="formula_10">F = [f T 1 , f T 2 , .</formula><p>.., f T N ] T denotes a N × 2 non-negative matrix which corresponds to the binary classification results of P, and each region P i is assigned with a label y i = arg max k={1,2} f ik , where f i = {f i1 , f i2 }. An indicator matrix is defined as Y = [y ik ] N ×2 with y ik = 1 if region P i is labeled as k, otherwise y ik = 0. We further adopt the color and depth information to form the affinity matrix A = [a ij ] N ×N :</p><formula xml:id="formula_11">a ij = exp(− c i − c j 2 2 2δ 2 1 ) exp(− |d i − d j | 2 2δ 2 2 ),<label>(7)</label></formula><p>where the first term defines the color distance of superpixel region P i and P j , and the second term defines the relative depth distance. Most of the elements of the affinity matrix A are zero except for those neighbouring P i and P j pairs. In order to better leverage the local smoothness, we use a two-hierarchy neighboring connection model, i.e., each region is not only connected to its neighboring regions but also connected to the regions that share the same boundaries with its neighboring regions. We set a ii = 0 to avoid self-reinforcement. Then the Laplacian propagation can be formulated to solve the following optimization functions:</p><formula xml:id="formula_12">F * = arg min F Q(F ) 2 ,<label>(8)</label></formula><formula xml:id="formula_13">Q(F ) = N i,j=1 a ij f i √ m ii − f j √ m jj 2 2 +µ N i=1 f i − y i 2 2 ,<label>(9)</label></formula><p>where parameter µ controls the balance between the smoothness constraint (the first term) and the fitting constraint (the second term). m ii is the element of the degree matrix M derived from affinity matrix A, and m ii = j a ij . This designed smoothness constraint not only considers local smoothness but also confines the regions within the same manifold to have the same label by constructing a smooth classifying function. This classifying function can change sufficiently slow along the coherent structure revealed by the original image <ref type="bibr" target="#b24">[25]</ref>. This optimization function Eq. 8 can be solved using an iteration algorithm as shown in <ref type="bibr" target="#b24">[25]</ref>, or it can be reformulated into a linear system. For efficiency, we set the derivative of the Q(F ) to zero and the optimal solution of Eq. 8 can be obtained by solving the following linear equation:</p><formula xml:id="formula_14">(I − αS)F * = Y,<label>(10)</label></formula><p>where I is an identity matrix and α = 1/(1 + µ). We further adopt Conjugate Gradient and preconditioner to solve this linear equation for fast convergence. After propagating from the high probability salient and non-salient regions, the final saliency map is normalized to [0,1] and it is denoted as S = F * . Two examples of the proposed propagation are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Those wrongly estimated regions in <ref type="figure" target="#fig_3">Figure 4b</ref> and <ref type="figure" target="#fig_3">Figure 4c</ref> are corrected in the final saliency maps produced by the Laplacian propagation. In our implementation, parameters τ 1 and τ 2 are adaptively determined by Otsu method <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATIONS</head><p>In this section, we evaluate the proposed method on three datasets, NLPR RGBD salient dataset <ref type="bibr" target="#b9">[10]</ref>, NJUDS2000 stereo datast <ref type="bibr" target="#b10">[11]</ref>, and LFSD dataset <ref type="bibr" target="#b42">[43]</ref>.</p><p>NLPR dataset <ref type="bibr" target="#b9">[10]</ref>. The NLPR RGBD salient dataset <ref type="bibr" target="#b9">[10]</ref>   <ref type="bibr" target="#b35">[36]</ref>, BSCA <ref type="bibr" target="#b12">[13]</ref>, MB+ <ref type="bibr" target="#b41">[42]</ref>, and LEGS <ref type="bibr" target="#b36">[37]</ref> are obtained from RGB image while the saliency maps of LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, GP <ref type="bibr" target="#b11">[12]</ref> are from RGBD image. different indoor and outdoor scenarios. We split this dataset into two part randomly: 750 for training and 250 for testing. NJUDS2000 dataset <ref type="bibr" target="#b10">[11]</ref>. The NJUDS2000 dataset contains 2000 stereo images, as well as the corresponding depth maps and manually labeled groundtruth. The depth maps are generated using an optical flow method. We also split this dataset into two part randomly: 1000 for training and 1000 for testing.</p><p>LFSD dataset <ref type="bibr" target="#b42">[43]</ref>. The LFSD dataset <ref type="bibr" target="#b42">[43]</ref> contains 100 images with depth information and manually labeled groundtruth. The depth information are captured with Lytro light field camera. All the images in this dataset are for testing.</p><p>Evaluation metrics. We compute the precision-recall (PR) curve, mean of average precision and recall, and F-measure score to evaluate the performance of different saliency detection methods. The PR curve indicates the mean precision and recall of the saliency map at different thresholds. The Fmeasure is defined as F β = (1+β 2 )×precision×recall β 2 ×precision+recall , where β 2 is set to 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>We use the randomly sampled 750 training images of NLPR dataset <ref type="bibr" target="#b9">[10]</ref> and the randomly sampled 1000 training images of NJUDS2000 dataset <ref type="bibr" target="#b10">[11]</ref> to train our deep learning framework. These randomly selected training dateset covers more than 1000 kinds of common objects under different circumstances. The remaining NLPR, NJUDS2000, and LFSD datesets are used to verify the generalization of the proposed method.</p><p>The proposed method is implemented using Matlab. We set the momentum in our network to 0.9 and the weight decay to be 0.0005. The learning rate of our network is gradually decreased from 1 to 0.001. Due to the "data-hungry" nature of CNN, the existing training data is insufficient for training, in addition to the dropout procedure, we also employed data augmentation to enrich our training dataset. Similar to <ref type="bibr" target="#b43">[44]</ref>, we adopted two different image augmentation operations, the first one consists of image translations and horizontal flipping and the other is to alter the intensities of the RGB channels. These data augmentations greatly enlarge our training dataset and make it possible for us to train the proposed CNN without overfitting. It took around 5 ∼ 7 days for our training to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Comparison</head><p>In this section, we compare our method with four stateof-the-art methods designed for RGB image (S-CNN <ref type="bibr" target="#b35">[36]</ref>, BSCA <ref type="bibr" target="#b12">[13]</ref>, MB+ <ref type="bibr" target="#b41">[42]</ref>, and LEGS <ref type="bibr" target="#b36">[37]</ref>), and three RGBD saliency methods designed specially for RGBD image (LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, and GP <ref type="bibr" target="#b11">[12]</ref>).</p><p>The results of these different methods are either provided by authors or achieved using the publicly available source codes. The qualitative comparisons of different methods on different scenes are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. As can be seen in the first and fifth rows of <ref type="figure" target="#fig_4">Figure 5</ref>, the salient object has a high color contrast with the background, as thus RGB saliency methods are able to detect salient object correctly. However, when the salient object shares similar color with the background, e.g., sixth, seventh, and eighth rows in <ref type="figure" target="#fig_4">Figure 5</ref>, it is difficult for existing RGB models to extract saliency. With the help of depth information, salient object can be easily detected by the proposed RGBD method. <ref type="figure" target="#fig_4">Figure 5</ref> also shows that the proposed method consistently outperforms all the other RGBD saliency methods (LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, and GP <ref type="bibr" target="#b11">[12]</ref>).</p><p>The quantitative comparisons on NLPR, NJUDS2000, and LFSD dataset are shown in <ref type="figure">Figure 6</ref> and <ref type="table" target="#tab_0">Table I</ref>. <ref type="figure">Figure 6</ref> and <ref type="table" target="#tab_0">Table I</ref> show that the proposed method performs favorably against the existing algorithms with higher precision, recall values and F-measure scores on all the three datasets. For the NLPR dataset, it is challenging as most of the salient object share similar color to the background. As a consequence, RGB saliency methods perform relative worse than RGBD saliency methods in terms of precision. By providing accurate depth map (NLPR dataset), LMH <ref type="bibr" target="#b9">[10]</ref> and GP <ref type="bibr" target="#b11">[12]</ref> methods perform well in both precision and recall. However, they performs not well when tested on the NJUDS2000 dataset and LFSD dataset. This is because these two datasets provide only the rough depth information (calculated from stereo images or using Light field camera), LMH <ref type="bibr" target="#b9">[10]</ref> and GP <ref type="bibr" target="#b11">[12]</ref> can only detect a small fraction of the salient objects (high precision but with low recall). ACSD <ref type="bibr" target="#b10">[11]</ref> works worse when the salient object lies in the same plane with the background, e.g., the third row in the <ref type="figure" target="#fig_4">Figure 5</ref>, and the bad quantitative results on the NLPR dataset. Both qualitative and quantitative results show that the proposed method performs better in terms of accuracy and robustness than the compared methods with RGBD input images. Saliency maps vs. features. In here we conduct a series of experiments to analyze the flexibility of the proposed framework and the effectiveness of Laplacian propagation.</p><p>Apart from previous heuristic saliency map merging algorithm <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we further compare our method with four other saliency map integration methods on three test dataset <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b42">[43]</ref> to show the flexibility of fusing different cues in feature level. These four integration methods are directly linear fusion (LF), fusing in CRF <ref type="bibr" target="#b21">[22]</ref>, the latest Multi-layer Cellular Automata (MCA) integration <ref type="bibr" target="#b12">[13]</ref>, and a CNN based fusion (denoted as CNN-F). To investigate the importance of saliency map quality, we test these saliency map merging methods on two set of inputs. The first set is from seven saliency maps computed by widely used features (similar to those seven saliency feature vectors computed in section III-A), and the second set is from more representative sophisticated saliency maps (obtained using three state-of-theart RGBD saliency detection methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>).</p><p>The original CRF fusion framework in <ref type="bibr" target="#b21">[22]</ref> is utilized for merging three color based saliency maps. In our implementation, we retrain this CRF framework for merging the seven adopted fundamental saliency maps and three sophisticated saliency maps respectively 1 .</p><p>For CNN-F, we utilize the same CNN architecture as shown in <ref type="figure" target="#fig_2">Fig. 3</ref> to perform the CNN based saliency map fusion, i.e., the same convolutional layers and fully connected layers except the input layer. More specifically, we formulate the saliency map merging as a binary logistic regression problem, which takes several saliency map patches as input (size 52 × 52 × 7 for fundamental saliency map merging and 52×52×3 for sophisticated saliency map merging), and output the probabilities of the pixel being salient and non-salient. CNN-F is trained in patch-wise manner. We collect training samples by cropping patches of size 52×52 from each saliency map using sliding window. We label a patch as salient if the central pixel is salient or 75% pixels in this patch are salient, otherwise it is labeled as non-salient. This CNN-F is trained on the cropped patches of the NLPR training set <ref type="bibr" target="#b9">[10]</ref> and NJUDS2000 training set <ref type="bibr" target="#b10">[11]</ref>.</p><p>The relevant comparison results of our proposed methods with these saliency map merging methods are shown in <ref type="figure">Figure  7</ref>, <ref type="table" target="#tab_0">Table II, Table III, and Table IV</ref>. "Fundamental fusion" represents the results of four merging methods performed on seven fundamental saliency maps. "Heuristic fusion" gives the results of two state-of-the-art heuristic saliency map merging methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, while "Sophisticated fusion" gives the results of four merging methods performed on three sophisticated saliency maps (calculated from the existing state-of-theart RGBD saliency detection methods LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, and GP <ref type="bibr" target="#b11">[12]</ref>).</p><p>For "Fundamental fusion" in <ref type="table" target="#tab_0">Table IV</ref>, all the existing saliency map merging methods (including deep learning framework) cannot achieve satisfactory performance. Even though feeding with the state-of-the-art sophisticated saliency maps, these saliency merging methods still perform worse than our saliency feature fusion without LP framework (0.8071 vs 0.8157), which further validates the flexibility of our feature level fusion. Note that 0.8157 are obtained from our initial saliency feature fusion network, which performs only on the pixel level and without considering spatial consistency. Our model achieves superior performance even though the input features are very simple (similar to the features used in "Fundamental fusion"). Compared to those methods using similar features (in "Fundamental fusion"), we can observe   <ref type="figure">Fig. 6</ref>: PR curves and F-measure curves of different methods on three datasets. Left: quantitative comparisons on the 250 test images of NLPR dataset <ref type="bibr" target="#b9">[10]</ref>. Middle: quantitative comparisons on the 1000 test images of NJUDS2000 dataset <ref type="bibr" target="#b10">[11]</ref>. Right: quantitative comparisons on the LFSD dataset <ref type="bibr" target="#b42">[43]</ref>.    <ref type="figure">Fig. 7</ref>: More examples to show the problem of saliency map merging methods. MCA and CNN F are the results of sophisticated fusion (fusing LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, and GP <ref type="bibr" target="#b11">[12]</ref>). "CNN F init" and "ours init" are the initial results of CNN F and proposed hyper-feature without Lapalacian propagation respectively. that fusing features is much more flexible than fusing saliency map.</p><p>Analysis of Laplacian Propagation. We then evaluate the effective of the proposed Laplacian propagation, and the optimized results of the existing methods using Laplacian propagation. The F-measure scores of our RGBD method without Laplacian propagation on three test dataset <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b42">[43]</ref> are shown in blue in <ref type="table" target="#tab_0">Table II, Table III, and Table  IV</ref>. These learned hyper-features still outperform the stateof-the-art approaches, while with LP we achieve almost 0.79, 0.79, and 0.84 F-measures. <ref type="figure" target="#fig_7">Figure 8</ref> shows some examples of the optimized results of the existing methods (LMH <ref type="bibr" target="#b9">[10]</ref>, ACSD <ref type="bibr" target="#b10">[11]</ref>, and GP <ref type="bibr" target="#b11">[12]</ref>) using Laplacian propagation. These quantitative and qualitative experimental evaluations further demonstrate that the proposed Laplacian propagation is able to refine the saliency maps of existing methods, which can be widely adopted as a post processing step.</p><p>Failure cases. <ref type="figure" target="#fig_6">Figure 9</ref> gives more visual results and some failure cases of our proposed method on RGBD images. Compared with the these two pictures, we can find that depth information is more helpful when the salient objects have high depth contrast with background or lie closer to the camera. Our method may fail when the salient object shares a very similar color and depth information with the background. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel RGBD saliency detection method. Our framework consists of three different modules. The first module generates various low level saliency feature vectors from the input image. The second module learns the interaction mechanism of RGB saliency features and depth-induced features and produces hyper-feature using CNN. Feeding with these hand-designed features can guide the learning process of CNN towards saliency-optimized. In the third module, we integrate a Laplacian propagation framework with CNN to obtain a spatially consistent saliency map. Both quantitative and qualitative experiment results show that the fused RGBD hyper-feature outperforms all the state-of-the-art methods.</p><p>We demonstrated that an optimized fusion leads to superior performance, and this flexible hyper-feature extraction framework can be further extended by including more saliency cues (e.g., flash cue <ref type="bibr" target="#b27">[28]</ref>). We aim to explore a deeper and more effective fusion network and extend it to other applications in our future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Example to show the problem of saliency map merging methods. (a) Original RGB image. (b) Original depth image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>xi−xj 2 2 2σ 2</head><label>22</label><figDesc>Lr )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Saliency feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of the proposed Laplacian propagation. (a) RGB image. (b) Depth image. (c) Saliency probability produced by the proposed CNN. (d) Background (non-salient) probability produced by the proposed CNN. (e) Refined saliency map using (c) and (d) as guidance. (f) The ground truth saliency map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>contains 1000 images captured by Microsoft Kinect in Visual comparisons of the proposed deep fusion framework with four RGB saliency methods and three RGBD saliency methods. The saliency maps of S-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>More visual results and some failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Examples to show the effectiveness of Laplacian propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Shengfeng</head><label></label><figDesc>He obtained his B.Sc. degree and M.Sc. degree from Macau University of Science and Technology, and the Ph.D degree from City University of Hong Kong. He is currently a Research Fellow at City University of Hong Kong. His research interests include computer vision, image processing, computer graphics, and deep learning. Jiawei Zhang received his BEng degree in Electronic Information Engineering from the University of Science and Technology of China in 2011 and master degree in Institute of Acoustics, Chinese Academy of Sciences in 2014. He is currently a Computer Science PhD student in City university of Hong Kong. Jiandong Tian received his B.S. Tech. degree in automation at Heilongjiang University, China, in 2005. He received his Ph.D. degree in Pattern Recognition and Intelligent System at Chinese Academy of Sciences, China, in 2011. He is currently an asassociate professor in computer vision at State Key Laboratory of Robotic, Shenyang Institute of Automation, Chinese Academy of Sciences. His research interests include pattern recognition and robot vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Yandong</head><label></label><figDesc>Tang received B.S. and M.S. degrees in the department of mathematics, Shandong University in 1984 and 1987. In 2002 he received the doctor's degree in applied mathematics from the University of Bremen, Germany. Currently he is a professor in Shenyang Institute of Automation, Chinese Academy of Sciences. His research interests include robot vision, pattern recognition and numerical computation. Qingxiong Yang received the B.E. degree in electronic engineering and information science from the University of Science and Technology of China, Hefei, China, in 2004, and the Ph.D. degree in electrical and computer engineering from the University of Illinois at Urbana-Champaign, Champaign, IL, USA, in 2010. He is currently an Assistant Professor with the Department of Computer Science, City University of Hong Kong, Hong Kong. His research interests reside in computer vision and computer graphics. He was a recipient of the Best Student Paper Award at the 2010 International Workshop on Multimedia Signal Processing and the Best Demo Award at the 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The F-measure scores of different approaches on three datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell cols="2">S-CNN</cell><cell>BSCA</cell><cell>MB+</cell><cell>LEGS</cell><cell>LMH</cell><cell>ACSD</cell><cell>GP</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">NLPR test set</cell><cell cols="2">0.5141</cell><cell>0.5634</cell><cell>0.6049</cell><cell>0.6335</cell><cell>0.6519</cell><cell>0.5448</cell><cell>0.7184</cell><cell>0.7823</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">NJUD test set</cell><cell cols="2">0.6096</cell><cell>0.6133</cell><cell>0.6156</cell><cell>0.6791</cell><cell>0.6381</cell><cell>0.6952</cell><cell>0.7246</cell><cell>0.7874</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">LFSD dataset</cell><cell cols="2">0.6982</cell><cell>0.7311</cell><cell>0.7029</cell><cell>0.7384</cell><cell>0.7041</cell><cell>0.7567</cell><cell>0.7877</cell><cell>0.8439</cell></row><row><cell>0 0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>The comparisons of F-measure scores for different saliency map merging approaches with or without LP on NLPR test dataset<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>LP?</cell><cell>LF</cell><cell cols="2">Fundamental fusion CRF MCA</cell><cell>CNN-F</cell><cell>LF</cell><cell cols="2">Sophisticated fusion CRF MCA</cell><cell>CNN-F</cell><cell cols="2">Heuristic fusion LMH GP</cell><cell>Ours</cell></row><row><cell>No</cell><cell>0.393</cell><cell>0.2991</cell><cell>0.3713</cell><cell>0.4667</cell><cell>0.7020</cell><cell>0.698</cell><cell>0.7017</cell><cell>0.6921</cell><cell>0.6519</cell><cell>0.718</cell><cell>0.7315</cell></row><row><cell>Yes</cell><cell>0.536</cell><cell>0.398</cell><cell>0.486</cell><cell>0.597</cell><cell>0.711</cell><cell>0.739</cell><cell>0.7623</cell><cell>0.737</cell><cell>0.665</cell><cell>0.7111</cell><cell>0.7823</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>The comparisons of F-measure scores for different saliency map merging approaches with or without Laplacian propagation (LP) on NJUD test dataset<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell>LP?</cell><cell>LF</cell><cell cols="3">Fundamental fusion CRF MCA CNN-F</cell><cell>LF</cell><cell cols="2">Sophisticated fusion CRF MCA</cell><cell>CNN-F</cell><cell cols="2">Heuristic fusion LMH GP</cell><cell>Ours</cell></row><row><cell>No</cell><cell>0.437</cell><cell>0.450</cell><cell>0.458</cell><cell>0.644</cell><cell>0.675</cell><cell>0.671</cell><cell>0.7376</cell><cell>0.7319</cell><cell>0.6381</cell><cell>0.7246</cell><cell>0.7447</cell></row><row><cell>Yes</cell><cell>0.605</cell><cell>0.609</cell><cell>0.632</cell><cell>0.731</cell><cell>0.698</cell><cell>0.741</cell><cell>0.742</cell><cell>0.7423</cell><cell>0.6810</cell><cell>0.7179</cell><cell>0.7874</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>The comparisons of F-measure scores for different saliency map merging approaches with or without LP on LFSD dataset<ref type="bibr" target="#b42">[43]</ref>.</figDesc><table><row><cell>RGB</cell><cell>Depth</cell><cell></cell><cell>GT</cell><cell>LMH</cell><cell>ACSD</cell><cell></cell><cell>GP</cell><cell>MCA</cell><cell cols="3">CNN F Init CNN F+LP</cell><cell>Our Init</cell><cell>Ours+LP</cell></row><row><cell></cell><cell>LP?</cell><cell>LF</cell><cell cols="3">Fundamental fusion CRF MCA CNN-F</cell><cell>LF</cell><cell cols="2">Sophisticated fusion CRF MCA</cell><cell>CNN-F</cell><cell cols="2">Heuristic fusion LMH GP</cell><cell>Ours</cell></row><row><cell></cell><cell>No</cell><cell>0.461</cell><cell>0.436</cell><cell>0.558</cell><cell>0.672</cell><cell>0.723</cell><cell>0.771</cell><cell>0.8071</cell><cell>0.706</cell><cell>0.704</cell><cell>0.7877</cell><cell>0.8157</cell></row><row><cell></cell><cell>Yes</cell><cell>0.616</cell><cell>0.693</cell><cell>0.654</cell><cell>0.757</cell><cell>0.762</cell><cell>0.792</cell><cell>0.802</cell><cell>0.800</cell><cell>0.718</cell><cell>0.7830</cell><cell>0.8439</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We adopt the implementation in http://www.cs.unc.edu/ ∼ vicente/code. html for training and testing. This CRF is trained on the NLPR training dataset<ref type="bibr" target="#b9">[10]</ref> and the NJUDS2000 training dataset<ref type="bibr" target="#b10">[11]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is bottom-up attention useful for object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1007" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative spatial saliency for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3506" to="3513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A time-of-flight depth sensorsystem description, issues and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Gokturk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bamji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="35" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmenting imple? objects using rgb-d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4406" to="4413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object-based rgbd image cosegmentation with mutex constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4428" to="4436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Second-order constrained parametric proposals and sequential search-based structured prediction for semantic segmentation in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1115" to="1119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pisa: Pixelwise image saliency by aggregating complementary appearance contrast measures with spatial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="2115" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A computational model of depth-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nordlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="734" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth matters: Influence of depth cues on visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="101" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Depth really matters: Improving visual salient region detection with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereoscopic visual attention model for 3d video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Salient region detection via integrating diffusion-based compactness and local contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3308" to="3320" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5878</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection with flash and no-flash image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection by multicontext deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supercnn: A superpixelwise convolutional neural network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Computational model of stereoscopic 3d visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Dasilva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lecallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ricordel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2151" to="2165" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mȇch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">She is currently a joint Ph.D. student of University of Chinese Academy of Sciences and City University of Hong Kong. Her research interests include illumination modeling, image processing</title>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Liangqiong Qu received the B.S. degree in automation from Central South University</orgName>
		</respStmt>
	</monogr>
	<note>saliency detection and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
