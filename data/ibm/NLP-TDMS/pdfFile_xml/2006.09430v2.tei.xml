<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 WASSERSTEIN EMBEDDING FOR GRAPH LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">HRL Laboratories, LLC</orgName>
								<orgName type="institution">‡ University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Naderializadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">HRL Laboratories, LLC</orgName>
								<orgName type="institution">‡ University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
							<email>gustavo@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">HRL Laboratories, LLC</orgName>
								<orgName type="institution">‡ University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
							<email>hhoffmann@hrl.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">HRL Laboratories, LLC</orgName>
								<orgName type="institution">‡ University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 WASSERSTEIN EMBEDDING FOR GRAPH LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Wasserstein Embedding for Graph Learning (WEGL), a novel and fast framework for embedding entire graphs in a vector space, in which various machine learning models are applicable for graph-level prediction tasks. We leverage new insights on defining similarity between graphs as a function of the similarity between their node embedding distributions. Specifically, we use the Wasserstein distance to measure the dissimilarity between node embeddings of different graphs. Unlike prior work, we avoid pairwise calculation of distances between graphs and reduce the computational complexity from quadratic to linear in the number of graphs. WEGL calculates Monge maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph. We evaluate our new graph embedding approach on various benchmark graph-property prediction tasks, showing state-of-the-art classification performance while having superior computational efficiency. The code is available at https://github.com/navid-naderi/WEGL. * Denotes equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many exciting and practical machine learning applications involve learning from graph-structured data. While images, videos, and temporal signals (e.g., audio or biometrics) are instances of data that are supported on grid-like structures, data in social networks, cyber-physical systems, communication networks, chemistry, and bioinformatics often live on irregular structures <ref type="bibr" target="#b2">(Backstrom &amp; Leskovec, 2011;</ref><ref type="bibr" target="#b54">Sadreazami et al., 2017;</ref><ref type="bibr" target="#b30">Jin et al., 2017;</ref><ref type="bibr" target="#b0">Agrawal et al., 2018;</ref><ref type="bibr" target="#b49">Naderializadeh et al., 2020)</ref>. One can represent such data as (attributed) graphs, which are universal data structures. Efficient and generalizable learning from graph-structured data opens the door to a vast number of applications, which were beyond the reach of classic machine learning (ML) and, more specifically, deep learning (DL) algorithms.</p><p>Analyzing graph-structured data has received significant attention from the ML, network science, and signal processing communities over the past few years. On the one hand, there has been a rush toward extending the success of deep neural networks to graph-structured data, which has led to a variety of graph neural network (GNN) architectures. On the other hand, the research on kernel approaches <ref type="bibr" target="#b23">(Gärtner et al., 2003)</ref>, perhaps most notably the random walk kernel <ref type="bibr" target="#b31">(Kashima et al., 2003)</ref> and the Weisfeiler-Lehman (WL) kernel <ref type="bibr" target="#b57">(Shervashidze et al., 2011;</ref><ref type="bibr" target="#b46">Morris et al., 2019;</ref>, remains an active field of study and the methods developed therein provide competitive performance in various graph representation tasks (see the recent survey by ).</p><p>To learn graph representations, GNN-based frameworks make use of three generic modules, which provide i) feature aggregation, ii) graph pooling (i.e., readout), and iii) classification . The feature aggregator provides a vector representation for each node of the graph, referred to as a node embedding. The graph pooling module creates a representation for the graph from its node embeddings, whose dimensionality is fixed regardless of the underlying graph size, and which can then be analyzed using a downstream classifier of choice. On the graph kernel side, one leverages a kernel to measure the similarities between pairs of graphs, and uses conventional kernel methods to perform learning on a set of graphs <ref type="bibr" target="#b26">(Hofmann et al., 2008)</ref>. A recent example of such methods is the framework provided by <ref type="bibr" target="#b59">Togninalli et al. (2019)</ref>, in which the authors propose a novel node embedding inspired by the WL kernel, and combine the resulting node embeddings with the Wasserstein distance <ref type="bibr" target="#b61">(Villani, 2008;</ref><ref type="bibr" target="#b35">Kolouri et al., 2017)</ref> to measure the dissimilarity between two graphs. Afterwards, they leverage conventional kernel methods based on the pairwise-measured dissimilarities to perform learning on graphs.</p><p>Considering the ever-increasing scale of graph datasets, which may contain tens of thousands of graphs or millions to billions of nodes per graph, the issue of scalability and algorithmic efficiency becomes of vital importance for graph learning methods <ref type="bibr" target="#b25">(Hernandez &amp; Brown, 2020;</ref>. However, both of the aforementioned paradigms of GNNs and kernel methods suffer in this sense. On the GNN side, acceleration of the training procedure is challenging and scales poorly as the graph size grows <ref type="bibr" target="#b4">(Bojchevski et al., 2019)</ref>. On the graph kernel side, the need for calculating the matrix of all pairwise similarities can be a burden in datasets with a large number of graphs, especially if calculating the similarity between each pair of graphs is computationally expensive. For instance, in the method proposed in <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>, the computational complexity of each calculation of the Wasserstein distance is cubic in the number of nodes (or linearithmic for the entropy-regularized distance).</p><p>To overcome these issues, inspired by the linear optimal transport framework of <ref type="bibr" target="#b62">(Wang et al., 2013)</ref>, we propose a linear Wasserstein Embedding for Graph Learning, which we refer to as WEGL. Our proposed approach embeds a graph into a Hilbert space, where the 2 distance between two embedded graphs provides a true metric between the graphs that approximates their 2-Wasserstein distance. For a set of M graphs, the proposed method provides:</p><p>1. Reduced computational complexity of estimating the graph Wasserstein distance <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref> for a dataset of M graphs from a quadratic complexity in the number of graphs, i.e., M (M −1) 2 calculations, to linear complexity, i.e., M calculations of the Wasserstein distance; and 2. An explicit Hilbertian embedding for graphs, which is not restricted to kernel methods, and therefore can be used in conjunction with any downstream classification framework.</p><p>We show that compared to multiple GNN and graph kernel baselines, WEGL achieves either stateof-the-art or competitive results on benchmark graph-level classification tasks, including classical graph classification datasets <ref type="bibr" target="#b32">(Kersting et al., 2020)</ref> and the recent molecular property-prediction benchmarks . We also compare the algorithmic efficiency of WEGL with two baseline GNN and graph kernel methods and demonstrate that it is much more computationally efficient relative to those algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>In this section, we provide a brief background on different methods for deriving representations for graphs and an overview on Wasserstein distances by reviewing the related work in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRAPH REPRESENTATION METHODS</head><p>Let G = (V, E) denote a graph, comprising a set of nodes V and a set of edges E ⊆ V 2 , where two nodes u, v ∈ V are connected to each other if and only if (u, v) ∈ E. 1 For each node v ∈ V, we define its set of neighbors as N v {u ∈ V : (u, v) ∈ E}. The nodes of the graph G may have categorical labels and/or continuous attribute vectors. We use a unified notation of x v ∈ R F to denote the label and/or attribute vector of node v ∈ V, where F denotes the node feature dimensionality. Moreover, we use w uv ∈ R E to denote the edge feature vector for any edge (u, v) ∈ E, where E denotes the edge feature dimensionality. Node and edge features may be present depending on the graph dataset under consideration.</p><p>To learn graph properties from the graph structure and its node/edge features, one can use a function ψ : G → H to map any graph G in the space of all possible graphs G to an embedding ψ(G) in a Hilbert space H. Kernel methods have been among the most popular ways of creating such graph embeddings. A graph kernel is defined as a function k : G 2 → R, where for two graphs G and G , k(G, G ) represents the inner product of the embeddings ψ(G) and ψ(G ) over the Hilbert space H. The mapping ψ could be explicit, as in graph convolutional neural networks, or implicit as in the case of the kernel similarity function k(·, ·) (i.e., the kernel trick). <ref type="bibr" target="#b37">Kriege et al. (2014)</ref> provide a thorough discussion on explicit and implicit embeddings for learning from graphs. <ref type="bibr" target="#b31">Kashima et al. (2003)</ref> introduced graph kernels based on random walks on labeled graphs. Subsequently, shortest-path kernels were introduced in <ref type="bibr" target="#b5">(Borgwardt &amp; Kriegel, 2005)</ref>. These works have been followed by graphlet and Weisfeiler-Lehman subtree kernel methods <ref type="bibr" target="#b56">(Shervashidze et al., 2009;</ref><ref type="bibr" target="#b45">Morris et al., 2017)</ref>. More recently, kernel methods using spectral approaches <ref type="bibr" target="#b36">(Kondor &amp; Pan, 2016)</ref>, assignment-based approaches <ref type="bibr" target="#b38">(Kriege et al., 2016;</ref><ref type="bibr" target="#b51">Nikolentzos et al., 2017)</ref>, and graph decomposition algorithms <ref type="bibr" target="#b52">(Nikolentzos et al., 2018)</ref> have also been proposed in the literature.</p><p>Despite being successful for many years, kernel methods often fail to leverage the explicit continuous features that are provided for the graph nodes and/or edges, making them less adaptable to the underlying data distribution. To alleviate these issues, and thanks in part to the prominent success of deep learning in many domains, including computer vision and natural language processing, techniques based on graph neural networks (GNNs) have emerged as an alternative paradigm for learning representations from graph-based data. In general, a GNN comprises multiple hidden layers, where at each layer, each node combines the features of its neighboring nodes in the graph to derive a new feature vector. At the GNN output, the feature vectors of all nodes are aggregated using a readout function (such as global average pooling), resulting in the final graph embedding ψ(G). More details on the combining and readout mechanisms of GNNs are provided in Appendix A.4. <ref type="bibr" target="#b33">Kipf and Welling (Kipf &amp; Welling, 2016)</ref> proposed a GNN architecture based on a graph convolutional network (GCN) framework. This work, alongside other notable works on geometric deep learning <ref type="bibr" target="#b13">(Defferrard et al., 2016)</ref>, initiated a surge of interest in GNN architectures, which has led to several architectures, including the Graph Attention network (GAT) <ref type="bibr" target="#b60">(Veličković et al., 2017)</ref>, Graph SAmple and aggreGatE (GraphSAGE) <ref type="bibr" target="#b24">(Hamilton et al., 2017)</ref>, and the Graph Isomorphism Network (GIN) . Each of these architectures modifies the GNN combining and readout functions and demonstrates state-of-the-art performance in a variety of graph representation learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">WASSERSTEIN DISTANCES</head><p>Let µ i denote a Borel probability measure with finite p th moment defined on Z ⊆ R d , with corresponding probability density function p i , i.e., dµ i (z) = p i (z)dz. The 2-Wasserstein distance between µ i and µ j defined on Z, Z ⊆ R d is the solution to the optimal mass transportation problem with 2 transport cost <ref type="bibr" target="#b61">(Villani, 2008)</ref>:</p><formula xml:id="formula_0">W 2 (µ i , µ j ) = inf γ∈Γ(µi,µj ) Z×Z z − z 2 dγ(z, z ) 1 2 ,<label>(1)</label></formula><p>where Γ(µ i , µ j ) is the set of all transportation plans γ ∈ Γ(µ i , µ j ) such that γ(A × Z ) = µ i (A) and γ(Z × B) = µ j (B) for any Borel subsets A ⊆ Z and B ⊆ Z . Due to Brenier's theorem <ref type="bibr" target="#b7">(Brenier, 1991)</ref>, for absolutely continuous probability measures µ i and µ j (with respect to the Lebesgue measure), the 2-Wasserstein distance can be equivalently obtained from</p><formula xml:id="formula_1">W 2 (µ i , µ j ) = inf f ∈M P (µi,µj ) Z z − f (z) 2 dµ i (z) 1 2 ,<label>(2)</label></formula><formula xml:id="formula_2">where M P (µ i , µ j ) = {f : Z → Z | f # µ i = µ j } and f # µ i represents the pushforward of measure µ i , characterized as B dµ j (z ) = f −1 (B) dµ i (z) for any Borel subset B ⊆ Z .<label>(3)</label></formula><p>The mapping f is referred to as a transport map <ref type="bibr" target="#b35">(Kolouri et al., 2017)</ref>, and the optimal transport map is called the Monge map. For absolutely continuous measures, the differential form of the above equation takes the following form, det(Df (z))p j (f (z)) = p i (z), which is referred to as the Jacobian equation. For discrete probability measures, when the transport plan γ is a deterministic optimal coupling, such a transport plan is referred to as a Monge coupling <ref type="bibr" target="#b61">(Villani, 2008)</ref>.</p><p>Recently, Wasserstein distances have been used for representation learning on graphs and images <ref type="bibr" target="#b59">(Togninalli et al., 2019;</ref><ref type="bibr" target="#b66">Zhang et al., 2020;</ref><ref type="bibr" target="#b3">Bécigneul et al., 2020)</ref>. In particular, <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref> proposed a Wasserstein kernel for graphs that involves pairwise calculation of the Wasserstein distance between graph representations. Pairwise calculation of the Wasserstein distance, however, could be expensive, especially for large graph datasets. In what follows, we apply the linear optimal transportation framework <ref type="bibr" target="#b62">(Wang et al., 2013)</ref> to define a Hilbertian embedding, in which the 2 distance provides a true metric between the probability measures that approximates W 2 . We show that in a dataset containing M graphs, this framework reduces the computational complexity from calculating M (M −1) 2 linear programs to M .  <ref type="bibr" target="#b62">Wang et al. (2013)</ref> and the follow-up works <ref type="bibr" target="#b55">(Seguy &amp; Cuturi, 2015;</ref><ref type="bibr" target="#b34">Kolouri et al., 2016;</ref><ref type="bibr" target="#b11">Courty et al., 2018)</ref> describe frameworks for isometric Hilbertian embedding of probability measures such that the Euclidean distance between the embedded images approximates W 2 . We leverage the prior work and introduce the concept of linear Wasserstein embedding for learning graph embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LINEAR WASSERSTEIN EMBEDDING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THEORETICAL FOUNDATION</head><p>We adhere to the definition of the linear Wasserstein embedding for continuous measures. However, all derivations hold for discrete measures as well. More precisely, let µ 0 be a reference probability measure defined on Z ⊆ R d , with a positive probability density function p 0 ,</p><formula xml:id="formula_3">s.t. dµ 0 (z) = p 0 (z)dz and p 0 (z) &gt; 0 for ∀z ∈ Z. Let f i denote the Monge map that pushes µ 0 into µ i , i.e., f i = argmin f ∈M P (µ0,µi) Z z − f (z) 2 dµ 0 (z). (4) Define φ(µ i ) (f i − id) √ p 0 , where id(z) = z, ∀z ∈ Z is the identity function. In cartography,</formula><p>such a mapping is known as the equidistant azimuthal projection, while in differential geometry, it is called the logarithmic map. The mapping φ(·) has the following characteristics (partially illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>):</p><p>1. φ(·) provides an isometric embedding for probability measures, i.e., using the Jacobian</p><formula xml:id="formula_4">equation p i = det(Df −1 i )p 0 (f −1 i ), where f i = φ(µi) √ p0 + id.</formula><p>2. φ(µ 0 ) = 0, i.e., the reference is mapped to zero.</p><formula xml:id="formula_5">3. φ(µ i ) − φ(µ 0 ) 2 = φ(µ i ) 2 = W 2 (µ i , µ 0 ), i.e., the mapping preserves distances to µ 0 . 4. φ(µ i ) − φ(µ j ) 2 ≈ W 2 (µ i , µ j ),</formula><p>i.e., the 2 distance between φ(µ i ) and φ(µ j ), while being a true metric between µ i and µ j , is an approximation of W 2 (µ i , µ j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding probability measures {µ</head><formula xml:id="formula_6">i } M i=1 via φ(·)</formula><p>requires calculating M Monge maps. The fourth characteristic above states that φ(·) provides a linear embedding for the probability measures. Therefore, we call it the linear Wasserstein embedding. The mapping φ(µ i ) could be thought as the Reproducing Kernel Hilbert Space (RKHS) embedding of the measure, µ i <ref type="bibr" target="#b48">(Muandet et al., 2017)</ref>. In practice, for discrete distributions, the Monge coupling is used, which could be approximated from the Kantorovich plan (i.e., the transport plan) via the so-called barycenteric projection <ref type="bibr" target="#b62">(Wang et al., 2013)</ref>. We here acknowledge the concurrent work by <ref type="bibr" target="#b43">Mialon et al. (2021)</ref>, where the authors use a similar idea to the linear Wasserstein embedding as a pooling operator for learning from sets of features. The authors demonstrate the relationship between their proposed optimal transport-based pooling operation and the widespread attention pooling methods in the literature. A detailed description of the capabilities of the linear Wasserstein embedding framework is included in Appendix A.1. We next provide the numerical details of the barycenteric projection <ref type="bibr" target="#b1">(Ambrosio et al., 2008;</ref><ref type="bibr" target="#b62">Wang et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NUMERICAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider a set of probability distributions {p</head><formula xml:id="formula_7">i } M i=1 , and let Z i = z i 1 , . . . , z i Ni T ∈ R Ni×d be an array containing N i i.i.d. samples from distribution p i , i.e., z i k ∈ R d ∼ p i , ∀k ∈ {1, . . . , N i }. Let us define p 0 to be a reference distribution, with Z 0 = z 0 1 , . . . , z 0 N T ∈ R N ×d , where N = 1 M M i=1 N i and z 0 j ∈ R d ∼ p 0 , ∀j ∈ {1, . . . , N }.</formula><p>The optimal transport plan between p i and p 0 , denoted by π * i ∈ R N ×Ni , is the solution to the following linear program,</p><formula xml:id="formula_8">π * i = argmin π∈Πi N j=1 Ni k=1 π jk z 0 j − z i k 2 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">Π i π ∈ R N ×Ni N i N j=1 π jk = N Ni k=1 π jk = 1, ∀k ∈ {1, . . . , N i }, ∀j ∈ {1, . . . , N } .</formula><p>The Monge map is then approximated from the optimal transport plan by barycentric projection via</p><formula xml:id="formula_10">F i = N (π * i Z i ) ∈ R N ×d .<label>(6)</label></formula><p>Note that the transport plan π i could split the mass in z 0 j ∈ Z 0 and distribute it on z i k s. The barycentric projection calculates the center of mass of the transportation locations for z 0 j to ensure that no mass splitting is happening (see <ref type="figure" target="#fig_5">Figure 5</ref>), and hence it approximates a Monge coupling. Finally, the embedding can be calculated by φ(</p><formula xml:id="formula_11">Z i ) = (F i − Z 0 )/ √ N ∈ R N ×d .</formula><p>With a slight abuse of notation, we use φ(p i ) and φ(Z i ) interchangeably throughout the paper. Due to the barycenteric projection, here, φ(·) is only pseudo-invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEGL: A LINEAR WASSERSTEIN EMBEDDING FOR GRAPHS</head><p>The application of the optimal transport problem to graphs is multifaceted. For instance, some works focus on solving the "structured" optimal transport concerning an optimal probability flow, where the transport cost comes from distances on an often unchanging underlying graph <ref type="bibr" target="#b40">(Léonard et al., 2016;</ref><ref type="bibr" target="#b18">Essid &amp; Solomon, 2018;</ref><ref type="bibr" target="#b58">Titouan et al., 2019)</ref>. Here, we are interested in applying optimal transport to measure the dissimilarity between two graphs <ref type="bibr" target="#b42">(Maretic et al., 2019;</ref><ref type="bibr" target="#b59">Togninalli et al., 2019;</ref><ref type="bibr" target="#b14">Dong &amp; Sawin, 2020)</ref>. Our work significantly differs from <ref type="bibr" target="#b42">(Maretic et al., 2019;</ref><ref type="bibr" target="#b14">Dong &amp; Sawin, 2020)</ref>, which measure the dissimilarity between non-attributed graphs based on distributions defined by their Laplacian spectra and is closer to <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>.</p><p>Our proposed graph embedding framework, termed Wasserstein Embedding for Graph Learning (WEGL), combines node embedding methods for graphs with the linear Wasserstein embedding explained in Section 3. More precisely, let Let h(·) be an arbitrary node embedding process, where h(</p><formula xml:id="formula_12">{G i = (V i , E i )} M i=1</formula><formula xml:id="formula_13">G i ) = Z i = z 1 , . . . , z |Vi| T ∈ R |Vi|×d .</formula><p>Having the node embeddings {Z i } M i=1 , we can then calculate a reference node embedding Z 0 (see Section 4.2 for details), which leads to the linear Wasserstein embedding φ(Z i ) with respect to Z 0 , as described in Section 3. Therefore, the entire embedding for each graph <ref type="figure">Figure 2</ref> visualizes this process.</p><formula xml:id="formula_14">G i , i ∈ {1, . . . , M }, is obtained by composing φ(·) and h(·), i.e., ψ(G i ) = φ(h(G i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NODE EMBEDDING</head><p>There are many choices for node embedding methods <ref type="bibr" target="#b10">(Chami et al., 2020)</ref>. These methods in general could be parametric or non-parametric, e.g., as in propagation/diffusion-based embeddings. Parametric embeddings are often implemented via a GNN encoder. The encoder can capture different graph properties depending on the type of supervision (e.g., supervised or unsupervised). Selfsupervised embedding methods have also been recently shown to be promising .</p><p>In this paper, for our node embedding process h(·), we follow a similar non-parametric propagation/diffusion-based encoder as in <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>. One of the appealing advantages of this framework is its simplicity, as there are no trainable parameters involved. In short, given a graph G = (V, E) with node features {x v } v∈V and scalar edge features {w uv } (u,v)∈E , we use the <ref type="figure">Figure 2</ref>: Our proposed graph embedding framework, WEGL, combines node embedding methods with the linear Wasserstein embedding framework described in Section 3. Given a graph Gi = (Vi, Ei), we first embed the graph nodes into a d-dimensional Hilbert space and obtain an array of node embeddings, denoted by h(Gi) = Zi ∈ R |V i |×d . We then calculate the linear Wasserstein embedding of Zi with respect to a reference Z0, i.e., φ(Zi), to derive the final graph embedding.</p><formula xml:id="formula_15">Reference Distribution ! " ! # ℎ(! " ) ℎ(! # ) Linear Wasserstein Embedding, '(⋅) Monge Coupling Monge Coupling Node embedding, ℎ(⋅) Transport Displacements Transport Displacements ) ! " = '(ℎ ! " ) ) ! # = '(ℎ ! # ) + " + # + ,</formula><p>following instantiation of equation 12 to define the combining function as</p><formula xml:id="formula_16">x (l) v = u∈Nv∪{v} w uv deg(u)deg(v) · x (l−1) u , ∀l ∈ {1, . . . , L}, ∀v ∈ V,<label>(7)</label></formula><p>where for any node v ∈ V, its degree deg(v) is defined as its number of neighbors in G augmented with self-connections, i.e., deg(v) 1 + |N v |. Note that the normalization of the messages between graph nodes by the (square root of) the two end-point degrees in equation 7 have also been used in other architectures, including <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>. For the cases where the edge weights are not available, including self-connection weights {w vv } v∈V , we set them to one. In Appendix A.5, we show how we use an extension of equation 7 to treat graphs with multiple edge features/labels. Finally, we let</p><formula xml:id="formula_17">z v = g {x (l) v } L l=0 represent the resultant embedding for each node v ∈ V, where g(·)</formula><p>is a local pooling process on a single node (not a global pooling), e.g., concatenation or averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CALCULATION OF THE REFERENCE DISTRIBUTION</head><p>To calculate the reference distribution, we use the k-means clustering algorithm on</p><formula xml:id="formula_18">M i=1 Z i with N = 1 M M i=1 N i centroids.</formula><p>Alternatively, one can calculate the Wasserstein barycenter <ref type="bibr" target="#b12">(Cuturi &amp; Doucet, 2014</ref>) of the node embeddings or simply use N samples from a normal distribution. While approximation of the 2-Wasserstein distance in the tangent space depends on the reference distribution choice, surprisingly, we see a stable performance of WEGL for different references. In Appendix A.2, we compare the performance of WEGL with respect to different references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we discuss the evaluation results of our proposed algorithm on multiple benchmark graph classification datasets. We use the PyTorch Geometric framework  for implementing WEGL. In all experiments, we use scikit-learn for the implementation of our downstream classifiers on the embedded graphs <ref type="bibr" target="#b8">(Buitinck et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MOLECULAR PROPERTY PREDICTION ON THE OPEN GRAPH BENCHMARK</head><p>We first evaluate our algorithm on the molecular property prediction task on the ogbg-molhiv dataset. This dataset is part of the Open Graph Benchmark , which involves node-level, link-level, and graph-level learning and prediction tasks on multiple datasets spanning diverse problem domains. The ogbg-molhiv dataset, in particular, is a molecular tree-like dataset, consisting of 41, 127 graphs, with an average number of 25.5 nodes and 27.5 edges per graph. Each graph is a molecule, with nodes representing atoms and edges representing bonds between them, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Validation ROC-AUC (%) Test ROC-AUC (%) GCN <ref type="bibr" target="#b33">(Kipf &amp; Welling, 2016)</ref> 83.8 ± 0.9 76.0 ± 1.2 GIN + Virtual Node  84.8 ± 0.7 77.1 ± 1.5 DeeperGCN <ref type="bibr" target="#b41">(Li et al., 2020)</ref> 84.3 ± 0.6 78.6 ± 1.2 HIMP  -78.8 ± 0.8 GNN GCN + GraphNorm  79.0 ± 1. it includes both node and edge attributes, characterizing the atom and bond features. The goal is to predict a binary label indicating whether or not a molecule inhibits HIV replication.</p><p>To train and evaluate our proposed method, we use the scaffold split provided by the dataset, and report the mean and standard deviation of the results across 10 different random seeds. We perform a grid search over a set of hyperparameters and report the configuration that leads to the best validation performance. We also report a virtual node variant of the graphs in our evaluations, where each graph is augmented with an additional node that is connected to all the original nodes in the graph. This node serves as a shortcut for message passing among the graph nodes, bringing any pair of nodes within at most two hops of each other. The complete implementation details can be found in Appendix A.5. <ref type="table" target="#tab_0">Table 1</ref> shows the evaluation results of WEGL on the ogbg-molhiv dataset in terms of the ROC-AUC (i.e., Receiver Operating Characteristic Area Under the Curve), alongside multiple GNNbased baseline algorithms. Specifically, we show the results using two classifiers: A random forest classifier <ref type="bibr" target="#b6">(Breiman, 2001)</ref> and an automated machine learning (AutoML) classifier using the Auto-Sklearn 2.0 library <ref type="bibr" target="#b19">(Feurer et al., 2020)</ref>. As the table demonstrates, while WEGL embeddings combined with random forest achieve a decent performance level, using AutoML further enhances the performance and achieves state-of-the-art test results on this dataset, showing the high expressive power of WEGL in large-scale graph datasets, without the need for end-to-end training.</p><p>Moreover, as an ablation study, we report the evaluation results on the ogbg-molhiv dataset using a random forest classifier, where the Wasserstein embedding module after the node embedding process is replaced with global average pooling (GAP) among the output node embeddings of each graph to derive the graph-level embedding. As the table demonstrates, there is a significant performance drop when using GAP graph embedding, which indicates the benefit of our proposed graph embedding method as opposed to average readout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TUD BENCHMARK DATASETS</head><p>We also consider a set of social network, bioinformatics and molecule graph datasets <ref type="bibr" target="#b32">(Kersting et al., 2020)</ref>. The social network datasets (IMDB-BINARY, IMDB-MULTI, COLLAB, REDDIT-BINARY, and REDDIT-MULTI-5K) lack both node and edge features. Therefore, in these datasets we use a one-hot representation of the node degrees as their initial feature vectors, as also used in prior work, e.g., . To handle the large scale of the REDDIT-BINARY and REDDIT-MULTI-5K and datasets, we clip the node degrees at 500.</p><p>Moreover, for the molecule (PTC-MR) and bioinformatics (ENZYMES and PROTEINS) datasets, we use the readily-provided node labels in <ref type="bibr" target="#b32">(Kersting et al., 2020)</ref> as the initial node feature vectors. Besides, for PTC-MR which has edge labels, as explained in Appendix A.5, we use an extension of equation 7 to use the one-hot encoded edge features in the diffusion process. To evaluate the performance of WEGL, we follow the methodology used in <ref type="bibr" target="#b65">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b50">Niepert et al., 2016;</ref>, where for each dataset, we perform 10-fold cross-validation with random splitting on the entire dataset, conducting a grid search over the desired set of hyperparameters as mentioned in Appendix A.5, and we then report the mean and standard deviation of the validation accuracies achieved during cross-validation. For this experiment, we use two ensemble classifiers, Method IMDB-B IMDB-M COLLAB RE-B RE-M5K PTC-MR ENZYMES PROTEINS DGCNN <ref type="bibr" target="#b67">(Zhang et al., 2018a)</ref> 69.2±3.0 45.6±3.4 71.2±1.9 87.8±2.5 49.2±1.2 58.6 38.9±5.7 72.9±3.5 GraphSAGE <ref type="bibr" target="#b24">(Hamilton et al., 2017)</ref> 68.8±4.5 47.6±3.5 73.9±1.7 84.3±1.9 50.0±1.3 63.9±7.7 -75.9±3.2 GIN  75.1±5.1 2.3±2.8 80.2±1.9 92.4±2.5 57.5±1.5 64.6±7.0 -76.2±2.8 GNTK <ref type="bibr" target="#b15">(Du et al., 2019)</ref> 76.9±3.6 52.8±4.6 83.6±1.0 --67.9±6.9 -75.6±4.2 CapsGNN <ref type="bibr" target="#b63">(Xinyi &amp; Chen, 2019)</ref> 73.1±4.8 50.3±2.6 79.6±0.9 -52.9±1.5 -54.7±5.7 76.3±3.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN</head><p>GraphNorm  76.0±3.7 -80.2±1.0 93.5±2.1 -64.9±7.5 -77.4±4.9</p><p>DGK <ref type="bibr" target="#b65">(Yanardag &amp; Vishwanathan, 2015)</ref>   <ref type="bibr" target="#b29">(Ivanov &amp; Burnaev, 2018)</ref> 74.5±5.8 51.5±3.6 73.9±1.9 87.9±2.5 50.5±1.9 -35.8±5.9 -GK WWL <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>   namely Random Forest and Gradient Boosted Decision Tree (GBDT), together with kernel-SVM with an RBF kernel (SVM-RBF). Given that the Euclidean distance in the embedding space approximates the 2-Wasserstein distance, the SVM-RBF classification results are comparable with those reported by <ref type="bibr" target="#b59">Togninalli et al. (2019)</ref>. <ref type="table" target="#tab_3">Table 2</ref> shows the classification accuracies achieved by WEGL on the aforementioned datasets as compared with several GNN and graph kernel baselines, whose results are extracted from the corresponding original papers. As the table demonstrates, our proposed algorithm achieves either state-of-the-art or competitive results across all the datasets, and in particular, it is among the top-three performers in all of them. This shows the effectiveness of the proposed linear Wasserstein embedding for learning graph-level properties across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">COMPUTATION TIME</head><p>As mentioned before, one of the most important advantages of WEGL as compared to other graph representation learning methods is its algorithmic efficiency. To evaluate that, we compare the wall-clock training and inference times of WEGL with those of GIN and the Wasserstein Weisfeiler-Lehman (WWL) graph kernel on five different TUD datasets (IMDB-B, MUTAG, PTC-MR, PROTEINS, and NCI1) 2 . For WEGL and WWL, we use the exact linear programming solver (as opposed to the entropy-regularized version). We carry out our experiments for WEGL and WWL on a 2.3 GHz Intel ® Xeon ® E5-2670 v3 CPU, while we use a 16 GB NVIDIA ® Tesla ® P100 GPU for GIN. As a reference, we implement GIN on the aforementioned CPU hardware as well. <ref type="figure" target="#fig_2">Figure 3</ref> shows how the training and inference run-times of WEGL, WWL and GIN scale with the number of graphs in the dataset, the average number of nodes per graph, and the average number of edges per graph. As the figure illustrates, while having similar or even better performance, training WEGL is several orders of magnitude faster than WWL and GIN, especially for datasets with larger numbers of graphs. Note that training WWL becomes very inefficient as the number of graphs in the dataset increases due to the pairwise distance calculation between all the graphs.</p><p>During inference, WEGL is slightly slower than GIN, when implemented on a GPU. However, on most datasets, WEGL is considerably faster in inference as compared to GIN implemented on a CPU. Moreover, both algorithms are significantly faster than WWL. Using GPU-accelerated implementations of the diffusion process in equation 7 and the entropy-regularized transport problem could potentially further enhance the computational efficiency of WEGL during inference.  <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref> and GIN . WEGL and WWL were implemented on a 2.3 GHz Intel ® Xeon ® E5-2670 v3 CPU. Moreover, GIN was implemented twice, once using the aforementioned CPU and once using a 16 GB NVIDIA ® Tesla ® P100 GPU, with 5 hidden layers, 300 training epochs and a batch size of 16 in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We considered the problem of graph property prediction and introduced the linear Wasserstein Embedding for Graph Learning, which we denoted as WEGL. Similar to <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>, our approach also relies on measuring the Wasserstein distances between the node embeddings of graphs. Unlike <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>, however, we further embed the node embeddings of graphs into a Hilbert space, in which their Euclidean distance approximates their 2-Wasserstein distance. WEGL provides two significant benefits: i) it has linear complexity in the number of graphs (as opposed to the quadratic complexity of <ref type="bibr" target="#b59">(Togninalli et al., 2019)</ref>), and ii) it enables the application of any ML algorithm of choice, such as random forest, gradient boosted decision tree, or even AutoML. We demonstrated WEGL's superior performance and highly efficient training on a wide range of benchmark datasets, including the ogbg-molhiv dataset and the TUD graph classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>Here we provide further details on the theoretical aspect of WEGL, our implementation details, and the sensitivity of the results to the choice of reference distribution. We also share our implementation code on the ogbg-molhiv dataset to help with the review process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DETAILED DISCUSSION ON LINEAR WASSERSTEIN EMBEDDING</head><p>The linear Wasserstein embedding used in WEGL is based on the Linear Optimal Transport (LOT) framework introduced in <ref type="bibr" target="#b62">(Wang et al., 2013)</ref>. The main idea is to compute the "projection" of the manifold of probability measures to the tangent space at a fixed reference measure. In particular, the tangent space at measure µ 0 is the set of vector fields V µ0 = {v : Z → R d | Z |v(z)| 2 dµ 0 (z) &lt; ∞} such that the inner product is the weighted 2 :</p><formula xml:id="formula_19">v i , v j µ0 = Z v i (z) · v j (z)dµ 0 (z).<label>(8)</label></formula><p>We can then define v i (·) f (·) − id(·), where f (·) is the optimal transport map from µ 0 to µ i . Note</p><formula xml:id="formula_20">that v i ∈ V µ0 , v 0 = 0, and v i − v 0 2 µ0 = v i 2 µ0 = v i , v i µ0 = Z f (z) − z 2 dµ 0 (z) = W 2 (µ i , µ 0 ).<label>(9)</label></formula><p>In the paper, we use φ(µ i ) = v i √ p 0 to turn the weighted-2 into 2 .</p><p>The discussion above assumes an absolutely continuous reference measure µ 0 . A more interesting treatment of the problem is via the generalized geodesics defined in <ref type="bibr" target="#b1">(Ambrosio et al., 2008)</ref>, connecting µ i and µ j and enabling us to use discrete reference measures. Following the notation in <ref type="bibr" target="#b1">(Ambrosio et al., 2008)</ref>, given the reference measure µ 0 , let Γ(µ i , µ 0 ) be the set of transport plans between µ i and µ 0 , and let Γ(µ i , µ j , µ 0 ) be the set of all measures on the product space Z × Z × Z such that the marginals over µ i and µ j are Γ(µ j , µ 0 ) and Γ(µ i , µ 0 ), respectively. Then the linearized optimal transport distance is defined as</p><formula xml:id="formula_21">d 2 LOT,µ0 (µ i , µ j ) = inf γ∈Γ(µi,µj ,µ0) Z×Z×Z z − z 2 dγ(z, z , z ).<label>(10)</label></formula><p>In a discrete setting, where µ i = 1 δ z l , we have</p><formula xml:id="formula_22">d 2 LOT,µ0 (µ i , µ j ) = min γ∈Γ(µi,µj ,µ0) 1 N i N j N Ni n=1 Nj m=1 N l=1 γ nml z n − z m 2 .<label>(11)</label></formula><p>See <ref type="figure" target="#fig_4">Figure 4a</ref> for a depiction of Equation equation 11's meaning. Finally, the idea of barycenteric projection used to approximate Monge couplings and provide a fixed-size representation is shown in <ref type="figure" target="#fig_4">Figure 4b</ref>.  </p><formula xml:id="formula_23">= [z i k ∼ pi] N i i=1</formula><p>from each distribution, together with the process of obtaining the linear Wasserstein embedding with respect to a reference distribution. In short, for each distribution pi, the embedding approximates the Monge-map (i.e., a vector field) from the reference samples Z0 to the target samples Zi by a barycentric projection of the optimal transport plan. Adding samples in the embedding space corresponds to adding their vector fields, which can be used to calculate (b) the mean distribution in the embedding space, i.e.,</p><formula xml:id="formula_24">φ −1 ( 1 M M i=1 φ(pi)</formula><p>) and (c)-(d) the Euclidean geodesics in this space, i.e., φ −1 (αφ(pi) + (1 − α)φ(pj)) for α ∈ [0, 1]. As can be seen, the calculated mean is the Wasserstein barycenter of the dataset, and the Euclidean geodesics in the embedding space follow the Wasserstein geodesics in the original space.</p><p>Next, to demonstrate the capability of the linear Wasserstein embedding, we present the following experiment. Consider a set of distributions {p i } M i=1 , where each p i is a translated and dilated ring distribution in R 2 , and N i samples are observed from p i , where N i and N j could be different for i = j. We then consider a normal distribution as the reference distribution and calculate the linear Wasserstein embedding with respect to the reference (See <ref type="figure" target="#fig_5">Figure 5a)</ref>. Given the pseudo-invertible nature of the embedding, to demonstrate the modeling capability of the framework, we calculate the mean in the embedding space (i.e., on the vector fields), and invert it to obtain the mean distribution p. <ref type="figure" target="#fig_5">Figure 5b</ref> shows the calculated mean, indicating that the linear Wasserstein embedding framework has successfully retrieved a ring distribution as the mean. Finally, we calculate Euclidean geodesics in the embedding space (i.e., the convex combination of the vector fields) between p i and p 0 , as well as between p i and p j , and show the inverted geodesics in <ref type="figure" target="#fig_5">Figures 5c and 5d</ref>, respectively. As the figures demonstrate, the calculated geodesics follow the Wasserstein geodesics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPROXIMATION ERROR OF THE EMBEDDING</head><p>Given the Euclidean distance in the embedding space is a transport-based distance (i.e., the socalled LOT distance) that approximates the Wasserstein distance, a natural question arises about the approximation error. Here we point the reader to the recent work of <ref type="bibr" target="#b44">Moosmüller &amp; Cloninger (2020)</ref> in which the authors provide bounds on how well the Euclidean distance in the embedding space approximates the 2-Wasserstein distance. In particular, the authors show that:</p><formula xml:id="formula_25">W 2 (µ i , µ j ) ≤ φ(µ i ) − φ(µ j ) 2 ≤ W 2 (µ i , µ j ) + f µj µi − f µj µ0 • f µ0 µi µi , where f</formula><p>µj µi is the optimal transport map from µ i to µ j . This inequality simply indicates that the approximation error is caused by conditioning the transport map to be obtained by composition of the optimal transport maps from µ i to µ 0 , and then from µ 0 to µ j . More importantly, it can be shown that if µ i and µ j are shifted and scaled versions of the reference measure, µ 0 , then the embedding is isometric (See <ref type="figure" target="#fig_0">Figure 1</ref> and 2 in <ref type="bibr" target="#b44">Moosmüller &amp; Cloninger (2020)</ref>).</p><p>Maybe a less interesting upper bound can also be obtained by the triangle inequality:</p><formula xml:id="formula_26">W 2 (µ i , µ j ) ≤ φ(µ i ) − φ(µ j ) 2 ≤ W 2 (µ i , σ) + W 2 (σ, µ j ),</formula><p>which ensures some regularity of the embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGULARITY OF THE EMBEDDING</head><p>A good question was raised during the feedback period, about the regularity of the proposed embedding. The regularity of the graph embedding will depend on both the regularity of node-embedding and the Wasserstein embedding. In the following, we avoid the discussion on regularity of the nodeembedding (as it is not the main focus of our work), and focus on the regularity of the Wasserstein embedding. To that end, we first point out several regularity characteristics pointed out in Appendix A of <ref type="bibr" target="#b44">Moosmüller &amp; Cloninger (2020)</ref>. Most notably Theorem 4.2 in their paper, shows an almost isometric property when the distortions are within an -tube around the set of shifts and scalings. In short, let µ ∈ P 2 (Z), R &gt; 0, &gt; 0, E be the set of all shifts and scalings, and</p><formula xml:id="formula_27">E µ,R = {h ∈ E : h µ ≤ R}, and let G µ,R, = {g ∈ L 2 (Z, µ) : ∃h ∈ E µ,R : g − h µ ≤ },</formula><p>which is the -tube around set of shifts and scalings. Now, assume µ 0 ∈ P 2 (Z) is the reference measure and both µ and µ 0 satisfy Caffarelli's regularity theorem. Then for g 1 , g 2 ∈ G µ,R, we have</p><formula xml:id="formula_28">0 ≤ φ(g 1# µ) − φ(g 2# µ) 2 − W 2 (g 1# µ, g 2# µ) ≤ C µ0,µ,R +C µ0,µ,R 2 ,</formula><p>where C µ0,µ,R andC µ0,µ,R are constants depending on µ 0 , µ, and R.</p><p>The results shown above can be used to derive regularity results for the linear Wasserstein embedding with respect to additive noise. We know that the addition of two random variables leads to a new random variable with its PDF being the convolution of the original PDFs. Therefore, for features</p><formula xml:id="formula_29">Z i = [z 1 , ..., z Ni ] T , letẐ i = [z 1 + e 1 ẑ1</formula><p>, ..., z Ni + e Ni ẑ N i ] T denote the noisy samples for e i ∼ η, where η is the noise distribution. Then the noisy samples,ẑ i , will be distributed according toμ i = µ i * η. For instance, for the Gaussian additive noise,μ i is the smoothed version of µ i . Therefore, there exists a transport map in g ∈ G µi,R, for which,μ i = g # µ i and g − id µi = W 2 (μ i , µ i ) ≤ , and we have:</p><formula xml:id="formula_30">0 ≤ φ(µ i ) − φ(μ i ) 2 ≤ (C µ0,µi,R + 1) +C µ0,µi,R 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SENSITIVITY TO THE CHOICE OF REFERENCE DISTRIBUTION</head><p>To measure the dependency of WEGL on the reference distribution choice, we changed the reference to a normal distribution (i.e., data-independent). We compared the results of WEGL using the new reference distribution to that using a reference distribution calculated via k-means on the training set. We used the ogbg-molhiv dataset with initial node embedding of size 300 and 4 diffusion layers. We ran the experiment with 100 different random seeds, and measured the test ROC-AUC of WEGL calculated with the two aforementioned reference distributions. <ref type="figure" target="#fig_6">Figure 6</ref> shows the results of this experiment, indicating that the choice of reference distribution is statistically insignificant.  A.3 LINEAR PROGRAMMING VS. ENTROPY REGULARIZATION In this paper, we used the Python Optimal Transport <ref type="bibr" target="#b22">(Flamary &amp; Courty, 2017)</ref> for the calculation of the optimal transport plans. During the feedback period, a point came up regarding the entropyregularized version of the OT problem <ref type="bibr" target="#b12">(Cuturi &amp; Doucet, 2014)</ref>, which reduces the complexity of the linear programming problem from being cubic, in the number of nodes, to being quadratic, using the Sinkhorn algorithm. Given the Sinkhorn algorithm's iterative nature, the computational gain of the method is prominent when calculating the transportation problem between graphs with a large number of nodes (e.g., larger than 10 3 ). However, the graph datasets used in this paper often have a small number of nodes (e.g., &lt; 50). In these settings, linear programming is efficient. To obtain any computational gain using the Sinkhorn algorithm, one would need to use a large regularization coefficient, which reduces the Sinkhorn algorithm's precision.</p><p>Here we ran an experiment on the ogbg-molhiv dataset. We obtain the transport plans between the node embeddings and the reference distribution using the linear programming solver (using ot.emd2 from (Flamary &amp; Courty, 2017)) and the Sinkhorn algorithm for the entropy-regularized problem (using ot.sinkhorn2 from (Flamary &amp; Courty, 2017)). We measure the calculation time as well as the calculated distances for both algorithms. For the Sinkhorn algorithm, we used four different regularization values. We report the mean and standard deviation of calculation time ratio, i.e., and the relative error of calculating the 2-Wasserstein distance, i.e., | W 2,Sinkhorn −W 2,LP W 2,LP | in <ref type="figure" target="#fig_7">Figure 7</ref>. As the figure shows, due to the small graph size, the LP solver is efficient and little to no gain can be obtained using the Sinkhorn algorithm. Nevertheless, in the case of dealing with larger graph sizes, the entropy regularized formulation should be the definite choice. Finally, for the entropy-regularized OT problem, more efficient solvers have been proposed that outperform the Sinkhorn algorithm <ref type="bibr" target="#b16">(Dvurechensky et al., 2018)</ref>. However, given the acceptable performance of the linear programming solver (at least for the graph datasets in this paper), we did not find it necessary to seek more efficient solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 INNER WORKING OF GNNS</head><p>In its most general form, a GNN consists of L hidden layers, where at the l th layer, each node v ∈ V aggregates and combines messages from its 1-hop neighboring nodes N v , resulting in the feature vector</p><formula xml:id="formula_31">x (l) v = Ψ combine x (l−1) v , (x (l−1) u , w uv ) u∈Nv , ∀l ∈ {1, . . . , L}, ∀v ∈ V,<label>(12)</label></formula><p>where Ψ combine (·) denotes a parametrized and differentiable combining function.</p><p>At the input layer, each node v ∈ V starts with its initial feature vector </p><formula xml:id="formula_32">x 0 v = x v ∈ R F ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 IMPLEMENTATION DETAILS</head><p>To derive the node embeddings, we use the diffusion process in equation 7 for the datasets without edge features/labels, i.e., all the social network datasets (IMDB-BINARY, IMDB-MULTI, COLLAB, REDDIT-BINARY, REDDIT-MULTI-5K, and REDDIT-MULTI-12K) and four of the molecule and bioinformatics datasets (ENZYMES, PROTEINS, D&amp;D, and NCI1). We specifically set w uv = 1 for any (u, v) ∈ E and also for all self-connections, i.e., w vv = 1, ∀v ∈ V.</p><p>The remaining datasets contain edge labels that cannot be directly used with equation 7. Specifically, each edge in the MUTAG and PTC-MR datasets has a categorical label, encoded as a one-hot vector of dimension four. Moreover, in the ogbg-molhiv dataset, each edge has three categorical features indicating bond type (five categories), bond stereochemistry (six categories) and whether the bond is conjugated (two categories). We first convert each categorical feature to its one-hot representation, and then concatenate them together, resulting in a binary 13-dimensional feature vector for each edge.</p><p>In each of the three aforementioned datasets, for any edge (u, v) ∈ E, let us denote its binary feature vector by w uv ∈ {0, 1} E , where E is equal to 4, 4, and 13 for MUTAG, PTC-MR, and ogbg-molhiv, respectively. We then use the following extension of the diffusion process in equation 7,</p><formula xml:id="formula_34">x (l) v = u∈V E e=1 w uv,e deg e (u)deg e (v) x (l−1) u , ∀l ∈ {1, . . . , L}, ∀v ∈ V,<label>(14)</label></formula><p>where for any e ∈ {1, . . . , E}, w uv,e denotes the e th element of w uv , and for any node v ∈ V, we define deg e (v) as its degree over the e th elements of the edge features; i.e., deg e (v) u∈V w uv,e . We assign vectors of all-one features to the self-connections in the graph; i.e., w vv,e = 1, ∀v ∈ V, ∀e ∈ {1, . . . , E}. Note that the formulation of the diffusion process in equation 14 can be seen as an extension of equation 7, where the underlying graph with multi-dimensional edge features is broken into E parallel graphs with non-negative single-dimensional edge features, and the parallel graphs perform message passing at each round/layer of the diffusion process.</p><p>For the ogbg-molhiv experiments in which virtual nodes were appended to the original molecule graphs, we set the initial feature vectors of all virtual nodes to all-zero vectors. Moreover, for any graph G i in the dataset with |V i | nodes, we set the edge features for the edge between the virtual node v virtual and each of the original graph nodes u ∈ V i as w uv virtual ,e = 1 |Vi| , ∀e ∈ {1, . . . , E}. The normalization by the number of graph nodes is included so as to regulate the degree of the virtual node used in equation 14. We also include the resultant embedding of the virtual node at the end of the diffusion process in the calculation of the graph embedding ψ(G i ).</p><p>In the experiments conducted on each dataset, once the node embeddings are derived from the diffusion process, we standardize them by subtracting the mean embedding and dividing by the standard deviation of the embeddings, where the statistics are calculated based on all the graphs in the dataset. Moreover, to reduce the computational complexity of estimating the graph embeddings for the ogbg-molhiv dataset, we further apply a 20-dimensional PCA on the node embeddings.</p><p>Published as a conference paper at ICLR 2021 • Initial Node Feature Dimensionality (for ogbg-molhiv only): {100, 300, 500}.</p><p>• Node Embedding Type: For a graph with F -dimensional initial node features, we consider using the following three types of node embedding:</p><formula xml:id="formula_35">-Concat: z v = x (0) v x (1) v . . . x (L) v</formula><p>∈ R (L+1)F , where denotes concatenation.</p><p>-Average:</p><formula xml:id="formula_36">z v = 1 L+1 L l=0 x (l) v ∈ R F . -Final: z v = x (L) v ∈ R F .</formula><p>A.6 TUD BENCHMARK -COMPLETE RESULTS Here, we report the comprehensive set of classification results of our proposed method, WEGL, for each of the node embedding types mentioned in Section A.5, using five different classifiers: Linear SVM, Kernel-SVM (SVM-RBF), Gradient Boosted Decision Trees (GBDT), Multi-Layer Perceptron (MLP), and Random Forest (RF). The results are shown in <ref type="table">Table 3</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Graphical representation of the linear Wasserstein embedding framework, where the probability distributions are mapped to the tangent space with respect to a fixed reference distribution. The figure is adapted from<ref type="bibr" target="#b35">Kolouri et al. (2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>denote a set of M individual graphs, each with a set of possible node features {x v } v∈Vi and a set of possible edge features {w uv } (u,v)∈Ei .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Average wall-clock time comparison of our proposed method, WEGL with WWL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of (a) the meaning behind γ nml used in the LOT distance in equation 11, and (b) the idea of the barycenteric projection, which provides a fixed-size representation (i.e., of size N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>the Embedding Space from $ % to $ " Euclidean Geodesic in the Embedding Space from $ &amp; to $ " An experiment demonstrating the capability of the linear Wasserstein embedding. (a) A simple dataset consisting of shifted and scaled noisy ring distributions {pi} M i=1 , where we only observe samples Zi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>ROC-AUC (%) results on ogbg-molhiv dataset, when the reference distribution is calculated by k-means (Section 4.2) on the training dataset (denoted as k-means), compared to when it is fixed to be a normal distribution (denoted as Normal). With a p-value= 0.05, the choice of the template is statistically insignificant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Comparing the performance of the linear programming (LP) solver with the Sinkhorn algorithm (on entropy regularized OT problem) on the ogbg-molhiv dataset for various regularization parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>and the sequential application of GNN layers, as in equation 12, computes intermediate feature vectors {x (l) v } L l=1 . At the GNN output, the feature vectors of all nodes from all layers go through a global pooling (i.e., readout) function Ψ readout (·), resulting in the final graph embedding ψ(G) = Ψ readout x (l) v v∈V,l∈{0,...,L} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Graph classification results on the ogbg-molhiv dataset. The results for GCN and GIN are reported from. The best validation and test results are shown in bold.</figDesc><table><row><cell>1</cell><cell>78.8 ± 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Graph classification accuracy (%) of our method and comparison with the state-of-the-art GNNs and graph kernels (GKs) on various TUD graph classification tasks. The results for DGCNN are reported from (</figDesc><table /><note>Errica et al., 2020). The top-three performers on each dataset are shown in bold.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this definition includes both directed and undirected graphs, where in the latter case, for each edge (u, v) ∈ E, the reverse edge (v, u) is also included in E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The complete classification results of WEGL on these datasets can be found in Appendix A.6.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t Sinkhorn t LP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We gratefully acknowledge funding by the United States Air Force under Contract No. FA8750-19-C-0098. Gustavo K. Rohde also acknowledges funding by NIH grant GM130825. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force and DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale analysis of disease pathways in the human interactome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSB</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gradient flows: in metric spaces and in the space of probability measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Gigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Savaré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised random walks: predicting and recommending links in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fourth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04804</idno>
		<title level="m">Optimal transport graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is pagerank all you need for scalable graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15 th International Workshop on Mining and Learning with Graphs (MLG)</title>
		<meeting>the 15 th International Workshop on Mining and Learning with Graphs (MLG)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on Data Mining (ICDM&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Polar factorization and monotone rearrangement of vector-valued functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Brenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications on pure and applied mathematics</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="375" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">API design for machine learning software: Experiences from the scikit-learn project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaques</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop: Languages for Data Mining and Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Tie-Yan Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03294</idno>
		<title level="m">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning wasserstein embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mélanie</forename><surname>Ducoffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast computation of Wasserstein barycenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v32/cuturi14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 31st International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Sawin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03892</idno>
		<title level="m">COPT: Coordinated optimal transport on graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangcheng</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosong</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational optimal transport: Complexity by accelerated gradient descent is better than by sinkhorn&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dvurechensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kroshnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2196" to="2220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HygDF6NFPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quadratically regularized optimal transport on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montacer</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1961" to="1986" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Autosklearn 2.0: The next generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04074</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Gin</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pot python optimal transport library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>R&amp;apos;emi Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courty</surname></persName>
		</author>
		<ptr target="https://pythonot.github.io/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom B Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04305</idno>
		<title level="m">Measuring the algorithmic efficiency of neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kernel methods in machine learning. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1171" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlWWJSFDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2186" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting organic reaction outcomes with Weisfeiler-Lehman network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2607" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20 th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20 th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://www.graphlearning.io/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A continuous linear optimal transport approach for pattern analysis in image datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Akif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="453" to="462" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal mass transport: Signal processing and machine-learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Rim</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Slepcev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="43" to="59" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The multiscale Laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Explicit versus implicit graph feature maps: A computational phase transition for walk kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fredrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lazy random walks and optimal transport on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Léonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1864" to="1915" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GOT: An optimal transport framework for graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireille</forename><forename type="middle">El</forename><surname>Hermina Petric Maretic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Gheche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Chierchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13876" to="13887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A trainable optimal transport embedding for feature aggregation and its relationship to attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ZK6vTvb84s" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Linear optimal transport embedding: Provable fast wasserstein distance computation and classification for nonlinear problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Moosmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Cloninger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman graph kernels: Global-local feature maps of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Kernel mean embedding of distributions: A review and beyond. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Wireless power control via counterfactual optimization of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Naderializadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07631</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stratis Limnios, and Michalis Vazirgiannis. A degeneracy framework for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2595" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distributedgraph-based statistical approach for intrusion detection in cyber-physical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamidreza</forename><surname>Sadreazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal and Information Processing over Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Principal geodesic analysis for probability measures under the optimal transport metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Seguy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3312" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimal transport for structured data with application on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vayer Titouan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chapel</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Laetitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flamary</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/titouan19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36 th International Conference on Machine Learning</title>
		<meeting>the 36 th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Wasserstein Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabetta</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6436" to="6446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Optimal transport: Old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A linear optimal transportation framework for quantifying and visualizing variations in sets of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Slepčev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="269" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21 st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21 st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">RetGK: Graph kernels based on return probabilities of random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arye</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">HYPERPARAMETERS We use the following set of hyperparameters to perform a grid search over in each of the experiments: • Random Forest: min_samples_leaf ∈ {1, 2, 5}, min_samples_split ∈ {2, 5, 10}, and n_estimators ∈ {25</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<title level="m">• Gradient Boosted Decision Tree (GBDT): min_samples_leaf ∈ {1, 2, 5}, min_samples_split ∈ {2, 5, 10}, n_estimators ∈ {25, 50, 100, 150, 200}, and max_depth ∈ {1</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Svm-Linear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><surname>Svm-Rbf: C ∈ {10 −2</surname></persName>
		</author>
		<title level="m">10 5 }. • Multi-Layer Perceptron (MLP): hidden_layer_sizes ∈ {(128)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>256, 128)}</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Auto-Sklearn 2.0 searches over a space of 42 hyperparameters using Bayesian optimization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Auto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Ml</surname></persName>
		</author>
		<editor>Feurer et al.</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<title level="m">• Number of Diffusion Layers in equation 7 and equation 14: L ∈ {3</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
