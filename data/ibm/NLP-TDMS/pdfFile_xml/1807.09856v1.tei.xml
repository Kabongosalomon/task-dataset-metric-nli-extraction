<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where are the Blobs: Counting by Localization with Point Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issam</forename><forename type="middle">H</forename><surname>Laradji</surname></persName>
							<email>issamou@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
							<email>dvazquez@elementai.com</email>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
							<email>schmidtm@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Element AI</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Where are the Blobs: Counting by Localization with Point Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detectionbased method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using pointlevel annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object counting is an important task in computer vision with many applications in surveillance systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, traffic monitoring <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, ecological surveys <ref type="bibr" target="#b0">[1]</ref>, and cell counting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In traffic monitoring, counting methods can be used to track the number of moving cars, pedestrians, and parked cars. They can also be used to monitor the count of different species such as penguins, which is important for animal conservation. Furthermore, it has been used for counting objects present in everyday scenes in challenging datasets where the objects of interest come from a large number of classes such as the Pascal VOC dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Many challenges are associated with object counting. Models need to learn the variability of the objects in terms of shape, size, pose, and appearance. Moreover, objects may appear at different angles and resolutions, and may be partially occluded (see <ref type="figure">Fig. 1</ref>). Also, the background, weather conditions, and illuminations can vary widely across the scenes. Therefore, the model needs to arXiv:1807.09856v1 [cs.CV] 25 Jul 2018 Penguins: 10 Penguins: 28 Persons: 5 <ref type="figure">Fig. 1</ref>. Qualitative results on the Penguins <ref type="bibr" target="#b0">[1]</ref> and PASCAL VOC datasets <ref type="bibr" target="#b1">[2]</ref>. Our method explicitly learns to localize object instances using only point-level annotations. The trained model then outputs blobs where each unique color represents a predicted object of interest. Note that the predicted count is simply the number of predicted blobs.</p><p>be robust enough to recognize objects in the presence of these variations in order to perform efficient object counting.</p><p>Due to these challenges, regression-based models such as "glance" and object density estimators have consistently defined state-of-the-art results in object counting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. This is because their loss functions are directly optimized for predicting the object count. In contrast, detection-based methods need to optimize for the more difficult task of estimating the location, shape, and size of the object instances. Indeed, perfect detection implies perfect count as the count is simply the number of detected objects. However, models that learn to detect objects often lead to worse results for object counting <ref type="bibr" target="#b8">[9]</ref>. For this reason, we look at an easier task than detection by focusing on the task of simply localizing object instances in the scene. Predicting the exact size and shape of the object instances is not necessary and usually poses a much more difficult optimization problem. Therefore, we propose a novel loss function that encourages the model to output instance regions such that each region contains a single object instance (i.e. a single point-level annotation). Similar to detection, the predicted count is the number of predicted instance regions (see <ref type="figure">Fig. 1</ref>). Our model only requires point supervision which is a weaker supervision than bounding-box, and per-pixel annotations used by most detection-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Consequently, we can train our model for most counting datasets as they often have point-level annotations.</p><p>This type of annotation is cheap to acquire as it requires lower human effort than bounding box and per-pixel annotations <ref type="bibr" target="#b13">[14]</ref>. Point-level annotations provide a rough estimate of the object locations, but not their sizes nor shapes. Our counting method uses the provided point annotations to guide its attention to the object instances in the scenes in order to learn to localize them. As a result, our model has the flexibility to predict different sized regions for different object instances, which makes it suitable for counting objects that vary in size and shape. In contrast, state-of-the-art density-based estimators often assume a fixed object size (defined by the Gaussian kernel) or a constrained environment <ref type="bibr" target="#b5">[6]</ref> which makes it difficult to count objects with different sizes and shapes.</p><p>Given only point-level annotations, our model uses a novel loss function that (i) enforces it to predict the semantic segmentation labels for each pixel in the image (similar to <ref type="bibr" target="#b13">[14]</ref>) and (ii) encourages it to output a segmentation blob for each object instance. During the training phase, the model learns to split the blobs that contain more than one point annotation and to remove the blobs that contain no point-level annotations.</p><p>Our experiments show that our method achieves superior object counting results compared to state-of-the-art counting methods including those that use stronger supervision such as per-pixel labels. Our benchmark uses datasets representing different settings for object counting: Mall <ref type="bibr" target="#b14">[15]</ref>, UCSD <ref type="bibr" target="#b15">[16]</ref>, and Shang-haiTech B <ref type="bibr" target="#b16">[17]</ref> as crowd datasets; MIT Traffic <ref type="bibr" target="#b17">[18]</ref>, and Park lot <ref type="bibr" target="#b4">[5]</ref> as surveillance datasets; Trancos <ref type="bibr" target="#b5">[6]</ref> as a traffic monitoring dataset; and Penguins <ref type="bibr" target="#b0">[1]</ref> as a population monitoring dataset. We also show counting results for the PASCAL VOC <ref type="bibr" target="#b1">[2]</ref> dataset which consists of objects present in natural, 'everyday' images. We also study the effect of using different parts of the proposed loss function against counting and localization performance.</p><p>We summarize our contributions as follows: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset.</p><p>The rest of the paper is organized as follows: Section 2 presents related works on object counting; Section 3 describes the proposed approach; and Section 4 describes our experiments and results. Finally, we present the conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object counting has received significant attention over the past years <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. It can be roughly divided into three categories <ref type="bibr" target="#b19">[20]</ref>: (1) counting by clustering, (2) counting by regression, and (3) counting by detection.</p><p>Early work in object counting use clustering-based methods. They are unsupervised approaches where objects are clustered based on features such as appearance and motion cues <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. Rabaud and Belongie <ref type="bibr" target="#b18">[19]</ref> proposed to use feature points which are detected by motion and appearance cues and are tracked through time using KLT <ref type="bibr" target="#b21">[22]</ref>. The objects are then clustered based on similar features. Sebastian et al. <ref type="bibr" target="#b20">[21]</ref> used an expectation-maximization method that cluster individuals in crowds based on head and shoulder features. These methods use basic features and often perform poorly for counting compared to deep learning approaches. Another drawback is that these methods only work for video sequences, rather than still images.</p><p>Counting by regression methods have defined state-of-the-art results in many benchmarks. They were shown to be faster and more accurate than other groups such as counting by detection. These methods include glance and density-based methods that explicitly learn how to count rather than optimize for a localizationbased objective. Lempitsky et al. <ref type="bibr" target="#b7">[8]</ref> proposed the first method that used object density to count people. They transform the point-level annotation matrix into a density map using a Gaussian kernel. Then, they train their model using a least-squares objective to predict the density map. One major challenge is determining the optimal size of the Gaussian kernel which highly depends on the object sizes. As a result, Zhang et al. <ref type="bibr" target="#b16">[17]</ref> proposed a deep learning method that adjusted the kernel size using a perspective map. This assumes fixed camera images such as those used in surveillance applications. Onoro-Rubio et al. <ref type="bibr" target="#b9">[10]</ref> extended this method by proposing a perspective-free multi-scale deep learning approach. However, this method cannot be used for counting everyday objects as their sizes vary widely across the scenes as it is highly sensitive to the kernel size.</p><p>A straight-forward method for counting by regression is 'glance' <ref type="bibr" target="#b8">[9]</ref>, which explicitly learns to count using image-level labels only. Glance methods are efficient if the object count is small <ref type="bibr" target="#b8">[9]</ref>. Consequently, the authors proposed a grid-based counting method, denoted as "subitizing", in order to count a large number of objects in the image. This method uses glance to count objects at different non-overlapping regions of the image, independently. While glance is easy to train and only requires image-level annotation, the "subitizing" method requires a more complicated training procedure that needs full per-pixel annotation ground-truth.</p><p>Counting by detection methods first detect the objects of interest and then simply count the number of instances. Successful object detection methods rely on bounding boxes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref> and per-pixel labels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> ground-truth. Perfect object detection implies perfect count. However, Chattopadhyay et al. <ref type="bibr" target="#b8">[9]</ref> showed that Fast RCNN <ref type="bibr" target="#b26">[27]</ref>, a state-of-the-art object detection method, performs worse than glance and subitizing-based methods. This is because the detection task is challenging in that the model needs to learn the location, size, and shape of object instances that are possibly heavily occluded. While several works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref> suggest that counting by detection is infeasible for surveillance scenes where objects are often occluded, we show that learning a notion of localization can help the model improve counting.</p><p>Similar to our method is the line of work proposed by Arteta et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. They proposed a method that detects overlapping instances based on optimizing a tree-structured discrete graphical model. While their method showed promising detection results using point-level annotations only, it performed worse for counting than regression-based methods such as <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our method is also similar to segmentation methods such as U-net <ref type="bibr" target="#b30">[31]</ref> which learns to segment objects using a fully-convolutional neural network. Unlike our method, U-net requires the full per-pixel instance segmentation labels, whereas we use point-level annotations only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Localization-based Counting FCN</head><p>Our model is based on the fully convolutional neural network (FCN) proposed by Long et al. <ref type="bibr" target="#b23">[24]</ref>. We extend their semantic segmentation loss to perform object counting and localization with point supervision. We denote the novel loss function as localization-based counting loss (LC) and, we refer to the proposed model as LC-FCN. Next, we describe the proposed loss function, the architecture of our model, and the prediction procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Proposed Loss Function</head><p>LC-FCN uses a novel loss function that consists of four distinct terms. The first two terms, the image-level and the point-level loss, enforces the model to predict the semantic segmentation labels for each pixel in the image. This is based on the weakly supervised semantic segmentation algorithm proposed by Bearman et al. <ref type="bibr" target="#b13">[14]</ref>. These two terms alone are not suitable for object counting as the predicted blobs often group many object instances together (see the ablation studies in Section 4). The last two terms encourage the model to output a unique blob for each object instance and remove blobs that have no object instances. Note that LC-FCN only requires point-level annotations that indicate the locations of the objects rather than their sizes, and shapes.</p><p>Let T represent the point annotation ground-truth matrix which has label c at the location of each object (where c is the object class) and zero elsewhere. Our model uses a softmax function to output a matrix S where each entry S ic is the probability that pixel i belongs to category c. The proposed loss function can be written as:</p><formula xml:id="formula_0">L(S, T ) = L I (S, T ) Image-level loss + L P (S, T ) Point-level loss + L S (S, T ) Split-level loss + L F (S, T ) False positive loss ,<label>(1)</label></formula><p>which we describe in detail next.</p><p>Image-level loss. Let C e be the set of classes present in the image. For each class c ∈ C e , L I increases the probability that the model labels at least one pixel as class c. Also, let C ¬e be the set of classes not present in the image. For each class c ∈ C ¬e , the loss decreases the probability that the model labels any pixel as class c. C e and C ¬e can be obtained from the provided ground-truth point-level annotations. More formally, the image level loss is computed as follows:</p><formula xml:id="formula_1">L I (S, T ) = − 1 |C e | c∈Ce log(S tcc ) − 1 |C ¬e | c∈C¬e log(1 − S tcc ) ,<label>(2)</label></formula><p>where t c = argmax i∈I S ic . For each category present in the image, at least one pixel should be labeled as that class. For classes that do not exist in the image, none of the pixels should belong to that class. Note that we assume that each image has at least one background pixel; therefore, the background class belongs to C e .</p><p>Point-level loss. This term encourages the model to correctly label the small set of supervised pixels I s contained in the ground-truth. I s represents the locations of the object instances. This is formally defined as,</p><formula xml:id="formula_2">L P (S, T ) = − i∈Is log(S iTi ) ,<label>(3)</label></formula><p>where T i represents the true label of pixel i. Note that this loss ignores all the pixels that are not annotated.</p><p>Split-level loss. L S discourages the model from predicting blobs that have two or more point-annotations. Therefore, if a blob has n point annotations, this loss enforces it to be split into n blobs, each corresponding to a unique object. These splits are made by first finding boundaries between object pairs. The model then learns to predict these boundaries as the background class. The model outputs a binary matrix F where pixel i is foreground if argmax k S ik &gt; 0, and background, otherwise.</p><p>We apply the connected components algorithm proposed by <ref type="bibr" target="#b31">[32]</ref> to find the blobs B in the foreground mask F. We only consider the blobs with two or more ground truth point annotationsB. We propose two methods for splitting blobs (see <ref type="figure">Fig. 2</ref>), 1. Line split method. For each blob b inB we pair each point with its closest point resulting in a set of pairs b P . For each pair (p i , p j ) ∈ b P we use a scoring function to determine the best segment E that is perpendicular to the line between p i and p j . The segment lines are within the predicted blob and they intersect the blob boundaries. The scoring function z(·) for segment E is computed as,</p><formula xml:id="formula_3">z(E) = 1 |E| i∈E S i0 ,<label>(4)</label></formula><p>which is the mean of the background probabilities belonging to segment E (where 0 is the background class). The best edge E best is defined as the set of pixels representing the edge with the highest probability of being background among all the perpendicular lines. This determines the 'most likely' edge of separation between the two objects. Then we set T b as the set of pixels representing the best edges generated by the line split method. 2. Watershed split method. This consists of global and local segmentation procedures. For the global segmentation, we apply the watershed segmentation algorithm <ref type="bibr" target="#b32">[33]</ref> globally on the input image where we set the ground-truth point-annotations as the seeds. The segmentation is applied on the distance transform of the foreground probabilities, which results in k segments where k is the number of point-annotations in the image. For the local segmentation procedure, we apply the watershed segmentation only within each blob b inB where we use the point-annotation ground-truth inside them as seeds. This adds more importance to splitting big blobs when computing the loss function. Finally, we define T b as the set of pixels representing the boundaries determined by the local and global segmentation.</p><p>Predicted blobs Line splits Watershed splits <ref type="figure">Fig. 2</ref>. Split methods. Comparison between the line split, and the watershed split. The loss function identifies the boundary splits (shown as yellow lines). Yellow blobs represent those with more than one object instance, and red blobs represent those that have no object instance. Green blobs are true positives. The squares represent the ground-truth point annotations. <ref type="figure">Fig. 2</ref> shows the split boundaries using the line split and the watershed split methods (as yellow lines). Given T b , we compute the split loss as follows,</p><formula xml:id="formula_4">L S (S, T ) = − i∈T b α i log(S i0 ),<label>(5)</label></formula><p>where S i0 is the probability that pixel i belongs to the background class and α i is the number of point-annotations in the blob in which pixel i lies. This encourages the model to focus on splitting blobs that have the most point-level annotations. The intuition behind this method is that learning to predict the boundaries between the object instances allows the model to distinguish between them. As a result, the penalty term encourages the model to output a single blob per object instance. We emphasize that it is not necessary to get the right edges in order to accurately count. It is only necessary to make sure we have a positive region on each object and a negative region between objects. Other heuristics are possible to construct a negative region which could still be used in our framework. For example, fast label propagation methods proposed in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> can be used to determine the boundaries between the objects in the image. Note that these 4 loss functions are only used during training. The framework does not split or remove false positive blobs at test time. The predictions are based purely on the blobs obtained from the probability matrix S.</p><p>False Positive loss. L F discourages the model from predicting a blob with no point annotations, in order to reduce the number of false positive predictions. The loss function is defined as where B f p is the set of pixels constituting the blobs predicted for each class (except the background class) that contain no ground-truth point annotations (note that S i0 is the probability that pixel i belongs to the background class). All the predictions within B f p are considered false positives (see the red blobs in <ref type="figure" target="#fig_1">Fig.  5</ref>). Therefore, optimizing this loss term results in less false positive predictions as shown in the qualitative results in <ref type="figure" target="#fig_1">Fig. 5</ref>. The experiments show that this loss term is extremely important for accurate object counting.</p><formula xml:id="formula_5">L F (S, T ) = − i∈B f p log(S i0 ),<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LC-FCN Architecture and Inference</head><p>LC-FCN can be any FCN architecture such as FCN8 architecture <ref type="bibr" target="#b23">[24]</ref>, Deeplab <ref type="bibr" target="#b35">[36]</ref>, Tiramisu <ref type="bibr" target="#b24">[25]</ref>, and PSPnet <ref type="bibr" target="#b25">[26]</ref>. LC-FCN consists of a backbone that extracts the image features. The backbone is an Imagenet pretrained network such as VGG16 or ResNet-50 <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. The image features are then upscaled using an upsampling path to output a score for each pixel i indicating the probability that it belongs to class c (see <ref type="figure" target="#fig_0">Fig. 3</ref>). We predict the number of objects for class c through the following three steps: (i) the upsampling path outputs a matrix Z where each entry Z ic is the probability that pixel i belongs to class c; then (ii) we generate a binary mask F , where pixel F ic = 1 if arg max k Z ik = c, and 0 otherwise; lastly (iii) we apply the connected components algorithm <ref type="bibr" target="#b31">[32]</ref> on F to get the blobs for each class c. The count is the number of predicted blobs (see <ref type="figure" target="#fig_0">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we describe the evaluation metrics, the training procedure, and present the experimental results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Evaluation Metric. For datasets with single-class objects, we report the mean absolute error (MAE) which measures the deviation of the predicted count p i from the true count c i , computed as 1</p><formula xml:id="formula_6">N i |p i − c i |.</formula><p>MAE is a commonly used metric for evaluating object counting methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. For datasets with multiclass objects, we report the mean root mean square error (mRMSE) as used in <ref type="bibr" target="#b8">[9]</ref> for the PASCAL VOC 2007 dataset. We measure the localization performance using the average mean absolute error (GAME) as in <ref type="bibr" target="#b5">[6]</ref>. Since our model predicts blobs instead of a density map, GAME might not be an accurate localization measure. Therefore, in section 4.3 we use the F-Score metric to assess the localization performance of the predicted blobs against the point-level annotation ground-truth.</p><p>Training Procedure. We use the Adam <ref type="bibr" target="#b40">[41]</ref> optimizer with a learning rate of 10 −5 and weight decay of 5 × 10 −5 . We use the provided validation set for early stopping only. During training, the model uses a batch size of 1 which can be an image of any size. We double our training set by applying the horizontal flip augmentation method on each image. Finally, we report the prediction results on the test set. We compare between three architectures: FCN8 <ref type="bibr" target="#b23">[24]</ref>; ResFCN which is FCN8 that uses ResNet-50 as the backbone instead of VGG16; and PSPNet <ref type="bibr" target="#b25">[26]</ref> with ResNet-101 as the backbone. We use the watershed split procedure in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Penguins Dataset <ref type="bibr" target="#b0">[1]</ref>. The Penguins dataset comprises images of penguin colonies located in Antarctica. We use the two dataset splits as in <ref type="bibr" target="#b0">[1]</ref>. In the 'separated' dataset split, the images in the training set come from different cameras than those in the test set. In the 'mixed' dataset split, the images in the training set come from the same cameras as those in the test set. In <ref type="table" target="#tab_0">Table 1</ref>, the MAE is computed with respect to the Max and Median count (as there are multiple annotators). Our methods significantly outperform theirs in all of the four settings, although their methods use depth features and the multiple annotations  <ref type="table">Table 2</ref>. Trancos dataset. Evaluation of our method against previous state-of-theart methods, comparing the mean absolute error (MAE) and the grid average mean absolute error (GAME) as described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>provided for each penguin. This suggests that LC-FCN can learn to distinguish between individual penguins despite the heavy occlusions and crowding.</p><p>Trancos Dataset <ref type="bibr" target="#b9">[10]</ref>. The Trancos dataset comprises images taken from traffic surveillance cameras located along different roads. The task is to count the vehicles present in the regions of interest of the traffic scenes. Each vehicle is labeled with a single point annotation that represents its location in the image. We observe in <ref type="table">Table 2</ref> that our method achieves new state-of-the-art results for counting and localization. Note that GAM E(L) subdivides the image using a grid of 4 L non-overlapping regions, and the error is computed as the sum of the mean absolute errors in each of these subregions. For our method, the predicted count of a region is the number of predicted blob centers in that region. This provides a rough assessment of the localization performance. Compared to the methods in <ref type="table">Table 2</ref>, LC-FCN does not require a perspective map nor a multi-scale approach to learn objects of different sizes. These results suggest that LC-FCN can accurately localize and count extremely overlapping vehicles.</p><p>Parking Lot <ref type="bibr" target="#b4">[5]</ref>. The dataset comprises surveillance images taken at a parking lot in Curitiba, Brazil. We used the PUCPR subset of the dataset where the first 50% of the images was set as the training set and the last 50% as the test set. The last 20% of the training set was set as the validation set for early stopping. The ground truth consists of a bounding box for each parked car since this dataset is primarily used for the detection task. Therefore, we convert them into pointlevel annotations by taking the center of each bounding box. <ref type="table">Table 5</ref> shows that LC-FCN significantly outperforms Glance in MAE. LC-FCN8 achieves only 0.21 average miscount per image although many images contain more than 20 parked cars. This suggests that explicitly learning to localize parked cars can perform  better in counting than methods that explicitly learn to count from image-level labels (see <ref type="figure" target="#fig_1">Fig. 5</ref> for qualitative results). Note that this is the first counting method being applied on this dataset.</p><p>MIT Traffic <ref type="bibr" target="#b2">[3]</ref>. This dataset consists of surveillance videos taken from a single fixed camera. It has 20 videos, which are split into a training set (Videos 1-8), a validation set (Videos 0-10), and a test set (Videos <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Each video frame is provided with a bounding box indicating each pedestrian. We convert them into point-level annotations by taking the center of each bounding box. <ref type="table">Table 5</ref> shows that our method significantly outperforms Glance, suggesting that learning a localization-based objective allows the model to ignore the background regions that do not contribute to the object count. As a result, LC-FCN is less likely to overfit on irrelevant features from the background. To the best of our knowledge, this is the first counting method being applied on this dataset.</p><p>Pascal VOC 2007 <ref type="bibr" target="#b1">[2]</ref>. We use the standard training, validation, and test split as specified in <ref type="bibr" target="#b1">[2]</ref>. We use the point-level annotation ground-truth provided by Bearman et al. <ref type="bibr" target="#b13">[14]</ref> to train our LC-FCN methods. We evaluated against the count of the non-difficult instances of the Pascal VOC 2007 test set. <ref type="table">Table 3</ref> compares the performance of LC-FCN with different methods proposed by <ref type="bibr" target="#b8">[9]</ref>. We point the reader to <ref type="bibr" target="#b8">[9]</ref> for a description of the evaluation metrics used in the with respect to mRMSE. We see that LC-FCN outperforms methods that explicitly learn to count although learning to localize objects of this dataset is a very challenging task. Further, LC-FCN uses weaker supervision than Aso-sub and Seq-sub as they require the full per-pixel labels to estimate the object count for different image regions.</p><p>Crowd Counting Datasets. <ref type="table" target="#tab_3">Table 4</ref> reports the MAE score of our method on 3 crowd datasets using the setup described in the survey paper <ref type="bibr" target="#b39">[40]</ref>. For this experiment, we show our results using ResFCN as the backbone with the Watershed split method. We see that our method achieves competitive performance for crowd counting. <ref type="figure">Fig. 4</ref> shows the predicted blobs of our model on a test image of the ShanghaiTech B dataset. We see that our model predicts a blob on the face of each individual. This is expected since the ground-truth point-level annotations are marked on each person's face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Localization Benchmark. Since robust localization is useful in many computer vision applications, we use the F-Score measure to directly assess the localization performance of our model. F-Score is a standard measure for detection as it considers both precision and recall, F-Score = 2TP 2TP+FP+FN , where the number of true positives (TP) is the number of blobs that contain at least one point annotation; the number of false positives (FP) is the number of blobs that contain no point annotation; and the number of false negatives (FN) is the number of point annotations minus the number of true positives. <ref type="table">Table 5</ref> shows the localization results of our method on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function Analysis.</head><p>We assess the effect of each term of the loss function on counting and localization results. We start by looking at the results of a model trained with the image-level loss L I and the point-level loss L P only. These two terms were used for semantic segmentation using point annotations <ref type="bibr" target="#b13">[14]</ref>. We observe in <ref type="figure" target="#fig_1">Fig. 5(b)</ref> that a model using these two terms results in a single blob that groups many object instances together. Consequently, this performs poorly in terms of the mean absolute error and the F-Score (see <ref type="table">Table 5</ref>). As a result, we introduced the split-level loss function L S that encourages the model to predict blobs that do not contain more than one point-annotation. We see in <ref type="figure" target="#fig_1">Fig. 5</ref>(c) that a model using this additional loss term predicts several blobs as object instances rather than one large single blob. However, since L I + L P + L S does not penalize the model from predicting blobs with no point annotations, it can often lead to many false positives. Therefore, we introduce the false positive loss L F that discourages the model from predicting blobs with no point annotations. By adding this loss term to the optimization, LC-FCN achieves significant improvement as seen in the qualitative and quantitative results (see <ref type="figure" target="#fig_1">Fig. 5(d)</ref> and <ref type="table">Table 5</ref>). Further, including only the split-level loss leads to predicting a huge number of small blobs, leading to many false positives which makes performance worse. Combining it with the false-positive loss avoids this issue which leads to a net improvement in performance. On the other hand, using only the false positive loss it tends to predict one huge blob.</p><p>Split Heuristics Analysis. In <ref type="figure">Fig. 6</ref> we show that the watershed split achieves better MAE on Trancos and Penguins validation sets. Further, using the watershed split achieves much faster improvement on the validation set with respect to the number of epochs. This suggests that using proper heuristics to identify the negative regions is important, which leaves an open area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose LC-FCN, a fully-convolutional neural network, to address the problem of object counting using point-level annotations only. We propose a novel loss function that encourages the model to output a single blob for each object instance. Experimental results show that LC-FCN outperforms current stateof-the-art models on the PASCAL VOC 2007, Trancos, and Penguins datasets which contain objects that are heavily occluded. For future work, we plan to explore different FCN architectures and splitting methods that LC-FCN can use to efficiently split between overlapping objects that have complicated shapes and appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We would like to thank the anonymous referees for their useful comments that significantly improved the paper. Issam Laradji is funded by the UBC Four-Year Doctoral Fellowships (4YF).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Given an input image, our model first extracts features using a backbone architecture such as ResNet. The extracted features are then upsampled through the upsampling path to obtain blobs for the objects. In this example, the model predicts the blobs for persons and bikes for an image in the PASCAL VOC 2007 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of LC-FCN trained with different terms of the proposed loss function. (a) Test images obtained from MIT Traffic, Parking Lot, Trancos, and Penguins. (b) Prediction results using only image-level and point-level loss terms. (c) Prediction results using image-level, point-level, and split-level loss terms. (d) Prediction results trained with the full proposed loss function. The green blobs and red blobs indicate true positive and false positive predictions, respectively. Yellow blobs represent those that contain more than one object instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Penguins datasets. Evaluation of our method against previous state-of-theart methods. The evaluation is made across the four setups explained in the dataset description.</figDesc><table><row><cell></cell><cell cols="2">Separated</cell><cell>Mixed</cell><cell></cell></row><row><cell>Method</cell><cell>Max</cell><cell>Median</cell><cell>Max</cell><cell>Median</cell></row><row><cell>Density-only [1]</cell><cell>8.11</cell><cell>5.01</cell><cell>9.81</cell><cell>7.09</cell></row><row><cell>With seg. and depth [1]</cell><cell>6.38</cell><cell>3.99</cell><cell>5.74</cell><cell>3.42</cell></row><row><cell>With seg and no depth [1]</cell><cell>5.77</cell><cell>3.41</cell><cell>5.35</cell><cell>3.26</cell></row><row><cell>Glance</cell><cell>6.08</cell><cell>5.49</cell><cell>1.84</cell><cell>2.14</cell></row><row><cell>LC-FCN8</cell><cell>3.74</cell><cell>3.28</cell><cell>1.62</cell><cell>1.80</cell></row><row><cell>LC-ResFCN</cell><cell>3.96</cell><cell>3.43</cell><cell>1.50</cell><cell>1.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3. PASCAL VOC. We compare against the methods proposed in<ref type="bibr" target="#b8">[9]</ref>. Our model evaluates on the full test set, whereas the other methods take the mean of ten random samples of the test set evaluation.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">mRMSE mRMSE-nz m-relRMSE m-relRMSE-nz</cell></row><row><cell cols="2">Glance-noft-2L [9]</cell><cell></cell><cell>0.50</cell><cell>1.83</cell><cell>0.27</cell><cell>0.73</cell></row><row><cell cols="3">Aso-sub-ft-3 × 3 [9]</cell><cell>0.42</cell><cell>1.65</cell><cell>0.21</cell><cell>0.68</cell></row><row><cell cols="2">Faster-RCNN [9]</cell><cell></cell><cell>0.50</cell><cell>1.92</cell><cell>0.26</cell><cell>0.85</cell></row><row><cell>LC-ResFCN</cell><cell></cell><cell></cell><cell>0.31</cell><cell>1.20</cell><cell>0.17</cell><cell>0.61</cell></row><row><cell>LC-PSPNet</cell><cell></cell><cell></cell><cell>0.35</cell><cell>1.32</cell><cell>0.20</cell><cell>0.70</cell></row><row><cell>Methods</cell><cell cols="4">UCSD Mall ShanghaiTech B</cell></row><row><cell>FCN-rLSTM [43]</cell><cell>1.54</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>MoCNN [45]</cell><cell>-</cell><cell>2.75</cell><cell>-</cell><cell></cell></row><row><cell cols="3">CNN-boosting [46] 1.10 2.01</cell><cell>-</cell><cell></cell></row><row><cell>M-CNN [17]</cell><cell>1.07</cell><cell>-</cell><cell>26.4</cell><cell></cell></row><row><cell>CP-CNN [47]</cell><cell>-</cell><cell>-</cell><cell>20.1</cell><cell></cell></row><row><cell>CSRNet [44]</cell><cell>1.16</cell><cell>-</cell><cell>10.6</cell><cell></cell></row><row><cell>LC-FCN8</cell><cell cols="2">1.51 2.42</cell><cell>13.14</cell><cell></cell></row><row><cell>LC-ResFCN</cell><cell cols="2">0.99 2.12</cell><cell>25.89</cell><cell></cell></row><row><cell>LC-PSPNet</cell><cell cols="2">1.01 2.00</cell><cell>21.61</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Crowd datasets MAE results.Fig. 4. Predicted blobs on a ShanghaiTech B test image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table .</head><label>.</label><figDesc>We show that LC-FCN achieves new state-of-the-art results</figDesc><table><row><cell></cell><cell cols="2">MIT Traffic</cell><cell cols="2">PKLot</cell><cell cols="2">Trancos</cell><cell cols="2">Penguins Separated</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>FS</cell><cell>MAE</cell><cell>FS</cell><cell>MAE</cell><cell>FS</cell><cell>MAE</cell><cell>FS</cell></row><row><cell>Glance</cell><cell>1.57</cell><cell>-</cell><cell>1.92</cell><cell>-</cell><cell>7.01</cell><cell>-</cell><cell>6.09</cell><cell>-</cell></row><row><cell>LI + LP</cell><cell>3.11</cell><cell>0.38</cell><cell>39.62</cell><cell>0.04</cell><cell>38.56</cell><cell>0.05</cell><cell>9.81</cell><cell>0.08</cell></row><row><cell cols="2">LI + LP + LS 1.62</cell><cell>0.76</cell><cell>9.06</cell><cell>0.83</cell><cell>6.76</cell><cell>0.56</cell><cell>4.92</cell><cell>0.53</cell></row><row><cell cols="2">LI + LP + LF 1.84</cell><cell>0.69</cell><cell>39.60</cell><cell>0.04</cell><cell>38.26</cell><cell>0.05</cell><cell>7.28</cell><cell>0.04</cell></row><row><cell>LC-ResFCN</cell><cell>1.26</cell><cell>0.81</cell><cell>10.16</cell><cell>0.84</cell><cell>3.32</cell><cell>0.68</cell><cell>3.96</cell><cell>0.63</cell></row><row><cell>LC-FCN8</cell><cell>0.91</cell><cell>0.69</cell><cell>0.21</cell><cell>0.99</cell><cell>4.53</cell><cell>0.54</cell><cell>3.74</cell><cell>0.61</cell></row><row><cell cols="9">Table 5. Quantitative results. Comparison of different parts of the proposed loss</cell></row><row><cell cols="5">function for counting and localization performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Fig. 6. Split Heuristics Analysis. Comparison between the watershed split method and the line split method against the validation MAE score.</figDesc><table><row><cell></cell><cell></cell><cell>2.8 × 101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Split method Trancos Penguins LC-ResFCN (L) 4.77 1.89 LC-ResFCN (W) 3.34 0.95</cell><cell>MAE on Trancos Val Set</cell><cell>8.1 1.2 × 101 1.9 × 101 5.3</cell><cell>5</cell><cell>10</cell><cell>Line Split 15 Epochs Wate rshed Split</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>MAE on Penguins Val Set</cell><cell>1.5 2.2 3.2 1.0</cell><cell>5</cell><cell>10</cell><cell>Line Split 15 Epochs Watershed Split</cell><cell>20</cell><cell>25</cell><cell>30</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Counting in the wild. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhanced semantic descriptors for functional scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pklot-a robust dataset for parking lot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Silva</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Koerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IbPRIA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Glastonbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Z</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Count-ception: Counting by Fully Convolutional Redundant Counting. ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
		<title level="m">Towards perspective-free object counting with deep learning. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Counting crowded moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Crowd counting and profiling: Methodology and evaluation. Modeling, Simulation and Visual Analysis of Crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unified crowd segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Ssd: Single shot multibox detector. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to detect cells using non-overlapping extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to detect partially overlapping instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Detecting overlapping instances in microscopy images using extremal region trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing connected component labeling algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Otoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoshani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging: Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The morphological approach to segmentation: the watershed transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Let&apos;s make block coordinate descent go fast: Faster greedy rules, message-passing, active-set complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>and superlinear convergence. arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sepehry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Virani</surname></persName>
		</author>
		<title level="m">Convergence rates for greedy kaczmarz algorithms, and faster randomized kaczmarz rules using the orthogonality graph. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet: A Large-Scale Hierarchical Image Database. CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Curry</surname></persName>
		</author>
		<title level="m">Nonnegative matrix factorization applied to reordered pixels of single images based on patches to achieve structured nonnegative dictionaries. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding traffic density from large-scale web camera data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
