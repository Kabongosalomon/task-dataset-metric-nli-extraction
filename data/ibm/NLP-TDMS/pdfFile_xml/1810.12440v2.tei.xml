<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TallyQA: Answering Complex Counting Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Acharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chester F. Carlson Center for Imaging Science</orgName>
								<orgName type="institution">Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chester F. Carlson Center for Imaging Science</orgName>
								<orgName type="institution">Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
							<email>kanan@rit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Chester F. Carlson Center for Imaging Science</orgName>
								<orgName type="institution">Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TallyQA: Answering Complex Counting Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world's largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields stateof-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Open-ended counting systems take in a counting question and an image to predict a whole number that answers the question. While object recognition systems now rival humans ), today's best open-ended counting systems perform poorly <ref type="bibr" target="#b5">(Kafle and Kanan 2017b;</ref><ref type="bibr" target="#b2">Chattopadhyay et al. 2017</ref>). This could be due to an inability to detect the correct objects or due to an inability to reason about them. To address this, we distinguish between simple and complex counting questions (see <ref type="figure">Fig. 1</ref>). Simple counting questions only require object detection, e.g., "How many dogs are there?" Complex questions require deeper analysis, e.g., "How many dogs are eating?"</p><p>Open-ended counting is a special case of visual question answering (VQA) <ref type="bibr" target="#b2">(Antol et al. 2015;</ref><ref type="bibr" target="#b6">Malinowski and Fritz 2014)</ref>, in which the goal is to answer open-ended questions about images. The best VQA systems pose it as a classification problem where the answer is predicted from convolutional visual features and the question <ref type="bibr" target="#b4">(Kafle and Kanan 2017a)</ref>. While this succeeds for many question types, it works poorly for counting <ref type="bibr" target="#b5">(Kafle and Kanan 2017b;</ref><ref type="bibr" target="#b2">Chattopadhyay et al. 2017)</ref>. Recently, better results were achieved by using region proposals generated by object detection algorithms <ref type="bibr" target="#b10">(Trott, Xiong, and Socher 2018;</ref><ref type="bibr" target="#b13">Zhang, Hare, and Prügel-Bennett 2018)</ref>. However, datasets mostly contain simple counting questions, as shown in <ref type="table" target="#tab_2">Table 1</ref>. Due to their rarity, complex questions need to be analyzed separately to determine if a model is capable of answering them.</p><p>This paper makes three major contributions: 1. We describe TallyQA, the world's largest open-ended counting dataset, which is over 2.5 times bigger than <ref type="figure">Figure 1</ref>: Counting datasets consist mostly of simple questions (top) that can be answered solely using object detection. We study complex counting questions (bottom) that require more than object detection using our new TallyQA dataset.</p><p>VQA2. TallyQA is designed to study both simple questions that require only object detection and complex questions that demand more. It will be made publicly available. 2. We propose the relational counting network (RCN), a new algorithm for counting that infers relationships between objects and background image regions. It is inspired by relation networks, with modifications to handle a dynamic number of image regions and to explicitly incorporate background information.</p><p>3. We show that RCN surpasses state-of-the-art methods for open-ended counting on both TallyQA and the HowMany-QA benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQA Datasets &amp; Counting</head><p>Popular VQA datasets contain a significant number of counting questions, e.g., about 7% in COCO-QA <ref type="bibr" target="#b7">(Ren, Kiros, and Zemel 2015)</ref>, 10% of VQA1 <ref type="bibr" target="#b2">(Antol et al. 2015</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms for Open-Ended Counting</head><p>Open-ended counting systems take as input a "How many ...?" question and an image and then output a count. This is a VQA sub-problem. For counting, there are two general approaches.  We hypothesize that the inability of VQA algorithms to count is due to the way their architectures are designed. These systems operate on image embeddings computed using a CNN. Mean pooling and weighted mean pooling (attention) operations may destroy information that can be used to determine how many objects of a particular type are present.</p><p>Counting Specific Systems. While counting has long been studied for specific computer vision problems <ref type="bibr" target="#b11">(Zhang et al. 2015;</ref><ref type="bibr" target="#b3">Dalal and Triggs 2005;</ref><ref type="bibr" target="#b10">Wang and Wang 2011;</ref><ref type="bibr" target="#b8">Ryan et al. 2009;</ref><ref type="bibr" target="#b6">Ren and Zemel 2017)</ref>, only recently has open-ended counting in natural scenes been studied. <ref type="bibr" target="#b2">Chattopadhyay et al. (2017)</ref> studied open-ended counting in typical scenes, and they evaluated three counting-specific methods: DETECT, GLANCE, and SUBITIZE. DETECT is built on top of an object detection algorithm, which was Fast R-CNN (Girshick 2015) in their implementation. DETECT works by finding the first noun in a question and then matching it to the closest category the detection algorithm has been trained for (e.g., COCO objects). GLANCE uses a shallow multi-layer perceptron (MLP) to regress for specific object counts from a CNN embedding, with the appropriate output unit chosen based on the first noun. SUBITIZE involves breaking the image into a grid, extracting image embeddings from each grid location, aggregating information across grids using an RNN, and then predicting the count for every class in the dataset. Although none of these methods are capable of handling complex questions, all of them outperformed MCB, which was a state-of-the-art VQA model.</p><p>Recently, <ref type="bibr" target="#b10">Trott, Xiong, and Socher (2018)</ref> and <ref type="bibr" target="#b13">Zhang, Hare, and Prügel-Bennett (2018)</ref> both created algorithms for open-ended counting in natural scenes that are built on top of object proposals generated by an object detection algorithm trained on Visual Genome. Trott et al. created the ILRC algorithm, which redefines counting as a sequential object selection problem. ILRC uses reinforcement learning to select the objects that need to be counted based on the question. Zhang et al. created a method that uses object detection and then constructs a graph of all detected objects based on how they overlap. Edges in the graph are removed based on several heuristics to ensure that duplicated objects are only counted once. ). Since neither of these algorithms performs any relational or comparative reasoning between the boxes, they may have an impaired ability to answer complex questions. Here, our RCN model applies relational reasoning to object-object and object-background pairs, giving it a more robust capability to answer complex and relational questions. Indeed, our experiments show that RCN outperforms other models on complex questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The TallyQA Dataset</head><p>Complex counting questions are rare in existing datasets. This prompted us to create TallyQA. TallyQA's test set is split into two parts: Test-Simple for simple counting questions and Test-Complex for complex counting questions. We gathered new complex questions using Amazon Mechanical Turk (AMT), and imported both simple and complex questions from other datasets. <ref type="table" target="#tab_2">Table 1</ref> shows the total number of questions in TallyQA compared to others, and it has over twice as many complex questions. The number of questions in the train and test sets by source is given in <ref type="table" target="#tab_4">Table 2</ref>. <ref type="figure" target="#fig_2">Fig. 4</ref> shows example images and questions. Test-Simple and Test-Complex contain images from only Visual Genome, and Train has images from COCO and Visual Genome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collecting New Complex Questions</head><p>To gather new complex questions, we developed targeted AMT tasks that yielded 19,500 complex questions for 17,545 unique images. These tasks were designed to fight the biases in earlier datasets, where simple counting questions were predominantly asked (Kafle and Kanan 2017a). TallyQA's images are drawn from both COCO and Visual Genome, which provides more variety than COCO alone. About 800 unique annotators provided QA pairs. For all tasks, annotators were not allowed to submit obviously simple questions, e.g., "How many x?" and "How many x in the photo?" We manually</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split</head><p>Questions Images  checked AMT questions to ensure they were complex, and we removed poor quality questions. We endeavored to ensure non-zero complex questions were difficult, e.g., "How many men are wearing glasses?" is not difficult if all of the men in the image are wearing glasses. To do this, annotators were told to ask questions in which there were counter examples, e.g., to ask "How many men are wearing glasses?" only if it had an answer greater than zero, and the contrary question "How many men are not wearing glasses?" had an answer greater than zero. We created a separate task to generate hard complex questions with zero as the answer. Annotators were asked to make questions about objects with attributes not observed in the image, e.g., asking the question "How many dogs have spots?" when there was a dog without spots in the image. Similar examples were shown to annotators before annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importing Questions from Other Datasets</head><p>TallyQA also contains questions imported from VQA2 and Visual Genome. A similar approach was used to create HowMany-QA <ref type="bibr" target="#b10">(Trott, Xiong, and Socher 2018)</ref> and TDIUC <ref type="bibr" target="#b5">(Kafle and Kanan 2017b)</ref>. We imported all questions beginning with the phrase "How many..." with answers that were whole numbers between 0-15. Following <ref type="bibr" target="#b5">Kafle and Kanan (2017b)</ref>, for VQA2, we required that 5 of the 10 annotators give the same answer. Although these questions were generated by humans, as seen in <ref type="table" target="#tab_2">Table 1</ref>, most are simple.</p><p>We also imported synthetic counting questions from TDIUC <ref type="bibr" target="#b5">(Kafle and Kanan 2017b)</ref>. These questions were gen-erated for COCO images using its semantic annotations. The creators used a variety of templates to introduce variation in the questions and used heuristics to avoid answer ambiguity. All template generated questions from TDIUC are simple. In addition to templates, we used their method for making "absurd" questions to create both simple and complex zero count questions. To do this, we first find the objects absent from an image based on its COCO annotations. Then, we randomly sample the counting questions from the rest of the dataset that ask about counting these objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifying Simple and Complex Questions</head><p>The Test-Complex dataset was made using only new, human vetted complex questions from AMT. Because simple questions are common in existing datasets like VQA2, we used imported questions to make Test-Simple. To do this, we developed a classifier to determine if a question was simple.</p><p>Our simple-complex classifier is made from a set of linguistic rules. First, any substrings such as "...in the photo?" or "...in the image?" were removed from the question. Then, we used SpaCy to do part of speech tagging on the remaining substring. It was classified as simple if it had only one noun, no adverbs, and no adjectives, otherwise it was deemed complex. This will classify questions such as "How many dogs?" as simple and "How many brown dogs?" as complex.</p><p>Every question classified as simple by our rules will be correct (i.e., the false positive rate is zero), making it suitable for creating Test-Simple, but it may sometimes classify simple questions as complex (i.e., the false negative rate is non-zero). For example, the question "How many men are wearing red hats to the left of the tree?" would be classified as complex by our classifier. However, if there was only a single person in the image then it is not truly a complex question, despite the apparent complexity. These kinds of questions are rare and our simple-complex classifier works robustly, but it is possible that it will underestimate the number of simple questions and overestimate the number of complex when used to characterize a dataset. For this reason, we only use human-vetted questions in TallyQA's Test-Complex set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Splits &amp; Statistics</head><p>TallyQA is split into one training split (Train) and two test splits: Test-Simple and Test-Complex. Using our simplecomplex classifier, Train was found to have 188,439 simple and 60,879 complex questions. The number of questions in each split is given in <ref type="table" target="#tab_4">Table 2</ref>. The test splits are comprised exclusively of Visual Genome imagery, and no images in the test splits are used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A New Framework for Complex Counting</head><p>Our RCN model, depicted in <ref type="figure">Fig. 3</ref>, is formulated as a modi-  <ref type="figure">RN (O, B)</ref> represents the RN responsible for inferring the relationship between each foreground and background region, and h γ is a neural network with parameters γ that predicts the final count.</p><p>The RN for predicting the relationship between foreground proposals in the context of question Q is given by</p><formula xml:id="formula_0">RN(O, O) = f φ1   i,j g θ1 (o i , o j , s ij , Q)   ,<label>(2)</label></formula><p>where f φ1 and g θ1 are neural networks with parameters φ 1 and θ 1 , respectively, that each output a vector, and the vector s ij encodes spatial information about the i-th and j-th proposals. Like the original RN model, the sum is computed over all n 2 pairwise combinations. Similarly, the RN for predicting the relationship of each proposal to the background is given by,</p><formula xml:id="formula_1">RN(O, B) = f φ2   i,j g θ2 (o i , b j , s ij , Q)   ,<label>(3)</label></formula><p>where f φ2 and g θ2 are neural networks with parameters φ 2 and θ 2 , respectively, that output vectors. RCN has two major innovations over the original RN approach. The original RN used raw CNN feature map indices as regions. This worked well for CLEVR, but this approach works poorly for real-world VQA datasets that require processing at higher resolutions (e.g., VQA2). RCN overcomes this problem by using region proposals. As input, the original RN model used the d 2 elements in a d × d convolutional feature map, which were each tagged with their spatial coordinates. This means it computed d 4 pairwise relationships. For recent direct VQA methods, a CNN feature map is typically 14 × 14, meaning that 38,416 comparisons would be needed per counting query. In contrast, RCN's proposal generator produces only 31.12 foreground regions and 16 background patches per image, so only 31.12 2 + (31.12 × 16) = 1466 comparisons are made, on average. By using proposals, RCN reduces the number of comparisons by a factor of 26 and scales to real-world imagery, whereas the original RN model used lower resolution imagery and was only evaluated on CLEVR (Johnson et al. 2017), a synthetic dataset that has simple geometric shapes and a plain background.</p><p>RCN's second innovation is the explicit incorporation of the background. For queries such as "How many dogs are laying in the grass?" it is necessary to consider background entities (stuff) that are ignored by object detection systems. RCN uses m image background patches, and computes the relationships of each region with each background patch, enabling the background to be studied with relatively few comparisons. In contrast, the original RN model did not explicitly deal with the background, but it was likely unnecessary due to the simple scenes in CLEVR. Explicitly modeling Figure 3: Our RCN model computes the relationship between foreground regions as well as the relationships between the these regions and the background to efficiently answer complex counting questions. In this example, the system needs to look at the relationship of each giraffe to each other and with the water (background). the background can help answer complex counting questions, which often involve attributes of background objects or relationships between objects and background entities.</p><p>Internally, RCN uses the spatial relationship between regions o i and o j to help predict the count. Using s ij is critical to ensuring each object is counted only once during prediction, and it enables RCN to learn to do non-maximal suppression to cope with overlapping proposals. The spatial relationship vector is given by</p><formula xml:id="formula_2">s ij = i , j , ξ ij , IoU ij , IoU ij A i , IoU ij A j ,<label>(4)</label></formula><p>where i and j encode the spatial information of each proposal individually, A i and A j are the area of proposals, ξ ij is the dot product between each proposal's CNN features to model how visually similar they are, and IoU ij is the intersection over union between the two proposals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training &amp; Implementation Details</head><p>The question Q is embedded using a one layer GRU that takes as input pre-trained 300-dimensional Glove vectors for each word in the question (Pennington, Socher, and Manning 2014) and outputs a vector of 1024 dimension. The GRU used in all models are regularized using a dropout of 0.3.</p><p>For foreground proposals, we use the boxes and CNN features produced by Faster R-CNN  with ResNet-101 as its backbone. The Faster R-CNN model is trained to predict boxes in Visual Genome, which contain a wide variety of objects and attributes. This approach for generating proposals was pioneered by <ref type="bibr" target="#b0">Anderson et al. (2018)</ref>, and has since been used by multiple VQA systems.</p><p>For the background patches, we extract ResNet-152 features from the entire image before the last pooling layer and then apply average pooling over these features to reduce them to a 4 × 4 grid. Each of these 2048-dimensional vectors represents a 112 × 112 pixel background region. In RCN, g θ has three hidden layers and g φ has one hidden layer, which each consist of 1024 rectified linear units (ReLUs). The outputs of these networks are then concatenated and passed to h γ , which has one hidden layer with 1024 units and ReLU activation. The softmax output layer treats counting as a classification task, and it is optimized using cross-entropy loss. RCN is trained using the Adam optimizer with a learning rate of 7e −4 and a batch size of 64 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we describe a series of experiments to evaluate the efficacy of multiple algorithms on both simple and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Evaluated</head><p>We compare RCN against two state-of-the-art models specifically for open-ended counting: Zhang, Hare, and Prügel-Bennett (2018) and IRLC <ref type="bibr" target="#b10">(Trott, Xiong, and Socher 2018</ref> The main difference is that we use the more recent YOLOv2 (Redmon and Farhadi 2017) method instead of Fast R-CNN. DETECT extracts the first noun from the question. It then finds the most semantically similar category that YOLOv2 was trained on to that noun based on word similarity, and then it outputs the total number of YOLOv2 boxes produced for that category. MUTAN, I-Only, and Q+I use ResNet-152 features. Q-Only, I-Only, Q+I, MUTAN, Zhang et al. , and RCN all use crossentropy loss and treat counting as a classification problem. Before evaluation, the output of all models was rounded to the nearest whole number and constrained to be within the range of values in the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Results for all methods on HowMany-QA and both of TallyQA's test sets are given in <ref type="table" target="#tab_7">Table 3</ref>  does not have an explicit mechanism for relational reasoning between objects and backgrounds, potentially impairing its ability to identify duplicates and compare attributes from different image regions.</p><p>Consistent with our claim that complex questions require more than detection, DETECT is the worst performer on Test-Complex. DETECT performs better on Test-Simple, but there is still a large gap between it and RCN.</p><p>To study the importance of the object-background model, we ran RCN without <ref type="figure">the RN (O, B)</ref>   <ref type="figure">Fig. 5c</ref>), RCN can infer an object's relative position to other objects. Since RCN is based on region proposals, it struggles when proposals do not align with question relevant objects <ref type="figure">(Fig. 5i</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Without Location Features</head><p>To assess the impact of using the spatial location information of each proposal, we conducted an experiment in which we removed the location features s ij given to RCN. For HowMany-QA, removing location caused a 5.4% decrease in accuracy (absolute). For TallyQA, it caused a decrease of 2.8% accuracy (absolute) for Test-Simple and 2.4% accuracy (absolute) for Test-Complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the Original RN</head><p>The original RN model uses raw CNN feature maps, rather than region proposals. Running the original RN model on HowMany-QA, it achieved 3.46 RMSE and about 20% less accuracy than RCN. RCN likely achieves better performance due to its improved architecture and due to using region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing RCN</head><p>To visualize RCN's inference process, we modified Grad-CAM <ref type="bibr" target="#b8">(Selvaraju et al. 2017)</ref>. Grad-CAM is a technique that, for a given prediction, generates a coarse heat map based on the gradient flow in the final convolutional layers. To adapt Grad-CAM to RCN, it is necessary to derive scores for each proposal. To do this, we first find the pairwise objectbackground score score(o i , b j ) using the gradient obtained at layer g θ2 . We then assign a score to each proposal using score(o i ) = max i,j score(o i , b j ). Scores for all proposals are then scaled from 0 to 1 and visualized on the original image. The results are shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>RCN achieved state-of-the-art results across all of the datasets, even outperforming Zhang et al., which is the best published result on VQA2's counting questions, and IRLC, which was the previous best result on HowMany-QA. The same regions and visual features were used across RCN, Zhang et al. , and IRLC, so the difference in performance is not due to using superior visual features, which is a frequent confound in many works. Our experiments showcased that there is a large performance gap between the ability for models to answer simple and complex questions. This gap (a) How many people are wearing long dresses?</p><p>(b) How many people are sitting on a horse?</p><p>(c) How many people are wearing glasses?</p><p>(d) How many people have a hat? (e) How many players are wearing red? (f) How many white cows are there?</p><p>(g) How many dogs are sleeping in the image?</p><p>(h) How many street lights are to be seen behind this man?</p><p>(i) How many of the planes are on the ground? <ref type="figure">Figure 5</ref>: Modified Grad-CAM visualizations show where RNC is looking to make predictions. The importance of each object proposals is proportional to the color intensity of the bounding boxes.</p><p>was especially large for RCN and the Zhang et al. method. A likely reason is that more data is required for complex questions to handle the full range of attributes and relations. We found that region based methods, such as RCN, IRLC, and the Zhang et al. model have better results compared to direct methods, e.g., <ref type="bibr">MUTAN (Ben-younes et al. 2017</ref>). However, all of these region based models, including ours, are not based on actual nameable objects but object proposals, which range from 10-100 in number for each image and can consist of many non-object regions and overlapping boxes. Intelligently pruning/refining of these proposals may improve performance of these systems. We tried simple non-maximal suppression to prune out the overlapping boxes for RCN, but it did not improve performance. We believe this to be due to the relational capacities of RCN which can learn to ignore duplicate or similar boxes based on the features and positions of the boxes more intelligently than off-the-shelf non-maximal suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we distinguished between simple and complex open-ended counting questions in VQA, where simple questions could be correctly answered using object detection alone. To do this, we created TallyQA, the world's largest dataset for open-ended counting using VQA, which will be made publicly available. We also described the RCN framework and showed that it can effectively answer both simple and complex counting questions compared to baseline models and state-of-the-art approaches for open-ended counting. RCN combines region proposals with relationship networks, enabling them to be efficiently used with high-resolution imagery. We found that RCN worked especially well compared to others on complex questions. Our work better defines the issues with open-ended counting, and sets the stage for future work on this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Histogram of answer counts for each of the three splits of TallyQA. Both Trott et al. and Zhang et al. operate on region proposals and loosely based on the idea of filtering out irrelevant boxes based on the question, i.e. selecting a subset of question relevant region proposals. However, successfully determining which boxes should be counted for a given question often requires comparing it with other object proposals (required for duplicate detection, comparative and positional reasoning, etc.), and the background (for modeling context, finding relative size, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>fied relation network (RN) (Santoro et al. 2017) that can reason about the nature of relationships between image regions. RCN uses the question Q to guide its processing of a list of n foreground region proposals, O = {o 1 , o 2 , . . . , o n }, and m background regions, B = {b 1 , b 2 , . . . , b m }, with o i ∈ R K and b j ∈ R K . Formally, our RCN model is the combination of two RN sub-networks, i.e., Count(O, B, Q) = h γ (RN(O, O) ⊕ RN(O, B)) , (1) where ⊕ denotes concatenation, RN (O, O) represents the RN that infers the relationship between foreground regions,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>component. As seen in (a) How many giraffes are there? GT: 2, DETECT: 2, Zhang:2, RCN: 2 (b) How many people are standing? GT: 2, DETECT: 4, Zhang: 3, RCN: 2 (c) How many people in the front row? GT: 8, DETECT: 22, Zhang: 6, RCN: 8 (d) How many chairs have a girl sitting on them? GT: 1, DETECT: 7, Zhang: 2, RCN: 1 (e) How many players are wearing red uniforms? GT: 3, DETECT: 11, Zhang: 4, RCN: 3 (f) How many strings does the instrument to the left have? GT: 4, DETECT: 3, Zhang: 1, RCN: 0 Example model outputs on TallyQA. While other models fail at positional reasoning questions (e.g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The number of counting questions for previous VQA datasets compared to TallyQA dataset.</figDesc><table><row><cell>than an image-blind (question-only) model. This was true</cell></row><row><cell>even though most of TDIUC's counting questions are simple.</cell></row><row><cell>This suggests these methods are primarily exploiting scene</cell></row><row><cell>and language priors. For VQA2, the best method (Teney et al.</cell></row><row><cell>2017) of the CVPR-2017 VQA Workshop challenge achieved</cell></row><row><cell>69% overall, but only 47% accuracy on number questions,</cell></row><row><cell>most of which are counting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Number of questions and images in TallyQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>min , x max ) and (y min , y max ) represent the top-left and bottom-right corners of proposal i, and W and H are the width and height of the image, respectively.</figDesc><table><row><cell>tor i = xmin W , ymin H , xmax W , ymax H , xmax−xmin W</cell><cell>The vec-, ymax−ymin H ,</cell></row><row><cell>where (x</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Performance breakdown on TallyQA and Howmany-QA datasets using accuracy (%) and RMSE.</figDesc><table /><note>complex counting questions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc>QA is the best-known dataset for openended counting. RCN, IRLC, and Zhang et al. all use identical region proposals and CNN features. RCN obtains the highest accuracy on HowMany-QA, outperforming IRLC, which was the best-known result. Zhang et al. achieves the third-highest accuracy. Kim, Jun, and Zhang (2018) used the Zhang et al. method to answer VQA2's counting questions. Although they achieved only third best overall in the CVPR 2018 VQA2 Workshop Challenge, they won for number questions. TallyQA. Example outputs for TallyQA are shown in Fig. 4. IRLC's authors were unable to share code with us, so we could not test IRLC on TallyQA. Zhang et al. uses the same Faster R-CNN region proposals and CNN features as RCN. For Test-Simple, RCN achieves the best accuracy, with Zhang et al. performing only slightly worse. On Test-Complex, RCN also achieves the highest accuracy. The next best method is again Zhang et al. , but there is a greater gap between the two models. This may be because Zhang et al.</figDesc><table><row><cell>: Performance on TallyQA using accuracy (%) and</cell></row><row><cell>RMSE showing the advantage of using background relation-</cell></row><row><cell>ships compared to a version of RCN that omits them.</cell></row><row><cell>HowMany-QA. HowMany-QA is made by combining</cell></row><row><cell>counting questions from VQA2 and Visual Genome, so good</cell></row><row><cell>performance on it serves as a surrogate for good performance</cell></row><row><cell>on VQA2. HowMany-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 ,</head><label>4</label><figDesc>this hurts performance for both simple and complex questions showing the value of the background model. Positional Reasoning Questions. Since RCN uses objectbased relational reasoning, we expect it to outperform other methods for positional reasoning questions. To study this, we filtered out positional reasoning questions from TallyQA's Test-Complex set using common qualifiers such as left, right, top, up, bottom, near, on, in, and then we measured accuracy for Zhang et al. and RCN. We found that RCN outperformed Zhang et al. 's model by 6.38% absolute for these questions, which further demonstrates RCN's efficacy.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Robik Shrestha for useful discussions. The lab thanks NVIDIA for the donation of a GPU, and we thank Alexander Trott for assistance with HowMany-QA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Girshick 2015] Girshick, R. 2015. Fast R-CNN. In ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Kanan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR. Visual question answering: Datasets, algorithms, and future challenges. CVIU</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DVQA: Understanding data visualizations via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kafle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07932</idno>
	</analytic>
	<monogr>
		<title level="m">Bilinear attention networks</title>
		<editor>INLG. [Kim</editor>
		<meeting><address><addrLine>Kanan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiros</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradientbased localization</title>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:1708.02711</idno>
		<title level="m">Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Socher ; Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>ICLR. [Wang and Wang</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Interpretable counting for visual question answering</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hare</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prügel-Bennett ;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
