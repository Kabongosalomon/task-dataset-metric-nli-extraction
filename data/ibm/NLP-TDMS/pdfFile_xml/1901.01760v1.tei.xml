<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Estimation with Spatial Contextual Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-07">7 Jan 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>yangruigang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Estimation with Spatial Contextual Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-07">7 Jan 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the importance of spatial contextual information in human pose estimation. Most state-of-the-art pose networks are trained in a multi-stage manner and produce several auxiliary predictions for deep supervision. With this principle, we present two conceptually simple and yet computational efficient modules, namely Cascade Prediction Fusion (CPF) and Pose Graph Neural Network (PGNN), to exploit underlying contextual information. Cascade prediction fusion accumulates prediction maps from previous stages to extract informative signals. The resulting maps also function as a prior to guide prediction at following stages. To promote spatial correlation among joints, our PGNN learns a structured representation of human pose as a graph. Direct message passing between different joints is enabled and spatial relation is captured. These two modules require very limited computational complexity. Experimental results demonstrate that our method consistently outperforms previous methods on MPII and LSP benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation refers to the problem of determining precise pixel location of important keypoints of human body. It serves as a fundamental tool to solve other high level tasks, such as human action recognition <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b28">28]</ref>, tracking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">51]</ref> and human-computer interaction <ref type="bibr" target="#b41">[41]</ref>. There are already a variety of solutions where remaining challenges include large change in appearance, uncommon body postures and occlusion.</p><p>Recent successful human pose estimation methods are based on Convolutional Neural Networks (CNNs). Stateof-the-art methods <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52]</ref> train pose networks in a multi-stage fashion. These networks produce several auxiliary prediction maps. Then the predictions are refined iteratively in different stages until the final result is produced. It needs to learn semantically strong appearance features and prevent gradient vanishing during training.</p><p>Spatial contextual correlation among different joints plays an important role in human pose estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">45]</ref>. <ref type="figure">Fig. 1</ref> shows the prediction maps of different stages. Rough locations of head and left knee are easy to identify in the first stage. However, joints like {right knee, left ankle} in <ref type="figure">Fig. 1</ref> are with large deformation and occlusion, which are hard to determine only based on the local regions. Fortunately, location of joints like left knee is associated with left ankle. So the prediction result of left knee in the first stage could be indicated as a prior to help infer the location of left ankle in the following stage.</p><p>Moreover, since human pose estimation is related to structure, it is important to design appropriate guideline to choose directions of information propagation for the joints that are unclear or occluded. Probabilistic Graphical Models (PGMs) are used to facilitate message passing among joints. In <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b11">12]</ref>, MRF or CRF is utilized to describe the distribution of human body. Nevertheless, the status of each joint needs to be sequentially updated, which means before updating the status of current joints, status of the previous joints is to be refreshed. The sequential nature of the updating scheme makes it easy to accumulate error. The multilevel compositional models <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b43">43]</ref> considered the relations of joints. These methods all rely on hierarchy structures. Pose grammars are based on the prior knowledge of the human body.</p><p>To make good use of the underlying spatial contextual information, we propose two conceptually simple and computational efficient modules to estimate body joints.</p><p>Our Contribution #1 To utilize the contextual information, we propose Cascade Prediction Fusion (CPF) to make use of auxiliary prediction maps. The prediction maps at previous stage could be deemed as a prior to support predictions in following stages. This procedure is different from that of <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b50">50]</ref>, where prediction maps were concatenated <ref type="figure">Figure 1</ref>. Pipeline of multi-stage prediction. A set of auxiliary predictions are generated. In the first stage, it is easy to identify easy joints while others with severe deformation are still confusing. Relative positions between joints help resolve ambiguity in the second stage. All joints converge to the final prediction in the third stage.</p><p>with or added to image feature maps and then fed to following huge CNN trunks. As shown in <ref type="figure">Fig. 2</ref>, we create a light-weight path to gradually accumulate auxiliary prediction maps for final accurate pose estimation. The predictions at different stages are with varied properties. Specifically, predictions from lower layers are with more accurate localization signals while those at higher layers are with stronger semantic information to distinguish among similar keypoints. Our network effectively fuses information from different stages by the shorter path created by CPF.</p><p>Our Contribution #2 We introduce the Pose Graph Neural Network (PGNN), which is flexible and efficient to learn a structured representation of body joints. Our PGNN is built on a graph that can be integrated in various pose estimation networks. Each node in the graph is associated with neighboring body parts. Spatial relations are thus captured through edge construction. Direct message passing between different nodes is enabled for precise prediction.</p><p>Ours is different in modeling spatial relation. PGNN is a novel way to adaptively select the message passing directions in parallel. Instead of defining an explicit sequential order for a human body structure, it dynamically arranges the update sequences. Via simultaneous update, we manage the short-and long-term relation. Finally, PGNN learns a structured graph representation to boost performance.</p><p>Our system is also end-to-end trainable, which not only estimates body location but also configures the spatial structures. We evaluate the system on two representative human pose benchmark datasets, i.e., MPII and LSP. It accomplishes new state-of-the-arts with high computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Pose Estimation The key of human pose estimation lies in joint detection and spatial relation configuration. Previous human pose estimation methods can be divided into two groups. The first is to learn feature representation using powerful CNN. These methods detect body joint location directly or predict the score maps for body joints. Early methods like DeepPose <ref type="bibr" target="#b47">[47]</ref> regressed joint locations with multiple stages. Later, Fan et al. <ref type="bibr" target="#b16">[17]</ref> combined local and global features to improve performance. To connect the input and output space, Carreira et al. <ref type="bibr" target="#b4">[5]</ref> iteratively concatenated the input image with previous prediction in each step. Following the paradigm of semantic segmentation <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b5">6]</ref>, methods of <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52]</ref> used Gaussian peaks to represent part locations. Then a fully convolutional neural network <ref type="bibr" target="#b33">[33]</ref> is applied to estimate body joint location. These methods can produce high quality representation and do not predict structure among body joints, however.</p><p>The other group focuses on modeling spatial relationship between body joints. The pictorial structures <ref type="bibr" target="#b36">[36]</ref> modeled spatial deformation by designing pairwise terms between different joints. To deal with human poses with large variation, a mixture model is learned for each joint. Yang et al. <ref type="bibr" target="#b53">[53]</ref> used a part mixture model to infer spatial relation with a tree structure. This structure may not capture very complicated relation. Subsequent methods introduced other models, such as loopy structure <ref type="bibr" target="#b49">[49]</ref> and poselet <ref type="bibr" target="#b36">[36]</ref> to further improve the performance.</p><p>Later methods <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref> modeled structures via CNN. Tompson et al. <ref type="bibr" target="#b46">[46]</ref> utilized the Markov Random Field (MRF) to model distribution of body parts. Convolutional priors were used in defining the pairwise terms of joints. The method of <ref type="bibr" target="#b10">[11]</ref> utilized geometrical transform kernels to capture relation of joints on feature maps.</p><p>Graph Neural Network Previous work on feature learning for graph-structure can be divided into two categories. One direction is to apply CNN to graphs. Based on graph Laplacian, methods of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">25]</ref> applied CNN to spectral domain. In order to operate CNN directly on graph, the method of <ref type="bibr" target="#b15">[16]</ref>   <ref type="figure">Figure 2</ref>. Framework. Our system takes an image as input, and generates the prediction maps. The architecture is with two components where CPF is for computing the prediction maps and the other PGNN is for refining these maps until final prediction.</p><p>line focuses on recurrently applying neural networks to each node of the graph. State of each node can be updated based on history and new information passing through edges. The Graph Neural Network (GNN) was first proposed in <ref type="bibr" target="#b40">[40]</ref>. It utilized multi-layer perceptrons (MLP) to learn hidden state of nodes in graphs. However, the contraction map assumption is a restriction. As an extension, the Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b27">[27]</ref> adopted recurrent gating function <ref type="bibr" target="#b7">[8]</ref> to update the hidden state and output sequences. Parameters of the final model can be effectively optimized by the back-propagation through time (BPTT) algorithm. Very recently, GGNNs was used in image classification <ref type="bibr" target="#b34">[34]</ref>, situation recognition <ref type="bibr" target="#b26">[26]</ref> and RGBD semantic segmentation <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we describe the two major components in our method. One is a cascaded multi-stage prediction module where previous-stage prediction serves as a prior to guide present prediction and accumulate auxiliary prediction as shown in <ref type="figure">Fig. 2(a)</ref>. The other is to model different parts in a graph, augmented by Pose Graph Neural Network (PGNN) to learn representation, as shown in <ref type="figure">Fig. 2</ref></p><formula xml:id="formula_0">(b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cascade Prediction Fusion (CPF)</head><p>For common pose estimation methods <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b50">50]</ref>, a set of prediction maps are iteratively refined for body parts. We propose CPF to take the underlying contextual information encoded in auxiliary prediction into consideration.</p><p>These prediction maps are in different semantic levels while all of them can be utilized for final predictions. As detailed in <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b12">13]</ref>, the lower-layer features focus on local appearance and describe details. It is crucial for accurate joints localization. Meanwhile, the global representations from higher layers help discriminate among different body joints. Our CPF is designed to gradually integrate different semantic information from lower to higher layers. <ref type="figure">Fig. 2</ref>(a) shows the way to incorporate CPF into Hourglass <ref type="bibr" target="#b35">[35]</ref> framework. It can be built on top of most multistage pose estimation frameworks and iterates from the first stage to the final predictions.</p><p>For stage i, instead of simply fusing the prediction map pred i−1 from last stage with pred i from current stage directly, we provide pred i−1 as a prior, which is used for producing pred i . Particularly, the coarse prediction map pred i−1 undergoes a 1 × 1 convolution to increase channels and is then merged with image features from stage i by using element-wise addition. pred i is generated by taking the fused feature map as input.</p><p>CPF is different from DenseNet <ref type="bibr" target="#b22">[22]</ref> and DLA <ref type="bibr" target="#b54">[54]</ref>. DenseNet emphasizes more on feature reuse and gradient vanish issues. DLA unifies semantic and spatial fusion in the feature level. In contrast, CPF focuses on exploring and aggregating the contextual information encoded in prediction maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph Neural Network (GNN)</head><p>Graph neural network (GNN) is a general model handling graph structured data. GNN takes the graph G = {K, E} as input where K and E represent the nodes and edges of the graph respectively. Each node k ∈ K is associated with a hidden state vector h k , which is recurrently updated. The hidden state vector at time step t is denoted as h t k . The hidden state is updated by taking as input its current state vector and the incoming messages x t k from its neighboring nodes N k . A is a function to collect messages from neighboring nodes. T is a function to update the hid-den state. Formally, the hidden state is updated as</p><formula xml:id="formula_1">x t k = A(h t−1 u |u ∈ N k ), h t k = T (h t−1 k , x t k ).</formula><p>(1)</p><p>In the following, we present our new GNN named PGNN for pose estimation.</p><p>Graph Construction Each node k in PGNN represents one body joint and each edge is defined as the connection between neighboring joints. <ref type="figure">Fig. 2(c)</ref> shows an example of how to construct a tree-like graph for human poses. The prediction maps are treated as unary maps learned from a backbone network, which will be detailed in Sec. 4. The hidden state of each node is initialized with its corresponding spatial prediction feature map derived from the original image. The status of node k is initialized as</p><formula xml:id="formula_2">h 0 k = F k (Θ, I), k ∈ {1 · · · K},<label>(2)</label></formula><p>where F indicates the backbone network, Θ is a set of parameters for the network, and I is the original input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Propagation</head><p>We use the constructed graph to exploit the semantic spatial relation and refine the appearance representation for each joint in steps. Before updating the hidden state of each node, it first aggregates messages of the hidden state at time step t − 1 from neighboring node k ′ . As demonstrated in <ref type="bibr" target="#b10">[11]</ref>, convolutional layers can be used as geometrical transform kernels. It advances message passing between feature maps. It is noted that the weights of convolution for different edges are not shared. So A is expressed as</p><formula xml:id="formula_3">x t k = k,k ′ ∈Ω W p,k h t−1 k ′ + b p,k ,<label>(3)</label></formula><p>where W p,k is the convolution weights and b p,k is the bias of the k th node. Ω is a set of connected edges. Eq. (4) gives the formulation of T . It updates the k th node with the aggregated messages and the t − 1 step of hidden state. We follow the same gating mechanism with GRU <ref type="bibr" target="#b27">[27]</ref> and enjoy more computational efficiency and less memory consumption. Again, we utilize convolution operations and do not share weights.</p><formula xml:id="formula_4">W z,k , U z,k , b z,k , W r,k , U r,k , b r,k , W h,k , U h,k and b h,</formula><p>k are the weights and biases for the k th node in the update function. With this method, the aggregated information is softly combined with its own memory, which can be expressed as</p><formula xml:id="formula_5">z t k = σ(W z,k x t k + U z,k h t−1 k + b z,k ), r t k = σ(W r,k x t k + U r,k h t−1 k + b r,k ), h t k = tanh(W h,k x t k + U h,k (r t k ⊙ h t−1 k ) + b h,k ), h t k = (1 − z t k ) ⊙ h t−1 k + z t k ⊙h t k .<label>(4)</label></formula><p>Output and Learning After T -time propagation, we get the final prediction</p><formula xml:id="formula_6">P k = h T k + h 0 k ,<label>(5)</label></formula><p>where h T k is the final hidden state collected from the corresponding node. h 0 k is the initialization hidden state, which encodes the appearance information of a joint. We get the final prediction by adding these two prediction maps. The graph network is trained by minimizing the ℓ 2 loss of</p><formula xml:id="formula_7">L 2 = 1 K K k=1 x,y || P k (x, y) − P k (x, y)|| 2 ,<label>(6)</label></formula><p>where (x, y) is the pixel location, P k (x, y) is the ground truth label at pixel (x, y). P k is the prediction map obtained in Eq. <ref type="bibr" target="#b4">(5)</ref>. The model is trained with back-propagation through time (BPTT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Graph Types</head><p>PGNN can handle a variety of graphs. It develops a novel message passing scheme so that each body receives message from specific neighboring joints. Intuitively, a fully connected graph is expected to be the ideal choice to collect information from all other joints. However, for some joints, such as head and ankle, it is hard to capture the relationship.</p><p>To address this problem, we utilize two types of structure, i.e., tree and loopy structure. It is not known beforehand which one is better. A tree is a simple structure, which captures the relation of neighboring joints. Loopy structure is more complex, allowing message passing in a loop. The structure we use in this paper is illustrated in <ref type="figure">Fig. 2(c)</ref> and (d). Although many tree-like or loopy graphs can be derived, PGNN tackles them in the same way. The graphs are undirected and enable bidirectional message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relationship to Other Methods</head><p>Most current state-of-the-art methods focus more on appearance learning of body parts. They capture spatial relation by enlarging the receptive fields. However, poses are with large variation, making the structure information in prediction feature maps still have the potential to boost the performance. Other models like Recurrent Neural Network (RNN) and Probabilistic Graphical Model (PGM) can also model the relation. We will detail the difference between PGNN and these models in the following.</p><p>PGNN vs. RNN RNN can be deemed as a special case of PGNN. It is also able to pass information across nodes of a graph, where each body part is denoted as a node and the joints relations are propagated through edges. However, the graph structure requires to be chains for RNN. For its construction, at each time step, the state of current node in RNN is updated by its current state and the hidden state of the parent node. It is different from our PGNN, which collects information from a set of neighboring nodes. Moreover, the order of RNN input is manually defined. A slightly inappropriate setting may destroy the naturally structured relationship of joints.</p><p>Tree-structured RNN <ref type="bibr" target="#b42">[42]</ref> can handle tree-structured data, which propagates information through a tree sequentially. In addition, before updating the state of subsequent layers L t , it must update the ancestors L t−1 at first. Contrarily, PGNN updates all states of the node simultaneously. In addition, RNN shares weights at different time steps. The transfer matrix between nodes in the graph is shared through T -time update. Note that in our model, each edge of the graph has different transformation weights.</p><p>PGNN vs. PGM PGNN is also closely related to probabilistic graphical model, which is widely used for pose estimation <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b10">11]</ref> to integrate joint associations. In fact, our model can be viewed as generalization of these models by designing specific update. As detailed in <ref type="bibr" target="#b46">[46]</ref>, for a body part r, the final marginal likelihood Q r is defined as</p><formula xml:id="formula_8">Q r = 1 Z v∈V (q r|v * q v + b v→r ),<label>(7)</label></formula><p>where V is a set of neighboring nodes of r. q v is the joint probability, q r|v is the conditional prior and b v→r is the bias, respectively. Z is the partition function. When the aggregation function is formulated as the product and update function is represented by Eq. <ref type="formula" target="#formula_8">(7)</ref>, PGNN is degraded to the MRF model. With these derivations, it becomes clear that PGNN is a more general model to integrate joint associations by designing specific graph structure and making its own way to update and aggregate functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backbone Networks</head><p>To verify the generality of our method, we use two backbone networks. One is our modified ResNet-50 <ref type="bibr" target="#b20">[20]</ref> and the other is the widely used 8-stack Hourglass <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">ResNet-50</head><p>ResNet has demonstrated its power on many high-level tasks, including object detection <ref type="bibr" target="#b20">[20]</ref> and instance segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>. To show the generalization ability, we first modify the ResNet-50 network with a few novel steps for human pose estimation. It achieves decent results, even comparable with using other much deeper networks.</p><p>Our strategy is to first convert the vanilla ResNet-50 into a fully convolutional network by removing the final classification and average pooling. We integrate CPF in ResNet-50 and further improve the results using PGNN. The following two techniques are also used to adapt ResNet-50.</p><p>Feature Pyramid Network (FPN) As introduced in Sec. 3.1, multi-stage prediction is very important for training a pose network. To this end, we adopt Feature Pyramid Network (FPN) <ref type="bibr" target="#b30">[30]</ref> in the vanilla ResNet-50. FPN leverages the pyramid shape in networks for prediction at different feature levels. Similar to FPN, we also use a lateral connection (1 × 1 conv) to merge the information from both bottom-up and top-down pathways. Finally, we produce three auxiliary predictions at three different levels.</p><p>Dilated Convolution Dilated Convolution <ref type="bibr" target="#b5">[6]</ref> is used to enlarge the receptive field without introducing extra parameters. An input image is down-sampled 32 times after fed into the vanilla ResNet-50. However, the feature map is too coarse to precisely localize the joints. To address this problem, we first decrease the stride of convolution layers of the last two blocks from 2px to 1px. This results in shrinking the receptive field. Since for human pose estimation as demonstrated in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b50">50]</ref>, the spatial information needs to be captured by a large enough area, we replace the 3 × 3 convolution layers of the last two blocks with the dilated convolution. Finally, we reduce the stride to 8px.</p><p>Other Implementation Details All models are implemented by Torch <ref type="bibr" target="#b13">[14]</ref>. We use ImageNet pre-trained model as the base and adopt RMSProp <ref type="bibr" target="#b44">[44]</ref> to optimize parameters. The network is trained in a total of 250 epochs with batch size 8. The initial learning rate is 0.001. It decreased by 10 times at the 200 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Hourglass</head><p>The 8-stack Hourglass (Hg) is adopted as the other backbone network to verify our method. It is much deeper than ResNet-50 and is widely adopted by many pose estimation frameworks. With CPF and PGNN integrated in Hourglass, we achieve new state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The network is implemented using Torch and optimized with RMSProp. The parameters are randomly initialized. We train the network in 300 epochs with batch size 6. The learning rate starts at 0.00025 and decreases by 10 times at the 240 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets We evaluate our CPF and PGNN on two representative benchmark datasets, i.e., MPII human pose dataset (MPII) <ref type="bibr" target="#b0">[1]</ref> and extended Leeds Sports Poses (LSP) <ref type="bibr" target="#b24">[24]</ref>. MPII contains about <ref type="bibr" target="#b25">25</ref>  <ref type="figure">Figure 3</ref>. Prediction results of PCKh @0.5 on MPII validation set. We compare the results by adding CPF and further integrating PGNN. The backbone is ResNet-50 and PGNN passes message with a tree-like structure. over 40,000 annotated poses. We use the same setting as that in <ref type="bibr" target="#b45">[45]</ref> to split training and validation sets. The dataset is very challenging since it covers daily human activities with large pose variety. The LSP dataset consists of 11,000 training images and 1,000 testing ones from sport activities.</p><p>Data Augmentation During training, the input image is cropped and warped to size 256 × 256 according to the annotated body position and scale. We augment the dataset by scaling it with a factor ([0.75, 1.25]), rotation (±30), horizontal flipping and illumination adjustment to enhance data diversity, and further improve the robustness of the model for various cases. During testing, we crop the image with the given rough center location and scale of the person. For LSP dataset, we simply use the size and center of the image as rough scale and center.</p><p>Unary Maps For the two backbone networks, i.e., ResNet-50 and Hourglass, we take the last prediction score maps as the unary maps. The reason is that the prediction at the final stage is made based on feature with strong semantics, which gathers previous prediction through CPF. It is generally with decent prediction accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>To investigate the effectiveness of our proposed CPF and PGNN modules, we conduct ablative analysis on the validation set of MPII Human Pose dataset. We set the modified ResNet-50 as our baseline network. To show the efficacy of our models, all results are tested without flipping or multiscale testing.</p><p>CPF To evaluate the effectiveness of CPF, we compare results with and without CPF on modified ResNet-50. <ref type="figure">Fig. 3</ref> shows results on MPII validation set. "ResNet" refers to our modified ResNet-50. For ResNet-50 with CPF, some difficult joints like knee is 1.2% higher and result of elbow is improved by 0.6%.</p><p>In order to clearly demonstrate accuracy change in each stage. <ref type="figure">Fig. 5(b)</ref> shows the accuracy produced at different stages. It is observed that the accuracy increases gradually in steps on the ResNet. This manifests that our CPF effectively gathers information from previous predictions.</p><p>PGNN Other than adding CPF, we integrate PGNN to further enhance the accuracy. <ref type="figure">Fig. 3</ref> gives the experimental results where "PGNN" stands for the graph updated twice based on a tree-like structure. The accuracy of parts such as difficult joints of elbow, wrist is further improved. The reason is that the contextual information propagated from confident parts through graph helps reduce error.</p><p>We use two types of graphs in our experiments. They are tree-and loopy-like graphs in PGNN. <ref type="figure" target="#fig_0">Fig. 4</ref> presents the results using different PGNN structures. They are comparable -connecting the parts including {elbow, ankle} with other easy parts consistently improves performance. In our experiments, a naive loopy structure, shown in <ref type="figure">Fig. 2(d)</ref>, is used. We simply add extra connections, i.e. shoulderwrist, ankle-hip, and shoulder-hip. It is notable that performance of these two types of graphs with the same number of steps are consistent. We thus believe allowing information to propagate between neighboring joints is of great importance. More sophisticated structures may further improve the performance, which will be our future work.</p><p>We also conduct experiments to compare results when propagating different times (i.e. with varying propagation number T ) in the system. The results are shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. The performance increases by a small amount when increasing T , and saturates quickly at T = 3. We also notice that propagation is important in the first 2 steps. For the tree-like graph, as revealed in the comparison when applying T = 0, T = 1 and T = 2, we obtain the improvement of around 0.5% and 0.3%, respectively. Similar results are observed when using the loopy-like graph. However, the performance begins to drop at T = 3. Since it is hard to capture the semantic information between too far away joints, but instead confuses prediction at current joint. Complexity In <ref type="figure">Fig. 5(a)</ref>, we compare the number of parameters and computational complexity between Hourglass, previous method PRM <ref type="bibr" target="#b52">[52]</ref> and our model. We note that the PRM adds 13.5% extra parameters compared with Hourglass, while our model only increases parameters by 0.8%. Additionally, we introduce very limited computation overhead (measured by GFLOPs) on Hourglass, contrary to much increased computational cost from PRM. Our results are with higher quality and decent computational efficiency.</p><p>Visual Analysis In <ref type="figure">Fig. 6</ref>, we visualize results of baseline and our model. The baseline model has difficulty in distinguishing among symmetric parts and uncommon body postures. For example, in {col.1, row.2} of <ref type="figure">Fig. 6</ref>, ankle with large deformation is hard to identify with inherent ambiguity. Our proposed CPF and PGNN provide an effective way to utilize contextual information to reduce the confusion. As a result, the associated joints knee help inferring the precise location of ankle in our model as shown in {col.2 and row.2} of <ref type="figure">Fig 6.</ref> More results on MPII and LSP generated by our method are shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results on LSP</head><p>Tab. 2 gives comparison with person-centric annotation. The results are evaluated with PCK scores at threshold 0.2. Following previous methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">52]</ref>, we add the MPII training set to the extended LSP training set. Our modified ResNet-50 outperforms most of the methods trained with deeper networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>We have presented effective Cascade Prediction Fusion (CPF) and Pose Graph Neural Network (PGNN) to explore contextual information for human pose estimation. CPF makes use of rich contextual information encoded in the auxiliary score maps to produce enhanced prediction. PGNN, differently, is adopted to provide an explicit information propagation scheme to refine prediction. These two components are independent while beneficial to each other in human pose estimation. They are also general for most existing pose estimation networks to boost performance. Our future work will be to extend our framework to 3D and video data for deeper understanding of the temporal and spatial relationship.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Results at different timesteps with tree-like and loopylike structure of PCKh @0.5. The backbone is ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>(a) Statistics of parameter numbers, GFLOPs and accuracy on three frameworks, i.e., baseline Hourglass, PRM and our method respectively. (b) Prediction accuracy at different stages with modified ResNet-50. Results on MPII test set produced by different backbone networks, i.e. Hourglass and ResNet-50. Hg-ours and ResNet-50ours are both trained with CPF and PGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>used a special hash function. The other</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CPF Prediction</cell></row><row><cell></cell><cell>. . . . . . . . . . . .</cell><cell>. . . . . .</cell><cell>. . . . . .</cell><cell>. . . . . .</cell></row><row><cell></cell><cell>pred 1</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>pred 2</cell><cell></cell><cell>pred 7</cell></row><row><cell>Elem-wise Sum</cell><cell cols="2">(a) Cascade Prediction Fusion (CPF)</cell><cell></cell></row><row><cell>Conv + BN + ReLU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>GRU unit</cell></row><row><cell></cell><cell></cell><cell></cell><cell>z</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell></cell><cell></cell><cell></cell><cell>r</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>(c) Tree</cell><cell>(d) Loopy</cell><cell></cell><cell></cell></row><row><cell cols="2">Predefined Pose Graph</cell><cell cols="2">(b) Pose Graph Neural Network (PGNN)</cell><cell>Final Prediction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The score maps are of size H × W × C where H is the height, W is the width, and C is the channel size. In our experiments, the size of C depends on different datasets. W and H are all with size 64, which is 1/8 of the original input image.</figDesc><table><row><cell>T = 3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.45 88.43</cell><cell></cell><cell></cell></row><row><cell>T = 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.56 88.61</cell><cell></cell></row><row><cell>T = 1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.29 88.38</cell><cell></cell><cell></cell></row><row><cell>T = 0</cell><cell></cell><cell>. 87 82 87 82 .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>87.4</cell><cell>87.6</cell><cell>87.8</cell><cell>88</cell><cell>88.2</cell><cell>88.4</cell><cell>88.6</cell><cell>88.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Loopy</cell><cell>Tree</cell><cell></cell><cell></cell><cell></cell></row></table><note>Evaluation Criteria We use the Percentage Correct Key- points (PCK) to evaluate results on the LSP dataset. For MPII, we use PCKh [1], a modified version of PCK. It nor- malizes the distance errors with respect to the size of head.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Results of PCKh @0.5 on the MPII test set. Note that ResNet is our modified ResNet-50. ResNet-50 and Hg are all trained with CPF and PGNN.4.2. Experimental Results on MPIIAccuracy Tab. 1 lists our results on MPII test set. "Hgours" is trained on MPII combined with LSP. The results are produced with five-scale input with horizontal flip testing. Our method trained based on Hourglass yields result 92.5% PCKh at threshold 0.5, which is the highest on this dataset at the time of paper submission. For the challenging parts such as knee and ankle, we obtain improvement of 2.4% and 3.0% compared to the baseline Hourglass, respectively. Particularly, our method outperforms the method<ref type="bibr" target="#b12">[13]</ref> with CRF as well. It is noteworthy that accuracy of our method (ResNet-ours) is also higher than the baseline ResNet-50, which proves the generalization ability.Methods Head Sho. Elb. Wri. Hip Knee Ank. Mean Belagiannis&amp;Zisserman [2] 95.2 89.0 81.5 77.0 83.7 87.0 82.8 85.2 Lifshitz et al. [29] 96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7 Pishchulin et al. [37] 97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 Insafutdinov et al. [23] 97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1 Wei et al. [50] 97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5 Bulat&amp;Tzimiropoulos [4] 97.2 92.1 88.1 85.2 92.2 91.4 88.7 90.7 Yang et al.Table 2. Comparison of PCK @0.2 on the LSP dataset. ResNet is short for ResNet-50. Both backbones are trained with CPF and PGNN.</figDesc><table><row><cell></cell><cell>[52]</cell><cell>98.3 94.5 92.2 88.9 94.4 95.0 93.7 93.9</cell></row><row><cell></cell><cell>ResNet-ours</cell><cell>98.5 94.0 89.9 86.9 92.3 93.5 92.7 92.5</cell></row><row><cell></cell><cell>Hg-ours</cell><cell>98.4 94.8 92.0 89.4 94.4 94.8 93.8 94.0</cell></row><row><cell>Methods</cell><cell>Head Sho. Elb. Wri. Hip Knee Ank. Mean</cell></row><row><cell cols="2">Pishchulin et al. [36] 74.3 49.0 40.8 34.1 36.5 34.4 35.2 44.1</cell></row><row><cell>Tompson et al. [46]</cell><cell>95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6</cell></row><row><cell>Carreira et al. [5]</cell><cell>95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3</cell></row><row><cell>Tompson et al. [45]</cell><cell>96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0</cell></row><row><cell cols="2">Hu&amp;Ramanan et al. [21] 95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4</cell></row><row><cell cols="2">Pishchulin et al. [37] 94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4</cell></row><row><cell>Lifshitz et al. [29]</cell><cell>97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0</cell></row><row><cell>Gkioxary et al. [18]</cell><cell>96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1</cell></row><row><cell>Rafi et al. [39]</cell><cell>97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3</cell></row><row><cell cols="2">Insafutdinov et al. [23] 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5</cell></row><row><cell>Wei et al. [50]</cell><cell>97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5</cell></row><row><cell>Chu et al. [13]</cell><cell>98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5</cell></row><row><cell>Chou et al. [10]</cell><cell>98.2 96.8 92.2 88.0 91.3 89.1 84.9 91.8</cell></row><row><cell>Chen et al. [7]</cell><cell>98.1 96.5 92.5 88.5 90.2 89.6 86.0 91.9</cell></row><row><cell>Yang et al. [52]</cell><cell>98.5 96.7 92.5 88.7 91.1 88.6 86.0 92.0</cell></row><row><cell>Newell et al. [35]</cell><cell>98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9</cell></row><row><cell>ResNet-ours</cell><cell>98.2 96.4 91.6 87.1 91.2 88.0 83.6 91.2</cell></row><row><cell>Hg-ours</cell><cell>98.6 97.0 92.8 88.8 91.7 89.8 86.6 92.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00389</idno>
		<title level="m">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive occlusion state estimation for human pose tracking under selfocclusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="661" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crf-cnn: Modeling structured information in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Example output on the LSP and MPII test data</title>
		<imprint/>
	</monogr>
	<note>Figure 7</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An expressive deep model for human action parsing from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning hierarchical poselets for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06484</idno>
		<title level="m">Deep layer aggregation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recursive compositional models for vision: Description and review of recent work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
