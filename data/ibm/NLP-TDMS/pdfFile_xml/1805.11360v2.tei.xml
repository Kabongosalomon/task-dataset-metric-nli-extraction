<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Naver Search</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inho</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Naver Search</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence matching is widely used in various natural language tasks such as natural language inference, paraphrase identification, and question answering. For these tasks, understanding logical and semantic relationship between two sentences is required but it is yet challenging. Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences, previous methods of attention mechanism simply use a summation operation which does not retain original features enough. Inspired by DenseNet, a densely connected convolutional network, we propose a densely-connected co-attentive recurrent neural network, each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers. It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer. To alleviate the problem of an ever-increasing size of feature vectors due to dense concatenation operations, we also propose to use an autoencoder after dense concatenation. We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching. Experimental results show that our architecture, which retains recurrent and attentive features, achieves state-of-the-art performances for most of the tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic sentence matching, a fundamental technology in natural language processing, requires lexical and compositional semantics. In paraphrase identification, sentence matching is utilized to identify whether two sentences have identical meaning or not. In natural language inference also known as recognizing textual entailment, it determines whether a hypothesis sentence can reasonably be inferred from a given premise sentence. In question answering, sentence matching is required to determine the degree of matching 1) between a query and a question for question retrieval, and 2) between a question and an answer for answer selection. However identifying logical and semantic relationship between two sentences is not trivial due to the problem of the semantic gap <ref type="bibr" target="#b6">(Liu et al. 2016)</ref>.</p><p>Recent advances of deep neural network enable to learn textual semantics for sentence matching. Large amount of annotated data such as Quora (Csernai 2017), SNLI (Bowman Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr">et al. 2015)</ref>, and MultiNLI (Williams, Nangia, and Bowman 2017) have contributed significantly to learning semantics as well. In the conventional methods, a matching model can be trained in two different ways <ref type="bibr" target="#b2">(Gong, Luo, and Zhang 2018)</ref>. The first methods are sentence-encoding-based ones where each sentence is encoded to a fixed-sized vector in a complete isolated manner and the two vectors for the corresponding sentences are used in predicting the degree of matching. The others are joint methods that allow to utilize interactive features like attentive information between the sentences.</p><p>In the former paradigm, because two sentences have no interaction, they can not utilize interactive information during the encoding procedure. In our work, we adopted a joint method which enables capturing interactive information for performance improvements. Furthermore, we employ a substantially deeper recurrent network for sentence matching like deep neural machine translator (NMT) <ref type="bibr">(Wu et al. 2016)</ref>. Deep recurrent models are more advantageous for learning long sequences and outperform the shallower architectures. However, the attention mechanism is unstable in deeper models with the well-known vanishing gradient problem. Though <ref type="bibr">GNMT (Wu et al. 2016</ref>) uses residual connection between recurrent layers to allow better information and gradient flow, there are some limitations. The recurrent hidden or attentive features are not preserved intact through residual connection because the summation operation may impede the information flow in deep networks.</p><p>Inspired by Densenet , we propose a densely-connected recurrent network where the recurrent hidden features are retained to the uppermost layer. In addition, instead of the conventional summation operation, the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better. The proposed architecture shown in <ref type="figure" target="#fig_0">Figure 1</ref> is called DRCN which is an abbreviation for Densely-connected Recurrent and Co-attentive neural Network. The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information. Furthermore, to alleviate the problem of an ever-increasing feature vector size due to concatenation operations, we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure. DRCN is, to our best knowledge, the first generalized version of DenseRNN which is expandable to deeper layers with the property of Dashed arrows indicate that a group of RNN-layer, concatenation and AE can be repeated multiple (N ) times (like a repeat mark in a music score). The bottleneck component denoted as AE, inserted to prevent the ever-growing size of a feature vector, is optional for each repetition. The upper right diagram is our specific architecture for experiments with 5 RNN layers (N = 4).</p><p>controllable feature sizes by the use of an autoencoder. We evaluate our model on three sentence matching tasks: natural language inference, paraphrase identification and answer sentence selection. Experimental results on five highly competitive benchmark datasets (SNLI, MultiNLI, QUORA, TrecQA and SelQA) show that our model significantly outperforms the current state-of-the-art results on most of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Earlier approaches of sentence matching mainly relied on conventional methods such as syntactic features, transformations or relation extraction <ref type="bibr" target="#b11">(Romano et al. 2006;</ref><ref type="bibr" target="#b16">Wang, Smith, and Mitamura 2007)</ref>. These are restrictive in that they work only on very specific tasks.</p><p>The developments of large-scale annotated datasets <ref type="bibr" target="#b0">(Bowman et al. 2015;</ref><ref type="bibr">Williams, Nangia, and Bowman 2017)</ref> and deep learning algorithms have led a big progress on matching natural language sentences. Furthermore, the wellestablished attention mechanisms endowed richer information for sentence matching by providing alignment and dependency relationship between two sentences. The release of the large-scale datasets also has encouraged the developments of the learning-centered approaches to semantic representation. The first type of these approaches is sentence-encoding-based methods <ref type="bibr" target="#b2">(Conneau et al. 2017;</ref><ref type="bibr" target="#b2">Choi, Yoo, and goo Lee 2017;</ref><ref type="bibr" target="#b8">Nie and Bansal 2017;</ref><ref type="bibr" target="#b12">Shen et al. 2018)</ref> where sentences are encoded into their own sentence representation without any cross-interaction. Then, a classifier such as a neural network is applied to decide the relationship based on these independent sentence representations. These sentence-encoding-based methods are simple to extract sentence representation and are able to be used for transfer learning to other natural language tasks <ref type="bibr" target="#b2">(Conneau et al. 2017</ref>). On the other hand, the joint methods, which make up for the lack of interaction in the former methods, use cross-features as an attention mechanism to express the word-or phrase-level alignments for performance improvements <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017;</ref><ref type="bibr" target="#b1">Chen et al. 2017b;</ref><ref type="bibr" target="#b2">Gong, Luo, and Zhang 2018;</ref><ref type="bibr">Yang et al. 2016)</ref>.</p><p>Recently, the architectural developments using deeper layers have led more progress in performance. The residual connection is widely and commonly used to increase the depth of a network stably <ref type="bibr">Wu et al. 2016</ref>). More recently, Huang et al. ) enable the features to be connected from lower to upper layers using the concatenation operation without any loss of information on lower-layer features.</p><p>External resources are also used for sentence matching. Chen et al. <ref type="bibr">(Chen et al. 2017a;</ref><ref type="bibr" target="#b1">Chen et al. 2017b</ref>) used syntactic parse trees or lexical databases like WordNet to measure the semantic relationship among the words and Pavlick et al. <ref type="bibr" target="#b8">(Pavlick et al. 2015)</ref> added interpretable semantics to the paraphrase database.</p><p>Unlike these, in this paper, we do not use any such external resources. Our work belongs to the joint approaches which uses densely-connected recurrent and co-attentive information to enhance representation power for semantic sentence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we describe our sentence matching architecture DRCN which is composed of the following three components: (1) word representation layer, (2) attentively connected RNN and (3) interaction and prediction layer. We denote two input sentences as P = {p 1 , p 2 , · · · , p I } and Q = {q 1 , q 2 , · · · , q J } where p i /q j is the i th /j th word of the sentence P /Q and I/J is the word length of P /Q. The overall architecture of the proposed DRCN is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation Layer</head><p>To construct the word representation layer, we concatenate word embedding, character representation and the exact matched flag which was used in <ref type="bibr" target="#b2">(Gong, Luo, and Zhang 2018)</ref>.</p><p>In word embedding, each word is represented as a ddimensional vector by using a pre-trained word embedding method such as GloVe <ref type="bibr" target="#b9">(Pennington, Socher, and Manning 2014)</ref> or Word2vec <ref type="bibr" target="#b7">(Mikolov et al. 2013</ref>). In our model, a word embedding vector can be updated or fixed during training. The strategy whether to make the pre-trained word embedding be trainable or not is heavily task-dependent. Trainable word embeddings capture the characteristics of the training data well but can result in overfitting. On the other hand, fixed (non-trainable) word embeddings lack flexibility on task-specific data, while it can be robust for overfitting, especially for less frequent words. We use both the trainable embedding e tr pi and the fixed (non-trainable) embedding e f ix pi to let them play complementary roles in enhancing the performance of our model. This technique of mixing trainable and non-trainable word embeddings is simple but yet effective. The character representation c pi is calculated by feeding randomly initialized character embeddings into a convolutional neural network with the max-pooling operation. The character embeddings and convolutional weights are jointly learned during training.</p><p>Like (Gong, Luo, and Zhang 2018), the exact match flag f pi is activated if the same word is found in the other sentence.</p><p>Our final word representational feature p w i for the word p i is composed of four components as follows:</p><formula xml:id="formula_0">e tr pi = E tr (p i ), e f ix pi = E f ix (p i ) c pi = Char-Conv(p i ) p w i = [e tr pi ;e f ix pi ; c pi ; f pi ].</formula><p>(1)</p><p>Here, E tr and E f ix are the trainable and non-trainable (fixed) word embeddings respectively. Char-Conv is the characterlevel convolutional operation and [· ; ·] is the concatenation operator. For each word in both sentences, the same above procedure is used to extract word features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Densely connected Recurrent Networks</head><p>The ordinal stacked RNNs (Recurrent Neural Networks) are composed of multiple RNN layers on top of each other, with the output sequence of previous layer forming the input sequence for the next. More concretely, let H l be the l th RNN layer in a stacked RNN. Note that in our implementation, we employ the bidirectional LSTM (BiLSTM) as a base block of H l . At the time step t, an ordinal stacked RNN is expressed as follows:</p><formula xml:id="formula_1">h l t =H l (x l t , h l t−1 ) x l t = h l−1 t .<label>(2)</label></formula><p>While this architecture enables us to build up higher level representation, deeper networks have difficulties in training due to the exploding or vanishing gradient problem.</p><p>To encourage gradient to flow in the backward pass, residual connection ) is introduced which bypasses the non-linear transformations with an identity mapping. Incorporating this into (2), it becomes</p><formula xml:id="formula_2">h l t = H l (x l t , h l t−1 ) x l t = h l−1 t + x l−1 t .</formula><p>(3)</p><p>However, the summation operation in the residual connection may impede the information flow in the network . Motivated by Densenet , we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The densely connected recurrent neural networks can be described as h l</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottleneck component</head><p>Our network uses all layers' outputs as a community of semantic knowledge. However, this network is a structure with increasing input features as layers get deeper, and has a large number of parameters especially in the fully-connected layer. To address this issue, we employ an autoencoder as a bottleneck component. Autoencoder is a compression technique that reduces the number of features while retaining the original information, which can be used as a distilled semantic knowledge in our model. Furthermore, this component increased the test performance by working as a regularizer in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction and Prediction Layer</head><p>To extract a proper representation for each sentence, we apply the step-wise max-pooling operation over densely connected recurrent and co-attentive features (pooling in <ref type="figure" target="#fig_0">Fig. 1</ref>). More specifically, if the output of the final RNN layer is a 100d vector for a sentence with 30 words, a 30 × 100 matrix is obtained which is max-pooled column-wise such that the size of the resultant vector p or q is 100. Then, we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows:</p><formula xml:id="formula_3">v = [p; q; p + q; p − q; |p − q|].<label>(7)</label></formula><p>Here, the operations +, − and | · | are performed elementwise to infer the relationship between two sentences. The element-wise subtraction p − q is an asymmetric operator for one-way type tasks such as natural language inference or answer sentence selection.</p><p>Finally, based on previously aggregated features v, we use two fully-connected layers with ReLU activation followed by one fully-connected output layer. Then, the softmax function is applied to obtain a probability distribution of each class. The model is trained end-to-end by minimizing the multi-class cross entropy loss and the reconstruction loss of autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our matching model on five popular and wellstudied benchmark datasets for three challenging sentence matching tasks: (i) SNLI and MultiNLI for natural language inference; (ii) Quora Question Pair for paraphrase identification; and (iii) TrecQA and SelQA for answer sentence selection in question answering. Additional details about the above datasets can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We initialized word embedding with 300d GloVe vectors pre-trained from the 840B Common Crawl corpus (Pennington, Socher, and Manning 2014), while the word embeddings for the out-of-vocabulary words were initialized randomly. We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network. For the densely-connected recurrent layers, we stacked 5 layers each of which have 100 hidden units. We set 1000 hidden units with respect to the fullyconnected layers. The dropout was applied after the word and character embedding layers with a keep rate of 0.5. It was also applied before the fully-connected layers with a keep rate of 0.8. For the bottleneck component, we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2. The batch normalization was applied on the fully-connected layers, only for the one-way type datasets. The RMSProp optimizer with an initial learning rate of 0.001 was applied. The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve. All weights except embedding matrices are constrained by L2 regularization with a regularization constant λ = 10 −6 . The sequence lengths of the sentence are all different for each dataset: 35 for SNLI, 55 for MultiNLI, 25 for Quora question pair and 50 for TrecQA. The learning parameters were selected based on the best performance on the dev set. We employed 8 different randomly initialized sets of parameters with the same model for our ensemble approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI and MultiNLI</head><p>We evaluated our model on the natural language inference task over SNLI and MultiNLI datasets. <ref type="table" target="#tab_2">Table 2</ref> shows the results on SNLI dataset of our model with other published models. Among them, ESIM+ELMo and LM-Transformer are the current state-of-the-art models. However, they use additional contextualized word representations from language models as an externel knowledge. The proposed DRCN obtains an accuracy of 88.9% which is a competitive score although we do not use any external knowledge like ESIM+ELMo and LM-Transformer. The ensemble model achieves an accuracy of 90.1%, which sets the new state-ofthe-art performance. Our ensemble model with 53m parameters (6.7m×8) outperforms the LM-Transformer whose the number of parameters is 85m. Furthermore, in case of the encoding-based method, we obtain the best performance of 86.5% without the co-attention and exact match flag. <ref type="table" target="#tab_3">Table 3</ref> shows the results on MATCHED and MISMATCHED problems of MultiNLI dataset. Our plain DRCN has a competitive performance without any contextualized knowledge. And, by combining DRCN with the ELMo, one of the contextualized embeddings from language models, our model outperforms the LM-Transformer which has 85m parameters with fewer parameters of 61m. From this point of view, the combination of our model with a contextualized knowledge   is a good option to enhance the performance.</p><p>Quora Question Pair <ref type="table" target="#tab_6">Table 4</ref> shows our results on the Quora question pair dataset. BiMPM using the multiperspective matching technique between two sentences reports baseline performance of a L.D.C. network and basic multi-perspective models <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref>. We obtained accuracies of 90.15% and 91.30% in single and ensemble methods, respectively, surpassing the previous state-of-the-art model of DIIN.</p><p>TrecQA and SelQA </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Ablation study We conducted an ablation study on the SNLI dev set as shown in <ref type="table">Table 6</ref>, where we aim to exam-Models Accuracy (%) Siamese-LSTM <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref> 82.58 MP LSTM <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref> 83.21 L.D.C. <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref> 85.55 BiMPM <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref> 88   <ref type="bibr" target="#b4">(He, Gimpel, and Lin 2015)</ref> 0.762 0.830 HyperQA <ref type="bibr" target="#b14">(Tay, Luu, and Hui 2017)</ref> 0.770 0.825 PR+CNN <ref type="bibr" target="#b11">(Rao, He, and Lin 2016)</ref> 0.780 0.834 DRCN 0.804 0.862 clean version HyperQA <ref type="bibr" target="#b14">(Tay, Luu, and Hui 2017)</ref> 0.801 0.877 PR+CNN <ref type="bibr" target="#b11">(Rao, He, and Lin 2016)</ref> 0.801 0.877 BiMPM <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref>   ine the effectiveness of our word embedding technique as well as the proposed densely-connected recurrent and coattentive features. Firstly, we verified the effectiveness of the autoencoder as a bottleneck component in <ref type="formula" target="#formula_1">(2)</ref>. Although the number of parameters in the DRCN significantly decreased as shown in <ref type="table" target="#tab_2">Table 2</ref>, we could see that the performance was rather higher because of the regularization effect. Secondly, we study how the technique of mixing trainable and fixed word embeddings contributes to the performance in models (3-4). After removing E tr or E f ix in eq. (1), the performance degraded, slightly. The trainable embedding E tr seems more effective than the fixed embedding E f ix . Next, the effectiveness of dense connections was tested in models (5-9). In (5-6), we removed dense connections only over co-attentive or recurrent features, respectively. The result shows that the dense connections over attentive features are more effective. In <ref type="formula" target="#formula_3">(7)</ref>, we removed dense connections over both co-attentive and recurrent features, and the performance degraded to 88.5%.</p><p>In <ref type="formula">(8)</ref>  only over recurrent and co-attentive features. It means that only the word embedding features are densely connected to the uppermost layer while recurrent and attentive features are connected to the upper layer using the residual connection. In (9), we removed additional dense connection over word embedding features from (8). The results of (8-9) demonstrate that the dense connection using concatenation operation over deeper layers, has more powerful capability retaining collective knowledge to learn textual semantics. The model <ref type="formula">(10)</ref> is the basic 5-layer RNN with attention and (11) is the one without attention. The result of (10) shows that the connections among the layers are important to help gradient flow. And, the result of (11) shows that the attentive information functioning as a soft-alignment is significantly effective in semantic sentence matching. The performances of models having different number of recurrent layers are also reported in <ref type="figure" target="#fig_2">Fig. 2</ref>. The models (5-9) which have connections between layers, are more robust to the increased depth of network, however, the performances of (10-11) tend to degrade as layers get deeper. In addition, the models with dense connections rather than residual connections, have higher performance in general. <ref type="figure" target="#fig_2">Figure 2</ref> shows that the connection between layers is essential, especially in deep models, endowing more representational power, and the dense connection is more effective than the residual connection.  Word Alignment and Importance Our denselyconnected recurrent and co-attentive features are connected to the classification layer through the max pooling operation such that all max-valued features of every layer affect the loss function and perform a kind of deep supervision ). Thus, we could cautiously interpret the classification results using our attentive weights and max-pooled positions. The attentive weights contain information on how two sentences are aligned and the numbers of max-pooled positions in each dimension play an important role in classification. <ref type="figure" target="#fig_3">Figure 3</ref> shows the attention map (α i,j in eq. (5)) on each layer of the samples in <ref type="table">Table 1</ref>. The Avg(Layers) is the average of attentive weights over 5 layers and the gray heatmap right above the Avg(Layers) is the rate of max-pooled positions. The darker indicates the higher importance in classification. In the figure, we can see that tight, competing and bicycle are more important words than others in classifying the label. The word tight clothing in the hypothesis can be inferred from spandex in the premise. And competing is also inferred from race. Other than that, the riding is matched with pedaling, and pair is matched with two. Judging by the matched terms, the model is undoubtedly able to classify the label as an entailment, correctly.</p><p>In <ref type="figure" target="#fig_3">Figure 3 (b)</ref>, most of words in both the premise and the hypothesis coexist except white and gray. In attention map of layer 1, the same or similar words in each sentence have a high correspondence (gray and white are not exactly matched but have a linguistic relevance). However, as the layers get deeper, the relevance between white building and gray building is only maintained as a clue of classification (See layer 5). Because white is clearly different from gray, our model determines the label as a contradiction.</p><p>The densely connected recurrent and co-attentive features are well-semanticized over multiple layers as collective knowledge. And the max pooling operation selects the softpositions that may extract the clues on inference correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Error Analysis</head><p>We conducted a linguistic error analysis on MultiNLI, and compared DRCN with the ESIM, DIIN and CAFE. We used annotated subset provided by the MultiNLI dataset, and each sample belongs to one of the 13 linguistic categories. The results in table 7 show that our model generally has a good performance than others on most categories. Especially, we can see that ours outperforms much better on the Quantity/Time category which is one of the most difficult problems. Furthermore, our DRCN shows the highest mean and the lowest stddev for both MATCHED and MISMATCHED problems, which indicates that it not only results in a competitive performance but also has a consistent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduce a densely-connected recurrent and co-attentive network (DRCN) for semantic sentence matching. We connect the recurrent and co-attentive features from the bottom to the top layer without any deformation. These intact features over multiple layers compose a community of semantic knowledge and outperform the previous deep RNN models using residual connections. In doing so, bottleneck components are inserted to reduce the size of the network. Our proposed model is the first generalized version of DenseRNN which can be expanded to deeper layers with the property of controllable feature sizes by the use of an autoencoder. We additionally show the interpretability of our model using the attentive weights and the rate of maxpooled positions. Our model achieves the state-of-the-art performance on most of the datasets of three highly challenging natural language tasks. Our proposed method using the collective semantic knowledge is expected to be applied to the various other natural language tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material Datasets</head><p>A. SNLI is a collection of 570k human written sentence pairs based on image captioning, supporting the task of natural language inference <ref type="bibr" target="#b0">(Bowman et al. 2015)</ref>. The labels are composed of entailment, neutral and contradiction. The data splits are provided in <ref type="bibr" target="#b0">(Bowman et al. 2015)</ref>. B. MultiNLI, also known as Multi-Genre NLI, has 433k sentence pairs whose size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (FACE-TO-FACE, TELE-PHONE, 9/11, TRAVEL, LETTERS, OUP, SLATE, VERBATIM, GOVERNMENT and FICTION) of written and spoken English data. Also, there are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time. The data splits are provided in (Williams, Nangia, and Bowman 2017). C. Quora Question Pair consists of over 400k question pairs based on actual quora.com questions. Each pair contains a binary value indicating whether the two questions are paraphrase or not. The training-dev-test splits for this dataset are provided in <ref type="bibr" target="#b16">(Wang, Hamza, and Florian 2017)</ref>. D. TrecQA provided in <ref type="bibr" target="#b16">(Wang, Smith, and Mitamura 2007)</ref> was collected from TREC Question Answering tracks 8-13. There are two versions of data due to different pre-processing methods, namely clean and raw <ref type="bibr" target="#b11">(Rao, He, and Lin 2016)</ref>. We evaluate our model on both data and follow the same data split as provided in <ref type="bibr" target="#b16">(Wang, Smith, and Mitamura 2007)</ref>. We use official evaluation metrics of MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), which are standard metrics in information retrieval and question answering tasks. E. SelQA consists of questions generated through crowdsourcing and the answer senteces are extracted from the ten most prevalent topics (Arts, Country, Food, Historical Events, Movies, Music, Science, Sports, Travel and TV) in the English Wikipedia. We also use MAP and MRR for our evaluation metrics, and the data splits are provided in <ref type="bibr" target="#b6">(Jurczyk, Zhai, and Choi 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization on the comparable models</head><p>We study how the attentive weights flow as layers get deeper in each model using the dense or residual connection. We used the samples of the SNLI dev set in <ref type="table">Table 1</ref>. <ref type="figure">Figure 4</ref> and 5 show the attention map on each layer of the models of DRCN, Table 6 (8), and Table 6 (9). In the model of <ref type="table">Table 6</ref> (8), we replaced the dense connection with the residual connection only over recurrent and co-attentive features. And, in the model of Table 6 (9), we removed additional dense connection over word embedding features from Table 6 (8). We denote the model of Table 6 (9) as Res1 and the model of Table 6 (8) as Res2 for convenience.</p><p>In <ref type="figure">Figure 4</ref>, DRCN does not try to find the right alignments at the upper layer if it already finds the rationale for the prediction at the relatively lower layer. This is expected that the DRCN use the features of all the preceding layers as a collective knowledge. While Res1 and Res2 have to find correct alignments at the top layer, however, there are some misalignments such as competing and bicyclists rather than competing and race in Res2 model.</p><p>In the second example in <ref type="figure">Figure 5</ref>, although the DRCN couldn't find the clues at the lower layer, it gradually finds the alignments, which can be a rationale for the prediction. At the 5th layer of DRCN, the attentive weights of gray building and white building are significantly higher than others. On the other hand, the attentive weights are spread in several positions in both Res1 and Res2 which use residual connection. <ref type="figure">Figure 4</ref>: Visualization of attentive weights on the entailment example. The premise is "two bicyclists in spandex and helmets in a race pedaling uphill." and the hypothesis is "A pair of humans are riding their bicycle with tight clothing, competing with each other.". The attentive weights of DRCN, Res1, and Res2 are presented from left to right. <ref type="figure">Figure 5</ref>: Visualization of attentive weights on the contradiction example. The premise is "Several men in front of a white building." and the hypothesis is "Several people in front of a gray building.". The attentive weights of DRCN, Res1, and Res2 are presented from left to right.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>General architecture of our Densely-connected Recurrent and Co-attentive neural Network (DRCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0.802 0.875 Comp.-Aggr. (Bian et al. 2017)0.821 0.899 IWAN<ref type="bibr" target="#b13">(Shen, Yang, and Deng 2017)</ref> 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of models on every layer in ablation study. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of attentive weights and the rate of max-pooled position. The darker, the higher. See supplementary materials for a comparison with other models that use the residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Premise two bicyclists in spandex and helmets in a race pedaling uphill. Hypothesis A pair of humans are riding their bicycle with tight clothing, competing with each other. Label {entailment; neutral; contradiction} Premise Several men in front of a white building. Hypothesis Several people in front of a gray building. Label {entailment; neutral; contradiction}Table 1: Examples of natural language inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) for natural language inference on SNLI test set. |θ| denotes the number of parameters in each model.</figDesc><table><row><cell>Models</cell><cell cols="2">Accuracy (%) MATCHED MISMATCHED</cell></row><row><cell>ESIM (Williams, Nangia, and Bowman 2017)</cell><cell>72.3</cell><cell>72.1</cell></row><row><cell>DIIN (Gong, Luo, and Zhang 2018)</cell><cell>78.8</cell><cell>77.8</cell></row><row><cell>CAFE (Tay, Tuan, and Hui 2017)</cell><cell>78.7</cell><cell>77.9</cell></row><row><cell>LM-Transformer (Radford et al. 2018)</cell><cell>82.1</cell><cell>81.4</cell></row><row><cell>DRCN</cell><cell>79.1</cell><cell>78.4</cell></row><row><cell>DIIN* (Gong, Luo, and Zhang 2018)</cell><cell>80.0</cell><cell>78.7</cell></row><row><cell>CAFE* (Tay, Tuan, and Hui 2017)</cell><cell>80.2</cell><cell>79.0</cell></row><row><cell>DRCN*</cell><cell>80.6</cell><cell>79.5</cell></row><row><cell>DRCN+ELMo*</cell><cell>82.3</cell><cell>81.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Classification accuracy for natural language infer- ence on MultiNLI test set. * denotes ensemble methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows the performance of</cell></row><row><cell>different models on TrecQA and SelQA datasets for answer</cell></row><row><cell>sentence selection task that aims to select a set of candidate</cell></row><row><cell>answer sentences given a question. Most competitive models</cell></row><row><cell>(Shen, Yang, and Deng 2017; Bian et al. 2017; Wang, Hamza,</cell></row><row><cell>and Florian 2017; Shen et al. 2017) also use attention methods</cell></row><row><cell>for words alignment between question and candidate answer</cell></row><row><cell>sentences. However, the proposed DRCN using collective</cell></row><row><cell>attentions over multiple layers, achieves the new state-of-</cell></row><row><cell>the-art performance, exceeding the current state-of-the-art</cell></row><row><cell>performance significantly on both datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy for paraphrase identification on Quora question pair test set. * denotes ensemble methods.</figDesc><table><row><cell>Models</cell><cell>MAP MRR</cell></row><row><cell>Raw version</cell><cell></cell></row><row><cell>aNMM (Yang et al. 2016)</cell><cell>0.750 0.811</cell></row><row><cell>PWIM (He and Lin 2016)</cell><cell>0.758 0.822</cell></row><row><cell>MP CNN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Performance for answer sentence selection on TrecQA and selQA test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Accuracy (%) of Linguistic correctness on MultiNLI dev sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). [Williams, Nangia, and Bowman 2017] Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. [Wu et al. 2016] Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. [Yang et al. 2016] Yang, L.; Ai, Q.; Guo, J.; and Croft, W. B. 2016. anmm: Ranking short answer texts with attention-based neural matching model. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, 287-296. ACM.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = H l (x l t , h l t−1 ) x l t = [h l−1 t ; x l−1 t ].(4)The concatenation operation enables the hidden features to be preserved until they reach to the uppermost layer and all the previous features work for prediction as collective knowledge).Densely-connected Co-attentive networksAttention mechanism, which has largely succeeded in many domains(Wu et al. 2016;<ref type="bibr" target="#b15">Vaswani et al. 2017)</ref>, is a technique to learn effectively where a context vector is matched conditioned on a specific sequence. Given two sentences, a context vector is calculated based on an attention mechanism focusing on the relevant part of the two sentences at each RNN layer. The calculated attentive information represents soft-alignment between two sentences. In this work, we also use an attention mechanism. We incorporate co-attentive information into densely connected recurrent features using the concatenation operation, so as not to lose any information(Fig. 1). This concatenated recurrent and co-attentive features which are obtained by densely connecting the features from the undermost to the uppermost layers, enrich the collective knowledge for lexical and compositional semantics.The attentive information a pi of the i th word p i ∈ P against the sentence Q is calculated as a weighted sum of h qj 's which are weighted by the softmax weights as follows :a pi = J j=1 α i,j h qj α i,j = exp(e i,j ) J k=1 exp(e i,k ) e i,j = cos(h pi , h qj )(5)Similar to the densely connected RNN hidden features, we concatenate the attentive context vector a pi with triggered vector h pi so as to retain attentive information as an input to the next layer: h l t = H l (x l t , h l t−1 ) x l t = [h l−1 t ; a l−1 t ; x l−1 t ].(6)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A compare-aggregate model with dynamic-clip attention for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04289</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. Chen et al. 2017a] Chen, Q.; Zhu, X.; Ling, Z.-H.; and Inkpen, D. 2017a. Natural language inference with external knowledge</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<idno>arXiv:1802.05577</idno>
	</analytic>
	<monogr>
		<title level="m">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ternational Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
	<note>and Lin</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-perspective sentence similarity modeling with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distance-based selfattention network for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhai</forename><surname>Jurczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi ; Jurczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1512" to="1522" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adding semantics to data-driven paraphrasing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Radford et al. 2018. Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning loss functions for semi-supervised learning via discriminative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin ;</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lavelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management<address><addrLine>Santos, Wadhawan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Noisecontrastive estimation for answer selection with deep neural networks</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling</title>
		<idno type="arXiv">arXiv:1709.08294</idno>
		<idno>arXiv:1801.10296</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adaptive convolutional filter generation for natural language understanding</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inter-weighted alignment network for sentence pair modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng ; Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enabling efficient question answer retrieval via hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luu</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui ;</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<ptr target="CoRRabs/1707.07847" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui ;</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<idno>arXiv:1704.04565</idno>
	</analytic>
	<monogr>
		<title level="m">Neural paraphrase identification of questions with noisy pretraining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><forename type="middle">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>What is the jeopardy model? a quasisynchronous grammar for qa</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
