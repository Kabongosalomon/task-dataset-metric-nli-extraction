<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decompressing Knowledge Graph Representations for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
							<email>xiangk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyang</forename><surname>Chen</surname></persName>
							<email>xianyangc@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decompressing Knowledge Graph Representations for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the problem of predicting missing relationships between entities in knowledge graphs through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves state-of-the-art performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement. Code has been released 1 . * Equal contribution, in alphabetical order. 1 https://github.com/shawnkx/Decom</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, knowledge bases (KBs) such as Freebase <ref type="bibr" target="#b0">(Bollacker et al. 2008)</ref>, WordNet <ref type="bibr" target="#b13">(Miller 1995)</ref>, Yago <ref type="bibr" target="#b18">(Suchanek, Kasneci, and Weikum 2007)</ref> has proven useful in many tasks, including reading comprehension, recommendation system, information retrieval, etc. These KBs collect facts related to the real world as directed graphs (knowledge graphs), in which entities (nodes) are connected by their relationships (edges). As a result, a fact is represented by a triple (s, r, o), i.e., a relation r between a subject entity s and an object entity o.</p><p>Although these knowledge graphs contain millions of entities, they are usually incomplete, i.e., some relation-ships between entities are missing. Accordingly, extensive research has been done on predicting those missing links through learning low-dimensional embedding representations of entities and relations <ref type="bibr" target="#b1">(Bordes et al. 2011;</ref><ref type="bibr" target="#b2">2013;</ref><ref type="bibr" target="#b21">Yang et al. 2014;</ref><ref type="bibr" target="#b11">Krompaß, Baier, and Tresp 2015;</ref><ref type="bibr" target="#b14">Nickel et al. 2015;</ref><ref type="bibr" target="#b14">Nguyen et al. 2016;</ref><ref type="bibr" target="#b8">Feng et al. 2016;</ref><ref type="bibr" target="#b19">Trouillon et al. 2016;</ref><ref type="bibr" target="#b4">Das et al. 2016;</ref><ref type="bibr" target="#b3">Cai and Wang 2017;</ref><ref type="bibr" target="#b20">Xie et al. 2017;</ref><ref type="bibr" target="#b5">Dettmers et al. 2018;</ref><ref type="bibr" target="#b12">Lacroix, Usunier, and Obozinski 2018;</ref><ref type="bibr" target="#b7">Ebisu and Ichise 2018;</ref><ref type="bibr" target="#b15">Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b18">Sun et al. 2019)</ref>. Considering the large size of knowledge graphs with millions of facts, current popular link predictors tend to be fast and shallow, utilizing simple scoring functions and small embedding sizes, but at the potential expense of learning less expressive features.</p><p>In this work, different from the majority of prior studies, the goals of which are to design new scoring functions, such as TransE <ref type="bibr" target="#b2">(Bordes et al. 2013)</ref>, DistMult <ref type="bibr" target="#b21">(Yang et al. 2014)</ref>, ComplEx <ref type="bibr" target="#b19">(Trouillon et al. 2016)</ref>, <ref type="bibr">RotatE (Sun et al. 2019)</ref>, etc, we propose a general and effective method which could be applied to these models to boost their performance without explicitly increasing the embedding size or changing the scoring function. In more details, the original embeddings of entities and relations will be mapped to a more expressive and robust space by decompressing functions, then these link prediction models will be trained in this new space. Our method is simple and general enough to be applied to existing link prediction models and experimental results on different benchmark knowledge graphs and popular link prediction models demonstrate that our method could boost the scores with just a small amount of extra parameters.</p><p>Specifically, our contributions are as follows:</p><p>• We propose DeCom, a simple but effective decompressing method to significantly improve the performance of many existing knowledge graph embedding models.</p><p>• Without the need of increasing the embedding size explicitly, DeCom is able to help link prediction models save a lot of storing space and GPU memory.</p><p>• By employing convolutional neural network as the decompressing network, DeCom-based models only add a few extra parameters to original models, thus being highly parameter-efficient. Even if we use the fully connected network as the decompressing network, the number of extra parameters will still be constant and not growing with knowledge graph size. • Experiments on several benchmark knowledge graphs and link prediction models show the effectiveness of our model; among them, RESCAL + DeCom achieves stateof-the-art result on FB15k-237 across all evaluation metrics.</p><p>score es er eo fs fr fo SF <ref type="figure">Figure 1</ref>: The architecture of DeCom-based link predicting models. Embeddings of subject, object and relations, e s , e o , e r will be first fed into their corresponding DeCom layers to obtain more expressive and robust features, then the original scoring function (SF) will compute a score based on these decompressing features. The final score represents the probability of the input triple (s, r, o) being true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Formally, a knowledge graph G consists of entities (vertices) E and relations (edges) R, and could be represented by triples (facts) {(s, r, o)} ⊆ E × R × E. Each fact (triple) represent a relationship r ∈ R between one subject entity s ∈ E and one object entity o ∈ E. Commonly, there are millions of entities in one knowledge graph but a lot of links (relationships) between them are missing. Therefore, completing these missing links is referred as Knowledge Base Completion (KBC), or more specifically, Link Prediction. Most literature approaches the link prediction task by learning low-dimensional embedding vectors of knowledge graph entities and relations, known as Knowledge Graph Embedding (KGE). They formalize the problem into finding a scoring function g : E × R × E → R, which is able to compute a score of each triple <ref type="bibr">(s, r, o)</ref>, indicating whether this triple should be true or false. Intuitively, a promising scoring function should be able to assign higher scores to true triples than false ones. Within some of the recent models, a non-linearity such as the logistic sigmoid function is applied to the scoring function to give a corresponding probability prediction. RESCAL RESCAL (Nickel, Tresp, and Kriegel 2011) is a powerful link prediction model, the scoring function of which is a bilinear product between subject and object entities' embeddings and a full rank matrix for each relation. Due to its large number of parameters, RESCAL suffers from overfitting issue and explicitly increasing the relation embedding dimension will quadratically boost the number of its parameters.</p><p>DistMult In order to mitigate the above issue, Dist-Mult <ref type="bibr" target="#b21">(Yang et al. 2014</ref>), a special case of RESCAL, employs a diagonal matrix to represent each relation so that the number of parameters grows linearly in terms of the embedding size. The resulting scoring function is equivalent to the inner product of three vectors. However, DistMult could not handle asymmetric relations, as (s, r, o) and (o, r, s) will be assigned to the same score.</p><p>ComplEx To model asymmetric relations, Com-plEx <ref type="bibr" target="#b19">(Trouillon et al. 2016)</ref> extends DistMult from the real space to the complex space. Even though each relation matrix of ComplEx is still diagonal, the subject and object entity embeddings for the same entity are no longer equivalent, but complex conjugates, which introduces asymmetry into the tensor decomposition and thus enables ComplEx to model asymmetric relations.</p><p>Trade off between parameter growth and model performance Due to the large number of entities (vertices) in a knowledge graph, the number of parameters and computational costs are two essential aspects to evaluate a link prediction model. Specifically, since the number of entity and relation embedding parameters are |E| × D e and |R| × D r , where D e and D r are the entity and relation embedding dimension respectively, a large embedding size will lead to a unmanageable number of parameters. For example, applying DistMult with an embedding size of 400 to the whole Freebase needs more than 100 GB memory to store its parameters.</p><p>As a consequence, from <ref type="table" target="#tab_0">Table 1</ref>, it is easy to find that all these popular scoring functions are simple and only contain some basic operations, such as matrix multiplications and vector products, etc. Also, they tend to set the dimensionality of entities' and relations' embedding size relatively low (around 200). As a result, these simple, small and fast models could be applicable in real-world scenarios. The drawback is that these relatively low-dimensional embeddings may not have the capacity to model the semantic of the knowledge graph very well (high bias), posing a negative effect on model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring Function</head><p>DeCom Scoring Function RESCAL (Nickel, Tresp, and Kriegel 2011)</p><formula xml:id="formula_0">e s T W r e o f s (e s ) T f r (W r )f o (e o ) DistMult (Yang et al. 2014) e s , e r , e o f s (e s ), f r (e r ), f o (e o )</formula><p>ComplEx <ref type="bibr" target="#b19">(Trouillon et al. 2016</ref>) In all, the choice of knowledge graph embedding size, though rarely discussed, is an important problem to be addressed. In this work, we propose DeCom, a simple but effective method to decompress the low-dimensional embedding to a high dimensional space. Furthermore, DeCom has the following attractive aspects:</p><formula xml:id="formula_1">Re( e s , e r , e o ) Re( f s (e s ), f r (e r ), f o (e o ) )</formula><p>• DeCom does not explicitly increase the embedding size so that the model is still able to be scaled in a manageable way.</p><p>• DeCom is able to not only implicitly increase the expressiveness but extract more robust features from the original embedding to achieve better performance as well, thus reduce the risk of overfitting.</p><p>• DeCom learns a more general representation through the decompressing network which could be easily incorporated into many existing link predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation and Approach Overview</head><p>Despite a large amount of literature that designs new scoring functions, there have been limited discussions about how large the embedding size should be. Sharma, Talukdar, and others (2018) examine the geometry of knowledge graph embeddings and their experiment results suggest that for multiplicative methods (DistMult, ComplEx, etc.), increasing entity and relation embedding size leads to decreasing conicity (a high value of conicity would imply that the vectors lie in a narrow cone centered at origin) which might improve link prediction performance. It also has been proved that several bi-linear methods can be fully-expressive (i.e. there exists an assignment of values to the embeddings that accurately separates the correct triples from incorrect ones) given large enough embedding size <ref type="bibr" target="#b10">(Kazemi and Poole 2018)</ref>. However, they also show that the upper bound of embedding size for full-expressiveness is O(|E||R|), where |E| is the number of entities and |R| is the number of relations in knowledge graphs, which is not feasible even on a smallscale toy knowledge graph. People may be encouraged to use larger embedding size by above observations, but it is not just modeling scalability that sets them back. In fact, as shown in our experiment results in <ref type="table" target="#tab_4">Table 3</ref> (rows 6 vs. 13 and 14 vs. 21) , increasing embedding size does not guarantee better performance. The same phenomenon has been observed for word embeddings, and Yin and Shen (2018) explains this phenomenon under the bias-variance tradeoff framework: larger embed-ding size leads to decreased bias (better reconstruct the factorized coorcurrence matrix), but increased variance (overfit to the noise in the matrix). The same analysis can be applied to knowledge graph embedding as well, considering RESCAL, for example, what it essentially does is a tensor decomposition</p><formula xml:id="formula_2">X = ERE T</formula><p>where X ∈ R |E|×|R|×|E| is the tensor that represents the training graph, X ijk = +1 if there is a relation j from entity i to entity k, and X ijk = −1 if there is none. E ∈ R |E|×d is the entity embedding matrix where E i· is the embedding vector of the ith entity and d is the embedding size. R ∈ R d×|R|×d is the relation tensor where R ·j· is the embedding matrix of the j relation. This decomposition can be lossless with large enough embedding size, but if we do obtain such an embedding, the performance on evaluation and test sets will be zero -as the training graph tensor X is corrupted from the true graph tensorX by randomly flipping some of its entries. Moreover, for the link prediction task, we are exclusively evaluating those corrupted entries. It is popular for recent studies to prove that their model is fully expressive (unbiased with large enough d), but it may not be relevant to actual model performance, as the variance plays an important role here. Since DistMult is a special case of RESCAL and ComplEx generalizes DistMult to complex space, this analysis can be applied to them, and other multiplicative models, as well.</p><p>Motivated by the scalability issue and bias-variance tradeoff, we propose to use a shallow neural network to decompress low-dimensional embedding vectors to a higherdimensional space before applying the scoring functions. The intuition is that the low-dimensional embeddings will store the compressed information about entities/relations, and the decompressing network will project this compressed representation into a higher-dimensional space which is easier for the simple scoring functions to handle, thus achieving the low bias of high dimensional embedding with much fewer parameters. On the other hand, the decompressing network must learn the general information about the knowledge graph, making it more robust to noise and have lower variance. Less number of total parameters also suggests the model is less prone to overfitting.  <ref type="table">Table 2</ref>: Performance of different models w/ and w/o decompressing on the testset of FB15k-237 dataset. C-DeCom and F-DeCom denotes the CNNs-based and FCNNs-based decompressing functions. *-En means only decompressing entity embeddings and *-Rel shows that only relation embeddings are decompressed. d e andd e denote the dimension of entity features before and after decompressing layer. d r andd r represent the dimension of relation features before and after decompressing layer. '-' denotes no decompressing layer in the model. 'vanilla expansion' means explicitly increasing the embedding dimension (same notation are followed in other tables).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Approach DeCom</head><p>We denote the decompressing functions as f es (·), f r (·), f eo (·) ∈ R d → R D for the subject entity e s , relation r and objective entity e o respectively, where d and D are the original and projected embedding sizes repsectively. For any scoring function g(h, r, t), we could simply incorportate our decompressing function and change it into g(f h (h), f r (r), f t (t)). For example, the scoring function of DistMult is h, r, t , and after inserting the decompressing layer, we can change it into f h (h), f r (r), f t (t ). <ref type="table" target="#tab_0">Table 1</ref> shows more examples about scoring functions w/ and w/o decomressing operations. <ref type="figure">Figure 1</ref> shows the DeCom-based knowledge graph embedding model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decompressing Functions</head><p>Theoretically, DeCom could be implemented by any kinds of architectures, such as fully-connected, convolutional and recurrent neural networks. In this work, we mainly explore decompressing functions via convolutional neural networks (CNNs) and fully connected neural networks (FCNNs). Because the embedding has no sequential nature, the recurrent neural network has not been explored in this work. Furthermore, we need to point out that decompressing functions, f es (·), f r (·), f eo (·) are independent and do not need to be same.</p><p>CNNs-based DeCom (C-DeCom) Because of the high parameter efficiency and fast computation speed, CNNs are suitable to represent decompressing functions. Details of the CNNs-based decompressing function are as follows: for a batch of triples, we first look up their embedding vectors from entity and relation embedding tables. Then we feed them into one layer of 1-D CNN followed by the batch normalization and dropout, and use the final output to train a knowledge embedding model. Here, Batch normalization (Ioffe and Szegedy 2015) and dropout <ref type="bibr" target="#b17">(Srivastava et al. 2014</ref>) are employed to speed up training and prevent overfitting. Generally, this method could be easily incorporated into any non-parametric scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FCNNs-based decompressing function (F-DeCom)</head><p>Similar to C-DeCom, F-DeCom employs a linear layer to decompress the input features into a higher dimensional feature space.  DeCom, decompressing functions could be applied on 1) just entities 2) just relations and 3) both entities and relations.</p><p>Hyperparamerter Settings. In order to make our results comparable, for each link predicting baseline model, we keep most of the hyperparameters and training strategies the same between the original model and DeCom-enhanced model. All models are trained for 500 epochs, embedding size is 100, and other hyperparameters are chosen based on the performance on the validation set by grid search. For DistMult <ref type="bibr" target="#b21">(Yang et al. 2014)</ref> and ComplEx (Trouillon et al. 2016), following <ref type="bibr" target="#b5">Dettmers et al. (2018)</ref>, 1-1 training strategy is employed, and Adagrad <ref type="bibr" target="#b6">(Duchi, Hazan, and Singer 2011)</ref> is used as the optimizer; besides, we regularize these two models by forcing their entity embeddings to have a L2 norm of 1 after parameter updating and the pairwise margin-based ranking loss (margin=1.0) (Bordes et al.   <ref type="table" target="#tab_4">Tables 2 and 3.</ref> 2013) is employed. Furthermore, we find that regularizing entity embeddings after the decompressing layer to have a L2 norm of 1 could effectively prevent overfitting and make the training process stable. The range of the learning rate of Adagrad is {0.08, 0.10, 0.12}. For RESCAL (Nickel, Tresp, and Kriegel 2011), we apply 1-N <ref type="bibr" target="#b5">(Dettmers et al. 2018</ref>) training strategy, employ Adam (Kingma and Ba 2014) as the optimizer and set binary cross entropy as the loss function. The range of the learning rate of Adam is {0.01, 0.005, 0.001, 0.0005}. Because RESCAL's relations are represented as full-rank matrices, and it's not intuitive to decompress a low-dimensional vector into a matrix by convolution, we only experiment it with fully connected networks.</p><p>For each model's corresponding DeCom-enhanced model, in order to make them comparable, the training strategies such as the optimizer, 1-1 or 1-N training, hyperparameters grid search range, etc, remain the same. Besides that, hyperparameters of the decompressing function are selected via grid search according to the performance on the validation set. The ranges of hyperparameters of the DeCom layer for the grid search are set as follows: for C-DeCom, the number of kernel {2, 3, 4}, the size of kernel {3, 4}, for F-DeCom, the dimension of decompressed features are {200, 400}, for RESCAL relations only, pre-decompress dimension <ref type="bibr">{100, 200, 400, 1000, 2000}.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>Link prediction results on two datasets of three baseline models and their corresponding DeCom-based models are shown in Tables 2, 3 and 4.</p><p>DeCom vs. no DeCom: DeCom-based knowledge graph embedding models outperform their corresponding baseline models significantly, which demonstrates the expressive power of DeCom. Also note that the DeCom models also outperform the baseline models with explicitly increased embedding size, indicating that they are more robust to over-fitting.</p><p>C-DeCom vs. F-DeCom: The F-DeCom is able to generally obtain better scores but is more prone to be overfitting because from row 16 and rows 2, 8 in <ref type="table" target="#tab_4">Tables 2 and 3</ref> respectively, the best F-DeCom feature size is 200 instead of 400. One reason that F-DeCom achieving higher scores is that it could extract features from all embedding dimensions but C-DeCom is only able to extract features in the range of kernels.</p><p>DeCom-En vs. DeCom-Rel: From related rows in Table 2 and 3, just decompressing relation features could obtain slightly better result. We attribute this to that modelling relation between entities is more complicated which needs more expressive and robust features from DeCom.</p><p>DeCom vs. others In <ref type="table" target="#tab_5">Table 4</ref> we collect the scores of best configurations from <ref type="table">Table 2</ref> and 3 and compare them with some other recent works. Especially, DeCom-based RESCAL link prediction models achieve state-of-the-art performance on the FB15k-237 dataset across all metrics.</p><p>We further note that DeCom could assist the original model to achieve higher improvement on the dataset with a larger number of relations. Specifically, link prediction models with DeCom achieve +16% and +5% averaged improvement on FB15k-237 and WN18RR. We attribute this to that WN18RR is simpler in structure and the original embedding already has the ability to extract meaningful features from the small number of relations. Explicitly increasing embedding size also makes baseline performance worse on WN18RR, which suggests that 100 dimensions may be enough. Therefore, models trained on FB15k-237 benefit more from DeCom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and Discussion</head><p>Parameter and Running Time Efficiency</p><p>The decompressing layer is able to map the original embedding to a more expressive and robust feature space. One nat-  ural question is: what if we explicitly increase the embedding size? Therefore, we increase the embedding size from 100 to 400 to match the feature size after decompressing layer and compare them from different perspectives. The result is shown in <ref type="table" target="#tab_7">Table 5</ref>. It is clear to find that models with decompressing layer not only achieve better performance, but are much more parameter efficient with a little sacrifice of prediction speed. Especially, ComplEx with large embedding size instead harms the performance. We attribute this to the overfitting of two many parameters. Comparing with C-DeCom, F-DeCom obtains better scores with a little more parameters and slower decoding speed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why is DeCom effective?</head><p>We think that there are two main reasons to explain the effectiveness of DeCom: 1) Implicitly increasing the feature dimension to improve model's expressiveness by decompressing functions. 2) Learning more robust features. To further understand this fact, decompressing functions are designed to keep the size of input features (original embedding) and output ones the same. Specifically, we set the output feature size of F-DeCom and C-DeCom the same as the input embedding dimension, i.e., 100. The result is shown in <ref type="table" target="#tab_9">Table 6</ref>. Despite there is no increase in embedding size, the DeCom models still achieve the performance improvement, suggesting that they could learn more robust embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In order to predict the missing links in knowledge graphs, knowledge graph embedding (KGE) methods have been extensively studied in recent years. For example, RESCAL (Nickel, Tresp, and Kriegel 2011) employs a bilinear product between vector embeddings for each subject and object entity and a full rank matrix for each relation. TransE <ref type="bibr" target="#b2">(Bordes et al. 2013)</ref> implicitly models relations through representing each relation as a bijection between source and target entities. DistMult <ref type="bibr" target="#b21">(Yang et al. 2014)</ref>, as a special case of RESCAL, uses a diagonal matrix for each representation so that the amount of parameters grows linearly. ComplEx <ref type="bibr" target="#b19">(Trouillon et al. 2016</ref>) extends DistMult through modeling asymmetric relations by introducing complex embeddings. <ref type="bibr">RotatE (Sun et al. 2019</ref>) models the relation as a rotation operation from the subject entity to the object entity in the complex vector space. Most prior methods are based on simple operations and shallow neural network, which make them fast, scalable and memory-efficient, however, these properties also restrict the expressiveness of learned features. Concurrently, in order to mitigate this problem, <ref type="bibr" target="#b5">Dettmers et al. (2018)</ref> (ConvE) employs 2-D convolution operations on the subject entity and relation embedding vectors, after they are reshaped to matrices and concatenated. However, the reshaping and concatenation operations and applying 2-D convolution on word embeddings are not intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, in order to increase expressiveness and robustness of shallow link predictors, we propose, DeCom, a flexible decompressing mechanism which is able to map low-dimensional embeddings to a more expressive and robust space by adding just a few extra parameters. DeCom could be easily incorporated into many existing knowledge graph embedding models and experimental results show that it could boost the performance of many popular link predictors on several knowledge graphs and obtain state-of-the-art results on FB15k-237 across all evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>lists several popular scoring functions g from the literature. In these models, entities and relations are represented by low-dimensional embedding vectors, except for RESCAL where the relations are represented by full-rank matrices.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Scoring function g(s, r, o) of some link prediction models w/ and w/o decompressing (DeCom) layer, where · denotes the generalized dot product, f s , f r , f o represents decompressing operations for the subject entity, relation and object entity respectively and e s , e s and e s represent the embedding of the subject entity, relation and object entity respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of different models w/ and w/o decompressing on the testset of WN18RR dataset. We follow the standard evaluation protocol of this task. For each test triple (s, r, o), we corrupt subjecst or the objects in the knowledge graph into (s, r, o ) or (s , r, o). Then we rank the triples and see how good the ground truth is ranked. Triples that are different from the ground truth but are also correct are filtered. Mean Reciprocal Rank (MRR) and Hit@N (H@N) where N∈ {1, 3, 10}, are standard evaluation measures for these datasets and are reported in our experiments.</figDesc><table><row><cell>Experiments</cell></row><row><cell>We experiment with three bi-linear knowledge graph em-</cell></row><row><cell>bedding models, i.e., RESCAL (Nickel, Tresp, and Kriegel</cell></row><row><cell>2011), DistMult (Yang et al. 2014) and ComplEx (Trouil-</cell></row><row><cell>lon et al. 2016) on two benchmark datasets, i.e., FB15k-</cell></row><row><cell>237 (Toutanova and Chen 2015) and WN18RR (Dettmers et al. 2018), to show that our proposed method could con-sistently boost the performance of knowledge graph embed-ding methods. Experimental Settings Benchmark Datasets. FB15k (Bordes et al. 2013) and WN18 (Bordes et al. 2013) are widely used for evaluat-ing knowledge graph embedding methods. Toutanova and Chen (2015) shows that FB15k contains a large number of inverse relations and most test triples can be inferred from its reverse relation in the training set, so they delete the re-verse relations from FB15k and propose FB15k-237. There are 14,541 entities and 237 kinds of relations in FB15k-237. Similarly, Dettmers et al. (2018) removes the reverse relations from WN18 and propose WN18RR. Therefore, in this paper, we evaluate our methods on FB15k-237 and WN18RR. There are 40,934 entities and 11 types of rela-tions in WN18RR. Evaluation Protocol. Different DeCom strategies In order to further explore DeCom, various decompressing strategies are explored and the details are the following : • Different Decompressing functions: in this work, two de-compressing functions, F-DeCom and C-DeCom, are ex-plored in our experiments. • Decompressing objects: Because of the high flexibility of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Performance of different models w/ and w/o decompressing on the testset of FB15k-237 and WN18RR datasets.</cell></row><row><cell>Results of [♠] and [♥] are taken from Dettmers et al. (2018) and Sun et al. (2019). -adv sample stands for RotatE without</cell></row><row><cell>adversarial sampling, which should be a more fair comparison. For each DeCom-enhanced result, the best result is selected</cell></row><row><cell>from all DeCom settings from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison between models with different types of DeCom layers on the validation set of FB15k-237. The speed is calculated by the number of triples processed per second during predicting (validation) time. DeCom size means the size of features after decompressing layer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The robustness comparison between DeCom and original models on FB15k-237 validation set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04071</idno>
		<title level="m">Adversarial learning for knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01426</idno>
		<title level="m">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toruse: Knowledge graph embedding on a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by flexible translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Typeconstrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">International semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07297</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08140</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A threeway model for collective learning on multi-relational data</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards understanding the geometry of knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 16th international conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05908</idno>
		<title level="m">An interpretable knowledge transfer model for knowledge base completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Shen, Y</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="887" to="898" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>On the dimensionality of word embedding</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
