<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Improved Baseline for Sentence-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-27">27 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Improved Baseline for Sentence-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-27">27 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence-level relation extraction (RE) aims at identifying the relationship between two entities in a sentence. Many efforts have been devoted to this problem, while the best performing methods are still far from perfection. In this report, we revisit two problems that affect the performance of existing RE models, namely ENTITY REPRESENTATION and NOISY OR ILL-DEFINED LABELS. Our improved baseline model, incorporated with entity representations with typed markers, achieves an F 1 of 74.6% on TACRED, significantly outperforms previous SOTA methods. Furthermore, the presented new baseline achieves an F 1 of 91.1% on the refined Re-TACRED dataset, demonstrating that the pre-trained language models achieve unexpectedly high performance on this task. We release our code 1 to the community for future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the fundamental information extraction (IE) tasks, relation extraction (RE) aims at identifying the relationship(s) between two entities in a given piece of text from a pre-defined set of relationships of interest. For example, given the sentence "Bill Gates founded Microsoft together with his friend Paul Allen in 1975" and an entity pair ("Bill Gates", "Microsoft"), an RE system is expected to predict the relation to be ORG:FOUNDED BY. On this task, SOTA models based on pre-trained language models <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> have gained significant success.</p><p>Recent work on RE can be divided into two lines. One focuses on injecting external knowledge into language models. Methods of such, including ERNIE <ref type="bibr" target="#b15">(Zhang et al., 2019)</ref> and Know-BERT <ref type="bibr" target="#b8">(Peters et al., 2019)</ref>, take pre-trained entity embedding as input to the transformer. Similarly, K-Adapter <ref type="bibr">(Wang et al., 2020)</ref> introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model, and LUKE <ref type="bibr" target="#b12">(Yamada et al., 2020)</ref> extends the pretraining objective of masked language modeling to entities and proposes an entity-aware selfattention mechanism. The other line of work focuses on fine-tuning pre-trained language models on entity-linked text using RE-oriented objectives. Specifically, <ref type="bibr">BERT-MTB (Baldini Soares et al., 2019)</ref> proposes a matching-the-blanks objective that decides whether two relation instances share the same entities. Despite being extensively studied, the performance of existing RE classifiers is still far from perfection. On the most commonly used benchmark TACRED <ref type="bibr" target="#b14">(Zhang et al., 2017)</ref>, the SOTA F 1 result only increases from 70.1% (BERT LARGE ) to 72.7% (LUKE) after applying pre-trained language models to this task. It is unclear what building block is still missing in a satisfactory RE system.</p><p>In this work, we discuss two obstacles that have hindered the performance of existing RE models. First, the RE task provides a structured input of both the raw sentences and side information of the subject and object entities such as entity names, spans, and NER types. In order to feed the structured input into pre-trained language models, one necessary step is to transform the structured inputs into plain text. However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities. Moreover, all splits of TACRED, including the training, development, and test set, contains a large portion of noisy or ill-defined labels, causing the model performance to be underestimated. <ref type="bibr" target="#b0">Alt et al. (2020)</ref> relabeled the development and test set, and found that 6.62% of labels are incorrect. <ref type="bibr" target="#b9">Stoica et al. (2021)</ref> examined the definitions of pre-defined relationships and found a part of them to be ambiguous. Based on their refined set of relationships, Stoica et al. further re-annotated the TACRED dataset using an improved annotation strategy to ensure highquality labels. To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentence-level RE, which leads to promising improvement of performance over existing entity representation techniques.</p><p>We evaluate our improved baseline model on three different versions of TA-CRED: the original TACRED <ref type="bibr" target="#b14">(Zhang et al., 2017)</ref>, TACREV <ref type="bibr" target="#b0">(Alt et al., 2020)</ref>, and Re-TACRED <ref type="bibr" target="#b9">(Stoica et al., 2021)</ref>.</p><p>Using RoBERTa  as the backbone, our improved baseline model achieves an F 1 of 74.6% and 83.2% on the original TACRED and TACREV, respectively, significantly outperforming previous methods. Particularly, our baseline model achieves an F 1 of 91.1% on Re-TACRED. It demonstrates that pre-trained language models achieve much better results than shown in previous work on the sentence-level RE task. Moreover, we notice that the high noisy rate of TACRED makes the RE models more reliant on the side information of entities. We suggest that Re-TACRED is a better evaluation benchmark for future work on sentence-level RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Improved RE Baseline</head><p>In this section, we first formally define the sentence-level RE task in Section 2.1, and then present our model architecture and entity representation techniques in Section 2.2 and Section 2.3. Finally, we evaluate whether our model can generalize to unseen entities in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>In this paper, we focus on sentence-level RE. Specifically, given a sentence x and an entity pair (e s , e o ), the task of sentence-level RE is to predict the relationship r between e s and e o from R∪{NA}, where R is a pre-defined set of relationships of interest, e s , e o are identified as the subject and object entity, respectively. Entity pairs that do not express relation from R are labeled NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>Our RE classifier is an extension of previous transformer-based <ref type="bibr" target="#b2">(Devlin et al., 2019;</ref> RE models <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref>. Given the input sentence x, we first mark the entity spans and entity types using techniques presented in Section 2.3, then feed the processed sentence into a pre-trained language model to get its contextual embedding. Finally, we feed the hidden states of the subject and object entities in the language model's last layer, i.e., h subj and h obj , into the softmax classifier:</p><formula xml:id="formula_0">z = ReLU W proj h subj , h obj , P(r) = exp(W r z + b r ) r ′ ∈R∪{NA} exp(W r ′ z + b r ′ ) , where W proj ∈ R 2d×d , W r , W r ′ ∈ R d , b r , b r ′ ∈ R are model parameters.</formula><p>In inference, the classifier returns the relationship with the maximum probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Representation</head><p>For sentence-level RE, the names, spans, and NER types of entities are provided as the input. The RE models need to transform such side information of entities into text to allow it to be captured by the pre-trained language models. Previous work <ref type="bibr" target="#b14">(Zhang et al., 2017;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019;</ref><ref type="bibr">Wang et al., 2020)</ref> has proposed different entity representation techniques. In this context, we conduct a comprehensive evaluation of the following techniques on TACRED:</p><p>• Entity mask <ref type="bibr" target="#b14">(Zhang et al., 2017)</ref>. This technique uses a special [SUBJ-TYPE] or [OBJ-TYPE] token to mask the subject and object entities in the original sentence, where TYPE is substituted with the corresponding entity type. This technique was originally proposed for the PA-LSTM model <ref type="bibr" target="#b14">(Zhang et al., 2017)</ref>, and was later adopted by transformerbased models such as SpanBERT <ref type="bibr" target="#b4">(Joshi et al., 2020)</ref>. <ref type="bibr" target="#b14">Zhang et al. (2017)</ref>   • Entity marker (punct) <ref type="bibr">(Wang et al., 2020;</ref><ref type="bibr" target="#b17">Zhou et al., 2021)</ref>. This technique is a variant of the previous technique that marks entity spans using punctuation. It modifies the original sentence to "@ SUBJ @ ... # OBJ #".</p><p>• Typed entity markers <ref type="bibr" target="#b16">(Zhong and Chen, 2021)</ref> This technique further adds the NER types into entity markers. It introduces special tokens S:TYPE , /S:TYPE , O:TYPE , /O:TYPE , where TYPE is the corresponding entity type given by NER, and modifies the sentence to " S:TYPE SUBJ /S:TYPE ... O:TYPE OBJ /O:TYPE ".</p><p>• Typed entity marker (punct) We propose a variant of the typed entity marker technique that marks the entity span and NER type of entities without introducing new special tokens. We enclose the subject and object entities with "@" and "#", respectively. We also represent the subject and object entity types using plain text, which is prepended to the entity spans, and is enclosed by "*" for subjects or "∧" for objects. The modified sentence is "@ * SUBJ-NER * SUBJ @ ... # ∧ OBJ-NER ∧ OBJ # ".</p><p>The above techniques all seek to provide combined representations for different entity information. Differently, the typed entity marker represents all of entity names, entity spans, and entity types, while entity marker and entity mask ignore entity types and entity names, respectively. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of different representation techniques. For each technique, we also provide an example of the processed sentence. We observe that: (1) Typed entity markers (original and punct) outperform other techniques by a 2 SUBJ and OBJ are respectively the original token spans of subjet and object entities in the sentence.  notable margin. Especially, the RoBERTa model achieves an F 1 score of 74.6% using the typed entity marker (punct), which is significantly higher than the SOTA result of 72.7% <ref type="bibr" target="#b12">(Yamada et al., 2020)</ref>. It shows that all types of entity information are helpful to the RE task. It also shows that keeping entity names in the input improves the generalization of RE models, contradicting the claim by <ref type="bibr" target="#b14">Zhang et al. (2017)</ref>. <ref type="formula">(2)</ref> Although the original and punct versions of entity representation techniques represent the same types of entity information, they do make a difference in model performance. Particularly, introducing special tokens deteriorates severely model performance on RoBERTa. On RoBERTa LARGE , entity marker underperforms entity marker (punct) by 0.7%, typed entity marker underperforms typed entity marker (punct) by 3.6%, while entity mask gets a much worse result of 60.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inference with Unseen Entities</head><p>Some previous work <ref type="bibr" target="#b13">(Zhang et al., 2018)</ref> claims that entity names may leak superficial clues of the relation types, allowing heuristics to hack the benchmark. They show that neural RE classifier can achieve high evaluation results only based on the subject and object entity names even without putting them in the context of the original sentence. They also suggest that RE classifiers trained without entity masks may not generalize well to unseen entities. However, as the provided NER types  <ref type="table">Table 3</ref>: F 1 (in %) on the original and revisited test set of TACRED. * marks re-implemented results from <ref type="bibr" target="#b0">Alt et al. (2020)</ref>. † marks re-implemented results from <ref type="bibr" target="#b9">Stoica et al. (2021)</ref>. ‡ marks our re-implemented results.</p><p>We report the median of F 1 on 5 runs training using different random seeds. We fail to run the officially released code of KnowBERT because of environment errors.  in RE datasets are usually coarse-grained, using entity masks may lead to the loss of entity information. Using entity masks also contradicts existing methods of injecting entity knowledge into RE classifiers <ref type="bibr" target="#b15">(Zhang et al., 2019;</ref><ref type="bibr" target="#b8">Peters et al., 2019;</ref><ref type="bibr">Wang et al., 2020)</ref>. If RE classifiers should not consider entity names, it is unreasonable to suppose that the RE classifiers can be improved by external knowledge graphs.</p><p>To evaluate whether the RE classifier can generalize to unseen entities, we propose a filtered evaluation setting. Specifically, we remove all test instances containing entities from the training set. This results in a filtered test set of 4,599 instances that only contain sentences with entities that are unseen during training. We present the evaluation results on the filtered test set in <ref type="table" target="#tab_3">Table 2</ref>. Note that as the label distributions of the original and filtered test set are different, their results are not directly comparable. Still, typed entity marker consistently outperforms entity mask, which shows that RE classifiers can learn from entity names and generalize to unseen entities. Our finding is consistent with <ref type="bibr" target="#b7">Peng et al. (2020)</ref>, whose work suggests that entity mentions can provide more information than entity types to improve the RE classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets. We evaluate our model on three different versions of TACRED: TACRED <ref type="bibr" target="#b14">(Zhang et al., 2017)</ref>, TACREV <ref type="bibr" target="#b0">(Alt et al., 2020)</ref>, and Re-TACRED <ref type="bibr" target="#b9">(Stoica et al., 2021)</ref>. The statistics of the three datasets are shown in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>Model Configurations. Our model is implemented based on HuggingFace's Transformers <ref type="bibr" target="#b11">(Wolf et al., 2020)</ref>. Most hyper-parameters are set following Baldini Soares et al. <ref type="bibr">(2019)</ref>. Specifically, our model is optimized with Adam (Kingma and Ba, 2015) using the learning rate of 5e−5 on BERT BASE , and 3e−5 on BERT LARGE and RoBERTa LARGE , with a linear warmup <ref type="bibr" target="#b3">(Goyal et al., 2017)</ref> for the first 6% steps followed by a linear decay to 0. We use a batch size of 64 for all models. We fine-tune the model for 5 epochs on all datasets. The best model checkpoint is selected based on dev F 1 . For other compared methods, we rerun their officially released code using the recommended hyperparameters in their papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The experimental results are shown in <ref type="table">Table 3</ref>. Using RoBERTa LARGE as the backbone, our improved RE baseline achieves the best results on all datasets. However, we notice that on Re-TACRED, the gain from typed entity marker is much smaller compared to TACRED and TACREV, decreasing from 3.1 − 3.9% and 2.0 − 3.4% to 0.2 − 0.8% of F 1 . This observation could be attributed to the high noise rate in the training set of TACRED, in which case introducing additional entity information can reduce the influence of noisy labels. This shows that noisy labels in TACRED could make model predictions more dependent on the side information of entities such as NER types. Therefore, we suggest future work on sentence-level RE to adopt Re-TACRED as the evaluation benchmark in order to avoid such bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this report, we revisit two technical problems in existing sentence-level RE models, namely entity representation and noisy or ill-defined labels. We propose an improved RE baseline model, which significantly outperforms existing RE models. Especially, our model achieves an F 1 score of 91.1% on the Re-TACRED dataset, showing that pre-trained language models already achieve satisfactory performance on this task. We hope the proposed model can benefit future research in sentence-level RE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>claim that this technique prevents RE models from over-fitting predictions to entity names. Bill [/E1] was born in [E2] Seattle [/E2]. PERSON was born in O:CITY Seattle /O:CITY .Typed entity marker (punct) @ * person * Bill @ was born in # ∧ city ∧ Seattle #.</figDesc><table><row><cell>Method</cell><cell>Input Example</cell><cell></cell><cell cols="3">BERT BASE BERT LARGE RoBERTa LARGE</cell></row><row><cell>Entity mask</cell><cell cols="2">[SUBJ-PERSON] was born in [OBJ-CITY].</cell><cell>69.6</cell><cell>70.6</cell><cell>60.9</cell></row><row><cell>Entity marker</cell><cell cols="3">[E1] 68.4</cell><cell>69.7</cell><cell>70.7</cell></row><row><cell>Entity marker (punct)</cell><cell cols="2">@ Bill @ was born in # Seattle #.</cell><cell>68.7</cell><cell>69.8</cell><cell>71.4</cell></row><row><cell>Typed entity marker</cell><cell>S:PERSON Bill</cell><cell cols="2">/S:71.5</cell><cell>72.9</cell><cell>71.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.9</cell><cell>72.7</cell><cell>74.6</cell></row><row><cell></cell><cell></cell><cell>• Entity</cell><cell>marker</cell><cell>(Zhang et al.,</cell><cell>2019;</cell></row><row><cell></cell><cell></cell><cell cols="4">Baldini Soares et al., 2019). This technique</cell></row><row><cell></cell><cell></cell><cell cols="4">introduces special tokens [E1], [/E1],</cell></row><row><cell></cell><cell></cell><cell cols="4">[E2], [/E2] to enclose the spans of subject</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test F 1 (in %) of different entity representation techniques on TACRED. For each technique, we also provide the processed input of an example sentence "Bill was born in Seattle". Typed entity marker (original and punct) significantly outperforms others. and object entities, and modify the sentence to "[E1] SUBJ [/E1] ... [E2] OBJ [/E2]" 2 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test F 1 on the filtered test set of TACRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Statistics of different versions of TACRED.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wzhouad/ RE_improved_baseline</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3661" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Re-tacred: Addressing shortcomings of the tacred dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Emmanouil Antonios Platanios, and Barnabás Póczos</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2020. K-adapter: Infusing knowledge into pre-trained models with adapters</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
