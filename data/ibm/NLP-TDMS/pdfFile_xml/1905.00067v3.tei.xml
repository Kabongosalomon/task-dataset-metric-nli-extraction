<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
						</author>
						<title level="a" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing popular methods for semi-supervised learning with Graph Neural Networks (such as the Graph Convolutional Network) provably cannot learn a general class of neighborhood mixing relationships. To address this weakness, we propose a new model, MixHop, that can learn these relationships, including difference operators, by repeatedly mixing feature representations of neighbors at various distances. MixHop requires no additional memory or computational complexity, and outperforms on challenging baselines. In addition, we propose sparsity regularization that allows us to visualize how the network prioritizes neighborhood information across different graph datasets. Our analysis of the learned architectures reveals that neighborhood mixing varies per datasets. 1  We use "like", as graph edges are not axis-aligned.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) establish state-ofthe-art performance for many Computer Vision applications <ref type="bibr" target="#b14">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b20">Szegedy et al., 2015)</ref>. CNNs consist of a series of convolutional layers, each parameterized by a filter with pre-specified spatial dimensions. CNNs are powerful because they are able to learn a hierarchy of translation invariant feature detectors.</p><p>The success of CNNs in Computer Vision and other domains has motivated researchers <ref type="bibr" target="#b4">(Bruna et al., 2014;</ref><ref type="bibr" target="#b7">Defferrard et al., 2016;</ref><ref type="bibr" target="#b13">Kipf &amp; Welling, 2017)</ref> to extend the convolutional operator from regular grids, in which the structure is fixed and repeated everywhere, to graph-structured data, where nodes' neighborhoods can greatly vary in structure across the graph. Generalizing convolution to graph struc-Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</p><formula xml:id="formula_0">W (i)Â H (i) H (i+1) H (i+1) = σ(ÂH (i) W (i) ) (a) Traditional graph convolution. H (i+1) H (i+1) = σ(Â 0 H (i) W (i) 0 |Â 1 H (i) W (i) 1</formula><p>|...) (b) Our mixed feature model. <ref type="figure">Figure 1</ref>: Feature propagation in traditional graph convolution methods (a), compared to ours (b). We show the latent feature for red node in layer i + 1 given node features in layer i. The traditional graph convolution case only aggregates from immediate neighborsÂH <ref type="bibr">(i)</ref> . In our MixHop the feature vector H (i+1) is a learned combination of the node's neighborsÂ j H (i) at multiple distances j.</p><p>tures should allow models to learn location-invariant node and neighborhood features.</p><p>Early extensions of the graph convolution (GC) operator were theoretically motivated <ref type="bibr" target="#b4">(Bruna et al., 2014)</ref>, but (1) required quadratic computational complexity in number of nodes and therefore were not scalable to large graphs, and (2) required the graph to be completely observed during training, targeting only the transductive setting. <ref type="bibr" target="#b7">Defferrard et al. (2016)</ref> and <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref> propose GC approximations that are computationally-efficient (linear complexity, in the number of edges), and can be applied in inductive settings, where the test graphs are not observed during training.</p><p>However, said approximations limit the representational capacity of the model. In particular, if we represent an image as a graph of pixel nodes, where edges connect adjacent pixels, GC approximations applied on the pixel graphs will be unable to learn Gabor-like 1 filters. Gabor filters are fundamental to the human visual cognitive system <ref type="bibr" target="#b5">(Daugman, 1980;</ref><ref type="bibr" target="#b6">1985)</ref>. Further, these filters are automatically recovered by training CNNs on natural images (see <ref type="bibr" target="#b14">Krizhevsky et al. (2012)</ref>; <ref type="bibr" target="#b15">Lee et al. (2009)</ref> for visualizations). Their automatic recovery implies their usefulness for hierarchical object representations and scene understanding, as guided by the optimization (e.g. classification) objective. Since Graphs are generic data structures that can encode data from various domains (e.g. images, chemical compounds, social, and biological networks), realizing Gabor-like filters in Graph domains ought to yield a general advantage.</p><p>In this work, we address the limitations of the approximations that prevent these models from capturing the graph analogue of Gabor filters. Our proposed method, MixHop, allows full linear mixing of neighborhood information (as illustrated in <ref type="figure">Figure 1</ref>), at every message passing step. Specifically, our contributions are the following:</p><p>• We formalize Delta Operators and their generalization, Neighborhood Mixing, to analyze the expressiveness of graph convolution models. We show that popular graph convolution models (e.g. GCN of <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>) cannot learn these representations.</p><p>• We propose MixHop, a new Graph Convolutional layer that mixes powers of the adjacency matrix. We prove that MixHop can learn a wider class of representations without increasing the memory footprint or computational complexity of previous GCN models.</p><p>• We provide a method of learning to divide modeling capacity among various widths and depths of a MixHop model, yielding powerful compact GCN architectures. These architectures conveniently also allow visual inspection of which aspects of a graph are important.</p><p>We demonstrate our method on node classification tasks. Our code is available on github.com/samihaija/mixhop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>Graph G with n nodes and m edges has a feature matrix X ∈ R n×s0 with s 0 features per node, and training labels Y I , annotating a partial set of nodes with the c possible classes. The output of the task, Y O , is an assignment of labels to the nodes, Y ∈ [0, 1] n×c . Let A denote the adjacency matrix of G, where a non-zero entry A ij indicates an edge between nodes i and j. We consider the case of a binary adjacency matrix (A ∈ {0, 1} n×n ), but this notation can be extended w.l.o.g. to weighted graphs. Let I n be the n × n identity matrix, and D be the degree matrix, D = diag(d), where d ∈ Z n is the degree vector with d j = i A ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Message Passing</head><p>Message Passing algorithms can be used to learn models over graphs <ref type="bibr" target="#b8">(Gilmer et al., 2017)</ref>. In such models, each graph node (and optionally edge) holds a latent vector, initialized to the node's input features Each node repeatedly passes its current latent vector to, and aggregates incoming messages from, its immediate neighbors. After l steps of message passing and feature aggregation, every node outputs a representation which can be used for an upstream task e.g. node classification, or entire graph classification. The l steps (message passing and aggregation) can be parametrized and trained via Backprop-Through-Structure algorithms <ref type="bibr" target="#b9">(Goller &amp; Kuchler, 1996)</ref>, to minimize an objective measured using the node representations as output by the l'th step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph Convolutional Networks</head><p>We refer to the Graph Convolutional Network proposed by <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref> as the vanilla GCN. The vanilla GCN Graph Convolutional (GC) Layer is defined as:</p><formula xml:id="formula_1">H (i+1) = σ( AH (i) W (i) ),<label>(1)</label></formula><p>where H (i) ∈ R n×si and H (i+1) ∈ R n×si+1 are the input and output activations for layer i, W (i) ∈ R si×si+1 is a trainable weight matrix and σ is an element-wise activation function, and A is a symmetrically normalized adjacency matrix with self-connections, A = D − 1 2 (A + I n )D − 1 2 . A GCN model with l layers is then defined as:</p><formula xml:id="formula_2">H (i) = X if i = 0 σ( AH (i−1) W (i−1) ) if i ∈ [1 .. l],<label>(2)</label></formula><p>and the output Y O can be set as to function of H (l) . The vanilla GCN can be described as a message passing algorithm, where a node's latent representation at step i is defined as an average of its neighbors' representations from step i − 1, multiplied by W (i−1) . See <ref type="bibr" target="#b8">Gilmer et al. (2017)</ref>.</p><p>The vanilla GCN makes three simplifying assumptions: (1) it is a Chebyshev rank-2 approximation of multiplication in the Graph Fourier basis, defined to be the eigenbasis of the graph Laplacian; (2) it assumes that the two coefficients of the Chebyshev polynomials multiply to -1; (3) a renormalization trick adds self-connections (identity matrix) to A before, rather than after, normalization. These simplifications reduce the computational complexity and prevent exploding/vanishing gradients. However, it simplifies the definition of convolution to become a simple neighborhoodaveraging operator: this is obvious from Equation 1 -the features are left-multiplied by normalized adjacency A, effectively replacing each row in the feature matrix, by the average of its neighbors (and itself, due to renormalization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Semi-supervised Node Classification</head><p>We are interested in semi-supervised node classification tasks. To train a GCN model on such a task, we select row slices from the output matrix Y O , corresponding to nodes with known labels in Y I , on which a loss and its gradients are evaluated. The gradient of the loss is backpropagated through the GC layers where they get multiplied by A , spreading gradients to unlabeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Architecture</head><p>We are interested in higher-order message passing, where nodes receive latent representations from their immediate (first-degree) neighbors and from further N-degree neighbors at every message passing step. In this section, we motivate and detail a model with trainable aggregation parameters that can choose how to mix latent information from neighbors at various distances.</p><p>Our analysis starts with the Delta Operator, a subtraction operation between node features collected from different distances. The vanilla GCN is unable to learn such a feature representation. Before introducing our model, we give one formal definition:</p><p>Definition 1 Representing Two-hop Delta Operator: A model is capable of representing a two-hop Delta Operator if there exists a setting of its parameters and an injective mapping f , such that the output of the network becomes</p><formula xml:id="formula_3">f σ AX − σ A 2 X ,<label>(3)</label></formula><p>given any adjacency matrix A, features X, and activation function σ.</p><p>Learning such an operator should allow models to represent feature differences among neighbors, which is necessary, for example, for learning Gabor-like filters on the graph manifold. To provide a concrete example regarding graphs, consider an online social network. In this setting, Delta Operators allow a model to represent users that live around the "boundary" of social circles <ref type="bibr" target="#b17">(Perozzi &amp; Akoglu, 2018)</ref>.</p><p>To learn an approximate feature for American person with a popular German friend, who might have most immediate friends speaking English, but many friends-of-friends speaking German. This person can be represented by learning a convolutional filter contrasting the English and German languages of one-hop and two-hop neighbors.</p><p>Note that in the Definition 1 we allow not learning the direct form of two-hop Delta Operators, but a transformation of it, as long as that transformation can be inverted (i.e. f is injective).</p><p>In Sections 3.1 -3.3, we analyze the extent to which various GCN models can learn the Delta Operator. We generalize this definition and analysis in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MixHop Graph Convolution Layer</head><p>We propose replacing the Graph Convolution (GC) layer defined in Equation 1, with: where the hyper-parameter P is a set of integer adjacency powers, A j denotes the adjacency matrix A multiplied by itself j times, and denotes column-wise concatenation. The difference between our proposed layer and a vanilla GCN is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Note that setting P = {1} exactly recovers the original GC layer. Further, note that A 0 is the identity matrix I n , where n is the number of nodes in the graph. We depict a model with P = {0, 1, 2} in <ref type="figure">Figure  1b</ref>. In our model, each layer contains |P | distinct parameter matrices, each of which can be a different size. By default, we set all |P | matrices to have the same dimensionality; however, in Section 4.2, we explain how we utilize sparsifying regularizers on the learnable weight matrices to produce dataset-specific model architectures that slightly outperform our default settings.</p><formula xml:id="formula_4">H (i+1) = j∈P σ A j H (i) W (i) j ,<label>(4)</label></formula><formula xml:id="formula_5">H (i−1) × A × W (i) H (i) (a) GCN layer. H (i−1) × A 0 × W (i) 0 × A 1 × W (i) 1 × A 2 × W (i) 2 H (i) (b) Our proposed layer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computational Complexity</head><p>There is no need to calculate A j . We calculate A j H (i) with right-to-left multiplication. Specifically, if j = 3, we calculate A 3 H (i) as A A AH (i) . Since we store A as a sparse matrix with m non-zero entries, an efficient implementation of our layer (Equation 4) takes O(j max × m × s i ) computational time, where j max is the largest element in P and s i is the feature dimension of H <ref type="bibr">(i)</ref> . Under the realistic assumptions of j max m and s l m, running an l-layer model takes O(lm) computational time. This matches the computational complexity of the vanilla GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Representational Capability</head><p>Since each layer outputs the multiplication of different adjacency powers in different columns, the next layer's weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MixHop Graph Convolution Layer</head><p>Inputs:</p><formula xml:id="formula_6">H (i−1) , A Parameters: {W (i) j } j∈P j max := max P B := H (i−1) for j = 1 to j max do B := AB if j ∈ P then O j := BW (i) j end if end for H (i) := j∈P O j Return: H (i)</formula><p>can learn arbitrary linear combinations of the columns. By assigning a positive coefficient to a column produced by some A power, and assigning a negative coefficient to another, the model can learn a Delta Operator. In contrast, vanilla GCNs are not capable of representing this class of operations, even when stacked over multiple layers. Proof of Theorem 1. The output of an l-layer vanilla GCN has the following form:</p><formula xml:id="formula_7">σ( A(σ( A · · · σ( AXW (0) ) · · · )W (l−2) )W (l−1) ).</formula><p>For the simplicity of the proof, let's assume that ∀i, s i = n. In a particular case, when σ(x) = x and X = I n , this reduces to A l W * , where W * = W (0) W (1) · · · W (l−1) . Suppose the network is capable of representing a two-hop Delta Operator. This means that there exists an injective map f and a value for</p><formula xml:id="formula_8">W * , such that ∀ A, A l W * = f ( A − A 2 ). Setting A = I n , we get that W * = f (0). Let C 1,2        0.5 0.5 0 · · · 0 0.5 0.5 0 · · · 0 0 0 1 · · · 0 . . . . . . . . . . . . . . . 0 0 0 · · · 1       </formula><p>be the symmetrically normalized adjacency matrix with self-connections corresponding to the graph having a single edge between vertices 1 and 2. Setting A = C 1,2 , we get C 1,2 W * = f (0). Since we already have that f (0) = W * , we get that (I n − C 1,2 )W * = 0, which proves that the w * 1 = w * 2 , where w * i is the i-th row of W * . Since the choice of vertices 1 and 2 was arbitrary, we have that all rows of W * are equal to each other. Therefore, rank( A l W * ) ≤ 1, which implies that outputs of mapping f should be at most rank-one matrices. Thus, f cannot be injective, proving that vanilla GCN cannot represent two-hop Delta Operators.</p><p>Proof of Theorem 2. A two-layer model, defined using Equation 4 with P = {0, 1, 2} recovers the two-hop delta operator defined in Equation 3. We start by redefining the feature vector H (1) learned by the first layer of the model by pulling out the element-wise activation function σ and expanding the concatenation operator found in the layer definition:</p><formula xml:id="formula_9">H (1) = j∈{0,1,2} σ A j XW (0) j = σ     j∈{0,1,2} A j XW (0) j     = σ I N XW (0) 0 AXW (0) 1 A 2 XW (0) 2 ,</formula><p>We can now set W = I s0 . The expression above can be simplified to</p><formula xml:id="formula_10">H (1) = σ 0 AX A 2 X .</formula><p>The feature vector H (1) can be plugged into the equation for the second layer that has linear activation function:</p><formula xml:id="formula_11">H (2) = I N H (1) W (1) 0 AH (1) W (1) 1 A 2 H (1) W (1) 2 .</formula><p>Setting the weights for the second layer as W</p><formula xml:id="formula_12">(1) 1 = W (1) 2 = 0, and W (1) 0 =   0 I s0 −I s0   ,<label>(5)</label></formula><p>makes</p><formula xml:id="formula_13">H (2) = σ AX − σ A 2 X 0 0 .</formula><p>This shows that our GCN can successfully represent the two-hop Delta Operators according to the Definition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">General Neighborhood Mixing</head><p>We generalize Definition 1 from two-hops to multiple hops:</p><p>Definition 2 General layer-wise Neighbor-hood Mixing: A Graph Convolutional Network is capable of representing layer-wise neighborhood mixing if for any α 0 , α 1 , . . . , α m numbers, there exists a setting of its parameters and an injective mapping f , such that the output of the network becomes equal to</p><formula xml:id="formula_14">f   m j=0 α j σ A j X   (6)</formula><p>for any adjacency matrix A, features X, and activation function σ. </p><formula xml:id="formula_15">= W (1) 2 = · · · = W (1) m = 0.</formula><p>In other words, we utilize only zero-hops in the second layer, setting the zeroth-power weight matrix the following way:</p><formula xml:id="formula_16">W (1) 0 =    α 0 I s0 . . . α m I s0   <label>(7)</label></formula><p>This setting of parameters exactly recover the expression in Equation 6, for any adjacency matrix A and features X.</p><p>We note that the generalized Delta Operator in Definition 2 does not explicitly specify feature differences as in Definition 1; rather, the generalized form defines linear combinations of features (which includes subtraction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Graph Convolution Architectures</head><p>We have discussed a single layer of our model. In practice, one would stack multiple layers and interleave them with standard neural operators such as BatchNorm <ref type="bibr" target="#b11">(Ioffe &amp; Szegedy, 2015)</ref>, element-wise activation, and Dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref>. In this section, we discuss approaches to turning the MixHop GC layer into a MixHop GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Output Layer</head><p>The final layer of a GCN performs a key role for learning the learned latent space of the model on the dataset that is being trained on. As MixHop uniquely mixes features from different sets of information, we theorized that constraining the output layer may result in better outcomes for different tasks. In order to leverage this property, we define our output layer in the following way: We divide s l columns into sets of size c and compute</p><formula xml:id="formula_17">Y O = s l /c k=1 q k H (l) * ,(id l /c : (i+1)s l /c) , then Y O = softmax( Y O ).</formula><p>Here the subscript on H (l) selects c contiguous columns and the scalars q k ∈ [0, 1] define a valid distribution (output of a softmax). This results in the model being forced to choose which features it wants to prioritize by putting more weight on that feature. We obtain the model parameters W (j) i for all i, j and q 1 , . . . q s l c , by minimizing cross-entropy loss, measured only on nodes with known labels i.e. similar to <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning Adjacency Power Architectures</head><p>As mentioned, our model learns multiple weight matrices W (i) j , one per adjacency power used in the model. By default, we set all W (i) j to be the same size, which effectively assigns the same capacity to adjacency powers A j for all j ∈ P . We intuit that different sizes of W (i) j may be more appropriate for different tasks and datasets; as such, we are interested in learning how to automatically size W (i) j . For vanilla GCNs, such an architecture search is relatively inexpensive -the parameters are the number of layers and their widths. In contrast, searching over the architecture space of our model is multiplicatively O(l × |P |) more expensive, as each architecture involves choices on how to divide each layer width s i among the adjacency powers. To address this limitation, we propose using a lasso regularization to automatically learn an architecture for our model <ref type="bibr" target="#b10">(Gordon et al., 2018)</ref>. In particular, we train our architecture in stages: We discuss the learned architectures in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Design</head><p>Given the model described above, a number of natural questions arise. In this section, we aim to design experiments which answer the following hypothesises:</p><p>• H1: The MixHop model learns delta operators.</p><p>• H2: Higher order graph convolutions using neighborhood mixing can outperform existing approaches (e.g. vanilla GCNs) on real semi-supervised learning tasks. • H3: When learning a model architecture for MixHop the best performing architectures differ for each graph. To answer these questions, we design three experiments.</p><p>• Synthetic Experiments: This experiment uses a family of synthetic graphs which allow us to vary the correlation (or homophily) of the edges in a generated graph, and observe how different graph convolutional approaches respond. As homophily is decreased in the network, nodes are more likely to connect to those with different labels, and a model that better captures delta operators should have superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Real-World Experiments: This experiment evaluates</head><p>MixHop's performance on a variety of noisy real world datasets, comparing against challenging baselines. • Model Visualization Experiment: This experiment shows how an appropriately regularized MixHopmodel can learn different, task-dependent, architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We conduct semi-supervised node classification experiments on synthetic and real-world datasets.</p><p>Synthetic Datasets: Our synthetic datasets are generated following <ref type="bibr" target="#b12">Karimi et al. (2017)</ref>. We generate 10 graphs, each with a different homophily coefficient (ranging from 0.0 to 0.9 at 0.1 intervals) that indicates the likelihood of a node forming a connection to a neighbor with the same label. For example, a node in the homophily = 0.9 graph with 10 edges, will have on average 9 edges to a same-label neighbor. All graphs contain 5000 nodes. The features for all synthetic nodes were sampled from overlapping multi-Gaussian distributions. We randomly partition each graph into train, test, and validation node splits, all of equal size. See Appendix for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real World Datasets:</head><p>The experiments with real-world datasets follow the methodology proposed in <ref type="bibr" target="#b23">Yang et al. (2016)</ref>. In addition to using the classic dataset split, (which have 20 samples per label), we evaluate against against a set of random splits with 100 samples per label. We will release our test splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training</head><p>For all experiments, we construct a 2-layer network of our model using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. We train our models using a Gradient Descent optimizer for a maximum of 2000 steps, with an initial learning rate of 0.05 that decays by 0.0005 every 40 steps. We terminate training if validation accuracy does not improve for 40 consecutive steps; as a result, most runs finish in less than 200 steps. We use 5 × 10 −4 L2 regularization on the weights, and dropout input and hidden layers. We note that the citation datasets are extremely sensitve to initializations; as such, we run all models 100 times, sort by the validation accuracy, and finally report the test accuracy for the top 50 runs. For all models we ran (our models in <ref type="table" target="#tab_0">Tables 1 &amp; 3</ref>, and all models in <ref type="table">Table  3</ref>), we use a latent dimension of 60; Our default architecture evenly divided 60 dimensions are divided evenly to all |P | powers. Our learned architectures spread them unevenly, see Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on Synthetic Graphs</head><p>We present our results on the synthetic datasets in <ref type="figure">Figure  4</ref>. We show average accuracy for each baseline against the homophily of the graph. We use a dense (MLP) model that does not ingest any adjacency information as a control. As expected, all models perform better as the homophily of the synthetic graph increases. At low levels of homophily, when nodes are rarely adjacent to neighbors with the same label, we observe that MixHop performs significantly better than the most competitive baseline. Interestingly, we notice that the GAT model performs significantly worse than the features-only control. This suggests that the added attention mechanism of the GAT model relies heavily on homophily in node neighborhoods.</p><p>For each level of homophily, we measured the number of delta operators learned by our model. We present these metrics in <ref type="figure" target="#fig_2">Figure 3</ref>. We observe that for low levels of homophily, our model uses 2.5X of its model capacity on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Citeseer Cora Pubmed ManiReg <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref> 60.1 59.5 70.7 SemiEmb <ref type="bibr" target="#b22">(Weston et al., 2012)</ref> 59.6 59.0 71.1 LP <ref type="bibr" target="#b25">(Zhu et al., 2003)</ref> 45.3 68.0 63.0 DeepWalk <ref type="bibr" target="#b18">(Perozzi et al., 2014)</ref> 43.2 67.2 65.3 ICA <ref type="bibr" target="#b17">(Lu &amp; Getoor, 2003)</ref> 69.1 75.1 73.9 Planetoid <ref type="bibr" target="#b23">(Yang et al., 2016)</ref> 64.7 75.7 77.2 Vanilla GCN <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref> 70  learning delta operators compared with higher homophily. This follows intuition: as the nodes cluster around likelabeled neighbors, the need to identify meaningful feature differences between neighbors at different distances drops significantly. These results strongly suggest that the learned delta operators play a role in the success of MixHop in <ref type="figure">Figure 4</ref>. For this experiment, we trained our model over the synthetic datasets under one constraint: input layer weights W (j) 0 are shared across all powers j ∈ P . This allows us to examine sub-columns in the following layer W (j) 1 . Specifically, we count the number of times a feature, coming out of the first layer, is assigned values of opposite signs in W (j) 1 . We restrict the analysis to only values of W (j) 1 with magnitude larger than the median in the corresponding column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Node Classification Results</head><p>We show two sets of semi-supervised node classfication results using different splits of our datasets. Because these datasets are taken from the real world, they are inherently noisy, and it is unlikely that achieving 100% classification accuracy is possible even when given a significant amount of labeled training data. Instead, we are interested in the sparse classification task, namely how well our model is able to improve on previous work while being resilient to noise, even with limited information.</p><p>In <ref type="table" target="#tab_4">Table 2</ref>, we demonstrate how our model performs on common splits taken from <ref type="bibr" target="#b23">Yang et al. (2016)</ref>. Accuracy numbers above double-line are copied from <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>. Numbers below the double-line are our methods, with P = {1} being equivalent to vanilla GCNs. ± represents the standard deviation of 50 runs with different random initializations. All MixHop models are of same capacity. These splits utilize only 20 labeled nodes per class during training. We achieve a test accuracy of 71.4%, 81.9%, and 80.8% on Citeseer, Cora, and Pubmed respectively. Interestingly, for Citeseer, we see that the learned architecture was equal to the original architecture (and so the models performed the same). In <ref type="table">Table 3</ref>, we demonstrate how our model performs using random splits with more training information available. These splits utilize 100 nodes per class during training. We achieve a test accuracy of 77.0%, 87.2%, and 83.9% on Citeseer, Cora, and Pubmed respectively.</p><p>As MixHop is able to pull in linear combinations of features from farther distances, it can extract meaningful signals in extremely sparse settings. We believe this explains why Mix-Hop outperforms baseline methods in both sets of dataset  <ref type="table">Table 3</ref>: Classification results on random partitions of <ref type="bibr" target="#b23">(Yang et al., 2016)</ref> datasets. splits. The results of these experiments confirm our hypothesis (H2) that higher order graph convolution methods with neighborhood mixing can outperform existing methods on real datasets.</p><formula xml:id="formula_18">X 500 n=19717 × A 0 × 500 17 W (1) 0 × A 1 × 23 W (1) 1 × A 2 × 20 W (1) 2 n 60 × A 0 × 60 3 W (2) 0 × A 1 × 6 W (2) 1 × A 2 × 3 W (2) 2 n 12 (a) Pubmed X 1433 n=2708 × A 0 × 1433 24 W (1) 0 × A 1 × 18 W (1) 1 × A 2 × 18 W (1) 2 n 60 × A 0 × 60 0 W (2) 0 × A 1 × 7 W (2) 1 × A 2 × 7 W (2) 2 n 14 (b) Cora</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Visualizing Learned Architectures</head><p>Figure 5 depicts the learned architectures for two of the citation datasets. We note that each dataset prefers its own architecture. For example, Cora prefers to have zero-capacity on the 0th power of the adjacency matrix (effectively ignoring the features of each node) in the second layer. Not shown (for space reasons) is Citeseer, which prefers the default parameter settings with the same weight capacity across all powers. All three real datasets had different final architectures, which confirms our hypothesis (H3) that different architectures are optimal for different graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>? uses adjacency powers but for embedding learning. Abu-El-Haija et al. <ref type="formula" target="#formula_1">(2018)</ref>; <ref type="bibr" target="#b2">Atwood &amp; Towsley (2016)</ref> use adjacency powers for feature propagation on graphs, but they combine the powers at the end of the network (right before classification), and <ref type="bibr" target="#b16">Lee et al. (2018)</ref> combine them at the input. We intermix information from the powers layerwise, enabling our method to learn neighborhood mixing e.g. delta operators, which contrast the features of immediate neighbors from those further away. <ref type="bibr" target="#b7">Defferrard et al. (2016)</ref> uses more Chebyshev polynomials (i.e. higher-rank) Graph Convolution, but their model underperforms our baseline <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>, allowing us to hypothesize that message passing along edges outperforms explicit alignment onto the graph Fourier Basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we analyzed the expressive power of popular methods for semi-supervised learning with Graph Neural Networks and we showed they cannot learn general neighborhood mixing functions. To address this, we have proposed a graph convolutional layer that utilizes multiple powers of the adjacency matrix. Repeated application of this layer allows a model to learn general mixing of neighborhood information, including averaging and delta operators in the feature space, without additional memory or computational complexity. Utilizing L2 group lasso regularization on these stacked layers allows us to learn a unique architecture that is optimized for each dataset. Our experimental results showed that higher order graph convolution methods can achieve state of the art performance on several node classification tasks. Our analysis of the experimental results showed that neighborhood difference operators are especially useful in graphs which do not have high homophily (correlation between edges and labels). While we focused this paper on applying our proposal to the most popular models for graph convolution, it is possible to implement our method in more sophisticated frameworks including the recent GAT <ref type="bibr" target="#b21">(Velickovic et al., 2018)</ref>. Other recent work like <ref type="bibr" target="#b24">(Ying et al., 2018)</ref>, which focuses on hierarchical pooling for community-aware graph representation might also be extended to use general neighborhood mixing layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Vanilla GC layer (a), using adjacency A, versus our GC layer (b), using powers of A. Orange denotes an input activation matrix, with one row per node; green denotes the trainable parameters; and red denotes the layer output. Left vs right-multiplication is specified by the relative position of the multiplicand to the × operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Amount of model capacity devoted to learning delta operators at different levels of homophily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Learned MixHop Architectures. Note how different parameter sizes (green boxes) are learned for the two datasets. For example, Group-Lasso regularization on Cora removes all capacity for the zeroth power in the second GC layer. For space, all matrices are plotted transposed and output layer (Section 4.1) has been ommitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Theorem 1</head><label>1</label><figDesc>The vanilla GCN defined by Equation 2 is not capable of representing two-hop Delta Operators. MixHop GCN (using layers defined in Equation 4) can represent two-hop Delta Operators.</figDesc><table><row><cell>Theorem 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Theorem 3 GCNs defined using Equation 1 are not capable of representing general layer-wise neighborhood mixing.</figDesc><table><row><cell>Theorem 4 GCNs defined using our proposed method</cell></row><row><cell>(Equation 4) are capable of representing general layer-wise</cell></row><row><cell>neighborhood mixing.</cell></row><row><cell>(0)</cell></row><row><cell>j</cell></row><row><cell>(1)</cell></row><row><cell>1</cell></row></table><note>Proof of Theorem 3. This trivially follows from Theorem 1: if the vanilla GCN cannot recover a two-hop Delta Operator, defined in Equation 3, it cannot recover the Delta Operator generalization in Equation 6. Proof of Theorem 4. The proof steps closely resemble the proof of Theorem 2. Our GCN with P = {0, . . . , m} can represent the target function, by setting the first layer weight matrices as W= I s0 , ∀j ∈ P and setting all but the zeroth second layer weight matrices as W</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>MixHop with P = {1} (baseline) 70.7±0.73 81.1±0.84 79.9±0.78 MixHop: default architecture (ours) 71.4±0.81 81.8±0.62 80.0±1.1 MixHop: learned architecture (ours) 71.4±0.81 † 81.9±0.40 80.8±0.58 Experiments run on Node Classification citation datasets created by<ref type="bibr" target="#b23">Yang et al. (2016)</ref>. † The learned architecture for Citeseer is equivalent to default architecture, so the results are the same.</figDesc><table><row><cell>.3</cell><cell>81.5</cell><cell>79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics. Numbers of nodes (n), edges (m), features, classes (c), and labeled nodes (|Y P I | from the Planetoid splits, |Y R I | from our random splits).</figDesc><table><row><cell>Accuracy</cell></row><row><cell>Test</cell></row><row><cell>Homophily</cell></row><row><cell>Figure 4: Synthetic dataset results. MLP does not utilize</cell></row><row><cell>graph with (homophilic) edges, but only node features.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>0±1.1 78.3±0.54 Chebyshev<ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref> 74.2±0.5 85.5±0.4 81.8±0.5 Vanilla GCN<ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref> 76.7±0.43 86.1±0.34 82.2±0.29 GAT (Velickovic et al., 2018) 74.8±0.42 83.0±1.1 81.8±0.18 MixHop: default architecture (ours) 76.3±0.41 87.0±0.51 83.6±0.68 MixHop: learned architecture (ours) 77.0±0.54 87.2±0.32 83.8±0.44</figDesc><table><row><cell>Model</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>2-Layer MLP</cell><cell>70.6±1</cell><cell>69.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Information Sciences Institute, University of Southern California 2 Google AI, New York. Correspondence to: Sami &lt;sami@haija.org&gt;, Bryan &lt;bperozzi@acm.org&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors acknowledge support from the Defense Advanced Research Projects Agency (DARPA) under award FA8750-17-C-0106, and acknowledge discussions with Jesse Dodge and Leto Peel, respectively, on Group Lasso regularization and synethetic experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>N-Gcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLG KDD Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-dimensional spectral analysis of cortical receptive field profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Research</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of the Optical Society of America</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visibility of minorities in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Genois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<idno>arxiv/1702.00150</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Higher-order graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<idno>arxiv/1809.07697</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering communities and anomalies in attributed graphs: Interactive visual exploration and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<idno>24:1- 24:40</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003-01" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Link-based classification</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeplearning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
