<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
							<email>zhaofanqiu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating 3 × 3 × 3 convolutions with 1 × 3 × 3 convolutional filters on spatial domain (equivalent to 2D CNN) plus 3 × 1 × 1 convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today's digital contents are inherently multimedia: text, audio, image, video and so on. Images and videos, in particular, become a new way of communication between In- * This work was performed when Zhaofan Qiu was visiting Microsoft Research as a research intern. The codes and model of our P3D ResNet are publicly available at: https://github.com/ZhaofanQiu/pseudo-3d-residual-networks ternet users with the proliferation of sensor-rich mobile devices. This has encouraged the development of advanced techniques for a broad range of multimedia understanding applications. A fundamental progress that underlies the success of these technological advances is representation learning. Recently, the rise of Convolutional Neural Networks (CNN) convincingly demonstrates high capability of learning visual representation especially in image domain. For instance, an ensemble of residual nets <ref type="bibr" target="#b5">[7]</ref> achieves 3.57% top-5 error on the ImageNet test set, which is even lower than 5.1% of the reported human-level performance. Nevertheless, video is a temporal sequence of frames with large variations and complexities, resulting in difficulty in learning a powerful and generic spatio-temporal representation. One natural way to encode spatio-temporal information in videos is to extend the convolution kernels in CNN from 2D to 3D and train a brand new 3D CNN. As such, the networks have access not only the visual appearance present in each video frame, but also the temporal evolution across consecutive frames. While encouraging performances are reported in recent studies <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b31">33]</ref>, the training of 3D CNN is very computationally expensive and the model size also has a quadratic growth compared to 2D CNN. Take a widely adopted 11-layer 3D CNN, i.e., C3D <ref type="bibr" target="#b29">[31]</ref> networks, as an example, the model size reaches 321MB which is even larger than that (235MB) of a 152-layer 2D ResNet (ResNet-152) <ref type="bibr" target="#b5">[7]</ref>, making it extremely difficult to train a very deep 3D CNN. More importantly, directly fine-tuning ResNet-152 with frames in Sports-1M dataset <ref type="bibr" target="#b8">[10]</ref> may achieve better accuracy than C3D trained on videos from scratch as shown in <ref type="figure">Figure 1</ref>. Another alternative solution of producing spatio-temporal video representation is to utilize pooling strategy or Recurrent Neural Networks (RNN) over the representations of frames, which are often the activations of the last pooling layer or fully-connected layer in a 2D CNN. This category of approaches, however, only build temporal connections on the high-level features at the top layer while leaving the correlations in the low-level forms, e.g., edges at the bottom layers, not fully exploited.</p><p>We demonstrate in this paper that the above limitations can be mitigated by devising a family of bottleneck building blocks that leverages both spatial and temporal convolutional filters. Specifically, the key component in each block is a combination of one 1 × 3 × 3 convolutional layer and one layer of 3 × 1 × 1 convolutions in a parallel or cascaded fashion, that takes the place of a standard 3 × 3 × 3 convolutional layer. As such, the model size is significantly reduced and the advantages of pre-learnt 2D CNN in image domain could also be fully leveraged by initializing the 1 × 3 × 3 convolutional filters with 3 × 3 convolutions in 2D CNN. Furthermore, we propose a novel Pseudo-3D Residual Net (P3D ResNet) that composes each designed block in different placement throughout a whole ResNet-like architecture to enhance the structural diversity of the network. As a result, the temporal connections in our P3D ResNet are constructed at every level from bottom to top and the learnt video representations encapsulate information related to objects, scenes and actions in videos, making them generic for various video analysis tasks.</p><p>The main contribution of this work is the proposal of a family of bottleneck building blocks that simulates 3D convolutions in an economic and effective way. This also leads to the elegant view of how different blocks should be placed for learning very deep networks and a new P3D ResNet is presented for video representation learning. Through an extensive set of experiments, we demonstrate that our P3D ResNet outperforms several state-of-the-art models on five different benchmarks and three different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly group the methods for video representation learning into two categories: hand-crafted and deep learning-based methods.</p><p>Hand-crafted representation learning methods usually start by detecting spatio-temporal interest points and then describe these points with local representations. In this scheme, Space-Time Interest Points (STIP) <ref type="bibr" target="#b13">[15]</ref>, Histogram of Gradient and Histogram of Optical Flow <ref type="bibr" target="#b14">[16]</ref>, 3D Histogram of Gradient <ref type="bibr" target="#b9">[11]</ref> and SIFT-3D <ref type="bibr" target="#b21">[23]</ref> are proposed by extending representations from image domain to measure the temporal dimension of 3D volumes. Recently, Wang et al. propose dense trajectory features, which densely sample local patches from each frame at different scales and then track them in a dense optical flow field <ref type="bibr" target="#b32">[34]</ref>.</p><p>The most recent approaches for video representation learning are to devise deep architectures. Karparthy et al. stack CNN-based frame-level representations in a fixed size of windows and then leverage spatio-temporal convolutions for learning video representation <ref type="bibr" target="#b8">[10]</ref>. In <ref type="bibr" target="#b23">[25]</ref>, the famous two-stream architecture is devised by applying two CNN architectures separately on visual frames and staked optical flows. This architecture is further extended by exploiting multi-granular structure <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21]</ref>, convolutional fusion <ref type="bibr" target="#b4">[6]</ref>, key-volume mining <ref type="bibr" target="#b37">[39]</ref> and temporal segment networks <ref type="bibr" target="#b34">[36]</ref> for video representation learning. In the work by Wang et al. <ref type="bibr" target="#b33">[35]</ref>, the local ConvNet responses over the spatio-temporal tubes centered at the trajectories are pooled as the video descriptors. Fisher Vector <ref type="bibr" target="#b18">[20]</ref> is then used to encode these local descriptors to a global video representation. Recently, the LSTM-RNN networks have been successfully employed for modeling temporal dynamics in videos. In <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b35">37]</ref>, temporal pooling and stacked LSTM network are leveraged to combine frame-level (optical flow images) representation and discover long-term temporal relationships for learning a more robust video representation. Srivastava et al. <ref type="bibr" target="#b26">[28]</ref> further formulate the video representation learning task as an autoencoder model based on the encoder and decoder LSTMs.</p><p>It can be observed that most aforementioned deep learning-based methods treat video as a frame/optical flow image sequence for video representation learning while leaving the temporal evolution across consecutive frames not fully exploited. To tackle this problem, 3D CNN proposed by Ji et al. <ref type="bibr" target="#b6">[8]</ref> is one of the earlier works to directly learn the spatio-temporal representation of a short video clip. Later in <ref type="bibr" target="#b29">[31]</ref>, Tran et al. devise a widely adopted 11layer 3D CNN (C3D) for learning video representation over 16-frame video clips in the context of large-scale supervised video datasets, and temporal convolutions across longer clips (100 frames) are further exploited in <ref type="bibr" target="#b31">[33]</ref>. However, the capacity of existing 3D CNN architectures is extremely limited with expensive computational cost and memory demand, making it hard to train a very deep 3D CNN. Our method is different that we not only propose the idea of simulating 3D convolutions with 2D spatial convolutions plus 1D temporal connections which is more economic, but also integrate this design into a deep residual learning framework for video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">P3D Blocks and P3D ResNet</head><p>In this section we firstly define the 3D convolutions for video representation learning which can be naturally decoupled into 2D spatial convolutions to encode spatial information and 1D temporal convolutional filters for temporal dimension. Then, a new family of bottleneck building blocks, namely Pseudo-3D (P3D), to leverage both spatial and temporal convolutional filters is devised in the residual learning framework. Finally, we develop a novel Pseudo-3D Residual Net (P3D ResNet) composing each P3D block at different placement in ResNet-like architecture and further compare its several variants through experimental studies in terms of both performance and time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Convolutions</head><p>Given a video clip with the size of c × l × h × w where c, l, h and w denotes the number of channels, clip length, height and width of each frame, respectively, the most natural way to encode the spatio-temporal information is to utilize 3D convolutions <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b29">31]</ref>. 3D convolutions simultaneously model the spatial information like 2D filters and construct temporal connections across frames. For simplicity, we denote the size of 3D convolutional filters as d × k × k where d is the temporal depth of kernel and k is the kernel spatial size. Hence, suppose we have 3D convolutional filters with size of 3 × 3 × 3, it can be naturally decoupled into 1×3×3 convolutional filters equivalent to 2D CNN on spatial domain and 3 × 1 × 1 convolutional filters like 1D CNN tailored to temporal domain. Such decoupled 3D convolutions can be regarded as a Pseudo 3D CNN, which not only reduces the model size significantly, but also enables the pre-training of 2D CNN from image data, endowing Pseudo 3D CNN more power of leveraging the knowledge of scenes and objects learnt from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pseudo-3D Blocks</head><p>Inspired by the recent successes of Residual Networks (ResNet) <ref type="bibr" target="#b5">[7]</ref> in numerous challenging image recognition tasks, we develop a new family of building modules named Pseudo-3D (P3D) blocks to replace 2D Residual Units in ResNet, pursuing spatio-temporal encoding in ResNet-like architectures for videos. Next, we will recall the basic design of Residual Units in ResNet, followed by presenting how to devise our P3D blocks. The bottleneck building architecture on each P3D block is finally elaborated.</p><p>Residual Units. ResNet consists of many staked Residual Units and each Residual Unit could be given by</p><formula xml:id="formula_0">xt+1 = h (xt) + F (xt) ,<label>(1)</label></formula><p>where x t and x t+1 denote the input and output of the tth Residual Unit, h (x t ) = x t is an identity mapping and F is a non-linear residual function. Hence, Eq.(1) can be rewritten as</p><formula xml:id="formula_1">(I + F) · xt = xt + F · xt := xt + F (xt) = xt+1, (2)</formula><p>where F · x t represents the result of performing residual function F over x t . The main idea of ResNet is to learn the additive residual function F with reference to the unit inputs x t which is realized through a shortcut connection, instead of directly learning unreferenced non-linear functions. P3D Blocks design. To develop each 2D Residual Unit in ResNet into 3D architectures for encoding spatiotemporal video information, we modify the basic Residual Unit in ResNet following the principle of Pseudo 3D as introduced in Section 3.1 and devise several Pseudo-3D Blocks. The modification is not straightforward for involvement of two design issues. The first issue is about whether the modules of 2D filters on spatial dimension (S) and 1D filters on temporal domain (T) should directly or indirectly influence each other. Direct influence within the two types of filters means that the output of spatial 2D filters is connected as the input to the temporal 1D filters (i.e., in a cascaded manner). Indirect influence between the two filters decouples the connection such that each kind of filters is on a different path of the network (i.e., in a parallel fashion). The second issue is whether the two kinds of filters should both directly influence the final output. As such, direct influence in this context denotes that the output of each type of filters should be directly connected to the final output.</p><p>Based on the two design issues, we derive three different P3D blocks as depicted in <ref type="figure" target="#fig_0">Figure 2</ref>, respectively, named as P3D-A to P3D-C. Detailed comparisons about their architectures are provided as following:</p><p>(1) P3D-A: The first design considers stacked architecture by making temporal 1D filters (T) follow spatial 2D filters (S) in a cascaded manner. Hence, the two kinds of filters can directly influence each other in the same path and only the temporal 1D filters are directly connected to the final output, which could be generally given by</p><formula xml:id="formula_2">(I + T · S) · xt := xt + T (S (xt)) = xt+1.<label>(3)</label></formula><p>(2) P3D-B: The second design is similar to the first one except that indirect influence between two filters are adopted and both filters are at different pathways in a parallel fashion. Although there is no direct influence between S and T, both of them are directly accumulated into the final output, which could be expressed as shortcut connection from S to the final output, making the output x t+1 as</p><formula xml:id="formula_3">(I + S + T) · xt := xt + S (xt) + T (xt) = xt+1.</formula><formula xml:id="formula_4">(I + S + T · S) · xt := xt + S (xt) + T (S (xt)) = xt+1. (5)</formula><p>Bottleneck architectures. When specifying the architecture of 2D Residual Unit, the basic 2D block is modified with a bottleneck design for reducing the computation complexity. In particular, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a), instead of a single spatial 2D filters (3 × 3 convolutions), the Residual Unit adopts a stack of 3 layers including 1 × 1, 3 × 3, and 1 × 1 convolutions, where the first and last 1 × 1 convolutional layers are applied for reducing and restoring dimensions of input sample, respectively. Such bottleneck design makes the middle 3 × 3 convolutions as a bottleneck with smaller input and output dimensions. Thus, we follow this elegant recipe and utilize the bottleneck design to implement our proposed P3D blocks. Similar in spirit, for each P3D block which purely consists of one spatial 2D filters (1×3×3 convolutions) and one temporal 1D filters (3×1×1 convolutions), we additionally place two 1 × 1 × 1 convolutions at both ends of the path, which are responsible for reducing and then increasing the dimensions. Accordingly, the dimensions of the input and output of both the spatial 2D and temporal 1D filters are reduced with this bottleneck design. The detailed bottleneck building architectures on all the three P3D blocks are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(b) to 3(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pseudo-3D ResNet</head><p>In order to verify the merit of the three P3D blocks, we first develop three P3D ResNet variants, i.e., P3D-A ResNet, P3D-B ResNet and P3D-C ResNet by replacing all the Residual Units in a 50-layer ResNet (ResNet-50) <ref type="bibr" target="#b5">[7]</ref> with one certain kind of P3D block, respectively. The comparisons of performance and time efficiency between the basic ResNet-50 and the three P3D ResNet variants are presented. Then, a complete version of P3D ResNet is proposed by mixing all the three P3D blocks from the viewpoint of structural diversity. Comparisons between P3D ResNet variants. The comparisons are conducted on UCF101 <ref type="bibr" target="#b25">[27]</ref> video action recognition dataset. Specifically, the architecture of ResNet-50 is fine-tuned on UCF101 videos. We set the input as 224×224 image which is randomly cropped from the resized 240 × 320 video frame. Moreover, following <ref type="bibr" target="#b34">[36]</ref>, we freeze the parameters of all Batch Normalization layers except for the first one and add an extra dropout layer with 0.9 dropout rate to reduce the effect of over-fitting. After fine-tuning ResNet-50, the networks will predict one score for each frame and the video-level prediction score is calculated by averaging all frame-level scores. The architectures of three P3D ResNet variants are all initialized with ResNet-50 except for the additional temporal convolutions and are further fine-tuned on UCF101. For each P3D ResNet variant, the dimension of input video clip is set as 16 × 160 × 160 which is randomly cropped from the resized non-overlapped 16-frame clip with the size of 16 × 182 × 242. Each frame/clip is randomly horizontally flipped for augmentation. In the training, we set each minibatch as 128 frames/clips, which are implemented with multiple GPUs in parallel. The network parameters are optimized by standard SGD and the initial learning rate is set as 0.001, which is divided by 10 after every 3K iterations. The training is stopped after 7.5K iterations. <ref type="table" target="#tab_1">Table 1</ref> shows the performance and time efficiency of ResNet-50 and our Pseudo-3D ResNet variants on UCF101. Overall, all the three P3D ResNet variants (i.e., P3D-A ResNet, P3D-B ResNet and P3D-C ResNet) exhibit better performance than ResNet-50 with only a small increase in  Mixing different P3D Blocks. Further inspired from the recent success of pursuing structural diversity in the design of very deep networks <ref type="bibr" target="#b36">[38]</ref>, we devise a complete version of P3D ResNet by mixing different P3D blocks in the architecture to enhance structural diversity, as depicted in <ref type="figure" target="#fig_2">Figure  4</ref>. Particularly, we replace Residual Units with a chain of our P3D blocks in the order P3D-A→P3D-B→P3D-C. Table 1 also details the performance and speed of the complete P3D ResNet. By additionally pursuing structural diversity, P3D ResNet makes the absolute improvement over P3D-A ResNet, P3D-B ResNet and P3D-C ResNet by 0.5%, 1.4% and 1.2% in accuracy respectively, indicating that enhancing structural diversity with going deep could improve the power of neural networks.</p><formula xml:id="formula_5">P3D-A P3D-B P3D-C P3D-A P3D-B P3D-C ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spatio-Temporal Representation Learning</head><p>We further validate the complete design of our P3D ResNet on a deeper 152-layer ResNet <ref type="bibr" target="#b5">[7]</ref> and then produce a generic spatio-temporal video representation. The learning of P3D ResNet here was conducted on Sports-1M dataset <ref type="bibr" target="#b8">[10]</ref>, which is one of the largest video classification benchmark. It roughly contains about 1.13 million videos annotated with 487 Sports labels. There are 1K-3K videos per label and approximately 5% of the videos are with more than one label. Please also note that about 9.2% video URLs were dead when we downloaded the videos. Hence, we conducted the experiments on the remaining 1.02 million videos and followed the official split, i.e., 70%, 10% and 20% for training, validation and test set, respectively.</p><p>Network Training. For efficient training on the large Sports-1M training set, we randomly select five 5-second short videos from each video in the set. During training, the settings of data augmentation and mini-batch are the same as those in Section 3.3 except that the dropout rate is set as 0.1. The learning rate is also initialized as 0.001, and divided by 10 after every 60K iterations. The optimization will be complete after 150K batches.</p><p>Network Testing. We evaluate the performance of the learnt P3D ResNet by measuring video/clip classification accuracy on the test set. Specifically, we randomly sample 20 clips from each video and adopt a single center crop per clip, which is propagated through the network to obtain a clip-level prediction score. The video-level score is computed by averaging all the clip-level scores of a video.</p><p>We compare the following approaches for performance evaluation: (1) Deep Video (Single Frame) and (Slow Fusion) <ref type="bibr" target="#b8">[10]</ref>. The former performs a CNN which is similar to the architecture in <ref type="bibr" target="#b12">[14]</ref> on one single frame from each clip to predict a clip-level score and fuses multiple frames in each clip with different temporal extent throughout the network to achieve the clip-level prediction. (2) Convolutional Pooling <ref type="bibr" target="#b35">[37]</ref> exploits max-pooling over the final convolutional layer of GoogleNet <ref type="bibr" target="#b28">[30]</ref> across each clip's frames.</p><p>(3) C3D <ref type="bibr" target="#b29">[31]</ref> utilizes 3D convolutions on a clip volume to model the temporal information and the whole architecture could be trained on Sports-1M dataset from scratch or finetuned from the pre-trained model on I380K internal dataset collected in <ref type="bibr" target="#b29">[31]</ref>. (4) ResNet-152 <ref type="bibr" target="#b5">[7]</ref>. In this run, a 152layer ResNet is fine-tuned and employed on one frame from each clip to produce a clip-level score.</p><p>The performances and comparisons are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Overall, our P3D ResNet leads to a performance boost against ResNet-152 (2D CNN) and C3D (3D CNN) by 1.8% and 5.3% in terms of top-1 video-level accuracy, respectively. The results basically indicate the advantage of exploring spatio-temporal information by decomposing 3D learning into 2D convolutions in spatial space and 1D operations in temporal dimension. As expected, Deep Video (Slow Fusion) fusing temporal information throughout the networks exhibits better performance than Deep Video (Sin- convolutions improves Deep Video (Slow Fusion). This somewhat indicates that P3D ResNet is benefited from the principle of structural diversity in network design. It is also not surprise that the performances of P3D ResNet are still lower than Convolutional Pooling which performs temporal pooling on 120 frames' clips with frame rate of 1 fps, making the clip length over 120s. In contrast, we take 16 consecutive frames as a basic unit which only covers less than 0.5s but has strong spatio-temporal connections, making our P3D ResNet with better generalization capability. <ref type="figure" target="#fig_3">Figure 5</ref> further visualizes the insights in the learnt P3D ResNet model. Following <ref type="bibr" target="#b34">[36]</ref>, we adopt DeepDraw toolbox [1], which conducts iterative gradient ascent on the input clip of white noises. During learning, it evaluates the model's violation of class label and back-propagates the gradients to modify the input clip. Thus, the final generated input clip could be regarded as the visualization of class knowledge inside P3D ResNet. We select four categories, i.e., tai chi, horizontal bar, motorcycle racing and boxing, for visualization. As illustrated in the figure, P3D ResNet model could capture both spatial visual patterns and temporal motion. Take the category of tai chi as an example, our model generates a video clip in which a person is displaying different poses, depicting the process of this action.</p><p>P3D ResNet Representation. After training our P3D ResNet architecture on Sports-1M dataset, the networks could be utilized as a generic representation extractor for any video analysis tasks. Given a video, we select 20 video clips and each clip is with 16-frame long. Each video clip is then input into the learnt P3D ResNet architecture and the 2,048 dimensional activations of pool5 layer are output as the representation of this clip. Finally, all the clip-level representations in a video are averaged to produce a 2,048 dimensional video representation. We refer to this representation as P3D ResNet representation in the following evaluations unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Video Representation Evaluation</head><p>Next, we evaluate our P3D ResNet video representation on three different tasks and five popular datasets, i.e., UCF101 <ref type="bibr" target="#b25">[27]</ref>, ActivityNet <ref type="bibr" target="#b0">[2]</ref>, ASLAN <ref type="bibr" target="#b11">[13]</ref>, YUPENN <ref type="bibr" target="#b1">[3]</ref> and Dynamic Scene <ref type="bibr" target="#b22">[24]</ref>. UCF101 and ActivityNet are two of the most popular video action recognition benchmarks. UCF101 consists of 13,320 videos from 101 action categories. Three training/test splits are provided by the dataset organisers and each split in UCF101 includes about 9.5K training and 3.7K test videos. The ActivityNet dataset is a large-scale video benchmark for human activity understanding. The latest released version of the dataset (v1.3) is exploited, which contains 19,994 videos from 200 activity categories. The 19,994 videos are divided into 10,024, 4,926 and 5,044 videos for training, validation and test set, respectively. It is also worth noting that the labels of test set are not publicly available and thus the performances on ActivityNet dataset are all reported on validation set. ASLAN is a dataset on action similarity labeling task, which is to predict the similarity between videos. The dataset includes 3,697 videos from 432 action categories. We follow the strategy of 10-fold cross validation with the official data splits on this set. Furthermore, YUPENN and Dynamic Scene are two sets for the scenario of scene recognition. In between, YUPENN is comprised of 14 scene categories each containing 30 videos. Dynamic Scene consists of 13 scene classes with 10 videos per class. The training and test procedures on both datasets follow the standard leave-one-video-out protocol.</p><p>Comparison with the state-of-the-art. We first compare with several state-of-the-art techniques in the context of video action recognition on three splits of UCF101 and ActivityNet validation set. The performance comparisons are summarized in <ref type="table">Table 3</ref> and 4, respectively. We briefly group the approaches on UCF101 into three categories: end-to-end CNN architectures which are fine-tuned on UCF101 in the upper rows, CNN-based video representation extractors with linear SVM classifier in the middle rows and approaches fused with IDT in the bottom rows. It is worth noting that most recent end-to-end CNN architectures on UCF101 often employ and fuse two or multiple types of inputs, e.g., frame, optical flow or even audio. Hence, the performances by exploiting only video frames and late fusing the scores on two inputs of video frames plus optical flow are both reported. As shown in <ref type="table">Table 3</ref>, <ref type="table">Table 3</ref>. Performance comparisons with the state-of-the-art methods on UCF101 (3 splits). TSN: Temporal Segment Networks <ref type="bibr" target="#b34">[36]</ref>; TDD: Trajectory-pooled Deep-convolutional Descriptor <ref type="bibr" target="#b33">[35]</ref>; IDT: Improved Dense Trajectory <ref type="bibr" target="#b32">[34]</ref>. We group the approaches into three categories, i.e., end-to-end CNN architectures which are fine-tuned on UCF101 at the top, CNN-based video representation extractors with linear SVM classifier in the middle and approaches fused with IDT at the bottom. For the methods in the first direction, we report the performance of only taking frames and frames plus optical flow (in brackets) as inputs, respectively.</p><p>Method Accuracy End-to-end CNN architecture with fine-tuning Two-stream ConvNet <ref type="bibr" target="#b23">[25]</ref> 73.0% (88.0%) Factorized ST-ConvNet <ref type="bibr" target="#b27">[29]</ref> 71.3% (88.1%) Two-stream + LSTM <ref type="bibr" target="#b35">[37]</ref> 82.6% (88.6%) Two-stream fusion <ref type="bibr" target="#b4">[6]</ref> 82.6% (92.5%) Long-term temporal ConvNet <ref type="bibr" target="#b31">[33]</ref> 82.4% (91.7%) Key-volume mining CNN <ref type="bibr" target="#b37">[39]</ref> 84.5% (93.1%) ST-ResNet <ref type="bibr" target="#b2">[4]</ref> 82.2% (93.4%) TSN <ref type="bibr" target="#b34">[36]</ref> 85.7% (94.0%) CNN-based representation extractor + linear SVM C3D <ref type="bibr" target="#b29">[31]</ref> 82.3% ResNet-152 83.5% P3D ResNet 88.6% Method fusion with IDT IDT <ref type="bibr" target="#b32">[34]</ref> 85.9% C3D + IDT <ref type="bibr" target="#b29">[31]</ref> 90.4% TDD + IDT <ref type="bibr" target="#b33">[35]</ref> 91.5% ResNet-152 + IDT 92.0% P3D ResNet + IDT 93.7%</p><p>the accuracy of P3D ResNet can achieve 88.6%, making the absolute improvement over the best competitor TSN on the only frame input and ResNet-152 in the first and second category by 2.9% and 5.1%, respectively. Compared to <ref type="bibr" target="#b35">[37]</ref> which operates LSTM over high-level representations of frames to explore temporal information, P3D ResNet is benefited from the temporal connections throughout the whole architecture and outperforms <ref type="bibr" target="#b35">[37]</ref>. P3D ResNet with only frame input is still superior to <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b35">37]</ref> when fusing the results on the inputs of both frame and optical flow.</p><p>The results also consistently indicate that fusing two kinds of inputs (performances in brackets) leads to apparent improvement compared to only using video frames. This motivates us to learn P3D ResNet architecture with other types of inputs in our future works. Furthermore, P3D ResNet utilizing 2D spatial convolutions plus 1D temporal convolutions exhibits significantly better performance than C3D which directly uses 3D spatio-temporal convolutions. By combining with IDT <ref type="bibr" target="#b32">[34]</ref> which are hand-crafted features, the performance will boost up to 93.7%. In addition, by performing the recent state-of-the-art encoding method <ref type="bibr" target="#b20">[22]</ref> on the activations of res5c layer in P3D ResNet, the accuracy can achieve 90.5%, making the improvement over the representation from pool5 layer in P3D ResNet by 1.9%.</p><p>The results across different evaluation metrics constantly indicate that video representation produced by our P3D ResNet attains a performance boost against baselines on ActivityNet validation set, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Specifically, P3D ResNet outperforms IDT, C3D, VGG 19 and ResNet-152 by 10.4%, 9.3%, 8.5% and 3.7% in terms of Top-1 accuracy, respectively. There is also a large performance gap between C3D and ResNet-152. This is mainly due to data shift that the categories in ActivityNet are mostly human activities in daily life, which are quite different from those sport-related data in Sports-1M benchmark, resulting in not satisfying performance by C3D learnt purely on Sports-1M data. Instead, ResNet-152 trained on ImageNet image data is found to be more helpful in this case. P3D ResNet which pre-trains 2D spatial convolutions on image data and learns 1D temporal convolutions on video data fully leverages the knowledge from two domains, successfully boosting up the performance. The second task is action similarity labeling challenge, which is to answer a binary question of "does a pair of videos present the same action?" Following the experimental settings in <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b29">31]</ref>, we extract the outputs of four layers in P3D ResNet, i.e., prob, pool5, res5c and res4b35 layer as four types of representation for each 16-frame video clip. The video-level representation is then obtained by averaging all clip-level representations. Given each video pair, we calculate 12 different similarities on each type of video representation and thus generate a 48-dimensional vector for each pair. An L2 normalization is implemented on the 48d vector and a binary classifier is trained by using linear SVM. The performance comparisons on ASLAN are shown in <ref type="table">Table 5</ref>. Overall, P3D ResNet performs consistently better than both hand-crafted features and CNN-based representations across the performance metric of accuracy and area under ROC curve (AUC). In general, CNN-based representations exhibits better accuracy than hand-crafted fea-  tures. Unlike the observations on action recognition task, C3D significantly outperforms ResNet-152 on the scenario of action similarity labeling. We speculate that this may be the result of difficulty in interpreting the similarity between videos based on the ResNet-152 model learnt purely on image domain. In contrast, the video representation extracted by C3D which is trained on video data potentially has higher capability to distinguish between videos. At this point, improvements are also observed in P3D ResNet. This again indicates that P3D ResNet is endowed with the advantages of both C3D and ResNet-152 by pre-training 2D spatial convolutions on image data and learning 1D temporal connections on video data. The third experiment was conducted on scene recognition. <ref type="table" target="#tab_4">Table 6</ref> shows the accuracy of different methods. P3D ResNet outperforms the state-of-the-art hand-crafted features <ref type="bibr" target="#b3">[5]</ref> by 16.9% and 3.3% on Dynamic Scene and YUPENN benchmark, respectively. Compared to C3D and ResNet-152, P3D ResNet makes the absolute improvements by 1.4% and 0.3% on YUPENN, respectively.</p><p>The effect of representation dimension. <ref type="figure" target="#fig_4">Figure 6</ref> compares the accuracy of video representation with different dimensions on UCF101 by performing Principal Components Analysis on the original features of IDT, ResNet-152, C3D and P3D ResNet. Overall, video representation learnt by P3D ResNet consistently outperforms others at each dimension from 500 to 10. In general, higher dimensional representation provide better accuracy. An interesting observation is that the performance of ResNet-152 decreases more sharply than that of C3D and P3D ResNet when reducing the representation dimension. This somewhat reveals the weakness of ResNet-152 in generating video representa- ResNet. Specifically, we randomly select 10K videos from UCF101 and the video-level representation is then projected into 2-dimensional space using t-SNE. It is clear that video representations by P3D ResNet are better semantically separated than those of ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented Pseudo-3D Residual Net (P3D ResNet) architecture which aims to learn spatio-temporal video representation in deep networks. Particularly, we study the problem of simplifying 3D convolutions with 2D filters on spatial dimension plus 1D temporal connections. To verify our claim, we have devised variants of bottleneck building blocks for combining the 2D spatial and 1D temporal convolutions, and integrated them into a residual learning framework at different placements for structural diversity purpose. The P3D ResNet architecture learnt on Sports-1M dataset validate our proposal and analysis. Experiments conducted on five datasets in the context of video action recognition, action similarity labeling and scene recognition also demonstrate the effectiveness and generalization of the spatio-temporal video representation produced by our P3D ResNet. Performance improvements are clearly observed when comparing to other feature learning techniques.</p><p>Our future works are as follows. First, attention mechanism will be incorporated into our P3D ResNet for further enhancing representation learning. Second, an elaborated study will be conducted on how the performance of P3D ResNet is affected when increasing the frames in each video clip in the training. Third, we will extend P3D ResNet learning to other types of inputs, e.g., optical flow or audio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Three designs of Pseudo-3D blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>P3D-C: The last design is a compromise between P3D-A and P3D-B, by simultaneously building the direct influences among S, T and the final output. Specifically, to enable the direct connection between S and final output based on the cascaded P3D-A architecture, we establish a Bottleneck building blocks of Residual Unit and our Pseudo-3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>P3D ResNet by interleaving P3D-A, P3D-B and P3D-C. model size. The results basically indicate the advantage of exploring spatio-temporal information by our P3D blocks. Moreover, the speed of our P3D ResNet variants is very fast and could reach 8.6 ∼ 9.0 clips per second.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>gle Frame) which exploits only one single frame. Though the three runs of Deep Video (Slow Fusion), Convolutional Pooling and our P3D ResNet all capitalizes on temporal fusion, they are fundamentally different in the way of performing temporal connections. The performance of Deep Video (Slow Fusion) is as a result of carrying out temporal convolutions on spatial convolutions to compute activations, while Convolutional Pooling is by simply maxpooling the outputs of final convolutional layer across temporal frames. As indicated by the results, our P3D ResNet employing different combinations of spatial and temporal Visualization of class knowledge inside P3D ResNet model by using DeepDraw [1]. Four categories, i.e., tai chi, horizontal bar, motorcycle racing and boxing, are selected for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The accuracy of video representation learnt by different architectures with different dimensions. The performances reported in this figure are on UCF101 (3 splits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Video representation embedding visualizations of ResNet-152 and P3D ResNet on UCF101 using t-SNE<ref type="bibr" target="#b30">[32]</ref>. Each video is visualized as one point and colors denote different actions. tion, which is originated from domain gap that ResNet-152 is learnt purely on image data and may degrade the representational capability on videos especially when the feature dimension is very low. P3D ResNet, in comparison, is benefited from the exploration of knowledge from both image and video domain, making the learnt video representation more robust to the change of dimension.Video representation embedding visualization.Figure 7 further shows the t-SNE [32] visualization of embedding of video representation learnt by ResNet-152 and P3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Comparisons of different models on Sports-1M dataset in terms of accuracy, model size and the number of layers.</figDesc><table><row><cell>Video hit@1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>66.4%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>64.6%</cell><cell>Method</cell><cell>Depth</cell><cell>Model size</cell></row><row><cell></cell><cell>C3D</cell><cell>11</cell><cell>321MB</cell></row><row><cell></cell><cell>ResNet</cell><cell>152</cell><cell>235MB</cell></row><row><cell>61.1%</cell><cell>P3D ResNet</cell><cell>199</cell><cell>261MB</cell></row><row><cell>P3D ResNet ResNet</cell><cell>C3D</cell><cell></cell><cell></cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of ResNet-50 and different Pseudo-3DResNet variants in terms of model size, speed, and accuracy on UCF101 (split1). The speed is reported on one NVidia K40 GPU.</figDesc><table><row><cell>Method</cell><cell>Model size</cell><cell>Speed</cell><cell>Accuracy</cell></row><row><cell>ResNet-50</cell><cell>92MB</cell><cell>15.0 frame/s</cell><cell>80.8%</cell></row><row><cell>P3D-A ResNet</cell><cell>98MB</cell><cell>9.0 clip/s</cell><cell>83.7%</cell></row><row><cell>P3D-B ResNet</cell><cell>98MB</cell><cell>8.8 clip/s</cell><cell>82.8%</cell></row><row><cell>P3D-C ResNet</cell><cell>98MB</cell><cell>8.6 clip/s</cell><cell>83.0%</cell></row><row><cell>P3D ResNet</cell><cell>98MB</cell><cell>8.8 clip/s</cell><cell>84.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons in terms of pre-train data, clip length, Top-1 clip-level accuracy and Top-1&amp;5 video-level accuracy on Sports-1M. Method Pre-train Data Clip Length Clip hit@1 Video hit@1 Video hit@5</figDesc><table><row><cell>Deep Video (Single Frame) [10]</cell><cell>ImageNet1K</cell><cell>1</cell><cell>41.1%</cell><cell>59.3%</cell><cell>77.7%</cell></row><row><cell>Deep Video (Slow Fusion) [10]</cell><cell>ImageNet1K</cell><cell>10</cell><cell>41.9%</cell><cell>60.9%</cell><cell>80.2%</cell></row><row><cell>Convolutional Pooling [37]</cell><cell>ImageNet1K</cell><cell>120</cell><cell>70.8%</cell><cell>72.3%</cell><cell>90.8%</cell></row><row><cell>C3D [31]</cell><cell>-</cell><cell>16</cell><cell>44.9%</cell><cell>60.0%</cell><cell>84.4%</cell></row><row><cell>C3D [31]</cell><cell>I380K</cell><cell>16</cell><cell>46.1%</cell><cell>61.1%</cell><cell>85.2%</cell></row><row><cell>ResNet-152 [7]</cell><cell>ImageNet1K</cell><cell>1</cell><cell>46.5%</cell><cell>64.6%</cell><cell>86.4%</cell></row><row><cell>P3D ResNet (ours)</cell><cell>ImageNet1K</cell><cell>16</cell><cell>47.9%</cell><cell>66.4%</cell><cell>87.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance comparisons in terms of Top-1&amp;Top-3 classification accuracy, and mean AP on ActivityNet validation set. A linear SVM classifier is learnt on each feature.</figDesc><table><row><cell>Method</cell><cell>Top-1</cell><cell>Top-3</cell><cell>MAP</cell></row><row><cell>IDT [34]</cell><cell>64.70%</cell><cell>77.98%</cell><cell>68.69%</cell></row><row><cell>C3D [31]</cell><cell>65.80%</cell><cell>81.16%</cell><cell>67.68%</cell></row><row><cell>VGG 19 [26]</cell><cell>66.59%</cell><cell>82.70%</cell><cell>70.22%</cell></row><row><cell cols="2">ResNet-152 [7] 71.43%</cell><cell>86.45%</cell><cell>76.56%</cell></row><row><cell>P3D ResNet</cell><cell cols="3">75.12% 87.71% 78.86%</cell></row><row><cell cols="4">Table 5. Action similarity labeling performances on ASLAN</cell></row><row><cell cols="4">benchmark. STIP: Space-Time Interest Points; MIP: Motion In-</cell></row><row><cell cols="2">terchange Patterns; FV: Fisher Vector.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Model Accuracy</cell><cell>AUC</cell></row><row><cell>STIP [13]</cell><cell>linear</cell><cell>60.9%</cell><cell>65.3%</cell></row><row><cell>MIP [12]</cell><cell>metric</cell><cell>65.5%</cell><cell>71.9%</cell></row><row><cell>IDT+FV [19]</cell><cell>metric</cell><cell>68.7%</cell><cell>75.4%</cell></row><row><cell>C3D [31]</cell><cell>linear</cell><cell>78.3%</cell><cell>86.5%</cell></row><row><cell>ResNet-152 [7]</cell><cell>linear</cell><cell>70.4%</cell><cell>77.4%</cell></row><row><cell>P3D ResNet</cell><cell>linear</cell><cell>80.8%</cell><cell>87.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The accuracy performance of scene recognition on Dynamic Scene and YUPENN sets.</figDesc><table><row><cell cols="2">Method</cell><cell cols="5">Dynamic Scene YUPENN</cell></row><row><cell>[3]</cell><cell></cell><cell cols="2">43.1%</cell><cell></cell><cell cols="2">80.7%</cell></row><row><cell>[5]</cell><cell></cell><cell cols="2">77.7%</cell><cell></cell><cell cols="2">96.2%</cell></row><row><cell cols="2">C3D [31]</cell><cell cols="2">87.7%</cell><cell></cell><cell cols="2">98.1%</cell></row><row><cell cols="2">ResNet-152 [7]</cell><cell cols="2">93.6%</cell><cell></cell><cell cols="2">99.2%</cell></row><row><cell cols="2">P3D ResNet</cell><cell cols="2">94.6%</cell><cell></cell><cell cols="2">99.5%</cell></row><row><cell>Percentage on accuracy</cell><cell>40 50 60 70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>IDT</cell><cell>ResNet-152</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>C3D</cell><cell>P3D ResNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>500 30</cell><cell>300</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="2">Feature dimension</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic scene understanding: The role of orientation features in space and time in scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniildis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bags of spacetime energies for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion interchange patterns for action recognition in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gurovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The action similarity labeling challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On space-time interest points. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action recognition by learning deep multi-granular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchical video representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large margin dimensionality reduction for action similarity labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1022" to="1025" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Msr asia msm at thumos challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep quantization: Encoding convolutional activations with deep generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moving vistas: Exploiting motion for describing scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV- TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1604.04494</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05725</idno>
		<title level="m">Polynet: A pursuit of structural diversity in very deep networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
