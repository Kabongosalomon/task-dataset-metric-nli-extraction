<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ramé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémy</forename><surname>†1</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Optronics &amp; Missile Electronics</orgName>
								<orgName type="institution">Land &amp; Air Systems</orgName>
								<address>
									<addrLine>Thales 3 Valeo.ai</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent strategies achieved ensembling "for free" by fitting concurrently diverse subnetworks inside a single base network. The main idea during training is that each subnetwork learns to classify only one of the multiple inputs simultaneously provided. However, the question of how to best mix these multiple inputs has not been studied so far.</p><p>In this paper, we introduce MixMo, a new generalized framework for learning multi-input multi-output deep subnetworks. Our key motivation is to replace the suboptimal summing operation hidden in previous approaches by a more appropriate mixing mechanism. For that purpose, we draw inspiration from successful mixed sample data augmentations. We show that binary mixing in features -particularly with rectangular patches from CutMix -enhances results by making subnetworks stronger and more diverse.</p><p>We improve state of the art for image classification on CIFAR-100 and Tiny ImageNet datasets. Our easy to implement models notably outperform data augmented deep ensembles, without the inference and memory overheads. As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of research complementary to previous works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have shown exceptional performance in computer vision tasks, notably classification <ref type="bibr" target="#b47">[48]</ref>. However, among other limitations, obtaining reliable predictions remains challenging <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b68">69]</ref>. For additional robustness in real-world scenarios or to win Kaggle competitions, CNNs usually pair up with two practical strategies: data augmentation and ensembling.</p><p>Data augmentation reduces overfitting and improves * Equal contribution. † Correspondence to alexandre.rame@lip6.fr <ref type="figure">Figure 1</ref>: MixMo overview. We embed M = 2 inputs into a shared space with convolutional layers. The key point of our framework is the subsequent mixing block. Mixing with patches performs better than basic summing: 85.40% vs. <ref type="bibr" target="#b82">83</ref>.06% (MIMO <ref type="bibr" target="#b33">[34]</ref>) on CIFAR-100 with WRN-28-10.</p><p>generalization, notably by diversifying training samples <ref type="bibr" target="#b59">[60]</ref>. Traditional image augmentations are label-preserving: e.g flipping, cropping, etc <ref type="bibr" target="#b77">[78]</ref>. However, recent mixed sample data augmentation (MSDA) alters labels: multiple inputs and their labels are mixed proportionally to a ratio λ to create artificial samples. The seminal work Mixup <ref type="bibr" target="#b103">[104]</ref> linearly interpolates pixels while Manifold Mixup <ref type="bibr" target="#b90">[91]</ref> interpolates latent features in the network. Binary masking MSDAs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref> such as CutMix <ref type="bibr" target="#b100">[101]</ref> have since diversified mixed samples by pasting patches from one image onto another in place of interpolation. Aggregating diverse predictions from several neural networks significantly improves generalization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b97">98]</ref>, notably uncertainty estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b68">69</ref>]. An ensemble of several small networks usually performs better than one large network empirically <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59]</ref>. Yet, unfortunately, ensembling is costly in time and memory both at training and inference: this often limits applicability.</p><p>In this paper, we propose MixMo, a new generalized multi-input multi-output framework. To tackle these overheads found in traditional ensembling, we fit M ≥ 2 independent subnetworks within a single base network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b79">80]</ref>. This is possible as networks only leverage a subset of their weights. For example, "winning tickets" <ref type="bibr" target="#b21">[22]</ref> subnetworks perform similarly to the full network. Rather than pruning inactive filters <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b61">62]</ref>, we seek to fully use the available neural units and over parameterization.</p><p>The challenge is to prevent homogenization and enforce diversity among subnetworks with no structural differences. Thus, we consider M (input, label) pairs simultaneously in training: {(x i , y i )} 0≤i&lt;M . M images are treated simultaneously, as shown on <ref type="figure">Fig. 1</ref> with M = 2. The M inputs are encoded by M separate convolutional layers {c i } 0≤i&lt;M into a shared latent space before being mixed. The representation is then fed to the core network, which finally branches out into M dense layers {d i } 0≤i&lt;M . Diverse subnetworks naturally emerge as d i learns to classify y i from input x i . At inference, the same image is repeated M times: we obtain ensembling "for free" by averaging M predictions.</p><p>The key divergent point between MixMo variants lies in the multi-input mixing block. Should the merging be a basic summation, we would recover an equivalent formulation to MIMO <ref type="bibr" target="#b33">[34]</ref> (which first featured this multi-input multioutput strategy). Contrarily to classical fusion mechanisms that detect tensors' intercorrelations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, we seek features independence. This makes summing suboptimal.</p><p>Our main intuition is simple: we see summing as a balanced and restrictive form of Mixup <ref type="bibr" target="#b103">[104]</ref> where λ = 1 M . By analogy, we draw from the considerable MSDA literature to design a more appropriate mixing block. In particular, we leverage binary masking methods to ensure subnetworks diversity. Our framework allows us to create a new Cut-MixMo variant inspired by CutMix <ref type="bibr" target="#b100">[101]</ref>, and illustrated in <ref type="figure">Fig. 1</ref>: a patch of features from the first input is pasted into the features from the second input.</p><p>This asymmetrical mixing also raises new questions regarding information flow in the network's features. We tackle the imbalance between the multiple classification training tasks via a new weighting scheme. Conversely, MixMo's double nature as a new mixing augmentation in features yields important insights on traditional MSDA.</p><p>In summary, our contributions are threefold:</p><p>1. We propose a general framework, MixMo, connecting two successful fields: mixing samples data augmentations &amp; multi-input multi-output ensembling.</p><p>2. We identify the appropriate mixing block to best tackle the diversity/individual accuracy trade-off in subnetworks: our easy to implement Cut-MixMo benefits from the synergy between CutMix and ensembling.</p><p>3. We design a new weighting of the loss components to properly leverage the asymmetrical inputs mixing. We demonstrate excellent accuracy and uncertainty estimation with MixMo on CIFAR-10/100 and Tiny ImageNet. Specifically, Cut-MixMo with M = 2 reaches state of the art on these standard datasets: as exhibited by <ref type="figure" target="#fig_0">Fig. 2</ref>, it outperforms CutMix, MIMO and deep ensembles, at (almost) the same inference cost as a single network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Data augmentation</head><p>CNNs are known to memorize the training data <ref type="bibr" target="#b102">[103]</ref> and make overconfident predictions <ref type="bibr" target="#b28">[29]</ref> to the detriment of generalization on new test examples. Data Augmentation (DA) inflates the training dataset's size by creating artificial samples from available labeled data. Beyond slight perturbations (e.g. rotation), recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref> apply stronger transformations <ref type="bibr" target="#b36">[37]</ref>. CutOut <ref type="bibr" target="#b15">[16]</ref> and others <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b108">109]</ref> randomly delete regions of images in training. They prevent models from focusing on a single pixels region, similarly to how regularizations like Dropout <ref type="bibr" target="#b80">[81]</ref> or DropBlock <ref type="bibr" target="#b27">[28]</ref> force networks to leverage multiple features.</p><p>Mixed Sample Data Augmentation (MSDA) recently expanded the notion of DA. From pairs of labeled samples {(x i , y i ), (x k , y k )}, they create virtual samples:</p><formula xml:id="formula_0">(m x (x i , x k , λ), λy i + (1 − λ)y k ) where λ ∼ Beta(α, α)</formula><p>. <ref type="bibr" target="#b56">[57]</ref> shows that mixing the targets differently than this linear interpolation may cause underfitting and unstable learning. Indeed, approaches mainly focus on developing the most effective input mixing m x . In <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b103">104]</ref>, m x performs a simple linear interpolation between pixels: e.g in Mixup <ref type="bibr" target="#b103">[104]</ref>, m x (x i , x k , λ) = λx i + (1 − λ)x k . Theoretically, it regularizes outside the training distribution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b106">107]</ref> and applies label smoothing <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>CutMix draws from Mixup and CutOut <ref type="bibr" target="#b15">[16]</ref> by pasting a patch from x k onto x i : m x (x i , x k , λ) = 1 m x i + (1 − 1 m ) x k where represents the element-wise product and 1 m a binary mask with average value λ. CutMix randomly samples squares, which often leads to rectangular masks due to boundary effects. Such non-linear binary masking improves generalization <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b83">84]</ref> by increasing dataset: it creates new images with usually disjoint patches <ref type="bibr" target="#b32">[33]</ref>. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52]</ref> seek more diverse transformations via arbitrarily shaped masks: proposals range from cow-spotted masks <ref type="bibr" target="#b23">[24]</ref> to masks with irregular edges <ref type="bibr" target="#b32">[33]</ref>. As masking of discriminative regions may cause label misallocation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b91">92]</ref> try to alleviate this issue with costly saliency heatmaps <ref type="bibr" target="#b75">[76]</ref>. Yet, ResizeMix <ref type="bibr" target="#b72">[73]</ref> shows that they perform no better than random selection of patch locations.</p><p>In addition to Manifold Mixup <ref type="bibr" target="#b90">[91]</ref>, only a few works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b100">101]</ref> have tried to mix intermediate latent features as we do. Our goals and methods are however quite different, as shown later in Section 3.4. In brief, they mix deep features to smooth the decision boundaries, while we mix shallow features only so that inputs can remain distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ensembling</head><p>Like <ref type="bibr" target="#b94">[95]</ref>, we explore combining DA with another standard technique in machine learning: ensembling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49]</ref>. For improved performances, aggregated members should be both accurate and diverse <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b73">74]</ref>. Deep ensembles <ref type="bibr" target="#b49">[50]</ref> (DE) simultaneously train multiple networks with different random initializations <ref type="bibr" target="#b46">[47]</ref> converging towards different explanations for the training data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b96">97]</ref>.</p><p>Ensembling's fundamental drawback is the inherent computational and memory overhead, which increases linearly with the number of members. This bottleneck is typically addressed by sacrificing either individual performance or diversity in a complex trade-off. Averaging predictions from several checkpoints on the training process, i.e. snapshot ensembles <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b74">75]</ref>, fails to explore multiple local optima <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b96">97]</ref>. So does Monte Carlo Dropout <ref type="bibr" target="#b24">[25]</ref>. The recent BatchEnsemble <ref type="bibr" target="#b18">[19]</ref> is parameter-efficient, yet still requires multiple forward passes. TreeNets <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b82">83]</ref> by share low-level layers. Moth-erNets <ref type="bibr" target="#b93">[94]</ref> share first training epochs between members. However, sharing reduces diversity.</p><p>Very recently, the multi-input multi-output MIMO <ref type="bibr" target="#b33">[34]</ref> achieves ensemble almost "for free": all of the layers except the first convolutional and last dense layers are shared (≈ +1% #parameters). <ref type="bibr" target="#b79">[80]</ref> motivated a related Aggregated Learning to learn concise representations with arguments from information bottleneck <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b84">85]</ref>. The idea is that overparameterized CNNs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70]</ref> can fit multiple subnetworks <ref type="bibr" target="#b89">[90]</ref>. The question is how to prevent homogenization among the simultaneously trained subnetworks. Facing a similar challenge, <ref type="bibr" target="#b25">[26]</ref> includes stochastic channel recombination; <ref type="bibr" target="#b17">[18]</ref> relies on predefined binary masks; in GradAug <ref type="bibr" target="#b99">[100]</ref>, subnetworks only leverage the first channels up to a given percentage. In contrast, MIMO does not need structural differences among subnetworks: they learn to build their own paths while being as diverse as in DE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MixMo framework</head><p>We first introduce the main components of our MixMo strategy, summarized in <ref type="figure" target="#fig_1">Fig. 3</ref>: we mix multiple inputs to obtain multiple outputs via subnetworks. We highlight the key mixing block combining information from inputs, and our training loss based on a dedicated weighting scheme.</p><p>We mainly study M = 2 subnetworks here, both for clarity and as it empirically performs best in standard parameterization regimes. For completeness, we straightforwardly generalize to M &gt; 2 in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General overview</head><p>We leverage a training classification dataset D of i.i.d. pairs of associated image/label {x i , y i } |D| i=1 . We randomly sample a subset of |B| samples {x i , y i } i∈B that we randomly shuffle via permutation π. Our training batch is {(x i , x j ), (y i , y j )} i∈B,j=π(i) . The loss L MixMo is averaged over these |B| samples: the networks' weights are updated through backpropagation and gradient descent.</p><p>Let's focus on the training sample {(x 0 , x 1 ), (y 0 , y 1 )}. In MixMo, both inputs are separately encoded (see <ref type="figure">Fig. 1</ref>) into the shared latent space with two different convolutional layers (with 3 input channels each and no bias term): x 0 via c 0 and x 1 via c 1 . To recover a strictly equivalent formulation to MIMO <ref type="bibr" target="#b33">[34]</ref>, we simply sum the two encodings: c 0 (x 0 ) + c 1 (x 1 ). Indeed, MIMO merges inputs through channel-wise concatenation in pixels: MIMO's first convolutional layer (with 6 input channels and no bias term) hides the summing operation in the output channels.</p><p>Explicitly highlighting the underlying mixing leads us to consider a generalized multi-input mixing block M. This manifold mixing presents a unique opportunity to tackle the ensemble diversity/individual accuracy trade-off and to improve overall ensemble results (see Section 3.2). The shared representation M (c 0 (x 0 ), c 1 (x 1 )) feeds the next convolutional layers. We note κ the mixing ratio between inputs.</p><p>The core network C handles features that represent both inputs simultaneously. The dense layer</p><formula xml:id="formula_1">d 0 predictsŷ 0 = d 0 [C (M {c 0 (x 0 ), c 1 (x 1 )})]</formula><p>and targets y 0 , while d 1 targets y 1 . Thus, the training loss is the sum of two cross-entropies L CE weighted by parametrized function w r (defined in Section 3.3) to balance the asymmetry when κ = 0.5:</p><formula xml:id="formula_2">L MixMo = w r (κ)L CE (y 0 ,ŷ 0 ) + w r (1−κ)L CE (y 1 ,ŷ 1 ) . (1)</formula><p>At inference, the same input x is repeated twice: the core network C is fed the sum c 0 (x) + c 1 (x) that preserves maximum information from both encodings. Then, the diverse predictions are averaged: 1 2 (ŷ 0 +ŷ 1 ). This allows us to benefit from ensembling in a single forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mixing block M</head><p>The mixing block M -which combines both inputs into a shared representation -is the cornerstone of MixMo. Our main intuition was to analyze MIMO as a simplified Mixup variant where the mixing ratio κ is fixed to 0.5. MixMo generalized framework encompasses a wider range of variants inspired by MSDA mixing methods. Our first main variant -Linear-MixMo -fully extends Mixup. The mixing block is M Linear-MixMo (l 0 , l 1 ) = 2 [κl 0 + (1 − κ)l 1 ], where l 0 = c 0 (x 0 ), l 1 = c 1 (x 1 ) and κ ∼ Beta(α, α) with α the concentration parameter. The second and more effective variant Cut-MixMo adapts the patch mixing from CutMix:</p><formula xml:id="formula_3">M Cut-MixMo (l 0 , l 1 ) = 2 [1 M l 0 + (1 − 1 M ) l 1 ] , (2)</formula><p>where 1 M is a binary mask with area ratio κ ∼ Beta(α, α), valued at 1 either on a rectangle or on the complementary of a rectangle. In brief, a patch from c 0 (x 0 ) is pasted onto c 1 (x 1 ), or vice versa. As shown in Section 4.3.2, Cut-MixMo performs better than other strategies. Specifically, the binary mixing in Cut-MixMo advantageously replaces the linear interpolation in MIMO and Linear-MixMo: subnetworks are more accurate and more diverse.</p><p>First, binary mixing in M trains stronger individual subnetworks for the same reasons why CutMix improves over Mixup. In a nutshell, linear MSDAs <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b103">104]</ref> produce noisy samples <ref type="bibr" target="#b7">[8]</ref> that lead to robust representations. As MixMo tends to distribute different inputs on nonoverlapping channels (as discussed later in <ref type="figure" target="#fig_3">Fig. 4a</ref>), this regularization hardly takes place anymore in M Linear-MixMo . On the contrary, by masking features, we simulate common object occlusion problems. This spreads subnetworks' focus across different locations: the two classifiers are forced to find information relevant to their assigned input at disjoint locations. This occlusion remains effective as the receptive field in this first shallow latent space remains small.</p><p>Secondly, linear interpolation is fundamentally ill-suited to induce diversity as full information is preserved from both inputs. CutMix on the other hand explicitly increases dataset diversity by presenting patches of images that do not normally appear together. Such benefits can be directly transposed to M Cut-MixMo : binary mixing with patches increases randomness and diversity between the subnetworks. Indeed, in a similar spirit to bagging <ref type="bibr" target="#b5">[6]</ref>, different samples are given to the subnetworks. By deleting asymmetrical complementary locations from the two inputs, subnetworks will not rely on the same region and information. Overall, they are less likely to collapse on close solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss weighting w r</head><p>Asymmetries in the mixing mechanism can cause one input to overshadow the other. Notably when κ = 0.5, the predominant input may be easier to predict. We seek a weighting function w r to balance the relative importance of the two L CE in L MixMo . This weighting modifies the effective learning rate, how gradients flow in the network and overall how mixed information is represented in features. In this paper, we propose to weight via the parametrized:</p><formula xml:id="formula_4">w r (κ) = 2 κ 1/r κ 1/r + (1 − κ) 1/r .<label>(3)</label></formula><p>This defines a family of functions indexed by the parameter r, visualized for r = 3 in red on <ref type="figure" target="#fig_1">Fig. 3</ref>. See Appendix 6.1 for complementary visualizations. This power law provides a natural relaxation between two extreme configurations. The first extreme, r = 1, w 1 (κ) = 2κ, is in line with linear label interpolation in MSDA. The resulting imbalance in each subnetwork's contribution to L MixMo causes lopsided updates. While it promotes diversity, it also reduces regularization: the overshadowed input has a reduced impact on the loss. The opposite extreme, r → ∞, w ∞ (κ) → 1, removes reweighting. Consequently, w r inflates the importance of hard under-represented inputs, à la Focal Loss <ref type="bibr" target="#b57">[58]</ref>. However, minimizing the role of the predominant inputs destabilizes training. Overall, we empirically observe that moderate values of r perform best as they trade off pros and cons from both extremes. Interestingly, the proper weighting of loss components is also a central theme in multi-task learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b76">77]</ref>. While it aims at predicting several tasks from a shared input, MixMo predicts a shared task from several different inputs. Beyond this inverted structure, we have similar issues: e.g. gradients for one task can be detrimental to another conflicting task. Fortunately, MixMo presents an advantage: the exact ratios κ and 1 − κ of each task are known exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">From manifold mixing to MixMo</head><p>We have discussed at length how we extend multi-input multi-output frameworks by borrowing mixing protocols from MSDA. Now we reversely point out how our MixMo  diverges from MSDA schemes. At first glimpse, the idea is the same as manifold mixing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b90">91]</ref>: M = 2 inputs are encoded into a latent space to be mixed before being fed to the rest of the network. Yet, while they mix at varying depths, we only mix in the shallowest space. Specifically, we only mix in features -and not in pixels -to allow separate encodings of the inputs: they need to remain distinct in the mixed representation for the subsequent classifiers.</p><p>Hence our two key differences: first, MixMo uses two separated encoders (one for each input), and second, it outputs two predictions instead of a single one. Indeed, MS-DAs use a single classifier that targets a unique soft label reflecting the different classes via linear interpolation. MixMo instead chooses to fully leverage the composite nature of mixed samples and trains separated dense layers, d 0 and d 1 , ensembled "for free" at test time. Section 4.3.5 demonstrates that MixMo works because it also uses two different encoders c 0 and c 1 . While training two classifiers may seem straightforward in MSDA, it actually raises a troubling question: which input should each classifier predicts ? Having two encoders provides a simple solution: the network is divided in two subnetworks, one for each input. Their separability is easily observed: <ref type="figure" target="#fig_3">Fig. 4a</ref> shows the l 1 -norm of the 16 filters for the two encoders (WRN-28-10 on CIFAR-100). Each filter norm is far from zero in only one of the two encoders: c 0 (x 0 ) and c 1 (x 1 ) do not overlap much in the mixing space and can be treated differently by the subsequent layers.</p><p>This leads MixMo to use most available filters. Following the structured pruning literature <ref type="bibr" target="#b54">[55]</ref>, we consider in <ref type="figure" target="#fig_3">Fig. 4b</ref> that a filter (in a layer of the core network) is active if its l 1 -norm is at least 40% of the l 1 -norm from its layer's most active filter (see Appendix 6.2). This proportion of active filters decreases with width w: vanilla and CutMix networks do not fully use additional parameters. In contrast, MixMo better leverages over-parameterization thanks to its two encoders, classifiers and subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Generalization to M ≥ 2 subnetworks</head><p>Most of the framework is easily extended by optimizing</p><formula xml:id="formula_5">L MixMo = 0≤i&lt;M M κ 1/r i j κ 1/r j L CE (y i ,ŷ i ) with {κ i } ∼ Dir(α) from a Dirichlet distribution (see Appendix 6.3).</formula><p>The key change is that M now needs to handle more than 2 inputs: {c i (x i )} 0≤i&lt;M . While linear interpolation is easily generalized, Cut-MixMo has several possible extensions: in our experiments, we first linearly interpolate between M −1 inputs and then patch in a region from the M -th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate MixMo efficiency on standard image classification datasets: CIFAR-{10,100} <ref type="bibr" target="#b47">[48]</ref> and Tiny ImageNet <ref type="bibr" target="#b12">[13]</ref>. We equally track accuracies (Top{1,5}, ↑) and the calibrated Negative Log-Likelihood (NLL c , ↓). Indeed, <ref type="bibr" target="#b2">[3]</ref> shows that we should compare in-domain uncertainty estimations after temperature scaling (TS) <ref type="bibr" target="#b28">[29]</ref>: we thus split the test set in two and calibrate (after averaging in ensembles) with the temperature optimized on the other half, as in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b73">74]</ref>. We nonetheless report NLL (without TS) along with the Expected Calibration Error <ref type="bibr" target="#b64">[65]</ref> in Appendix 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We mostly study the Linear-MixMo and Cut-MixMo variants with M =2. We set hyper-parameter r=3 (see Section 4.3.3). α=2 performs better than 1 (see Appendix 6.8). In contrast, MIMO <ref type="bibr" target="#b33">[34]</ref> refers to linear summing, like Linear-MixMo, but with κ=0.5 instead of κ ∼ Beta(α, α).</p><p>Different mixing methods create a strong train-test distribution gap <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b59">60]</ref>. Thus, in Cut-MixMo we actually substitute M Cut-MixMo for M Linear-MixMo with probability 1 − p to accommodate for the summing in M at inference. We set the probability of patch mixing during training to p=0.5, with linear descent to 0 over the last twelfth of training epochs (see pseudocode 1 in Appendix).</p><p>When MixMo is combined with CutMix in pixels, the inputs may be:</p><formula xml:id="formula_6">(m x (x i , x k , λ), m x (x j , x k , λ )) with in- terpolated targets (λy i + (1 − λ)y k , λ y j + (1 − λ )y k )),</formula><p>where k, k are randomly sampled and λ, λ ∼ Beta(1).</p><p>MIMO duplicates samples b times via batch repetition: x i will be associated with x π(i) and x π (i) in the same batch if b=2. As the batch size remains fixed, the count of unique samples per batch and the learning rate is divided by b. Conversely, the number of steps is multiplied by b. Overall, this stabilizes training but multiplies its cost by b. We thus indicate an estimated (training/inference) overhead (wrt. vanilla training) in the time column of our tables. Note that some concurrent approaches also lengthen training: e.g. GradAug <ref type="bibr" target="#b99">[100]</ref> via multiple subnetworks predictions (≈ ×3).</p><p>We provide more details in Appendix 6.4 and will open source our PyTorch <ref type="bibr" target="#b88">[89]</ref> implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main results on CIFAR-100 and CIFAR-10</head><p>Tab. 1 reports averaged scores over 3 runs for our main experiment on CIFAR with WRN-28-10 <ref type="bibr" target="#b101">[102]</ref>. We re-use the hyper-parameters given in MIMO <ref type="bibr" target="#b33">[34]</ref>. Cut-MixMo reaches (85.40% Top1, 0.535 NLL c ) on CIFAR-100 with b=4: it surpasses our Linear-MixMo (83.08%, 0.656) and MIMO (83.06%, 0.661). Cut-MixMo sets a new state of the art when combined with CutMix (85.77%, 0.524). Results remain strong when b=2: Cut-MixMo (84.38%, 0.563) proves better on its own than traditional DE <ref type="bibr" target="#b49">[50]</ref> and previous MSDAs like Puzzle-Mix <ref type="bibr" target="#b45">[46]</ref> or CutMix <ref type="bibr" target="#b100">[101]</ref>. On CIFAR-10, we observe similar trends: Cut-MixMo reaches 0.081 in NLL c , 0.079 with CutMix. Yet, the costlier batch augmented Mixup BA <ref type="bibr" target="#b39">[40]</ref> edges it out in Top1. <ref type="figure" target="#fig_5">Fig. 5</ref> shows how MixMo grows stronger than DE (green curves) as width w in WRN-28-w increases. The parameterization becomes appropriate at w=4: Cut-MixMo (yellow curves) then matches DE -with half the parametersin <ref type="figure" target="#fig_5">Fig. 5a</ref> and its subnetworks match a vanilla network in <ref type="figure" target="#fig_5">Fig. 5b</ref>. Beyond, MixMo better uses over-parameterization: Cut-MixMo+CutMix surpasses DE+CutMix in NLL c for w≥5, and this is true in Top1 for w≥10. Compared to our strong Linear-MixMo+CutMix (purple curves), Cut-MixMo performs similarly in Top1, and better with CutMix for w≥4. While Linear-MixMo and DE learn from occlusion, Cut-MixMo also benefits from CutMix, notably from the induced label smoothing. Overall, Cut-MixMo, even without CutMix, significantly better estimates uncertainty.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The mixing block M</head><p>Tab. 2 compares performance for several mixing blocks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b100">101]</ref>. No matter the shape (illustrated in Appendix 6.7), binary masks perform better than linear mixing: the cow-spotted mask (84.17%, 0.561) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> notably performs well. The basic CutMix patching (84.38%, 0.563) is nevertheless more accurate and was our main focus. We further study the impact of patch mixing through the lens of the ensemble diversity/individual accuracy trade off. As in <ref type="bibr" target="#b73">[74]</ref>, we measure diversity via the pairwise ratio-error <ref type="bibr" target="#b0">[1]</ref> (d re , ↑), defined as the ratio between the number of different errors and simultaneous errors for two predictors. In <ref type="figure" target="#fig_7">Fig. 7 and 8</ref>, we average metrics over the last 10 epochs.</p><p>As argued in Section 3.2, patch mixing increases diversity compared to linear mixing in <ref type="figure" target="#fig_7">Fig. 7</ref>. As the probability p of patch mixing grows, so does diversity: from d re (p=0.0)≈0.78 (Linear-MixMo) to d re (p=0.5)≈0.85 (Cut-MixMo). We provide associated training dynamics in Appendix 6.6. In contrast, DE have d re ≈0.76 while MIMO has d re ≈0.77 on the same setup. Increasing p past 0.6 boosts diversity even more at the cost of subnetworks' accuracies: this is due to underfitting and an increased test-train distribution gap. p ∈ [0.5, 0.6] is thus the best trade off. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Weighting function w r</head><p>We analyze the impact of the parameter r in the reweighting function w r . Higher values tend to remove reweighting, as shown in Appendix 6.1: they strongly decrease diversity in <ref type="figure" target="#fig_8">Fig. 8</ref>. The opposite extreme with r=1 increases diversity via lopsided gradient updates but it degrades accuracy. We speculate it under-emphasizes hard samples. The range r ∈ [3, 6] strikes a good balance: results remain high and stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Generalization to M ≥ 2 subnetworks</head><p>We try to generalize MixMo to more than M = 2 subnetworks in <ref type="figure" target="#fig_9">Fig. 9</ref>. Cut-MixMo's subnetworks perform at 82.3% when M =2 vs. 79.5% when M =3. In MIMO, it's 79.8% vs. 77.7%. Because subnetworks do not share features, higher M degrades their results: only two can fit seamlessly. Ensemble Top1 overall decreases in spite of the additional predictions, as already noticed in MIMO <ref type="bibr" target="#b33">[34]</ref>. This reflects MixMo's strength in over-parametrized regimes, but also its limitations with fewer parameters when subnetworks underfit (recall previous <ref type="figure" target="#fig_5">Fig. 5</ref>). Facing similar findings, MIMO <ref type="bibr" target="#b33">[34]</ref> introduced input repetition so that subnetworks share their features, at the cost of drastically reducing diversity. Our generalization may be extended by future approaches whose mixing blocks (perhaps not inspired by MSDA) would tackle these issues. In Section 3.4, we compared MixMo and MSDA. Tab. 3 confirms the need for 2 encoders and 2 classifiers. With 1 classifier and linearly interpolated labels (in the same spirit as <ref type="bibr" target="#b9">[10]</ref>), the 2 encoders perform worse than 1 encoder. With 1 shared encoder and 2 classifiers, it is not clear which input each classifier should target. In the first naive , we randomly associate the 2 classifiers and the 2 inputs (encoded with the same encoder). This variant yields poor results. In ⊗, the first classifier tries to predict the label from the predominant input, the second targets the other input: ⊗ reaches 0.598 vs. 0.563 for Cut-MixMo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Multiple encoders and classifiers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness to image corruptions</head><p>Deep networks' results decrease when facing unfamiliar samples. To measure robustness to train-test distribution gaps, <ref type="bibr" target="#b37">[38]</ref> corrupted CIFAR-100 test images into CIFAR-100-c (more details in Appendix 6.4). As in Puzzle-Mix <ref type="bibr" target="#b45">[46]</ref>, we report WRN-28-10 results with and without Aug-Mix <ref type="bibr" target="#b38">[39]</ref>, a pixels data augmentation technique specifically introduced for this task. Tab. 4 shows that Cut-MixMo (b=4) best complements AugMix and reaches 71.1% Top1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pushing MixMo further: Tiny ImageNet</head><p>At a larger scale and with more varied 64 × 64 images, Cut-MixMo reaches a new state of the art of 70.24% on Tiny ImageNet <ref type="bibr" target="#b12">[13]</ref> in Tab. 5. We re-use the hyperparameters given in previous state of the art Puzzle-Mix <ref type="bibr" target="#b45">[46]</ref>. With w=1, PreActResNet-18 <ref type="bibr" target="#b35">[36]</ref> is not sufficiently parametrized for MixMo's advantages to express themselves on this challenging dataset. MixMo's full potential shines with wider networks: with w=2 and 44.9M parameters, Cut-MixMo reaches <ref type="bibr">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ensemble of MixMo</head><p>Since MixMo adds very little parameters (≈ +1%), we can combine independently trained MixMo like in DE. This ensembling of ensemble of subnetworks leads in practice to the averaging of M × N = 2 × N predictions. <ref type="figure">Fig. 10</ref> compares ensembling for vanilla networks and Cut-MixMo on CIFAR-100. We first recover the Memory Split Advantage <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b107">108]</ref> (MSA): at similar parameter counts, N =5 vanilla WRN-28-3 do better than a single vanilla WRN-28-7 (+0.10 in NLL c ). Cut-MixMo challenges this MSA: we bridge the gap between using one network or several smaller networks (−0.04 on same setup). Visually, Cut-MixMo's curves remain closer to the lower envelope: performances are less dependent on how the memory budget is split. This is because Cut-MixMo is effective mainly for larger architectures by better leveraging their parameters.</p><p>We also recover that wide vanilla networks tend to be less diverse <ref type="bibr" target="#b65">[66]</ref>, and thus gain less from ensembling <ref type="bibr" target="#b58">[59]</ref>: N =2 vanilla WRN-28-14 (83.47% Top1, 0.656 NLL c ) <ref type="figure">Figure 10</ref>: Ensemble effectiveness (NLL c /#params), for different widths w in WRN-28-w and numbers of members N . Standard data augmentations on CIFAR-100 with b=4. Curves interpolated through power laws <ref type="bibr" target="#b58">[59]</ref>.</p><p>perform not much better than N =2 WRN-28-7 (82.94%, 0.673). Contrarily, Cut-MixMo facilitates the ensembling of large networks with (86.58%, 0.488) vs. (85.50%, 0.516) (more comparisons in Appendix 6.9).</p><p>When combined with CutMix <ref type="bibr" target="#b100">[101]</ref>, Cut-MixMo previously set a new state of the art of 85.77% with N =1 WRN-28-10. Final Tab. 6 shows it further reaches 86.63% with N =2 and even 86.81% with N =3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce MixMo, a framework that generalizes the multi-input multi-output strategy and draws inspiration from recent data augmentations. MixMo can be analyzed as either an ensembling method or a new mixed samples data augmentation, while still remaining complementary to works from both lines of research. We also introduce a new weighting scheme to properly balance our losses for training. Overall, we demonstrated that different MixMo variants have excellent performance. In particular, Cut-MixMo with two subnetworks improves the state of the art on CIFAR-100, CIFAR-100-c and Tiny ImageNet.</p><p>MixMo better exploits large networks capacity and thus may become an important tool for real world projects. In future works, applications beyond vision tasks such as natural language processing could certainly be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>The sections in this Appendix follow a similar order to their related sections in the main paper. We first illustrate the reweighting of the loss components in Appendix 6.1. Appendix 6.2 elaborates on our analysis of filters activity. Appendix 6.3 clarifies our framework generalization with M &gt; 2 subnetworks. We describe in greater details our implementation in Appendix 6.4, and then our evaluation setting in 6.5. Appendix 6.6 showcases training dynamics. We provide a quick refresher on common MSDA techniques in Appendix 6.7. Appendix 6.8 studies the importance of α. Appendix 6.9 analyzes ensembles of Cut-MixMo with CutMix that reach state of the art. Finally, we provide a pseudocode in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Weighting function w r</head><p>As outlined in Section 3.3, the asymmetry in the mixing mechanism leads to asymmetry in the relative importance of the two inputs. Thus we reweight the loss components with function w r , defined as w r (κ) = 2 κ 1/r κ 1/r +(1−κ) 1/r . It rescales the mixing ratio κ through the use of a 1 r root operator. In the main paper, we have focused on r = 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Filters activity</head><p>We argued in Section 3.4 that MixMo better leverages additional parameters in wider networks. Concretely, a <ref type="table">Table 7</ref>: Proportion (%) of active filters in core network vs. width w for a WRN-28-w on CIFAR 100 and different activity thresholds t a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Width ta = 0.2 ta = 0.3 ta = 0.4 ta = 0.5 larger proportion of filters in large networks really help for classification as demonstrated in <ref type="figure" target="#fig_3">Fig. 4a</ref> and 4b in the main paper. Following common practices in the structured pruning literature <ref type="bibr" target="#b54">[55]</ref>, we used the l 1 -norm of convolutional filters as a proxy for importance. These 3D filters are of shape n i × k × k with n i the number of input channels and k the kernel size. In <ref type="figure" target="#fig_3">Fig. 4b</ref>, we arbitrarily defined a filter as active if its l 1 -norm is at least 40% of the highest filter l 1 -norm in that filter's layer. We report the average percentage of active filters across all filters in the core network C, for 3 learning strategies: vanilla, CutMix and Cut-MixMo.</p><p>The threshold t a = 0.4 was chosen for visualization purposes. Nevertheless, the observed trend in activity proportions remains for varying thresholds in Tab. 7. For example, for the lax t a = 0.2, CutMix uses 93.5% of filters vs. 98.5% for Cut-MixMo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Generalization to M &gt; 2 heads</head><p>We have mostly discussed our MixMo framework with M = 2 subnetworks. For better readability, we referred to the mixing ratios κ and 1 − κ with κ ∼ Beta(α, α). It's equivalent to a more generic formulation (κ 0 , κ 1 ) ∈ Dir 2 (α) from a symmetric Dirichlet distribution with concentration parameter α. This leads to the alternate equations L MixMo = i=0,1 w r (κ i )L CE (y i ,ŷ i ), where w r (κ i ) = 2 κ 1/r i j=0,1 κ 1/r j . Now generalization to the general case M ≥ 2 is straightforward. We draw a tuple {κ i } 0≤i&lt;M ∼ Dir M (α) and optimize the training loss:</p><formula xml:id="formula_7">L MixMo = M −1 i=0 w r (κ i )L CE (y i ,ŷ i ) ,<label>(4)</label></formula><p>where the new weighting naturally follows:</p><formula xml:id="formula_8">w r (κ i ) = M κ 1/r i M −1 j=0 κ 1/r j , ∀i ∈ {0, . . . , M − 1}. (5)</formula><p>The remaining point is the generalization of the mixing block M, that relies on the existence of MSDA methods for M &gt; 2 inputs. The linear interpolation can be easily expanded as in Mixup:</p><formula xml:id="formula_9">M Linear-MixMo ({l i }) = M M −1 i=0 κ i l i ,<label>(6)</label></formula><p>where l i = c i (x i ). However, extensions for other masking MSDAs have only recently started to emerge <ref type="bibr" target="#b44">[45]</ref>. For example, CutMix is not trivially generalizable to M &gt; 2, as the patches could overlap and hide important semantic components. In our experiments, a soft extension of Cut-MixMo performs best: it first linearly interpolates M − 1 inputs and then patches a region from the M -th:</p><formula xml:id="formula_10">M Cut-MixMo ({l i }) = M [1 M l k + (1 − 1 M ) M −1 i=0,i =k κ i 1 − κ k l i ],<label>(7)</label></formula><p>where 1 M is a rectangle of area ratio κ k and k sampled uniformly in {0, 1, . . . , M − 1}. However, it has been less successful than M = 2, as only two subnetworks can fit independently in standard parameterization regimes. Future work could design new framework components, such as specific mixing blocks, to tackle these limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Implementation details</head><p>We first used the popular image classification datasets CIFAR-100 and CIFAR-10 <ref type="bibr" target="#b47">[48]</ref>. They contain 60k 32 × 32 natural and colored images in respectively 100 classes and 10 classes, with 50k training images and 10k test images. At a larger scale, we study Tiny ImageNet <ref type="bibr" target="#b12">[13]</ref>, a downsampled version of ImageNet <ref type="bibr" target="#b14">[15]</ref>. It contains 200 different categories, 100k 64 × 64 training images (i.e. 500 images per class) and 10k test images.</p><p>Our code was adapted from the official MIMO [34] implementation 1 . For CIFAR, we re-use the hyperparameters from MIMO <ref type="bibr" target="#b33">[34]</ref>. The optimizer is SGD with learning rate of 0.1 b × batch-size 128 , batch size 64, linear warmup over 1 epoch, decay rate 0.1 at steps {75, 150, 225}, l 2 1 https://github.com/google/edward2/ regularization 3e-4. We follow standard MSDA practices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b100">101]</ref> and set the maximum number of epochs to 300. For Tiny ImageNet, we adapt PreActResNet-18-w, with w ∈ {1, 2, 3} times more filters. We re-use the hyperparameters from Puzzle-Mix <ref type="bibr" target="#b45">[46]</ref>. The optimizer is SGD with learning rate of 0.2 b , batch size 100, decay rate 0.1 at steps {600, 900}, 1200 epochs maximum, weight decay 1e-4. Our experiments ran on a single NVIDIA 12Go-TITAN X Pascal GPU. All results without a † were obtained with these training configurations. We will soon release our code and pre-trained models to facilitate reproducibility.</p><p>Batch repetition increases performances at the cost of longer training, which may be discouraging for some practitioners. Thus in addition to b = 4 as in MIMO <ref type="bibr" target="#b33">[34]</ref>, we often consider the quicker b = 2. Note that most of our concurrent approaches also increase training time: DE <ref type="bibr" target="#b49">[50]</ref> via several independent trainings, Puzzle-Mix <ref type="bibr" target="#b45">[46]</ref> via saliency detection (≈ ×2), GradAug <ref type="bibr" target="#b99">[100]</ref> via multiple subnetworks predictions (≈ ×3) or Mixup BA <ref type="bibr" target="#b39">[40]</ref> via 10 batch augmentations (≈ ×7 with our hardware on a single GPU).</p><p>MixMo operates in the features space and is complementary with pixels augmentations, i.e. cropping, Aug-Mix. The standard vanilla pixels data augmentation <ref type="bibr" target="#b34">[35]</ref> consists of 4 pixels padding, random cropping and horizontal flipping. When combined with CutMix, notably to benefit from multilabel smoothing, the input may be of the form: (m x (x i , x k , λ), x j ), where x k is randomly chosen in the whole dataset, and not only inside the current batch 2 . Moreover, M Cut-MixMo modifies by 1 M the visible part from mask 1 m (of area λ). We thus modify targets accordingly: (λ y i +(1−λ )y k , y j ) where λ = 1m 1 M 1 M . To fully benefit from b, we force the repeated x i to remain predominant in its b appearances: i.e., we swap x i and x k if λ &lt; 0.5. We see CutMix as a perturbation on the main batch sample.</p><p>Distributional uncertainty measures help when there is a mismatch between train and test data distributions. Thus <ref type="bibr" target="#b37">[38]</ref> introduced CIFAR-100-c on which AugMix performs best. AugMix sums the pixels from a chain of several augmentations and is complementary to our approach in features. We use default parameters 3 : the severity is set 3, the mixture's width to 3 and the mixture's depth to 4. We exclude operations in AugMix which overlap with CIFAR-100-c corruptions: thus, [equalize, posterize, rotate, solarize, shear_x, shear_y, translate_x, translate_y] remain. We disabled the Jensen-Shannon Divergence loss between predictions for the clean image and for the same image Aug-Mix augmented: that would otherwise triple the training time. For comparison of out-of-domain uncertainty estimations, we report NLL as in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b68">69]</ref>: indeed, the recommendation of <ref type="bibr" target="#b2">[3]</ref> to apply TS only stands for in-domain test set. <ref type="figure" target="#fig_0">Figure 12</ref>: Training dynamics. Higher probability p of binary mixing via patches increases diversity (lower right), and also subnetworks accuracy (lower left) but only up to p = 0.6. Around this value, we obtain best ensemble performances, in terms of accuracy (upper left) or uncertainty estimation (upper right). b = 2, r = 3, α = 3 with WRN-28-10 on CIFAR-100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Evaluation setting and metrics</head><p>We reproduce the experimental setting from CutMix <ref type="bibr" target="#b100">[101]</ref>, Manifold Mixup <ref type="bibr" target="#b90">[91]</ref> and other works such as the recent state-of-the-art ResizeMix <ref type="bibr" target="#b72">[73]</ref>: in absence of a validation dataset, results are reported at the epoch that yields the best test accuracy. For fair comparison, we apply this early stopping for all concurrent approaches. Nonetheless, for the sake of completeness, <ref type="table" target="#tab_8">Table 8</ref> shows results without early stopping on the main experiment (CIFAR with a standard WRN-28-10). We recover the exact same ranking among methods as in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Following recent works in ensembling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b73">74]</ref>, we have mainly focused on the NLL c metric for in-domain test set. Indeed, <ref type="bibr" target="#b2">[3]</ref> have shown that "comparison of [. . .] ensembling methods without temperature scaling (TS) <ref type="bibr" target="#b28">[29]</ref> might not provide a fair ranking". Nevertheless in <ref type="table" target="#tab_8">Table 8</ref>, we found that Negative Log-Likelihood (NLL) (without TS) leads to similar conclusions as NLL c (after TS).</p><p>The TS even mostly seems to benefit to poorly calibrated models, as shown by the calibration criteria Expected Calibration Error (ECE, ↓, 15 bins). ECE measures how confidences match accuracies. MixMo attenuates over-confidence in large networks and thus reduces ECE. In our case, combining ensembling and data augmentation improves calibration <ref type="bibr" target="#b94">[95]</ref>. Note that the appropriate measure of calibration is still under debate <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b104">105]</ref>. Notably, <ref type="bibr" target="#b2">[3]</ref> have also stated that, despite being widely used, ECE is biased and unreliable: we can confirm that we found ECE to be dependant to hyper-parameters and implementation details. Due to space constraints and these pitfalls, we have not included this controversial metric in the main paper. <ref type="figure" target="#fig_0">Fig. 12</ref> showcases training dynamics for probability p ∈ [0, 1] of patch mixing (see Section 4.3.2). In the remaining 1 − p, we interpolate features linearly. For p = 0, we recover our Linear-MixMo; for p = 0.5, we recover our Cut-MixMo. In all approaches, p is linearly reduced towards 0 beyond the <ref type="bibr">11 12</ref> of the training epochs, i.e. from epoch 275 to 300 on CIFAR. As we sum at inference, this reduces the train-test distribution gap and slightly increases individual accuracy during the final epochs (lower left in <ref type="figure" target="#fig_0">Fig. 12</ref>). Diversity is measured by the ratio-error, the ratio between the number of samples on which only one of the two predictor is wrong, divided by the number of samples on which they are both wrong. It is positively correlated with p. However, individual accuracies first increase with p until p = 0.6, then the tendency is reversed. Overall, best ensemble performances in terms of accuracy (Top1) and uncertainty (NLL) estimation are obtained with p ∈ [0.5, 0.6]. Most importantly, we note that the performance gaps are consistent and stable along training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Training dynamics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Mixed sample data augmentations</head><p>We have drawn inspiration from MSDA techniques to design our mixing block M. In particular, Section 4.3.2 compared different M based on recent papers. <ref type="figure" target="#fig_1">Fig. 13</ref> provides the reader a visual understanding of their behaviour, which we explain below.</p><p>MixUp <ref type="bibr" target="#b103">[104]</ref> linearly interpolates between pixels values: m x (x i , x k , λ) = λx i +(1−λ)x k . The remaining methods fall under the label of binary MSDA: m x (x i , x k , λ) = 1 m x i + (1 − 1 m ) x k with 1 m a mask with binary values {0, 1} and area of ratio λ. They diverge in how this mask is created. The horizontal concatenation, also found in <ref type="bibr" target="#b81">[82]</ref>, simply draws a vertical line such that every pixel to the left belongs to one sample and every pixel to the right belongs to the other. Similarly, we define a vertical concatenation with an horizontal line. PatchUp <ref type="bibr" target="#b19">[20]</ref> adapted DropBlock <ref type="bibr" target="#b27">[28]</ref>: a canvas C of patches is created by sampling for every spatial coordinate from the Bernoulli distribution Ber(λ ) (where λ is a recalibrated value of λ): if the drawn binary value is 1, a patch around that coordinate is set to 1 on the final binary mask 1 m . PatchUp was designed for in-manifold mixing with a different mask by channels. However, duplicating the same 2D mask in all channels for M performs better in our experiments. FMix <ref type="bibr" target="#b32">[33]</ref> selects a large contiguous region in one image and pastes it onto another. The binary mask is made of the top-λ percentile of pixels from a low-pass filtered 2D map G drawn from an isotropic Gaussian distribution. CowMix <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> selects a cow-spotted set of regions, and is somehow similar to FMix with a Gaussian filtered 2D map G. CutMix <ref type="bibr" target="#b100">[101]</ref> was inspired by CutOut <ref type="bibr" target="#b15">[16]</ref>. Formally, we sample a square with edges of length R √ λ, where R is the length of an image edge. Note that this sometimes leads to non square rectangles when the initially sampled square overlaps with the edge from the original image. We adjust our λ a posteriori <ref type="figure" target="#fig_1">Figure 13</ref>: Common MSDA procedures with λ = 0.5.</p><p>to fix this boundary effect. Regarding the hyper-parameters, we use in M those provided in the seminal papers, except for sampling of κ where we set α = 2 in all setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8.">Hyper-parameter α</head><p>In <ref type="figure" target="#fig_3">Fig. 14,</ref> we study the impact of different values of α, parameterizing the sampling law for κ ∼ Beta(α, α). For high values of α, the interval of κ narrows down around 0.5. Diversity is therefore decreased: we speculate this is because we do not benefit anymore from lopsided updates. The opposite extreme, when α=1, is equivalent to uniform distribution between 0 and 1. Therefore diversity is increased, at the cost of lower individual accuracy due to less stable training. For simplicity, we set α=2. Manifold-Mixup <ref type="bibr" target="#b90">[91]</ref> selected the same value on CIFAR-100. However, this value could be fine tuned on the target task: e.g. in <ref type="figure" target="#fig_3">Fig. 14, α=4</ref> seems to perform best for Cut-MixMo on CIFAR-100 with WRN-28-10 with r=3, p=0.5 and b=2.  <ref type="figure" target="#fig_5">Fig. 15</ref> plots performance for different widths w in WRN-28-w and varying number of ensembled networks N : two vertically aligned points have the same parameter budget. Indeed, the total number of parameters in our architectures has been used as a proxy for model complexity, as <ref type="figure" target="#fig_5">Figure 15</ref>: Ensemble effectiveness (NLL c /#params). We slide the width in WRN-28-w and numbers of members N . CutMix data augmentation. Interpolations through power laws <ref type="bibr" target="#b58">[59]</ref> when more than 2 points are available. in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59]</ref>. We compare ensembling with CutMix rather than standard pixels data augmentation, as previously done in <ref type="figure" target="#fig_6">Fig. 6</ref> from Section 4.6. CutMix induces additional regularization and label smoothing: empirically, it improves all our approaches. For a fixed memory budget, a single network usually performs worse than an ensemble of several medium-size networks: we recover the Memory Split Advantage even with CutMix. However, Cut-MixMo challenges this by remaining closer to the lower envelope. In other words, parameters allocation (more networks or bigger networks) has less impact on results. This is due to Cut-MixMo's ability to better use large networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9.">Ensemble of Cut-MixMo with CutMix</head><p>In <ref type="table" target="#tab_9">Table 9</ref>, we summarize several experiments on CIFAR-100. Among other things, we can observe that large vanilla networks tend to gain less from ensembling <ref type="bibr" target="#b58">[59]</ref>: e.g. 2 vanillas WRN-28-10 (83.17% Top1, 0.668 NLL c ) do not perform much better than 2 WRN-28-7 (82.94%, 0.673). This remains true even with CutMix: (85.74%, 0.571) vs. (85.52%, 0.573). We speculate this is related to wide networks' tendency to converge to less diverse solutions, as studied in <ref type="bibr" target="#b65">[66]</ref>. Contrarily, MixMo improves the ensembling of large networks, with (86.04%, 0.494) vs. (85.50%, 0.517) on the same setup. When additionally combined with CutMix, we obtain state of the art (86.63%, 0.479) vs. (85.90%, 0.498). This demonstrates the importance of Cut-MixMo in cooperation with standard pixels data augmentation. It attenuates the drawbacks from overparameterization This is of great importance for practical efficiency: it modifies the optimal network width for realworld applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.10.">Pseudo Code</head><p>Finally, the pseudocode in Algorithm 1 describes the procedure behind Cut-MixMo with M = 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Main results. CIFAR-100 with WRN-28-w. Our Cut-MixMo variant (patch mixing and M = 2) surpasses CutMix and deep ensembles (with half the parameters) by leveraging over-parameterization in wide networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>MixMo learning components. The asymmetry in mixing influences the balance of the training loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Filters l1-norms of the input encoders c0 and c1.(b) Proportion of active filters in the core network vs. width w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Influence of MixMo on network utilization. (a) The encoders have separate channels: the two subsequent classifiers can differentiate the two inputs. (b) Less filters are strongly active ( f i 1 ≥ 0.4× max f ∈layer f 1 ) in wider networks: Cut-MixMo reduces this negative point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Ensemble Top1 and NLLc.(b) Individual Top1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Parameters efficiency (metrics/#params). CIFAR-100 with WRN-28-w, b=4. Comparisons between (a) ensemble and some of their (b) individual counterparts.4.3. MixMo analysis on CIFAR-100 w/ WRN-28-10 4.3.1 Training timeWe have just seen that CutMix improves Linear-MixMo at varying widths w, but not enough to match Cut-MixMo in NLL c : CutMix can not fully compensate for the advantages from patch mixing over linear interpolation. We recover this finding inFig. 6, this time at varying batch repetition b ∈ {1, 2, 4} when w=10. Moreover, Cut-MixMo outperforms DE for the same training time. Indeed, MixMo variants trained with a given b matches the training time of DE with N =b networks. In the rest of this section, we set b=2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>NLL c (↓) improves with longer training, via batch repetitions (MixMo) or additional networks (DE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Diversity/accuracy as function of p with r=3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Diversity/accuracy as function of r with p=0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Ensemble/individual accuracies for M ≥ 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 illustrates</head><label>11</label><figDesc>how w r behaves for r ∈ {1, 2, 3, 4, 10} and r → ∞. The first extreme r = 1 matches the diagonal w r (κ) = 2κ, without rescaling of κ, similarly to what is customary in MSDA. Our experiments in Section 4.3.3 justified the initial idea to shift the weighting function closer to the horizontal and constant curve w r (κ) = 1 with higher r. In the other experiments, we always set r = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Curves of the reweighting operation that projects κ to the flattened ratio w r (κ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Diversity/accuracy as function of α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Main results: WRN-28-10 on CIFAR. Bold highlights best scores, † marks approaches not re-implemented.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell cols="2">CIFAR-10</cell></row><row><cell>Approach</cell><cell>Time Tr./Inf.</cell><cell>Top1 %, ↑</cell><cell>Top5 %, ↑</cell><cell>NLLc 10 −2 , ↓</cell><cell>Top1 %, ↑</cell><cell>NLLc 10 −2 , ↓</cell></row><row><cell>Vanilla</cell><cell></cell><cell cols="2">81.63 95.49</cell><cell>73.9</cell><cell>96.34</cell><cell>12.6</cell></row><row><cell>Mixup</cell><cell></cell><cell cols="2">83.44 95.92</cell><cell>65.7</cell><cell>97.07</cell><cell>11.2</cell></row><row><cell>Manifold Mixup  † CutMix</cell><cell>1/1</cell><cell cols="2">81.96 95.51 84.05 96.09</cell><cell>73.4 64.8</cell><cell>97.45 97.23</cell><cell>12.2 9.9</cell></row><row><cell>ResizeMix  †</cell><cell></cell><cell>84.31</cell><cell>-</cell><cell>-</cell><cell>97.60</cell><cell>-</cell></row><row><cell>Puzzle-Mix  †</cell><cell>2/1</cell><cell cols="2">84.31 96.46</cell><cell>66.8</cell><cell>-</cell><cell>-</cell></row><row><cell>GradAug  † + CutMix  †</cell><cell>3/1</cell><cell cols="2">84.14 96.43 85.51 96.86</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>Mixup BA  †</cell><cell>7/1</cell><cell>84.30</cell><cell>-</cell><cell>-</cell><cell>97.80</cell><cell>-</cell></row><row><cell>DE (2 Nets) + CutMix</cell><cell>2/2</cell><cell cols="2">83.17 96.37 85.74 96.82</cell><cell>66.4 57.1</cell><cell>96.67 97.52</cell><cell>11.1 8.6</cell></row><row><cell>MIMO</cell><cell></cell><cell cols="2">82.40 95.78</cell><cell>68.8</cell><cell>96.38</cell><cell>12.1</cell></row><row><cell>Linear-MixMo + CutMix</cell><cell>2/1</cell><cell cols="2">82.54 95.99 84.69 97.12</cell><cell>67.6 57.2</cell><cell>96.56 97.32</cell><cell>11.4 9.4</cell></row><row><cell>Cut-MixMo</cell><cell></cell><cell cols="2">84.38 96.94</cell><cell>56.3</cell><cell>97.31</cell><cell>8.9</cell></row><row><cell>+ CutMix</cell><cell></cell><cell cols="2">85.18 97.20</cell><cell>54.5</cell><cell>97.45</cell><cell>8.4</cell></row><row><cell>MIMO</cell><cell></cell><cell cols="2">83.06 96.23</cell><cell>66.1</cell><cell>96.74</cell><cell>11.4</cell></row><row><cell>Linear-MixMo + CutMix</cell><cell>4/1</cell><cell cols="2">83.08 96.26 85.47 97.04</cell><cell>65.6 55.8</cell><cell>96.91 97.68</cell><cell>10.8 8.7</cell></row><row><cell>Cut-MixMo</cell><cell></cell><cell cols="2">85.40 97.22</cell><cell>53.5</cell><cell>97.51</cell><cell>8.1</cell></row><row><cell>+ CutMix</cell><cell></cell><cell cols="2">85.77 97.42</cell><cell>52.4</cell><cell>97.73</cell><cell>7.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>M inspired by various MSDA approaches.</figDesc><table><row><cell>M</cell><cell>Mixup</cell><cell>Horiz.</cell><cell>Vertical</cell><cell>PatchUp 2D</cell><cell>FMix</cell><cell>CowMask</cell><cell>CutMix</cell></row><row><cell>approach</cell><cell>[104]</cell><cell>Concat.</cell><cell>Concat.</cell><cell>[20]</cell><cell>[33]</cell><cell>[23, 24]</cell><cell>[101]</cell></row><row><cell>Top1 ↑</cell><cell>82.5</cell><cell>82.78</cell><cell>84.00</cell><cell>84.16</cell><cell>83.76</cell><cell>84.17</cell><cell>84.38</cell></row><row><cell>NLLc ↓</cell><cell>0.676</cell><cell>0.627</cell><cell>0.573</cell><cell>0.581</cell><cell>0.602</cell><cell>0.561</cell><cell>0.563</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of encoders/classifiers.</figDesc><table><row><cell cols="3"># Enc. # Clas. NLL c ↓</cell></row><row><cell>1</cell><cell>1</cell><cell>0.604</cell></row><row><cell>2</cell><cell>1</cell><cell>0.666</cell></row><row><cell>1</cell><cell>2</cell><cell>0.687</cell></row><row><cell>1</cell><cell>2 ⊗</cell><cell>0.598</cell></row><row><cell>2</cell><cell>2</cell><cell>0.563</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Robustness comparison on CIFAR-100-c.</figDesc><table><row><cell>Approach</cell><cell>1 Net.</cell><cell>CutMix</cell><cell cols="2">Puzzle-Mix  †</cell><cell cols="5">DE (2 Nets) MIMO Linear-MixMo Cut-MixMo</cell></row><row><cell>AugMix</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Top1 ↑</cell><cell>52.2 67.8</cell><cell>51.93</cell><cell cols="3">58.09 70.46 53.8 69.9</cell><cell>53.6</cell><cell>55.6</cell><cell>70.4</cell><cell>57.0 71.1</cell></row><row><cell>Top5 ↑</cell><cell>73.7 87.5</cell><cell>72.03</cell><cell>77.3</cell><cell cols="2">87.7 74.9 88.9</cell><cell>74.9</cell><cell>76.1</cell><cell>89.4</cell><cell>77.4 89.5</cell></row><row><cell>NLL ↓</cell><cell>2.50 1.38</cell><cell>2.13</cell><cell>1.96</cell><cell cols="2">1.34 2.27 1.24</cell><cell>2.66</cell><cell>2.33</cell><cell>1.22</cell><cell>2.04 1.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>69.13%, 1.28) vs. (67.76%, 1.33) for CutMix. Compared to DE with 3 networks, Cut-MixMo performs {worse, similarly, better} for width w ∈ {1, 2, 3}. At (almost) the same numbers of parameters, Cut-MixMo when w=2 performs better (69.13%, 1.28) than DE with 4 networks when w=1 (67.51%, 1.31).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results: PreActResNet-18-w on Tiny ImageNet.</figDesc><table><row><cell cols="2">Width w (# params)</cell><cell cols="6">w = 1 (11.2M) w = 2 (44.9M) w = 3 (100.5M)</cell></row><row><cell>Approach</cell><cell>Time Tr./Inf.</cell><cell>Top1 %, ↑</cell><cell>NLLc ↓</cell><cell>Top1 %, ↑</cell><cell>NLLc ↓</cell><cell>Top1 %, ↑</cell><cell>NLLc ↓</cell></row><row><cell>Vanilla</cell><cell></cell><cell>62.56</cell><cell>1.53</cell><cell>64.80</cell><cell>1.51</cell><cell>65.78</cell><cell>1.53</cell></row><row><cell>Mixup</cell><cell></cell><cell>63.74</cell><cell>1.62</cell><cell>66.62</cell><cell>1.50</cell><cell>67.27</cell><cell>1.51</cell></row><row><cell>Manifold Mixup  †</cell><cell>1/1</cell><cell>58.70</cell><cell>1.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Co-Mixup  †</cell><cell></cell><cell>64.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CutMix</cell><cell></cell><cell>65.09</cell><cell>1.58</cell><cell>67.76</cell><cell>1.33</cell><cell>68.95</cell><cell>1.29</cell></row><row><cell>Puzzle-Mix  †</cell><cell>2/1</cell><cell>64.48</cell><cell>1.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DE (2 Nets)</cell><cell>2/2</cell><cell>65.53</cell><cell>1.39</cell><cell>68.06</cell><cell>1.37</cell><cell>68.38</cell><cell>1.36</cell></row><row><cell>DE (3 Nets)</cell><cell>3/3</cell><cell>66.76</cell><cell>1.34</cell><cell>69.05</cell><cell>1.29</cell><cell>69.36</cell><cell>1.28</cell></row><row><cell>DE (4 Nets)</cell><cell>4/4</cell><cell>67.51</cell><cell>1.31</cell><cell>69.94</cell><cell>1.24</cell><cell>69.72</cell><cell>1.26</cell></row><row><cell>Linear-MixMo Cut-MixMo</cell><cell>2/1</cell><cell>61.58 63.78</cell><cell>1.61 1.48</cell><cell>66.62 68.30</cell><cell>1.41 1.30</cell><cell>68.18 69.89</cell><cell>1.36 1.26</cell></row><row><cell>Linear-MixMo Cut-MixMo</cell><cell>4/1</cell><cell>62.91 64.44</cell><cell>1.51 1.48</cell><cell>67.03 69.13</cell><cell>1.41 1.28</cell><cell>68.38 70.24</cell><cell>1.38 1.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Best results for WRN-28-10 on CIFAR-100 via Cut-MixMo + CutMix<ref type="bibr" target="#b100">[101]</ref> + N -ensembling and b=4. Recent Top1 SoTAs: 85.23<ref type="bibr" target="#b72">[73]</ref>, 85.51<ref type="bibr" target="#b99">[100]</ref>, 85.74<ref type="bibr" target="#b107">[108]</ref>.</figDesc><table><row><cell cols="2">N # params</cell><cell>Top1 ↑</cell><cell>Average Top5 ↑</cell><cell>NLLc ↓</cell><cell>Best run Top1 ↑ Top5 ↑ NLLc ↓</cell></row><row><cell>1</cell><cell>36.6M</cell><cell cols="4">85.77 ± 0.14 97.36 ± 0.02 0.524 ± 0.005 85.92</cell><cell>97.36</cell><cell>0.518</cell></row><row><cell>2</cell><cell>73.2M</cell><cell cols="4">86.63 ± 0.19 97.73 ± 0.05 0.479 ± 0.003 86.75</cell><cell>97.80</cell><cell>0.475</cell></row><row><cell>3</cell><cell cols="5">109.8M 86.81 ± 0.17 97.85 ± 0.04 0.464 ± 0.002 86.94</cell><cell>97.83</cell><cell>0.464</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>WRN-28-10 on CIFAR without early stopping.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell></row><row><cell>Approach</cell><cell>Time Tr./Inf.</cell><cell>Top1 %, ↑</cell><cell>Top5 %, ↑</cell><cell>NLLc 10 −2 , ↓</cell><cell>NLL 10 −2 , ↓</cell><cell>ECE 10 −2 , ↓</cell><cell>Top1 %, ↑</cell><cell>NLLc 10 −2 , ↓</cell><cell>NLL 10 −2 , ↓</cell><cell>ECE 10 −2 , ↓</cell></row><row><cell>Vanilla</cell><cell></cell><cell cols="2">81.47 95.57</cell><cell>73.6</cell><cell>76.2</cell><cell>6.47</cell><cell>96.31</cell><cell>12.5</cell><cell>14.1</cell><cell>1.95</cell></row><row><cell>Mixup Hard PatchUp  †</cell><cell>1/1</cell><cell cols="2">83.15 95.75 83.87 -</cell><cell>66.3 -</cell><cell>67.3 66.0</cell><cell>1.62 -</cell><cell>97.00 97.47</cell><cell>11.3 -</cell><cell>11.5 11.4</cell><cell>0.97 -</cell></row><row><cell>CutMix</cell><cell></cell><cell cols="2">83.74 96.18</cell><cell>65.4</cell><cell>66.1</cell><cell>4.95</cell><cell>97.21</cell><cell>9.7</cell><cell>10.8</cell><cell>1.51</cell></row><row><cell>Puzzle-Mix  †</cell><cell>2/1</cell><cell cols="2">84.05 96.08</cell><cell>66.9</cell><cell>68.1</cell><cell>2.76</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GradAug  † + CutMix  †</cell><cell>3/1</cell><cell cols="2">83.98 96.28 85.25 96.85</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>Mixup BA  †</cell><cell>7/1</cell><cell>84.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DE (2 Nets) + CutMix</cell><cell>2/2</cell><cell cols="2">83.15 96.30 85.46 96.90</cell><cell>66.0 57.4</cell><cell>67.2 57.5</cell><cell>5.15 3.62</cell><cell>96.58 97.51</cell><cell>11.1 8.7</cell><cell>12.2 9.0</cell><cell>1.82 1.16</cell></row><row><cell>MIMO (M = 2)</cell><cell></cell><cell cols="2">82.04 95.75</cell><cell>69.1</cell><cell>72.4</cell><cell>6.32</cell><cell>96.33</cell><cell>12.1</cell><cell>13.4</cell><cell>1.89</cell></row><row><cell>Linear-MixMo + CutMix</cell><cell>2/1</cell><cell cols="2">81.88 95.97 84.55 96.95</cell><cell>67.8 57.4</cell><cell>70.3 57.5</cell><cell>6.20 2.54</cell><cell>96.55 97.34</cell><cell>11.4 8.9</cell><cell>12.5 9.3</cell><cell>1.67 1.34</cell></row><row><cell>Cut-MixMo</cell><cell></cell><cell cols="2">84.07 96.97</cell><cell>56.6</cell><cell>57.9</cell><cell>4.19</cell><cell>97.26</cell><cell>8.7</cell><cell>9.1</cell><cell>0.98</cell></row><row><cell>+ CutMix</cell><cell></cell><cell cols="2">85.17 97.28</cell><cell>54.4</cell><cell>54.5</cell><cell>2.13</cell><cell>97.33</cell><cell>8.5</cell><cell>8.6</cell><cell>0.88</cell></row><row><cell>MIMO (M = 2)</cell><cell></cell><cell cols="2">82.74 95.90</cell><cell>67.0</cell><cell>74.0</cell><cell>7.56</cell><cell>96.66</cell><cell>11.5</cell><cell>13.6</cell><cell>1.98</cell></row><row><cell>MIMO  † (M = 3)</cell><cell></cell><cell>82.0</cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>2.2</cell><cell>96.4</cell><cell>-</cell><cell>12.3</cell><cell>1.0</cell></row><row><cell>Linear-MixMo</cell><cell>4/1</cell><cell cols="2">82.53 96.08</cell><cell>65.8</cell><cell>68.5</cell><cell>6.64</cell><cell>96.78</cell><cell>10.8</cell><cell>11.8</cell><cell>1.80</cell></row><row><cell>+ CutMix</cell><cell></cell><cell cols="2">85.24 96.97</cell><cell>56.3</cell><cell>56.4</cell><cell>3.53</cell><cell>97.53</cell><cell>8.8</cell><cell>8.6</cell><cell>1.19</cell></row><row><cell>Cut-MixMo</cell><cell></cell><cell cols="2">85.32 97.12</cell><cell>53.6</cell><cell>54.8</cell><cell>4.53</cell><cell>97.42</cell><cell>8.1</cell><cell>8.4</cell><cell>1.15</cell></row><row><cell>+ CutMix</cell><cell></cell><cell cols="2">85.59 97.33</cell><cell>53.2</cell><cell>53.3</cell><cell>1.95</cell><cell>97.70</cell><cell>8.0</cell><cell>8.2</cell><cell>0.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Summary: WRN-28-w on CIFAR-100. b = 4. Top1 76.44 78.06 79.16 80.81 75.82 76.36 75.66 75.17 76.98 76.11 NLL Top1 77.95 80.70 80.85 83.14 78.51 80.74 79.81 79.85 80.78 81.20 NLL Top1 78.84 81.55 81.48 83.93 80.43 81.66 81.68 81.69 82.57 82.58 NLL Top1 79.75 82.55 82.18 84.60 80.95 83.06 83.11 83.34 83.97 84.31 NLL 84.05 83.17 85.74 83.08 85.47 85.40 85.77 86.04 86.63 NLL c 0.750 0.644 0.668 0.571 0.656 0.558 0.535 0.524 0.494 0.479 84.31 83.47 85.80 83.79 86.05 85.76 86.19 86.58 87.11 NLL c 0.730 0.645 0.656 0.569 0.648 0.545 0.527 0.518 0.488 0.473</figDesc><table><row><cell cols="2">Width Approach</cell><cell>1-Net</cell><cell>2-Nets</cell><cell cols="2">Linear-MixMo</cell><cell>Cut-MixMo</cell><cell>2-Cut-MixMos</cell></row><row><cell>w</cell><cell>CutMix</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell>c</cell><cell cols="6">0.921 0.815 0.776 0.695 0.841 0.824 0.824 0.846 0.7661 0.798</cell></row><row><cell></cell><cell># params</cell><cell>1.48M</cell><cell>2.95M</cell><cell></cell><cell cols="2">1.49M</cell><cell>2.99M</cell></row><row><cell>3</cell><cell>c</cell><cell cols="6">0.862 0.750 0.738 0.644 0.760 0.696 0.693 0.702 0.635 0.650</cell></row><row><cell></cell><cell># params</cell><cell>3.31M</cell><cell>6.62M</cell><cell></cell><cell cols="2">3.33M</cell><cell>6.66M</cell></row><row><cell>4</cell><cell>c</cell><cell cols="6">0.824 0.711 0.711 0.609 0.712 0.656 0.646 0.635 0.590 0.588</cell></row><row><cell></cell><cell># params</cell><cell>5.87M</cell><cell>11.74M</cell><cell></cell><cell cols="2">5.89M</cell><cell>11.79M</cell></row><row><cell>5</cell><cell>c</cell><cell cols="6">0.813 0.686 0.693 0.596 0.703 0.617 0.598 0.591 0.549 0.546</cell></row><row><cell></cell><cell># params</cell><cell>9.16M</cell><cell>18.32M</cell><cell></cell><cell cols="2">9.19M</cell><cell>18.39M</cell></row><row><cell></cell><cell>Top1</cell><cell cols="3">81.14 83.71 82.94 85.52 82.4</cell><cell cols="3">84.51 84.32 84.94 85.50 85.90</cell></row><row><cell>7</cell><cell>NLL c</cell><cell cols="6">0.764 0.648 0.673 0.573 0.675 0.581 0.562 0.543 0.516 0.498</cell></row><row><cell></cell><cell># params</cell><cell>17.92M</cell><cell>35.85M</cell><cell></cell><cell cols="2">17.97M</cell><cell>35.94M</cell></row><row><cell>10</cell><cell cols="2">Top1 81.63 # params 36.53M</cell><cell>73.07M</cell><cell></cell><cell cols="2">36.60M</cell><cell>73.21M</cell></row><row><cell>14</cell><cell cols="2">Top1 82.01 # params 71.55M</cell><cell>143.1M</cell><cell></cell><cell cols="2">71.64M</cell><cell>143.28M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following https://github.com/ildoonet/cutmix 3 https://github.com/google-research/augmix/ blob/master/cifar.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 20XX-AD011012262 made by GENCI. We acknowledge the financial support by the ANR agency in the chair VISA-DEEP (project number ANR-20-CHIA-0022-01).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1: Procedure for Cut-MixMo with M = 2 subnetworks / * Setup * / Parameters: First convolutions {c 0 , c 1 }, dense layers {d 0 , d 1 } and core network C, randomly initialized.  Extract features f i ← C(l i ) from core network <ref type="bibr" target="#b23">24</ref> Compute predictionsŷ 0 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of classifier selection methods for improving committee performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Aksela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MCS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pitfalls of in-domain uncertainty estimation and ensembling in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lyzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gridmix: Strong regularization through local context mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjune</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duhyeon</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bagging predictors. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Carratino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<title level="m">On mixup regularization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rich Caruana. Multitask learning. Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Imclr: Implicit contrastive learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Kyrillidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep ensembles on a fixed memory budget: One wide network or several thinner ones? ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadezhda</forename><surname>Chirkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lobacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MCS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Masksembles for uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Durasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient and scalable bayesian neural nets with rank-1 factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Akilesh Badrinaaraayanan, Vikas Verma, and Sarath Chandar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Patchup: A regularization technique for convolutional neural networks. ArXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep ensembles: A loss landscape perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intra-ensemble in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating scalable bayesian deep learning methods for robust computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fredrik K Gustafsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fmix: Enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training independent subnetworks for robust prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marton</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><forename type="middle">Roland</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Mingbo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Data augmentation revisited: Rethinking the distribution gap between clean and augmented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoxun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In UAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Co-mixup: Saliency guided joint mixup with supermodular diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Back propagation is sensitive to initial conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan B</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation, and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yann Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Howard</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Smoothmix: A simple yet effective data augmentation to train robust classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Zaigham</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Why M heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">On feature normalization and data augmentation. ArXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fencemask: A data augmentation approach for pre-extracted image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding mixup training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">On power laws in deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lobacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadezhda</forename><surname>Chirkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Kodryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry P</forename><surname>Vetrov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Affinity and diversity: Quantifying mechanisms of data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><forename type="middle">J</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Smullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Proving the lottery ticket hypothesis: Pruning is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">When does label smoothing help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A modern take on the bias-variance tradeoff in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Tantia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scicluna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Measuring calibration in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchuan</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Pensia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alliot</forename><surname>Nagle</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">When networks disagree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ensemble methods for hybrid neural networks. Neural networks for speech and image processing</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Resizemix: Mixing data with preserved object information and true labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dice: Diversity in deep ensembles via conditional redundancy adversarial estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A scalable laplace approximation for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hippolyt</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Efficient pattern recognition using a new transformation distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Sarmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Aggregated learning: A vector-quantization approach to learning neural network classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoumeh</forename><surname>Soflaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Al-Bashabsheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Improved mixedexample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael J Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Data augmentation using random image cropping and patching for deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Allerton Conference on Communication, Control and Computation</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Saliencymix: A saliency guided data augmentation strategy for better regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F M</forename><surname>Shahab Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mst</forename><surname>Sirazam Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wheemyung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taechoong</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Ho</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Automatic differentiation in ml: Where we are and where we should be going</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bart Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuleux</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Arnaud Bergeron, and Pascal Lamblin</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Attentive cutmix: An enhanced data augmentation approach for deep learning based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devesh</forename><surname>Walawalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Multiple networks are more efficient than one: Fast and accurate models via ensembles and cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Yair Movshovitz-Attias, and Elad Eban</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Mothernets: Rapid deep ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Wasay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuze</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<editor>MLSys</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Combining ensembles and data augmentation can harm your calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Calibration tests in multi-class classification: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Widmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Zachariah</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Bayesian deep learning and a probabilistic perspective of generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Stacked generalization. Neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Mixfeat: Mix feature in latent space learns discriminative space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumiyuki</forename><surname>Shiratani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidekazu</forename><surname>Iwaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Openreview preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Gradaug: A new regularization method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">How does mixup help with robustness and generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">When and how mixup improves calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Splitnet: Divide and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangsheng</forename><surname>Tin Lun Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
