<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LASAFT: LATENT SOURCE ATTENTIVE FREQUENCY TRANSFORMATION FOR CONDITIONED SOURCE SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehwa</forename><surname>Chung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Open University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonyoung</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LASAFT: LATENT SOURCE ATTENTIVE FREQUENCY TRANSFORMATION FOR CONDITIONED SOURCE SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-conditioned source separation, attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep-learning approaches have shown that Frequency Transformation (FT) blocks can significantly improve spectrogram-based single-source separation models by capturing frequency patterns. The goal of this paper is to extend the FT block to fit the multi-source task. We propose the Latent Source Attentive Frequency Transformation (LaSAFT) block to capture source-dependent frequency patterns. We also propose the Gated Point-wise Convolutional Modulation (GPoCM), an extension of Feature-wise Linear Modulation (FiLM), to modulate internal features. By employing these two novel methods, we extend the Conditioned-U-Net (CUNet) for multi-source separation, and the experimental results indicate that our LaSAFT and GPoCM can improve the CUNet's performance, achieving state-of-the-art SDR performance on several MUSDB18 source separation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Most of the deep learning-based models for Music Source Separation (MSS) are dedicated to a single instrument. However, this approach forces us to train an individual model for each instrument. Besides, trained models cannot use the commonalities between different instruments. A simple extension to multi-source separation is to generate several outputs at once. For example, models proposed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> generate multiple outputs. Although it shows promising results, this approach still has a scaling issue: the number of heads increases as the number of instrument increases, leading to <ref type="bibr" target="#b0">(1)</ref> performance degradation caused by the shared bottleneck, (2) inefficient memory usage.</p><p>Adopting conditioning learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> is a useful alternative. It can separate different instruments with the aid of the control mechanism. Since it does not need a multi-head output layer, there is no shared bottleneck. For example, the Conditioned-U-Net (CUNet) <ref type="bibr" target="#b2">[3]</ref> extends the U-Net <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> by exploiting Feature-wise Linear Modulation (FiLM) <ref type="bibr" target="#b6">[7]</ref>. It takes as input the spectrogram of a mixture and a control vector that indicates which instrument we want to separate and outputs the estimated spectrogram of the target instrument.</p><p>Meanwhile, recent spectrogram-based methods for Singing Voice Separation (SVS) <ref type="bibr" target="#b7">[8]</ref> or Speech Enhancement (SE) <ref type="bibr" target="#b8">[9]</ref> employed Frequency Transformation (FT) blocks to capture frequency patterns. Although stacking 2-D convolutions has shown remarkable results <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, it is hard to capture long-range dependencies along the frequency axis for fully convolutional networks with small sizes of kernels. FT blocks, which have fully-connected layers applied in a time-distributed manner, are useful to this end. Both models designed their building block to have a series of 2-D convolution layers followed by an FT block, reporting the state-of-the-art performance on SVS and SE, respectively.</p><p>In this paper, we aim to exploit FT blocks in a CUNet architecture. However, merely injecting an FT block to a CUNet does not inherit the spirit of FT block (although it does improve Source-to-Distortion Ratio (SDR) <ref type="bibr" target="#b11">[11]</ref> performance by capturing common frequency patterns observed across all instruments). We propose the Latent Source-Attentive Frequency Transformation (LaSAFT), a novel frequency transformation block that can capture instrument-dependent frequency patterns by exploiting the scaled dot-product attention <ref type="bibr" target="#b12">[12]</ref>. We also propose the Gated Point-wise Convolutional Modulation (GPoCM), a new modulation that extends the Feature-wise Linear Modulation (FiLM) <ref type="bibr" target="#b6">[7]</ref>. Our CUNet with LaSAFT and GPoCMs outperforms the existing methods on several MUSDB18 <ref type="bibr" target="#b13">[13]</ref> tasks. Our ablation study indicates that adding LaSAFT or replacing FiLMs with GPoCMs improves separation quality.</p><p>Our contributions are three-fold as follows:</p><p>• We propose LaSAFT, an attention-based novel frequency transformation block that captures instrumentdependent frequency patterns.</p><p>• We propose GPoCM, an extension of FiLM, to modulate internal features for conditioned source separation.</p><p>• Our model achieves state-of-the-art performance on several MUSDB18 tasks. We provide an ablation study to investigate the role of each component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE ARCHITECTURE</head><p>The baseline is similar to the CUNet <ref type="bibr" target="#b2">[3]</ref>. It consists of (1) the Conditioned U-Net and (2) the Condition Generator.</p><p>1. The Conditioned U-Net is a U-Net <ref type="bibr" target="#b4">[5]</ref> which takes a mixture spectrogram as input and outputs the estimated target spectrogram. It applies FiLM layers to modulate intermediate features with condition parameters generated by the condition generator.</p><p>2. The Condition Generator takes as input a condition vector and generates condition parameters. A condition vector is a one-hot encoding vector that specifies which instrument we want to separate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Conditioned U-Net</head><p>We extend the U-Net architecture used in <ref type="bibr" target="#b7">[8]</ref>, on which a state-of-the-art singing voice separation model is based. We first describe the common parts between ours and the original <ref type="bibr" target="#b7">[8]</ref>. As shown in the left part of <ref type="figure">Fig. 1</ref>, it consists of an encoder and a decoder: the encoder transforms the input mixture spectrogram M into a downsized spectrogram-like representation. The decoder takes it and returns the estimated target spectrogramT . It should be noted that spectrogram M andT are complex-valued adopting the Complex-as-Channel (CaC) separation method <ref type="bibr" target="#b7">[8]</ref>. In CaC, we view real and imaginary as separate channels. Thus, if the original mixture waveform is c-channeled (i.e., c = 2 for stereo), then the number of channels of M andT is (2c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. The baseline architecture</head><p>There are four types of components in the structure: 1 X 2 Convolution Layers are used for adjusting the number of channels. To increase the number of channels, we apply a 1 × 2 convolution withC output channels followed by ReLU <ref type="bibr" target="#b14">[14]</ref> activation to the given input M . Intermediate layers keep the number of channelsC. To restore the original number of channels, we apply another 1×2 convolution with (2c) output channels to the last intermediate block's output. Since the target spectrogram is complex-valued, we do not apply any activation functions.</p><p>An Intermediate Block transforms an input spectrogram-like tensor into an equally-sized tensor. For each block in the baseline, we use a Time-Frequency Convolution <ref type="bibr" target="#b7">[8]</ref> (TFC), a block of densely connected 2-D convolution layers <ref type="bibr" target="#b15">[15]</ref>. We denote the number of intermediate blocks in the encoder by L.</p><p>The decoder also has L blocks. There is an additional block between them.</p><p>A Down/Up-sampling Layer halves/doubles the scale of an input tensor. We use a strided/transposed-convolution. Skip Connections concatenate output feature maps of the same scale between the encoder and the decoder. They help the U-Net recover fine-grained details of the target. Unlike in the original U-Net <ref type="bibr" target="#b7">[8]</ref>, we modulate internal features in the decoder by applying FiLM layers, as shown in the right part in <ref type="figure">Fig. 1</ref>. Applying a FiLM is an effective way to condition a network, which applies the following operation to intermediate feature maps. We define a Film layer as follows:</p><formula xml:id="formula_0">F iLM (X i c |γ i c , β i c ) = γ i c · X i c + β i c<label>(1)</label></formula><p>where γ i c and β i c are parameters generated by the condition generator, and X i is the output of the i th decoder's intermediate block, whose subscript refers to the c th channel of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Condition Generator</head><p>The condition generator is a network that predicts condition parameters γ = (γ 1 1 , ..., γ L C ) and β = (β 1 1 , ..., β L C ). Our condition generator is similar to 'Fully-Connected Embedding' of the CUNet <ref type="bibr" target="#b2">[3]</ref> except for the usage of the embedding layer. It takes as input the one-hot encoding vector z ∈ {0, 1} |I| that specifies which one we want to separate among |I| instruments. The condition generator projects z into e z ∈ R E , the embedding of the target instrument, where E is the dimension of the embedding space.</p><p>It then applies a series of fully connected (i.e., linear or dense) layers, which doubles the dimension. We use ReLU <ref type="bibr" target="#b14">[14]</ref> as the activation function for each layer, and apply a dropout with p = 0.5 followed by a Batch Normalization (BN) <ref type="bibr" target="#b16">[16]</ref> to the output of each last two layers. The last hidden units are fed to two fully-connected layers to predict</p><formula xml:id="formula_1">γ = (γ 1 1 , ..., γ L C ) ∈ R |γ| and β = (β 1 1 , ..., β L C ) ∈ R |β| , where |γ| = |β| =CL.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Latent Source Attentive Frequency Transformation</head><p>We introduce the Time-Distributed Fully-connected layers (TDF) <ref type="bibr" target="#b7">[8]</ref>, an existing FT, and then propose our LaSAFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">TDF: a Frequency Transformation Block</head><p>A TDF is a series of two fully-connected layers. Suppose that the 2-D convolutional layer (e.g., TFC <ref type="bibr" target="#b7">[8]</ref>) takes an input representation X ∈ RC ×T ×F and outputs equally-sized tensor X . A TDF is applied separately and identically to each frame (i.e., X [i, j, :]) in a time-distributed fashion. Each layer is defined as consecutive operations: a fully-connected layer, BN, and ReLU. The number of hidden units is defined as F (l) /bf , where we denote the bottleneck factor by bf .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Extending TDF to the Multi-Source Task</head><p>Although injecting TDFs to the baseline also improves the SDR performance by capturing the common frequency patterns observed across all instruments (see §4), it does not inherit the spirit of TDF. To this end, we propose the Latent Source Attentive Frequency Transformation (LaSAFT) by adopting the scaled dot-product attention mechanism <ref type="bibr" target="#b12">[12]</ref>.</p><p>We first duplicate |I L | copies of the second layer of the TDF, as shown in the right side of <ref type="figure">Fig. 2</ref>, where |I L | refers to the number of latent instruments. |I L | is not necessarily the same as I for the sake of flexibility. For the given frame V ∈ R F , we obtain the |I L | latent instrumentdependent frequency-to-frequency correlations, denoted by V ∈ R F ×|I L | . We use components on the left side of <ref type="figure">Fig.  2</ref> to determine how much each latent source should be attended. LaSAFT takes as input the instrument embedding z e ∈ R 1×E . It has a learnable weight matrix K ∈ R |I L |×d k , where we denote the dimension of each instrument's hidden representation by d k . By applying a linear layer of size d k to z e , we obtain Q ∈ R d k . We now can compute the output of the LaSAFT as follows:</p><formula xml:id="formula_2">Attention(Q, K, V ) = sof tmax( QK T √ d k )V (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Latent Source Attentive Frequency Transformation</head><p>We apply a LaSAFT after each TFC in the encoder and after each Film/GPoCM layer in the decoder. We employ a skip connection for the final output of each block (i.e., it outputs X + lasaf t(X ), where X is an input of the lasaf t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gated Point-wise Convolutional Modulation</head><p>Before we describe the Gated Point-wise Convolutional Modulation (GPoCM), we first introduce the Point-wise Convolutional Modulation (PoCM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Point-wise Convolutional Modulation</head><p>The PoCM is an extension of FiLM. While FiLM does not have inter-channel operations, PoCM has them as follows:</p><formula xml:id="formula_3">P oCM (X i c |ω i c , β i c ) = β i c + j ω i cj · X i j (3)</formula><p>where ω i c = (ω i c1 ..., ω i cC ) and β i c are condition parameters, and X i is the output of the i th decoder's intermediate block, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Since this channel-wise linear combination can also be viewed as a point-wise convolution, we name it as PoCM. With inter-channel operations, PoCM can modulate features more flexibly and expressively than FiLM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Gated Point-wise Convolutional Modulation</head><p>In the decoder, we use the following 'Gated PoCM(GPoCM)' instead, as follows:</p><formula xml:id="formula_4">GP oCM (X i c |ω i c , β i c ) = σ(P oCM (X i c |ω i c , β i c )) X i c (4)</formula><p>where σ is a sigmoid and means the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>We use MUSDB18 dataset <ref type="bibr" target="#b13">[13]</ref>, which contains 86 tracks for training, 14 tracks for validation, and 50 tracks for test. Each track is stereo, sampled at 44100 Hz, consisting of the mixture and four sources (i.e., |I|=4): vocals, drums, bass, and other.</p><p>We train models using Adam <ref type="bibr" target="#b17">[17]</ref> with learning rate lr ∈ {0.0005, 0.001} depending on model depth. Each model is trained to minimize the mean squared error between the ground-truth STFT output and the estimated. For validation, we use mean absolute error of the target signal and the estimated. We apply data augmentation <ref type="bibr" target="#b18">[18]</ref> on the fly to obtain mixture clips comprised of sources from different tracks. We use SDR <ref type="bibr" target="#b11">[11]</ref> for the evaluation metric, by using the official tool 1 for MUSDB. We use the median SDR value over all the test set tracks for each run and report the mean SDR over three runs. More details are available online <ref type="bibr" target="#b1">2</ref>   <ref type="table">Table 1</ref>. An ablation study: dedicated means U-Nets for the single source separation, trained separately. FiLM CUNet refers the baseline in §2. The last row is our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We perform an ablation study to validate the effectiveness of the proposed methods compared with the baseline. In every model in the <ref type="table">Table 1</ref>, we use an FFT window size of 2048 and hop size of 1024 for spectrogram estimation. The configuration of the baseline (FiLM CUNET) is as follows: we use 7-blocked CUNet (i.e., L = 3), and we use a TFC <ref type="bibr" target="#b7">[8]</ref> for each block with the same configuration used in <ref type="bibr" target="#b7">[8]</ref>, where 5 convolution layers with kernel size 3 × 3 are densely connected, and the growth rate <ref type="bibr" target="#b15">[15]</ref> is set to be 24. We setC to be 24 as in <ref type="bibr" target="#b7">[8]</ref>. We use 32 for the dimensionality of the embedding space of conditions (i.e., R E ). We set |I L |, the number of latent instruments, to be 6.</p><p>In <ref type="table">Table 1</ref>, we can observe a considerable performance degradation when employing the existing method (FiLM CUNet), compared to the dedicated U-Net, which is trained separately for each instrument with the same configuration. Injecting TDF blocks to the baseline (FiLM CUNet + TDF) improves SDR by capturing the common frequency patterns. Replacing TDFs with LaSAFTs (FiLM CUNet + LaSAFT) significantly improves the average SDR score by 0.51dB, indicating that our LaSAFTs are more appropriate for multiinstrument tasks than TDFs. Our proposed model (GPoCM CUNet + LaSAFT) outperforms the others, achieving comparable but slightly inferior results to dedicated models.</p><p>Also, we compare against existing state-of-the-art models on the MUSDB18 benchmark. The first four rows of <ref type="table" target="#tab_2">Table 2</ref> show SDR scores of SOTA models. We take the SDR performance from the respective papers. For fair comparison, we use a 9-blocked 'GPoCM CUNet + LaSAFT' with the same frequency resolution as the other SOTA models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> (FFT window size = 4096). Our model yields comparable results against the existing methods and even outperforms the others on 'vocals' and 'other.' LaSAFT's ability, which allows the proposed model to extract latent instrument-attentive frequency patterns, significantly improves the SDR of 'other', since it contains various instruments such as piano and guitars. Also, existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATION TO PRIOR WORK</head><p>We first summarize the differences between the original CUNet <ref type="bibr" target="#b2">[3]</ref>, our baseline, and the proposed model as follows:</p><p>(1) our proposed model's U-Net is based on a generalized U-Net for source separation used in <ref type="bibr" target="#b7">[8]</ref>, but the U-Net of <ref type="bibr" target="#b2">[3]</ref> is more similar to the original U-Net <ref type="bibr" target="#b4">[5]</ref>, and <ref type="formula">(2)</ref> our baseline/proposed model applies FiLM/GPoCM to internal features in the decoder, but <ref type="bibr" target="#b2">[3]</ref> apply it in the encoder. The authors of <ref type="bibr" target="#b2">[3]</ref> tried to manipulate latent space in the encoder, assuming the decoder can perform as a general spectrogram generator, which is 'shared' by different sources. However, we found that this approach is not practical since it makes the latent space (i.e., the decoder's input feature space) more discontinuous. Via preliminary experiments, we observed that applying FiLMs in the decoder was consistently better than applying FilMs in the encoder. For multi-source separation, <ref type="bibr" target="#b3">[4]</ref> employed meta-learning, which is similar to conditioning learning. It also has external networks that generate parameters for the target instrument. Whiling we focus on modulating internal representations with GPoCM, <ref type="bibr" target="#b3">[4]</ref> focuses on generating parameters of the masking subnetwork. Also, models proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">19]</ref> operate in time domain, while ours in time-frequency domain.</p><p>While other multi-source separation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> estimate multiple sources simultaneously, we try to condition a shared U-Net. We expect that ours can be easily extended to more complicated tasks such as audio manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We propose LaSAFT that captures source-dependent frequency patterns by extending TDF to fit the multi-source task. We also propose the GPoCM that modulates features more flexibly and expressively than FiLM. We have shown that employing our LaSAFT and GPoCM in CUNet can significantly improve SDR performance. In future work, we try to reduce the number of parameters and the memory usage of LaSAFT to consider more latent instruments. Also, we extend the proposed model to audio manipulation tasks, where we can condition the model by providing various instructions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT)(No. 2020R1A2C1012624, 2019R1A6A3A13095526).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>PoCM layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>have shown that FT-based methods are beneficial for voice separation, which explains our model's excellent SDR performance on vocals. A comparison SDR performance of our models with other systems. ' * ' denotes model operating in time domain.</figDesc><table><row><cell>model</cell><cell cols="3">vocals drums bass other AVG</cell></row><row><cell>DGRU-DConv[1]</cell><cell>6.85</cell><cell>5.86</cell><cell>4.86 4.65 5.56</cell></row><row><cell>Meta-TasNet[4]  *</cell><cell>6.40</cell><cell>5.91</cell><cell>5.58 4.19 5.52</cell></row><row><cell>Nachmani[19]  *</cell><cell>6.92</cell><cell>6.15</cell><cell>5.88 4.32 5.82</cell></row><row><cell>D3Net [2]</cell><cell>7.24</cell><cell>7.01</cell><cell>5.25 4.53 6.01</cell></row><row><cell>proposed</cell><cell>7.33</cell><cell>5.68</cell><cell>5.63 4.87 5.88</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/sigsep/sigsep-mus-eval 2 https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dilated convolution with dilated gru for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2019-07-19" />
			<biblScope unit="page" from="4718" to="4724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">D3net: Densely connected multidilated densenet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditioned-u-net: Introducing a control mechanism in the u-net for multiple source separations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Meseguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Brocal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Society for Music Information Retrieval Conference, ISMIR</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meta-learning extractors for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="816" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigating u-nets with various intermediate blocks for spectrogram-based singing voice separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehwa</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21th International Society for Music Information Retrieval Conference, ISMIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Daewon Lee, and Soonyoung Jung</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Phasen: A phase-and-harmonicsaware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04697</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="106" to="110" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MUSDB18: a corpus for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>MUSDB18 -a corpus for music separation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
