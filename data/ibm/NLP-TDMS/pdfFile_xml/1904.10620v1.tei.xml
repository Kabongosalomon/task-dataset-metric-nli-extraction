<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Learning for Domain Adaptation of Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><forename type="middle">Yuan</forename><surname>Microsoft</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<email>nvasconcelos@ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Learning for Domain Adaptation of Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.</p><p>On the forward direction (i.e., "translation-tosegmentation", similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>), we propose a self-supervised learning (SSL) approach in training our segmentation adaptation model. Different from segmentation models trained on real data, the segmentation adaptation model is trained on both synthetic and real datasets, but the real data has no annotations. At every time, we may regard the predicted labels for real data with high confidence as the approximation to the ground truth</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent progress on image semantic segmentation <ref type="bibr" target="#b17">[18]</ref> has been driven by deep neural networks trained on large datasets. Unfortunately, collecting and manually annotating large datasets with dense pixel-level labels has been extremely costly due to large amount of human effort is required. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic images with computer-generated annotations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Despite this, the domain mismatch between the real images (target) and the synthetic data (source) cripples the models' performance. Domain adaptation addresses this domain shift problem. Specifically, we focus on the hard case of the problem where no labels from the target domain are available. This class of techniques is commonly referred to as Unsupervised Domain Adaptation.</p><p>Traditional methods for domain adaptation involve minimizing some measure of distance between the source and * This work was done when Yunsheng Li is an intern at Microsoft Cloud &amp; AI the target distributions. Two commonly used measures are the first and second order moment <ref type="bibr" target="#b1">[2]</ref>, and learning the distance metrics using Adversarial approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Both approaches have had good success in the classification problems (e.g., MNIST <ref type="bibr" target="#b15">[16]</ref>, USPS <ref type="bibr" target="#b6">[7]</ref> and SVHN <ref type="bibr" target="#b21">[22]</ref>); however, as pointed out in <ref type="bibr" target="#b36">[37]</ref>, their performance is quite limited on the semantic segmentation problem.</p><p>Recently, domain adaptation for semantic segmentation has made good progress by separating it into two sequential steps. It firstly translates images from the source domain to the target domain with an image-to-image translation model (e.g., CycleGAN <ref type="bibr" target="#b37">[38]</ref>) and then add a discriminator on top of the features of the segmentation model to further decrease the domain gap <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>. When the domain gap is reduced by the former step, the latter one is easy to learn and can further decrease the domain shift. Unfortunately, the segmentation model very relies on the quality of imageto-image translation. Once the image-to-image translation fails, nothing can be done to make it up in the following stages.</p><p>In this paper, we propose a new bidirectional learning framework for domain adaptation of image semantic segmentation. The system involves two separated modules: image-to-image translation model and segmentation adaptation model similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>, but the learning process involves two directions (i.e., "translation-to-segmentation" and "segmentation-to-translation"). The whole system forms a closed-loop learning. Both models will be motivated to promote each other alternatively, causing the domain gap to be gradually reduced. Thus, how to allow one of both modules providing positive feedbacks to the other is the key to success. labels, and then use them only to update the segmentation adaptation model while excluding predicted labels with low confidence. This process is referred as self-supervised learning, which aligns two domains better than one-trial learning that is widely used in existing approaches. Furthermore, better segmentation adaptation model would contribute to better translation model through our backward direction learning.</p><p>On the backward direction (i.e., "segmentation-totranslation"), our translation model would be iteratively improved by the segmentation adaptation model, which is different from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> where the image-to-image translation is not updated once the model is trained. For the purpose, we propose a new perceptual loss, which forces the semantic consistency between every image pixel and its translated version, to build the bridge between translation model and segmentation adaptation model. With the constraint in the translation model, the gap in visual appearance (e.g., lighting, object textures), between the translated images and real datasets (target) can be further decreased. Thus, the segmentation model can be further improved through our forward direction learning.</p><p>From the above two directions, both the translation model and the segmentation adaptation model complement each other, which helps achieve state-of-theart performance in adapting large-scale rendered image dataset SYNTHIA <ref type="bibr" target="#b27">[28]</ref>/GTA5 <ref type="bibr" target="#b26">[27]</ref>, to real image dataset, Cityscapes <ref type="bibr" target="#b4">[5]</ref>, and outperform other methods by a large margin. Moreover, the proposed method is general to different kinds of backbone networks.</p><p>In summary, our key contributions are:</p><p>1. We present a bidirectional learning system for semantic segmentation, which is a closed loop to learn the segmentation adaptation model and the image translation model alternatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We propose a self-supervised learning algorithm for the segmentation adaptation model, which incrementally align the source domain and the target domain at the feature level, based on the translated results.</p><p>3. We introduce a new perceptual loss to the image-toimage translation, which supervises the translation by the updated segmentation adaptation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation. When transferring knowledge from virtual images to real photos, it is often the case that there exists some discrepancy from the training to the test stage. Domain adaptation aims to rectify this mismatch and tune the models toward better generalization at testing <ref type="bibr" target="#b23">[24]</ref>. The existing work on domain adaptation has mainly focused on image classification <ref type="bibr" target="#b29">[30]</ref>. A lot of work aims to learn domain-invariant representations through minimizing the domain distribution discrepancy. Maximum Mean Discrepancy (MMD) loss <ref type="bibr" target="#b7">[8]</ref>, computing the mean of representations, is a common distance metric between two domains. As the extension to MMD, some statistics of feature distributions such as mean and covariance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> are used to match two different domains. Unfortunately, when the distribution is not Gaussian, solely matching mean and covariance is not enough to align the two different domains well. Adversarial learning <ref type="bibr" target="#b8">[9]</ref> recently becomes popular, and another kind of domain adaptation methods. It reduces the domain shift by forcing the features from different domains to fool the discriminator. <ref type="bibr" target="#b33">[34]</ref> would be the pioneer work, which introduces an adversarial loss on top of the high-level features of the two domains with the classification loss for the source dataset and achieves a better performance than the statistical matching methods. Expect for adversarial loss, some work proposed some extra loss functions to further decrease the domain shift, such as reweighted function for each class <ref type="bibr" target="#b3">[4]</ref>, and disentangled representations for separated matching <ref type="bibr" target="#b34">[35]</ref>. All of these methods work on simple and small classification datasets (e.g., MNIST <ref type="bibr" target="#b15">[16]</ref> and SVHN <ref type="bibr" target="#b21">[22]</ref>), and may have quite limited performance in more challenging tasks, like segmentation.</p><p>Domain Adaptation for Semantic Segmentation. Recently, more domain adaptation techniques are proposed for semantic segmentation models, since an enormous amount of labor-intensive work is required to annotate so many images that are needed to train high-quality segmentation networks. A possible solution to alleviate the human efforts is to train networks on virtual data which is labeled automatically. For example, GTA5 <ref type="bibr" target="#b26">[27]</ref> and SYHTHIA <ref type="bibr" target="#b27">[28]</ref> are two popular synthetic datasets of city streets with overlapped categories, similar views to the real datasets (e.g., CITYSCAPE <ref type="bibr" target="#b4">[5]</ref>, CamVid <ref type="bibr" target="#b0">[1]</ref>). Domain adaptation can be used to align the synthetic and the real datasets.</p><p>The first work to introduce domain adaptation for semantic segmentation is <ref type="bibr" target="#b12">[13]</ref>, which does the global and local alignments between two domains in the feature level. Curriculum domain adaptation <ref type="bibr" target="#b36">[37]</ref> estimates the global distribution and the labels for the superpixel, and then learns a segmentation model for the finer pixel. In <ref type="bibr" target="#b32">[33]</ref>, multiple discriminators are used for different level features to reduce domain discrepancy. In <ref type="bibr" target="#b30">[31]</ref>, foreground and background classes are separately treated for decreasing the domain shift respectively. All these methods target to directly align features between two domains. Unfortunately, the visual (e.g., appearance, scale, etc.) domain gap between synthetic and real data usually makes it difficult for the network to learn transferable knowledge.</p><p>Motivated by the recent progress of unpaired image-toimage translation work (e.g., CycleGAN <ref type="bibr" target="#b37">[38]</ref>, UNIT <ref type="bibr" target="#b16">[17]</ref>, MUNIT <ref type="bibr" target="#b13">[14]</ref>), the mapping from virtual to realistic data is regarded as the image synthesis problem. It can help re-duce the domain discrepancy before training the segmentation models. Based on the translated results, Cycada <ref type="bibr" target="#b11">[12]</ref> and DCAN <ref type="bibr" target="#b35">[36]</ref> further align features between two domains in feature level. By separately reducing the domain shift in learning, these approaches obtained the state-of-the-art performance. However, the performance is limited by the quality of image-to-image translation. Once it fails, nothing can be done in the following step. To address this problem, we introduce a bidirectional learning framework where both translation and segmentation adaption models can promote each other in a closed loop.</p><p>There are two most related work. In <ref type="bibr" target="#b5">[6]</ref>, the segmentation model is also used to improve the image translation, but not to adapt the source domain to the target domain since it is only trained on source data. <ref type="bibr" target="#b38">[39]</ref> also proposed a selftraining method for training the segmentation model iteratively. However, the segmentation model is only trained on source data and uses none of image translation techniques.</p><p>Bidirectional Learning. The kind of techniques were first proposed to solve the neural machine translation problem, such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>, which train a language translation model for both directions of a language pair. It improves the performance compared with the uni-direction learning and reduces the dependency on large amount of data. Bidirectional learning techniques were also extended to image generation problem <ref type="bibr" target="#b24">[25]</ref>, which trains a single network for both classification and image generation problem from both top-to-down and down-to-top directions. A more related work <ref type="bibr" target="#b28">[29]</ref> proposed bidirectional image translation (i.e., source-to-target, and target-to-source), then trained two classifiers on both domains respectively and finally fuses the classification results. By contrast, our bidirectional learning refers to translation boosting the performance of segmentation and vise verse. The proposed method is used to deal with the semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given the source dataset S with segmentation labels Y S (e.g., synthetic data generated by computer graphics) and the target dataset T with no labels (i.e., real data), we want to train a network for semantic segmentation, which is finally tested on the target dataset T . Our goal is to make its performance to be as close as possible to the model trained on T with ground truth labels Y T . The task is unsupervised domain adaptation for semantic segmentation. The task is not easy since the visual (e.g., lighting, scale, object textures, etc.) domain gap between S and T makes it difficult for the network to learn transferable knowledge at once.</p><p>To address this problem, the recent work <ref type="bibr" target="#b11">[12]</ref> proposed two separated subnetworks. One is image-to-image translation subnetwork F which learn to translate an image from S to T in absence of paired examples. The another is segmen- tation adaptation subnetwork M that is trained on translated results F(S), which have the same labels Y S to S, and the target images T with no labels. Both subnetworks are learnt in a sequential way shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. Such a two-stage solution has two advantages: 1) F helps decrease the visual domain gap; 2) when domain gap is reduced, M is easy to learn, causing better performance. However, the solution has some limitations. Once F is learnt, it is fixed. There is no feedback from M to boost the performance of F. Besides, one-trial learning for M seems to just learn limited transferable knowledge.</p><p>In this section, we propose a new learning framework which can address the above two issues well. We inherit the way of separated subnetworks, but employ a bidirectional learning instead (in Section 3.1), which uses a closed-loop to iteratively update both F and M. Furthermore, we introduce a self-supervised learning to allow M being selfmotivated in training (in Section 3.2). The network architecture and loss functions are presented in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bidirectional Learning</head><p>Our learning consists of two directions shown in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_0">(b).</formula><p>The forward direction (i.e., F → M) is similar to the behavior of previous sequential learning <ref type="bibr" target="#b11">[12]</ref>. We first train the image-to-image translation model F using images from T and S. Then, we get the translated results S = F(S). Note that F won't change the labels of S , which are the same to Y S (labels of S). Next, we train the segmentation adaptation model M using S with Y S and T . The loss function to learn M can be defined as:</p><formula xml:id="formula_1">M = λ adv adv (M(S ), M(T )) + seg (M(S ), Y S ), (1)</formula><p>where adv is adversarial loss that enforces the distance between the feature representations of S and the feature representations of T (obtained after S , T are fed into M) as small as possible. seg measures the loss of semantic segmentation. Since only S have the labels, we solely measure the accuracy for the translated source images S .</p><p>The backward direction (i.e., M → F) is newly added. The motivation is to promote F using updated M. In <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref>, a perceptual loss, which measures the distance of features obtained from a pre-trained network on object recognition, is used in the image translation network to improve the quality of translated result. Here, we use M to compute features for measuring the perceptual loss. By adding the other two losses: GAN loss and image reconstruction loss, the loss function for learning F can be defined as:</p><formula xml:id="formula_2">F = λ GAN [ GAN (S , T ) + GAN (S, T )] + λ recon [ recon (S, F −1 (S )) + recon (T , F(T )] + per (M(S), M(S )) + per (M(T ), M(T ),<label>(2)</label></formula><p>where three losses are computed symmetrically, i.e., S → T and T → S, to ensure the image-to-image translation consistent. The GAN loss GAN enforces two distributions between S and T similar to each other.</p><formula xml:id="formula_3">T = F −1 (T ),</formula><p>where F −1 is the reverse function of F that maps the image from T to S. The loss recon measures the reconstruction error when the image from S is translated back to S. per is the perceptual loss that we propose to maintain the semantic consistency between S and S or between T and T . That is, once we obtained an ideal segmentation adaptation model M, whether S and S , or T and T should have the same labels, even although there is the visual gap between S and S , or between T and T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised Learning for Improving M</head><p>In the forward direction (i.e., F → M), if the label is available for both the source domain S and the target domain T , the fully supervised segmentation loss seg is always the best choice to reduce the domain discrepancy. But in our case, the label for the target dataset is missing. As we known, self-supervised learning (SSL) has been used in semi-supervised learning before, especially when the labels of dataset are insufficient or noisy. Here, we use SSL to help promote the segmentation adaptation model M.</p><p>Based on the prediction probability of T , we can obtain some pseudo labels Y T with high confidence. Once we have the pseudo labels, the corresponding pixels can be aligned directly with S according to the segmentation loss. Thus, we modify the overall loss function used to learn M (in Equation 1) as:</p><formula xml:id="formula_4">M = λ adv adv (M(S ), M(T )) + seg (M(S ), Y S ) + seg (M(T ssl ), Y T ),<label>(3)</label></formula><p>where T ssl ⊂ T is a subset of the target dataset in which the pixels have the pseudo labels Y T . It can be empty at the beginning. When a better segmentation adaptation model M is achieved, we can use M to predict more high-confident labels for T , causing the size of T ssl to grow. The recent work <ref type="bibr" target="#b38">[39]</ref> also use SSL for segmentation adaptation. By contrast, SSL used in our work is combined with adversarial learning, which can work much better for the segmentation adaptation model. We use the illustration (shown in <ref type="figure">Figure 2</ref>) to explain the principle of this process. When we learn the segmentation adaptation model for the first time, T ssl is empty and the </p><formula xml:id="formula_5">Input: (S, Y S ), (T , T ssl = ∅), M (0) Output: M (K) N (F (K) ) for k ← 1 to K do (Bidirectional Learning) train F (k) with Equation 2 train M (k) 0 with Equation 1 for i ← 1 to N do (SSL) update T ssl with M (k) i−1 train M (k) i</formula><p>again with Equation 3 end for end for domain gap between S and T can be reduced with the loss shown in Equation 1. This process is shown in <ref type="figure">Figure 2</ref> (a). Then we pick up the points in the target domain T that have been well aligned with S to construct the subset T ssl . In the second step, we can easily shift T ssl to S and keep them being aligned with the help of the segmentation loss provided by the pseudo labels. This process is shown in the middle of <ref type="figure">Figure 2 (b)</ref>. Therefore, the amount of data in T that needs to be aligned with S is decreased. We can continue to shift the remaining data to S same as step 1, as shown the right side of <ref type="figure">Figure 2 (b)</ref>. It worth noting that SSL helps adversarial learning process focus on the rest data that is not fully aligned at each step, since adv can hardly change the data from S and T ssl that has been aligned well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network and Loss Function</head><p>In this section, we introduce the network architecture (shown in <ref type="figure" target="#fig_2">Figure 3</ref>), details of loss functions and the training process (shown in Algorithm 1). The network is mainly composed with two components -the image translation model and segmentation adaptation model. While the translation model is learned, the loss GAN and loss recon (shown in <ref type="figure" target="#fig_2">Figure 3</ref> and Equation 2) can be defined as:</p><formula xml:id="formula_6">GAN (S , T ) = E I T ∼T [D F (I T )] + E I S ∼S [1 − D F ((I S ))], recon (S, F −1 (S )) = E I S ∼S [||F −1 ((I S ))−I S || 1 ],</formula><p>where I S and I T are the input images from source and target dataset. I S is the translated image given by F. D F is the discriminator added to reduce the difference between I T and I S . For the reconstruction loss, L 1 norm is used to keep the cycle consistency between I S and F −1 (I S ) when F −1  is the reverse function of F. Here, we only show two losses for one direction, and GAN (S, T ), recon (T , F(T )) can be defined similarly.</p><formula xml:id="formula_7">M 0 (1) (F (1) ) M 2 (1) (F (1) ) M 0 (2) (F (2) ) M 2 (2) (F (2) ) ground truth</formula><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the perceptual loss per connects the translation model and segmentation adaptation model. When we learn the perceptual loss per for the translation model, instead of only keeping the semantic consistency between I S and its translated result I S , we add another term weighted by λ per recon , to keep the semantic consistency between I S and its corresponding reconstruction F −1 (I S ). With the new term, the translation model can be more stable especially for the reconstruction part. per is defined as: When the segmentation adaptation model is trained, it requires the adversarial learning with the loss adv and the self-supervised learning with the loss seg (shown in Equation 3). For adversarial learning, we add a discriminator D M to decrease the difference between the source and target probabilities shown in <ref type="figure" target="#fig_2">Figure 3</ref>. adv can be defined as:</p><formula xml:id="formula_8">adv (M(S ), M(T )) = E I T ∼T [D M (M(I T ))] + E I S ∼S [1 − D M (M(I S ))].</formula><p>The segmentation loss seg uses the cross-entropy loss. For the source image I S , seg can be defined as:</p><formula xml:id="formula_9">seg (M(S ), Y S ) = − 1 HW H,W C c=1 1 [c=y hw S ] log P hwc S ,</formula><p>where y S is the label map for I S , C is the number of classes, H and W are the height and width of the output probability map. P S is the source probability of the segmentation adaptation model which can be defined as P S = M(I S ).</p><p>For the target image I T , we need to define how to choose the pseudo label map y T for it. We choose to use a common method we call as "max probability threshold(MPT)" to filter the pixels with high prediction confidence in I T . Thus we can define y T as y T = argmax M(I T ) and the mask map for y T as m T = 1 [argmax M(I T )&gt;threshold] . Thus the segmentation loss for I T can be expressed as:</p><formula xml:id="formula_10">seg (M(T ssl ), Y T ) = − 1 HW H,W m hw T C c=1 1 [c=y hw T ] log P hwc T ,</formula><p>where P T is the target output of M.</p><p>We present the training processing in Algorithm 1. The training process consists of two loops. The outer loop is mainly to learn the translation model and the segmentation adaptation model through the forward direction and the backward direction. The inner loop is mainly used to implement the SSL process. In the following section, we will introduce how to choose the number of iteration for learning F, M, and how to estimate the MPT for SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>To know the effectiveness of bidirectional learning and self-supervised learning for improving M, we conduct some ablation studies. We use GTA5 <ref type="bibr" target="#b26">[27]</ref> as the source dataset and Cityscapes <ref type="bibr" target="#b4">[5]</ref> as the target dataset. The translation model is CycleGAN <ref type="bibr" target="#b37">[38]</ref> and the segmentation adaptation model is DeepLab V2 <ref type="bibr" target="#b2">[3]</ref> with the backbone ResNet101 <ref type="bibr" target="#b10">[11]</ref>. All the following experiments use the same model, unless it is specified.</p><p>Here, we first provide the description of notations used in the following ablation study and tables. M (0) is the initial model to start the bidirectional learning and is trained only with source data. M (1) is trained with source and target data with adversarial learning. For M (0) <ref type="figure" target="#fig_0">(F (1)</ref> ), a translation model F <ref type="bibr" target="#b0">(1)</ref> is used to translate the source data and then a segmentation model M (0) is learned based on the translated source data. M (k) i (F (k) ) for k = 1, 2 and i = 0, 1, 2 refers to the model of k-th iteration for the outer loop and i-th iteration for the inner loop in Algorithm 1. </p><formula xml:id="formula_11">M (0) (F (1) ) 41.1 M (1) 0 (F (1) ) 42.7 M (2) 0 (F (2) )</formula><p>43.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bidirectional Learning without SSL</head><p>We show the results obtained by the model trained in a bidirectional learning system without SSL. In <ref type="table" target="#tab_0">Table 1</ref>, M (0) is our baseline model that gives the lowerbound for mIoU. We find a similar performance between the model M <ref type="bibr" target="#b0">(1)</ref> and M (0) (F <ref type="bibr" target="#b0">(1)</ref> ) both of which achieve more than 7% improvement compared to M (0) and about 1.6% further improvement is given by M (1) (F <ref type="bibr" target="#b0">(1)</ref> ). It means segmentation adaptation model and the translation model can work independently and when combined together which is basically one iteration of the bidirectional learning they can be complementary to each other. We further show that through continue training the bidirectional learning system, in which case M (1) (F <ref type="bibr" target="#b0">(1)</ref> ) is used to replace M (0) for the backward direction, a better performance can be given by the new model M</p><p>(2) 0 (F <ref type="bibr" target="#b1">(2)</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bidirectional Learning with SSL</head><p>In this section, we show how the SSL can further improve the ability of segmentation adaption model and in return influence the bidirectional learning process. In <ref type="table" target="#tab_3">Table 2</ref>, we show results given by two iterations(k = 1, 2) based on Algorithm 1. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show the segmentation results and the corresponding mask map given by the max probability threshold (MPT) which is 0.9. In <ref type="figure" target="#fig_3">Figure 4</ref>, the white pixels are the ones with prediction confidence higher than MPT and the black pixels are the low confident pixels.</p><p>While k = 1, when model M (1) 0 (F <ref type="bibr" target="#b0">(1)</ref> ) is updated to M (1) 2 (F <ref type="bibr" target="#b0">(1)</ref> ) with SSL, the mIoU can be improved by 4.5%. We can find for each category when the IoU is below 50, a big improvement can be got from M <ref type="bibr" target="#b0">(1)</ref> ). It can prove our previous analysis in section 3.2 that with SSL the well aligned data from source and target domain can be kept and the rest data can be further aligned through the adversarial learning process.</p><formula xml:id="formula_12">(1) 0 (F (1) ) to M (1) 2 (F</formula><p>While k = 2, we first replace M (0) with M (1) 2 (F (1) ) to start the backward direction. Without SSL the mIoU is 44.3 which is a larger improvement compared to the results shown in <ref type="table" target="#tab_0">Table 1</ref>. It can further prove our discussion in section 4.1 about the importance role played by the segmentation adaptation model in the backward direction. Furthermore, we can find from <ref type="table" target="#tab_3">Table 2</ref>, although in the beginning of the second iteration the mIoU drops from 47.2 to 44.3, while SSL is induced, the mIoU can be promoted to 48.5   slope of pixel ratio <ref type="figure">Figure 5</ref>: Relationship between pixel ratio and the prediction confidence which outperforms the results in the first iteration. From the segmentation results shown in <ref type="figure" target="#fig_3">Figure 4</ref>, our findings can be further confirmed and the most important thing is as we improve the segmentation performance, the segmentation adaptation model can give more confident prediction which can be observed by the increasing white area in the mask map. It gives us the motivation to use the mask map to choose the threshold and number of iterations for the SSL process in Algorithm 1.</p><formula xml:id="formula_13">0 (F (1) ) 69% 42.7 M (1) 1 (F (1) ) 79% 46.8 M (1) 2 (F (1) ) 81% 47.2 M (1) 3 (F (1) )<label>81%</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyper Parameter Learning</head><p>We will describe how to choose the threshold to filter out data with high confidence and the iteration number N in Algorithm 1.</p><p>When we choose the threshold, we have to balance between two folds. On one hand, we desire the predicted labels with high confidence as many as possible (presented as white areas in <ref type="figure" target="#fig_3">Figure 4</ref>). On the other hand, we want to avoid inducing too much noise caused by the incorrect prediction, namely, the threshold should be as high as possible. We present the relationship of the prediction confidence (maximum class probability of per pixel from M) and the ratio between selected pixels and all pixels (i.e., percentage of all white areas shown in <ref type="figure" target="#fig_3">Figure 4</ref>) on the left side of <ref type="figure">Figure 5</ref>, then show the slope in the right side of <ref type="figure">Figure 5</ref>. We can find when the prediction confidence increases from 0.5 to 0.9, the ratio decreases almost linearly and the slope stays almost unchanged. But from 0.9 to 0.99, the ratio decreases much faster. Based on the observation, we choose the inflection point 0.9 as the threshold as the trade-off between the number and the quality of selected labels.</p><p>In order to further prove our choice, in <ref type="table" target="#tab_1">Table 3</ref>, we show segmentation results using different thresholds to the selfsupervised learning of M K N when K = 1 and N = 1 in Algorithm 1. As another option, we also consider soft threshold instead of hard one, namely, every pixel being weighted by its maximum class probability. We show the result on the bottom row. All the results confirm our analysis. When the threshold is lower than 0.9, the uncorrected prediction becomes the key issue to influence the performance of SSL. While we increase the threshold to 0.95, the SSL process is more sensitive to the number of pixels that can be used. When we use soft threshold, the result is still worse. It is probably because an amount of labeling noise are involved and the bad impact cannot be well alleviated by assigning a lower weight to the noise label. Thus, 0.9 seems to be a good choice for the threshold in the following experiments. For the iteration number N , we select a proper value according to the predicted labels as well. When N increases, the segmentation adaptation model becomes much stronger, causing more labels to be used for SSL. Once the pixel ratio for SSL stops increasing, it means that the learning for the segmentation adaptation model is converged and nearly no improved. We definitely increase the value of K to start another iteration. In <ref type="table" target="#tab_2">Table 4</ref>, we show some segmentation results with the theshold 0.9 as we increase the value of N . We can find the mIoU becomes better with the increasing of N . When N = 2 or 3, the mIoU almost stopped increasing, and the pixel ratio stay around the same. It may suggest that N = 2 is a good choice, and we use it in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we compare the results obtained between our method and the state-of-the-art methods.</p><p>Network Architecture. In our experiments, we choose to use DeepLab V2 <ref type="bibr" target="#b2">[3]</ref> with ResNet101 <ref type="bibr" target="#b10">[11]</ref> and FCN-8s <ref type="bibr" target="#b17">[18]</ref> with VGG16 <ref type="bibr" target="#b31">[32]</ref> as our segmentation model. They are initialized with the network pre-trained with ImageNet <ref type="bibr" target="#b14">[15]</ref>. The discriminator we choose for segmentation adaptation model is similar to <ref type="bibr" target="#b25">[26]</ref> which has 5 convolution layers with kernel 4 × 4 with channel numbers {64, 128, 256, 512, 1} and stride of 2. For each convolutional layer except the last one, a leaky ReLU <ref type="bibr" target="#b19">[20]</ref> parameterized by 0.2 is followed. For the image translation model, we follow the architecture of CycleGAN <ref type="bibr" target="#b37">[38]</ref> with 9 blocks and add the segmentation adaptation model as the perceptual loss.</p><p>Training. When training CycleGAN <ref type="bibr" target="#b37">[38]</ref>, the image is randomly cropped to the size 452 × 452 and it is trained for 20 epochs. For the first 10 epochs, the learning rate is 0.0002 and decreases to 0 linearly after 10 epochs. We set λ GAN = 1, λ recon = 10 in Equation 3 and set λ per = 0.1, λ per recon = 10 for the perceptual loss. When training the segmentation adaptation model, images are resized with the long side to be 1, 024 and the ratio is kept. Different parameters are used for DeepLab V2 <ref type="bibr" target="#b2">[3]</ref> and FCN-8s <ref type="bibr" target="#b17">[18]</ref>. For DeepLab V2 with ResNet 101, we use SGD as the optimizer. The initial learning rate is 2.5 × 10 −4 and decreased with 'poly' learning rate policy with power as 0.9. For FCN-8s with VGG16, we use Adam as the optimizer with momentum as 0.9 and 0.99. The initial learning rate is 1 × 10 −5 and decreased with 'step' learning rate policy with step size as 5000 and γ = 0.1. For both DeepLab V2 and FCN-8s, we use the same discriminator that is trained with Adam optimizer with initial learning rate as 1 × 10 −4 for DeepLab V2 and 1 × 10 −6 for FCN-8s. The momentum is set as 0.9 and 0.99. We set λ adv = 0.001 for ResNet101 and 1 × 10 −4 for FCN-8s in Equation 1. Dataset. As we have mentioned before, two synthetic datasets -GTA5 <ref type="bibr" target="#b26">[27]</ref> and SYNTHIA <ref type="bibr" target="#b27">[28]</ref> are used as the source dataset and Cityscapes <ref type="bibr" target="#b4">[5]</ref> is used as the target dataset. For GTA5 <ref type="bibr" target="#b26">[27]</ref>, it contains 24, 966 images with the resolution of 1914 × 1052 and we use the 19 common categories between GTA5 and Cityscapes dataset. For SYN-THIA <ref type="bibr" target="#b27">[28]</ref>, we use the SYNTHIA-RAND-CITYSCAPES set which contains 9, 400 images with the resolution 1280× 760 and 16 common categories with Cityscapes <ref type="bibr" target="#b4">[5]</ref>. For Cityscapes <ref type="bibr" target="#b4">[5]</ref>, it is splited into training set, validation set and testing set. The training set contains 2, 975 images with the resolution 2048 × 1024. We use the training set as the target dataset only. Since the ground truth labels for the testing set are missing, we have to use the validation set which contains 500 images as the testing set in our experiments. Comparison with State-of-Art. We compare the results between our method and the state-of-the-art method with two different backbone networks: ResNet101 and VGG16 respectively. We perform the comparison on two tasks: "GTA5 to Cityscapes" and "SYNTHIA to Cityscapes". In <ref type="table" target="#tab_4">Table 5</ref>, we present the adaptation result on the task "GTA5  to Cityscapes" with ResNet101 and VGG16. We can observe the role of backbone in all domain adaptation methods, namely ResNet101 achieves a much better result than VGG16. In <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19]</ref>, they mainly focus on featurelevel alignment with different adversarial loss functions. But working only on the feature level is not enough, even though the best result <ref type="bibr" target="#b35">[36]</ref> among them is still about 5% worse than our results. Cycada <ref type="bibr" target="#b11">[12]</ref> (we run their codes with ResNet101) and DCAN <ref type="bibr" target="#b35">[36]</ref> used the translation model followed by the segmentation adaptation model to further reduce the visual domain gap, and both achieved very similar performance. Ours uses similar loss function compared to Cycada <ref type="bibr" target="#b11">[12]</ref>, but with a new proposed bidirectional learning method, 6% improvement can be achieved. CBST <ref type="bibr" target="#b38">[39]</ref> proposed a self-training method, and further improved the performance with space prior information. For a fair comparison, we show the results that only use self-training. With VGG16, we can get 10.4% improvement. Therefore, we can find without bidirectional learning, the self-training method is not enough to achieve a good performance.</p><p>In <ref type="table" target="#tab_5">Table 6</ref>, we present the adaptation result on the task "SYNTHIA to Cityscapes" for both ResNet101 and VGG16.</p><p>The domain gap between SYNTHIA and Cityscapes is much larger than that of GTA5 and Cityscapes, and their categories are not fully overlapped. As the baseline results <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19]</ref> chosen for ResNet101 only use 13 categories, we also list results for the 13 categories for a fair comparison. We can find from <ref type="table" target="#tab_5">Table 6</ref>, as the domain gap increases, the adaptation result for Cityscapes is much worse compared to the result in <ref type="table" target="#tab_4">Table 5</ref>. For exam-ple, the category like 'road', 'sidewalk' and 'car' are more than 10% worse. And this problem will have a bad impact on the SSL because of the lower prediction confidence. But we can still achieve at least 4% better than most of other results given by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b32">33]</ref>. Performance Gap to Upper Bound. We use the target dataset with ground truth labels to train a segmentation model, which shares the same backbone that we used, to get the upper-bound result. For "GTA5 to Cityscapes" with 19 categories, the upper bounds are 65.1 and 60.3 for ResNet101 and VGG16 respectively. For "SYNTHIA to Cityscapes" with 13 categories for ResNet101 and 16 categories for VGG16, the upper bounds are 71.7 and 59.5. For our method, although the performance gap is 16.6 at least, it has been reduced significantly compared to other methods. However, it means there is still big room to improve the performance. We leave it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a bidirectional learning method with self-supervised learning for segmentation adaptation problem. We show via a lot of experiments that segmentation performance for real dataset can be improved when the model is trained bidirectionally and achieve the stateof-the-art result for multiple tasks with different networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sequential Learning vs Bidirectional Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Algorithm 1</head><label>21</label><figDesc>Self-supervised learning process Training process of our network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Network architecture and loss function target image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Segmentation result for each step in bidirectional learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>per (M(S), M(S )) = λ per E I S ∼S ||M(I S ) − M((I S ))|| 1 + λ per recon E I S ∼S [||M(F −1 ((I S ))) − M(I S )|| 1 ] Due to the symmetry, per (M(T ), M(T )) (shown in Equation 2) can be defined in a similar way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of bidirectinal learning</figDesc><table><row><cell cols="2">GTA5 → Cityscapes</cell></row><row><cell>model</cell><cell>mIoU</cell></row><row><cell>M (0)</cell><cell>33.6</cell></row><row><cell>M (1)</cell><cell>40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Influence of threshold</figDesc><table><row><cell></cell><cell cols="2">GTA5 → Cityscapes</cell><cell></cell></row><row><cell></cell><cell>model</cell><cell cols="2">threshold mIoU</cell></row><row><cell>M</cell><cell>(1) 1 (F (1) )</cell><cell>0.95</cell><cell>45.7</cell></row><row><cell>M</cell><cell>(1) 1 (F (1) )</cell><cell>0.9</cell><cell>46.8</cell></row><row><cell>M</cell><cell>(1) 1 (F (1) )</cell><cell>0.8</cell><cell>46.4</cell></row><row><cell>M</cell><cell>(1) 1 (F (1) )</cell><cell>0.7</cell><cell>45.9</cell></row><row><cell>M</cell><cell>(1) 1 (F (1) )</cell><cell>−</cell><cell>44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Influence of N</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GTA5 → Cityscapes</cell></row><row><cell></cell><cell cols="2">model</cell><cell cols="2">pixel ratio mIoU</cell></row><row><cell></cell><cell>M</cell><cell>(1) 0</cell><cell>66%</cell><cell>40.9</cell></row><row><cell>M</cell><cell>(1)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of bidirectional learning with self-supervised learning 69.0 12.7 69.5 9.9 19.5 22.8 31.7 15.3 73.9 11.3 67.2 54.7 23.9 53.4 29.7 4.6 11.6 26.1 32.5 33.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GTA5 → Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell></cell><cell>M (0)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k = 1</cell><cell cols="20">M M (1) 0 (F (1) ) 89.1 42.0 82.0 24.3 15.1 27.4 35.7 24.6 81.1 32.4 78.0 57.6 28.7 76.0 26.5 36.0 4.0 25.7 24.9 42.7 (1) 1 (F (1) ) 91.2 47.8 84.0 34.8 28.9 31.7 37.7 36.0 84.0 40.4 76.6 57.9 25.3 80.4 31.2 41.7 2.8 27.2 32.4 46.8 M (1) 2 (F (1) ) 91.4 47.9 84.2 32.4 26.0 31.8 37.3 33.0 83.3 39.2 79.2 57.7 25.6 81.3 36.3 39.7 2.6 31.3 33.5 47.2</cell></row><row><cell>k = 2</cell><cell cols="20">M M (2) 0 (F (2) ) 88.2 41.3 83.2 28.8 21.9 31.7 35.2 28.2 83.0 26.2 83.2 57.6 27.0 77.1 27.5 34.6 2.5 28.3 36.1 44.3 (2) 1 (F (2) ) 91.2 46.1 83.9 31.6 20.6 29.9 36.4 31.9 85.0 39.7 84.7 57.5 29.6 83.1 38.8 46.9 2.5 27.5 38.2 47.6 M (2) 2 (F (2) ) 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison results from GTA5 to Cityscapes 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19 65.0 12.0 28.6 4.5 31.1 42.0 42.7 AdaptSegNet[33] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 DCAN[36] 85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.50 26.9 11.6 41.7 CLAN[19] 87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 Ours 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 26.7 77.4 23.7 20.5 20.4 30.3 15.9 80.9 25.4 69.5 52.6 11.1 79.6 24.9 21.2 1.30 17.0 6.70 36.2 CLAN[19] 88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0 36.6 Ours 89.2 40.9 81.2 29.1 19.2 14.2 29.0 19.6 83.7 35.9 80.7 54.7 23.3 82.7 25.8 28.0 2.3 25.7 19.9 41.3</figDesc><table><row><cell>GTA5 → Cityscapes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison results from SYNTHIA to Cityscapes</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SYNTHIA → Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oracle</cell><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>t-light</cell><cell>t-sign</cell><cell>vegetation</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>ResNet101[11] 71.7</cell><cell cols="4">AdaptSegNet[33] 79.2 37.2 78.8 CLAN[19] 81.3 37.0 80.1 Ours 86.0 46.7 80.3</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell cols="10">9.9 16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3 14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>45.9 47.8 51.4</cell></row><row><cell></cell><cell>FCN wild[13]</cell><cell cols="3">11.5 19.6 30.8</cell><cell>4.4</cell><cell cols="2">0.0 20.3</cell><cell>0.1</cell><cell cols="4">11.7 42.3 68.7 51.2</cell><cell>3.8</cell><cell>54.0</cell><cell>3.2</cell><cell>0.2</cell><cell>0.6</cell><cell>20.2</cell></row><row><cell>VGG16[32] 59.5</cell><cell>Curriculum[37] CBST[39] DCAN[36]</cell><cell cols="16">65.2 26.1 74.9 69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 0.1 0.5 10.7 3.5 3.0 76.1 70.6 47.1 8.2 43.2 20.7 6.6 79.9 30.4 70.8 1.6 0.6 22.3 6.7 23.0 76.9 73.9 41.9 16.7 61.7 11.5 10.3 38.6 0.7 13.1 3.7 32.4</cell><cell>29.0 35.4 35.4</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">72.0 30.3 74.5</cell><cell>0.1</cell><cell cols="10">0.3 24.6 10.2 25.2 80.5 80.0 54.7 23.2 72.7 24.0</cell><cell>7.5</cell><cell>44.9</cell><cell>39.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially funded by NSF awards IIS-1546305 and IIS-1637941.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5077" to="5085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reweighted adversarial adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7976" to="7985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain stylization: A strong, simple baseline for synthetic to real image domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Daml: Domain adaptation metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04732</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09478</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01386</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bi-directional neural machine translation with synthetic parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11213</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pontes-Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08006</idno>
		<title level="m">Bidirectional learning for robust neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08824</idno>
		<title level="m">From source to target and back: symmetric bi-directional adaptive gan</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="86" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10349</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Domain adaptation meets disentangled representation learning and style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05827</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
