<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude&apos;s Variance Matters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
							<email>&lt;xinshao.wang@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queen&apos;s University Belfast</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Anyvision Research Team</orgName>
								<address>
									<country>UK. Correspondence to</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Queen&apos;s University Belfast</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude&apos;s Variance Matters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Work done at Queen&apos;s University Belfast and Anyvision. Derivative Manipulation for General Example Weighting, May, 2019 (Wang et al., 2019c) is a following work and generalisation of IMAE, March, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. Consequently, we have two key findings:</p><p>(1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust. First, we reveal its underfitting problem in practice. Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work. (2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the impact ratio between two examples are adjusted. Our solution is termed Improved MAE (IMAE). We prove IMAE's effectiveness using extensive experiments: image classification under clean labels, synthetic label noise, and real-world unknown noise. We conclude IMAE is superior to CCE, the most popular loss for training DNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we target at robust deep learning, which is indispensable when it comes to large-scale industrial applications. It is non-affordable to guarantee the quality of training data as its scale grows dramatically. Consequently, abnormal examples 1 generally exist in large-scale real-world scenarios <ref type="bibr" target="#b3">(Berrada et al., 2018)</ref>, which is caused by many factors, such as incomplete annotation, wrong labelling, subjectiveness, bias and so forth. Unfortunately, DNNs trained with categorical cross entropy (CCE) can fit random patterns <ref type="bibr" target="#b35">(Zhang et al., 2017)</ref>.</p><p>Great advances have been made towards training DNNs robustly when abnormal training examples exist <ref type="bibr">(Arpit et al., 2017;</ref><ref type="bibr" target="#b4">Chang et al., 2017;</ref><ref type="bibr" target="#b24">Ren et al., 2018;</ref><ref type="bibr" target="#b11">Jiang et al., 2018)</ref>. The robust loss function is one of them. In this paper, we study a so-claimed robust loss function, mean absolute error (MAE) following <ref type="bibr" target="#b5">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b36">Zhang &amp; Sabuncu, 2018)</ref>. According to the theoretical analysis of CCE and MAE in <ref type="bibr" target="#b5">(Ghosh et al., 2017)</ref>, CCE is sensitive to label noise while MAE is noise-tolerant. Thereafter, generalised cross entropy (GCE) <ref type="bibr" target="#b36">(Zhang &amp; Sabuncu, 2018)</ref> concludes MAE treats training samples equally, thus being noise-robust.</p><p>However, our empirical observation and technical analysis lead us to a contradictory and more reasonable conclusion.</p><p>Observation: In <ref type="table" target="#tab_1">Table 1</ref>, when 40% noise exists, compared with CCE, MAE underfits to clean training data points, thus fitting much fewer abnormal examples. <ref type="figure">Figure 1</ref>, MAE emphasises more on uncertain examples, whose probabilities of being classified to its labelled class are around 0.5, thus being noise-robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion: In</head><p>Specifically, according to <ref type="table" target="#tab_1">Table 1</ref>, MAE is much more noisetolerant than CCE. However, its ability of learning meaningful patterns is much weaker, fitting only 74.3% of the clean subset. We provide an intuitive interpretation for this 1 A training example is denoted as an observation-label pair, where the observation can be an image or video while the label defines its semantic information. We regard a training example as abnormal unrestrictedly whenever its observation and label are semantically unmatched, e.g., out-of-distribution examples (the observations contain only background or objects that do not belong to any training class), or examples with wrongly annotated labels. according to <ref type="figure">Figure 1</ref>: The variance of MAE's weight curve along with probability is only 0.09. As a result, the impact ratio between two examples is too small. <ref type="bibr">2</ref> The impact ratio reflects the relative impact of one example versus another for updating parameters. Due to MAE's small weight variance, informative samples cannot contribute enough against noninformative ones. Therefore, MAE cannot learn meaningful patterns well and is not widely used.</p><p>To adjust MAE's weight variance, we design an effective and simple solution, IMAE, which non-linearly transforms MAE's weighting scheme by an exponential function. On the one hand, by preserving MAE's overall weighting scheme, IMAE is noise-robust. On the other hand, by making the gradient magnitude's variance over training examples controllable, it learns meaningful patterns much better.</p><p>We demonstrate the effectiveness of IMAE under different scenarios. Most importantly, these empirical evidences justify that our interpretation of MAE's underfitting problem is reasonable and our proposed solution is superior. Our key findings are summarised as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We denote a training mini-batch as</p><formula xml:id="formula_0">X = {(x i , y i )} N i=1 , where there are N samples. (x i , y i ) represents i-th train-</formula><p>ing sample x i ∈ R D and its annotated class label y i ∈ {1, 2, ..., C}. D is the dimensionality of input samples and C is the number of all training classes. Let f θ be a deep neural network, which transforms x i to a representation f i = f θ (x i ) ∈ R E , E is the dimensionality of target space and θ indicates the parameters to be learned.</p><p>To optimise f θ during training, a linear classifier is generally trained jointly <ref type="bibr" target="#b18">(Liu et al., 2016)</ref>. In general, the  <ref type="bibr" target="#b9">(He et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Test set (Generalisation) linear classifier follows the output embeddings and is composed of one C-neuron fully connected (FC) layer, one softmax normalisation layer and one loss layer. The FC layer can be represented as z i = W f i ∈ R C , where W = [w 1 , w 2 , ..., w C ] ∈ R E×C consists of C weight vectors (the bias term is omitted for brevity). z ij = w j f i is a logit which indicates the compatibility between sample x i and class j. To produce the probabilities of sample x i belonging to different classes, we normalise its logit vector z i using a softmax function:</p><formula xml:id="formula_1">p(j|x i ) = exp(z ij ) C m=1 exp(z im ) ,<label>(1)</label></formula><p>where p(j|x i ) is the probability of sample x i being predicted to class j.</p><p>Let q(j|x i ) be the ground-truth probability of x i belonging to class j, i.e., q(j|x i ) = 1 if j = y i , q(j|x i ) = 0 otherwise. In the loss layer, if we use CCE, the minimisation objective per iteration is:</p><formula xml:id="formula_2">L CCE (X; f θ , W) = − 1 N N i=1 C j=1 q(j|x i ) log p(j|x i ) = − 1 N N i=1</formula><p>log p(y i |x i ).</p><p>(2) Figure 2: Pipeline of a softmax deep network. There are two reasons for analysing loss functions based on ∂L ∂z : (1) In gradient back-propagation, the gradients of examples in a mini-batch are fused when computing ∂L ∂z .</p><p>(2) Intermediate differences of ∂L ∂p lead to ultimate differences of ∂L ∂z . Therefore, our analysis of ∂L ∂z is more direct versus that of ∂L ∂p in <ref type="bibr" target="#b36">(Zhang &amp; Sabuncu, 2018)</ref>.</p><p>If MAE is applied, the minimisation objective becomes:</p><formula xml:id="formula_3">L MAE (X; f θ , W) = 1 N N i=1 C j=1 |p(j|x i ) − q(j|x i )| = 2 N N i=1 (1 − p(y i |x i )),<label>(3)</label></formula><p>where | · | is the absolute function.</p><p>In summary, we learn a softmax deep network g θ,W , which outputs logits:</p><formula xml:id="formula_4">z i = g θ,W (x i ) = W f θ (x i ) ∈ R C .</formula><p>In classification tasks, we use z = g θ,W (x) to produce logits for a test image x. While in verification or retrieval tasks <ref type="bibr" target="#b29">(Wang et al., 2019a;</ref>, we only use f = f θ (x) as an embedding function. The overall pipeline is described in <ref type="figure">Figure 2</ref>. The output of the softmax layer is p.</p><p>Definition 1 (Uncertain Examples). We define uncertain examples to be those data points whose p(y i |x i ) are around 0.5. Given an example x i , if its p(y i |x i ) is closer to 0.5, its uncertainty is higher.</p><p>Remark 1. This definition of uncertain examples is intuitive. If p(y i |x i ) is closer to 1, the confidence of x i being class y i is higher. If p(y i |x i ) is closer to 0, the confidence of x i belonging to one of other classes is higher. However, if p(y i |x i ) is around 0.5, we are more uncertain about whether x i being class y i . Therefore, we can understand uncertainty from the perspective of binary classification (Logistic Regression), i.e., whether x i being class y i or not.</p><p>Remark 2. We have the premise that abnormal (noisy) examples have smaller probabilities in general. This premise is widely used and demonstrated by our empirical observations. For example, in <ref type="figure" target="#fig_0">Figure 4</ref> and Tables 1, 6, the accuracy of noisy subset is less than that of clean subset consistently.</p><p>Remark 3. The uncertainty of an example is determined by its probability of being classified to its annotated label. This example can belong to one of the training classes (uncertain in-distribution example), or a class which does not exist in the training set (uncertain out-of-distribution example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gradient Magnitude Serving as Weight</head><p>As shown in <ref type="figure">Figure 2</ref>, g θ,W can be viewed as a black box and the update of θ and W is based on the back-propagation of logits' gradient. Therefore, an example's contribution can be measured by the magnitude of its partial derivative w.r.t. z. It can be regarded as example weighting that is naturally built-in in loss functions.</p><p>For brevity and clarity, we summarise the results here and put the detailed derivation in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Derivation of Softmax, CCE and MAE Layers</head><p>Softmax layer. According to Eq. (1), the derivation of softmax layer is:</p><formula xml:id="formula_5">∂p(y i |x i ) ∂z ij = p(y i |x i )(1 − p(y i |x i )), j = y i −p(y i |x i )p(j|x i ), j = y i<label>(4)</label></formula><p>CCE layer. According to Eq.</p><p>(2), we have</p><formula xml:id="formula_6">L CCE (x i ; f θ , W) = − log p(y i |x i ).<label>(5)</label></formula><p>Therefore, we obtain (the parameters θ, W are omitted),</p><formula xml:id="formula_7">∂L CCE (x i ) ∂p(j|x i ) = −p(y i |x i ) −1 , j = y i 0, j = y i .<label>(6)</label></formula><p>MAE layer. According to Eq. (3), we have</p><formula xml:id="formula_8">L MAE (x i ; f θ , W) = 2(1 − (p(y i |x i )).<label>(7)</label></formula><p>Therefore, we obtain as discussed in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_9">∂L MAE (x i ) ∂p(j|x i ) = −2, j = y i 0, j = y i .<label>(8)</label></formula><p>According to Eq. (6) and (4), we calculate ∂L CCE (x i )/∂z i :</p><formula xml:id="formula_10">∂L CCE (x i ) ∂z ij = p(y i |x i ) − 1, j = y i p(j|x i ), j = y i .<label>(9)</label></formula><p>Analogously, according to Eq. (8) and (4), we have: For simplicity, p y = p(y|x), and p j = p(j|x), j = y, j =y p j + p y = 1. Prior analysis on loss functions is based on the loss expression or ∂L ∂p . Instead, we are the first to study the differences of loss functions according to || ∂L ∂z || 1 . Our empirical evidences justifies its rationality. Note that we have L(p y ) = ∂L ∂py dp y , L(1) = 0, therefore L(p y ) = 1 py − ∂L ∂py dp y . We remark IMAE is neither symmetric nor bounded, which challenges the robustness theories studied in <ref type="bibr" target="#b5">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b36">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019e)</ref>.</p><formula xml:id="formula_11">∂L MAE (x i ) ∂z ij = 2p(y i |x i )(p(y i |x i ) − 1), j = y i 2p(y i |x i )p(j|x i ), j = y i .<label>(10)</label></formula><formula xml:id="formula_12">∂L ∂p ∂py ∂z ∂L ∂z = C j=1 ∂L ∂pj × ∂pj ∂z Loss Expression L = L(p y ) = 1 py − ∂L ∂py dp y ∂L ∂py ∂L ∂pj , j = y ∂py ∂zy ∂py ∂zj , j = y ∂L ∂zy ∂L ∂zj , j = y || ∂L ∂z || 1 CCE − log p y − 1 py 0 p y (1 − p y ) −p y p j p y − 1 p j 2(1 − p y ) MAE 2(1 − p y ) -2 0 p y (1 − p y ) −p y p j 2p y (p y − 1) 2p y p j 4p y (1 − p y ) IMAE 1 py exp(T py(1−py)) 2py(1−py) dp y exp(T py(1−py)) 2py(py−1) 0 p y (1 − p y ) −p y p j exp(T py(1−py)) −2 exp(T py(1−py))pj 2(1−py) exp(T p y (1 − p y ))</formula><p>Figure 3: Although the loss expression of IMAE is not an elementary function, we visualise it by integral, i.e., the area under curve from p y to 1.</p><p>Gradient magnitude treated as weight. In CCE and MAE, training samples are weighted because different ones own different gradient magnitude w.r.t. logit vector z. We choose to measure one gradient's magnitude by its L 1 norm because of its simpler statistics than other norms. If one sample's gradient is larger, its impact is larger during gradient back-propagation.</p><p>For CCE, based on Eq. (9), the weight of sample x i is:</p><formula xml:id="formula_13">w CCE (x i ) = || ∂L CCE (x i ) ∂z i || 1 = 2(1 − p(y i |x i )),<label>(11)</label></formula><p>where || · || 1 denotes L 1 norm. For MAE, based on Eq. (10), the weight of sample x i is:</p><formula xml:id="formula_14">w MAE (x i ) = || ∂L MAE (x i ) ∂z i || 1 = 4p(y i |x i )(1 − p(y i |x i )).</formula><p>(12) According to Eq. (11) and Eq. (12), in both CCE and MAE, examples' impact is determined by their probabilities being predicted to annotated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improved MAE</head><p>IMAE transforms MAE's weighting scheme non-linearly:</p><formula xml:id="formula_15">w IMAE (x i ) = exp(T p(y i |x i )(1 − p(y i |x i ))),<label>(13)</label></formula><p>where T controls the exponential base. In back-propagation, we simply scale the gradient w.r.t. logits as follows:</p><formula xml:id="formula_16">∂L IMAE (x i ) ∂z i = ∂L MAE (x i ) ∂z i w IMAE (x i ) w MAE (x i ) =&gt; || ∂L IMAE (x i ) ∂z i || 1 = w IMAE (x i ).<label>(14)</label></formula><p>IMAE is a family of robust losses when T changes. Its details are summarised in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Design Motivation: To Adjust Gradient Magnitude's Variance and Impact Ratio</head><p>Linear scaling also changes magnitude variance. However, it cannot adjust impact ratio, i.e., the ratio between two gradients' magnitude. That is why we have tried linear scaling and find it does not work.</p><p>Instead, the exponential function is non-linear so that the impact ratio of one sample versus another is re-adjusted compared with original MAE. The hyper-parameter T controls how significant gradient magnitude's variance and impact ratio are changed.</p><p>Furthermore, assuming that samples' probabilities are uniformly distributed, we compute the gradients' variance of MAE and IMAE over training data points:</p><formula xml:id="formula_17">σ MAE = 1 0 w 2 MAE (p) dp − ( 1 0 w MAE (p) dp) 2 (15) σ IMAE = 1 0 w 2 IMAE (p) dp − ( 1 0 w IMAE (p) dp) 2 .</formula><p>(16) We have σ MAE = 0.09. When T = 8, σ IMAE = 4.55.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussion of MAE and CCE</head><p>The weighting curves of CCE, MAE and IMAE are compared in <ref type="figure">Figure 1</ref>. Our key findings are summarised in the end of introduction. We further discuss them as follows:</p><p>• MAE's weighting scheme is appealing and practical in that samples with medium probabilities are emphasized. Generally, high-probability samples are clean and already trained well. While low-probability ones are highly likely to be noisy as a model improves during training. Although all samples are not trained well and probabilities are not meaningful at the beginning, it also does not hurt to focus on medium-probability ones.</p><p>• MAE's gradient magnitude's variance over data points is only 0.09. As a consequence, the impact ratio of one example versus another is too small. Therefore, the majority contribute almost equally. Therefore, MAE generally underfits to training data. • Does high loss value usually back-propagate high gradients to update parameters? The answer is NO. Therefore, those theorems based on loss values, e.g., symmetric or bounded conditions are insufficient for analysing robustness of DNNs <ref type="bibr" target="#b5">(Ghosh et al., 2017)</ref>. Actually, IMAE is neither symmetric nor bounded. However, it is proved to be noise-robust empirically.</p><p>These analytical discussions are demonstrated in our empirical studies in <ref type="table" target="#tab_8">Table 6</ref> and Figures 4, 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We demonstrate the effectiveness of IMAE as follows:</p><p>Outperforming the state-of-the-art. IMAE is compared with recent baselines in Sections 5.1 and 5.2 in different scenarios: (1) Clean labels;</p><p>(2) Synthetic symmetric and asymmetric noisy labels; (3) Realistic agnostic noise.</p><p>Analysis of the training dynamics of IMAE against CCE and MAE. We thoroughly visualise and compare the training dynamics of IMAE, CCE and MAE in Section 5.3 for empirical justification.</p><p>Supplementary studies. In our supplementary material, we further prove IMAE's effectiveness by: <ref type="formula" target="#formula_1">(1)</ref> The results on a video retrieval task (video person re-identification);</p><p>(2) The results of different stochastic optimisers;</p><p>(3) The ablation study of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification on CIFAR-100 with Synthetic Noise</head><p>Dataset. CIFAR-100 <ref type="bibr" target="#b12">(Krizhevsky, 2009</ref>) contains 100 classes, 500 images per class for training and 100 images per class for testing. The image size is 32 × 32.  Synthetic label noise generation.</p><p>(1) Class-independent (uniform or symmetric) noise: With a probability of r, the label of each image is replaced by one of the other class labels uniformly.</p><p>(2) Class-dependent (non-uniform or asymmetric) noise: The 100 classes of CIFAR-100 are grouped into 20 coarse ones. Every coarse one has 5 fine classes. Following <ref type="bibr" target="#b33">(Wang et al., 2019e)</ref>, we first randomly select 2 out of 5 classes, and then their labels are flipped to each other with a probability of r. r denotes the noise rate. All instances generated from the same original image by data augmentation share the same label. All test labels are kept intact.</p><p>Implementation details. We follow the settings of recent SL <ref type="bibr" target="#b33">(Wang et al., 2019e)</ref> and train ResNet44 <ref type="bibr" target="#b9">(He et al., 2016)</ref> for a fair comparison with their reported results. We also use the same data augmentation techniques: random horizontal flips and crops of 32 × 32 on the images after being padded with 4 pixels on each side. All networks are trained using SGD with a momentum of 0.9, a weight decay of 0.0005 and an initial learning rate of 0.1.</p><p>Baselines. IMAE is compared against standard CCE, MAE, and six recent robust training baselines: 1) Forward (or Backward) applies a noise-transition matrix to multiply the network's predictions (or losses) for label correction purpose <ref type="bibr" target="#b22">(Patrini et al., 2017)</ref>; 2) Bootstrapping learns on new labels generated by a convex combination (soft or hard combinations) of the original ones and their predictions <ref type="bibr" target="#b23">(Reed et al., 2015)</ref>. 3) D2L achieves noise-robustness by restricting the dimensionality expansion of learned subspaces during training <ref type="bibr" target="#b20">(Ma et al., 2018)</ref>; 4) SL boosts CCE with a noise-robust counterpart, i.e., reverse cross entropy <ref type="bibr" target="#b33">(Wang et al., 2019e)</ref>; 5) GCE aims to achieve a balance between MAE and CCE <ref type="bibr" target="#b36">(Zhang &amp; Sabuncu, 2018)</ref>; 6) Label Smoothing (LS) trains DNNs on softly smoothed labels instead of one-hot ones;</p><p>We remark that <ref type="bibr" target="#b15">(Lee et al., 2019)</ref> is not benchmarked for two reasons: (1) The used network is not ResNet-44 by checking with the authors;</p><p>(2) The proposed algorithm is orthogonal to ours because it targets at the inference stage and is a generative classifier on top of pre-trained deep representations. Our IMAE focuses on the training stage and is a softmax-based neural classifier.</p><p>Results. We display the results in <ref type="table" target="#tab_5">Tables 3 and 4</ref>. We observe that IMAE is superior to the state-of-the-art. We fix the random seed as 123 and do not use any random computational accelerator for the purpose of exact reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Classification on Clothing1M with Realistic Unknown Noise</head><p>Dataset. Clothing1M <ref type="bibr" target="#b34">(Xiao et al., 2015)</ref> contains one million clothing images of fourteen classes from online shopping websites. Its noise type is agnostic. The noise rate is around 38.46%. Additionally, it includes 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively. To compare fairly with existing algorithms without exploiting auxiliary information from trusted clean data, we also train only on the noisy training data.</p><p>Implementation details. We follow <ref type="bibr" target="#b22">(Patrini et al., 2017;</ref><ref type="bibr" target="#b27">Tanaka et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019e)</ref> and train ResNet50</p><p>initialised by pretrained ImageNet model <ref type="bibr" target="#b25">(Russakovsky et al., 2015)</ref>. We apply an SGD optimiser with a momentum of 0.9 and a weight decay of 0.00002. We set the initial learning rate to 0.01 and divide it by 10 after 10k and 15k iterations. We stop training at 30k iterations. Regarding data augmentation, a raw input image is warped to 256×256, followed by a random crop of 227×227 and a random horizontal mirroring. The batch size is 84. Every program is run on a single Tesla V100 GPU with 32 GB RAM.</p><p>Competitors. Some recent baselines are compared: 1) Sadaptation explicitly estimates latent true labels by an additional softmax layer <ref type="bibr" target="#b6">(Goldberger &amp; Ben-Reuven, 2017)</ref>; 2) Masking speculates the structure of a noise-transition matrix with human cognition <ref type="bibr" target="#b7">(Han et al., 2018)</ref>; 3) Joint Optim. iteratively optimises model's parameters and latent true labels <ref type="bibr" target="#b27">(Tanaka et al., 2018)</ref>. Others are introduced in Section 5.1. Note that <ref type="bibr" target="#b8">(Han et al., 2019)</ref> corrects labels gradually and  exploits meta-learning. They are not technically related and not benchmarked consequently.</p><p>Results. We display the results in <ref type="table" target="#tab_7">Table 5</ref>. IMAE outperforms the state-of-the-art, which proves IMAE's effectiveness under real-world scenarios with agnostic noise. Beyond, we remark that IMAE is much simpler than those competitors except CCE, MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Empirical Analysis of IMAE Against Basic Baselines CCE and MAE on CIFAR-10</head><p>Dataset. CIFAR-10 (Krizhevsky, 2009) contains 10 classes, 5k images per class for training and 1k images per class for testing. The image size is 32 × 32.</p><p>Implementation details 3 . We follow the study on CIFAR-10 in <ref type="bibr" target="#b9">(He et al., 2016)</ref>, which means we use exactly the same architectures (ResNet20, ResNet56) and training settings: a weight decay of 0.0001, a momentum of 0.9, a batch size of 128. The learning rate starts at 0.1, then is divided by 10 at 32k and 48k iterations. Training stops at 100k iterations. Data augmentation is the same as CIFAR-100. For IMAE, without tuning T case by case, we fix T = 0.5 when training data is clean and T = 8 when noise exists although noisy  rate is different. 4 4 More discussion about the hyper-parameter T is given in our supplementary material.</p><p>A well-accepted way to improve data fitting ability is increasing a model's capacity. Therefore, we train a shallower net ResNet20 and a deeper net ResNet56 for better analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">CIFAR-10 WITH INTACT LABELS</head><p>In <ref type="table" target="#tab_8">Table 6</ref>, we first compare IMAE with CCE and MAE on clean CIFAR-10 using different nets (ResNet20, ResNet56). We observe that IMAE is competitive with CCE and outperforms MAE significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">CIFAR-10 WITH CORRUPTED LABELS</head><p>Following <ref type="bibr" target="#b35">(Zhang et al., 2017;</ref><ref type="bibr">Arpit et al., 2017)</ref>, we test the robustness of deep models against corrupted labels. We evaluate on uniform noise because it is more challenging than asymmetric noise which is verified in <ref type="bibr" target="#b28">(Vahdat, 2017)</ref>.</p><p>Majority voting assumption. When generating uniform noise on CIFAR-10, even up to 80% noise rate, clean examples are still the majority because 80% labels are corrupted to other 9 classes evenly. We remark that the majority voting is our reasonable assumption. We believe that if the noise becomes the majority, it is hard to discover meaningful patterns. Being natural and intuitive, the majority define the meaningful data patterns to learn.</p><p>Results. The results are summarised in <ref type="table" target="#tab_8">Table 6</ref>. For more comprehensive and clear comparison, we display the training dynamics in Figures 4 (40% noise) and 10 (80% noise) of the supplementary material. Note that general learning objectives are high final testing accuracy, low accuracy on the noisy training subset, and high accuracy on the clean training subset. Therefore, we report the hybrid accuracy on the combination of testing set and clean training set. We have the following observations:</p><p>• Regarding CCE's test accuracies, the best is always much higher than the final. In <ref type="figure" target="#fig_0">Figures 4 and 10</ref>, as training goes, CCE always tries to fit the noisy training subset better. Therefore, CCE learns a lot of error information when severe noise exists. When it comes to MAE and IMAE, the gap between the best and final accuracies is significantly smaller than that of CCE regardless of net's capacity.</p><p>• The training accuracies on both noisy and clean subsets are compared. Whatever the noise rate and net's capacity are, CCE fits the noisy subset much more. Although MAE fits the noisy subset much less, it fits the clean subset worst. Instead, our IMAE fits the noisy subset little and the clean subset competitively with CCE.</p><p>• IMAE obtains the best hybrid accuracy consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>IMAE is a family of robust loss functions, inspired by the intrinsic example weighting scheme of MAE. Therefore, our work is related to some prior work about example weighting and robust loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Example Weighting</head><p>In <ref type="bibr" target="#b24">(Ren et al., 2018)</ref>, a meta-learning algorithm weights data points according to their gradient directions. The metalearning algorithm is optimised on a clean validation set. In contrast, our IMAE assigns weights to samples based on their gradient magnitude and does not require extra clean set. MentorNet <ref type="bibr" target="#b11">(Jiang et al., 2018)</ref> learns data-driven weighting scheme, which guides StudentNet to focus on samples whose labels are more trustful. In Active Bias <ref type="bibr" target="#b4">(Chang et al., 2017)</ref> and Focal Loss <ref type="bibr" target="#b17">(Lin et al., 2017)</ref>, uncertain and hard examples are emphasised, respectively. Other related work on weighting samples includes curriculum learning <ref type="bibr" target="#b2">(Bengio et al., 2009</ref>), self-paced learning <ref type="bibr" target="#b13">(Kumar et al., 2010)</ref>, and hard examples mining <ref type="bibr" target="#b26">(Shrivastava et al., 2016;</ref><ref type="bibr" target="#b30">Wang et al., 2019b)</ref>. In summary, what makes ours special is that the weighting scheme inherits from MAE, which is naturally built-in in the loss function without intuitive designing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Noise-Robust Theorems on Loss Functions</head><p>Noise-robust theorems on loss functions from the angle of symmetric and bounded conditions on loss values have been studied recently <ref type="bibr" target="#b5">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b36">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019e)</ref>. Does a robust loss function have to be symmetric or bounded? The answer is NO according to this work. Although IMAE is neither symmetric nor bounded, we have extensive empirical studies to support its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we firstly present a thorough study of CCE and MAE technically and empirically. Compared with previous work, we introduce our observations and new conclusions: 1) MAE underfits to meaningful patterns; 2) MAE is noisetolerant because of emphasising on medium-probability (uncertain) examples instead of treating all samples equally. Secondly, we claim gradient magnitude's variance matters.</p><p>As a consequence, we propose an effective and simple solution for addressing MAE's underfitting issue while preserving its noise-robustness. IMAE is a family of robust loss functions whose gradient magnitude's variance is adjustable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for IMAE</head><p>1. The Impact of T on Gradient Magnitude's Variance</p><p>Assuming samples' probabilities are uniformly distributed, we calculate the variances of IMAE's weighting curves with different T . As illustrated in Sec. 4 of the main paper, we rewrite the Eq. (13) (We use e to replace exp for brevity):</p><formula xml:id="formula_18">w IMAE (p) = e T ·p(1−p) ,<label>(17)</label></formula><p>where p is the probability of one randomly sampled example being predicted to its annotated label. According to Eq. (16) in the main paper, we have,</p><formula xml:id="formula_19">σ IMAE = 1 0 w 2 IMAE (p) dp − ( 1 0 w IMAE (p) dp) 2 = 1 0 e 2T p(1−p) dp − ( 1 0 e T p(1−p) dp) 2 = √ π erf √ 2T 2 e T 2 √ 2T − π erf 2 √ T 2 e T 2 T .<label>(18)</label></formula><p>erf is the error function. Therefore we obtain the weighting variances σ IMAE of IMAE with different T , as displayed in <ref type="table" target="#tab_6">Table 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Impact of T on Validation Accuracy</head><p>We visualise and compare the effect of T on CIFAR-10 test performance. These experiments follow exactly the same settings of the main paper.</p><p>We try two cases: (1). Training labels are intact (r = 0); (2). Training labels are corrupted randomly with a probability of 0.4 (r = 40%). In both cases, the test set is kept intact for evaluation. The backbone network is ResNet20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CIFAR-10 with intact training labels</head><p>The test results are shown and compared in <ref type="figure">Figure 6a</ref>. When training labels are clean, it is unhelpful to differentiate training samples in a high degree, e.g., the performance is even lower when T = 16. The final test accuracies are similar when T ranges from 0 to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CIFAR-10 with corrupted training labels</head><p>The results are presented and compared in <ref type="figure">Figure 6b</ref>. Because there exists 40% label noise, as training goes, the test accuracy drops, which means the model overfits noisy data gradually.</p><p>However, we observe that higher differentiation degree (larger T ) works better and is much less susceptible to overfitting to noisy data. In <ref type="figure">Figure 6b</ref>, the final test accuracies of IMAE-16 and IMAE-8 are much higher than those of other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Impact of T on Training Accuracy</head><p>Following the practice in the main paper, we also visualise and compare the accuracies on the training sets, which indicate how different models fit to training data as training goes, thus leading to different generalisation performance in the test phase. We present how each model fits its corresponding training set in <ref type="figure" target="#fig_3">Figure 7</ref>. 1 <ref type="figure">Figure 6</ref>: The accuracy on CIFAR-10 test set along with training iterations. We display the results when training on intact training set and corrupted training set. Better viewed in colour.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fitting of intact training set</head><p>As compared in <ref type="figure" target="#fig_3">Figure 7a</ref>, all models fit training data similarly when T ranges from 0 to 8. However, when T = 16, the differentiation degree becomes too large as shown in <ref type="table" target="#tab_6">Table 4</ref>. When differentiation degree is too large, only a quite small proportion of training data can contribute. Consequently, IMAE-16 underfits training data compared with other models. That is why IMAE-16 has the worst test performance as shown in <ref type="figure">Figure 6a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fitting of corrupted training set</head><p>The training accuracies of corrupted training set are displayed in <ref type="figure" target="#fig_3">Figure 7b</ref>. We have two observations:</p><p>• In cases where noise rate is high, as T increases, the fitting of training data first becomes better, and then becomes worse. Specifically, when T increases from 0 to 8, the training accuracy grows gradually, which means the fitting of training data becomes better. However, when T = 16, the weighting variance becomes very large <ref type="table" target="#tab_6">(Table 4)</ref>. As a result, IMAE-16's fitting of training data becomes much worse than IMAE-8's.</p><p>• Fitting corrupted training data better does not mean better generalisation performance. On the one hand, although IMAE-16 fits the training data much worse than IMAE-8 <ref type="figure" target="#fig_3">(Figure 7b</ref>), IMAE-16's test accuracy is slightly better than IMAE-8's ( <ref type="figure">Figure 6b</ref>). On the other hand, similar to IMAE-8, IMAE-4 fits its training data well <ref type="figure" target="#fig_3">(Figure 7b</ref>), but IMAE-4's test performance is much worse than IMAE-8's ( <ref type="figure">Figure 6b</ref>). In summary, the training accuracy (fitting of training data) is uninformative for estimating a model's generalisation performance according to our findings in Section 3.2. Therefore, it is better to optimise T on a validation set in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Choosing T in Practice</head><p>For empirical demonstration, since the overlap rate between corrupted and intact training sets is only (1 − r) = 60%, we treat the original intact training set as a validation set. The validation performance of IMAE-16, IMAE-8 and IMAE-4 is compared in <ref type="figure" target="#fig_4">Figure 8</ref>. We observe that IMAE-16 and IMAE-8 own similar validation performance, while IMAE-4's validation accuracy is lower. Furthermore, their validation performance is consistent with their test performance <ref type="figure">(Figure 6b</ref>). Therefore, we conclude that it is a good practice to optimise T on a validation set in different cases.  <ref type="bibr" target="#b37">(Zheng et al., 2016)</ref>. We remark three representatives: 1) The abnormal images with no person in 3rd row contain no semantic information at all.</p><p>2) The last abnormal image in 2nd or 3rd row may contain a person that does not belong to any person in the training set. 3) We cannot decide the object of interest without any prior when an image contains more than one object, e.g., the 2nd and 3rd last images in 2nd row contain two persons. Better viewed in colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Video person re-identification</head><p>Dataset and evaluation settings. MARS contains 20,715 videos of 1,261 persons <ref type="bibr" target="#b37">(Zheng et al., 2016)</ref>. There are 1,067,516 frames in total. Because person videos are collected by tracking and detection algorithms, abnormal examples exist as shown in <ref type="figure" target="#fig_5">Figure 9</ref>. The exact noise rate is unknown. Following standard settings, we use 8,298 videos of 625 persons for training and 12,180 videos of other 636 persons for testing. We report the cumulated matching characteristics (CMC) and mean average precision (mAP) results.</p><p>Implementation details. 1 Following <ref type="bibr" target="#b19">(Liu et al., 2017;</ref><ref type="bibr" target="#b30">Wang et al., 2019b)</ref>, we train GoogleNet V2. We also treat a video as an image set, which means we use only appearance information without exploiting latent temporal information. A video's representation is simply the average fusion of its frames' representations. We apply the same training settings for each loss. The learning rate starts from 0.01 and is divided by 2 every 10k iterations. We stop training at 50k iterations. We choose SGD optimiser with a weight decay of 0.0005 and momentum of 0.9. The batch size is set to 180. We use standard data augmentation: a 227 × 227 crop is randomly sampled and flipped after resizing an original image to 256 × 256. At testing, following (Wang et al., Results. We compare our method with CCE, MAE and GCE. We implement GCE with its best settings. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. IMAE outperforms other related methods by a significant margin. In this section, we study the performance of IMAE when different stochastic optimisers are used. The results are presented in <ref type="table" target="#tab_7">Table 5</ref>. We observe that IMAE's results are the best consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Derivation of Softmax, CCE and MAE Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Derivation of softmax layer</head><p>As the softmax layer is shared by CCE and MAE, we present the derivation of softmax layer first. Based on Eq. (1), we have</p><formula xml:id="formula_20">p(y i |x i ) −1 = 1 + j =yi exp(z ij − z iyi ).<label>(19)</label></formula><p>If j = y i , for left and right sides of Eq. (19), we calculate their derivatives w.r.t. z iyi simultaneously:</p><formula xml:id="formula_21">−1 p(y i |x i ) 2 ∂p(y i |x i ) z iyi = − j =yi exp(z ij − z iyi ) =&gt; ∂p(y i |x i ) z iyi = p(y i |x i )(1 − p(y i |x i )).<label>(20)</label></formula><p>If j = y i , analogously we have:</p><formula xml:id="formula_22">−1 p(y i |x i ) 2 ∂p(y i |x i ) z ij = exp(z ij − z iyi ) =&gt; ∂p(y i |x i ) z ij = −p(y i |x i )p(j|x i ).<label>(21)</label></formula><p>In summary, the derivation of softmax layer is:</p><formula xml:id="formula_23">∂p(y i |x i ) ∂z ij = p(y i |x i )(1 − p(y i |x i )), j = y i −p(y i |x i )p(j|x i ), j = y i<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Derivation of loss layer: CCE</head><p>According to Eq.</p><p>(2), we have L CCE (x i ; f θ , W) = − log p(y i |x i ).</p><p>Therefore, we obtain (the parameters are omitted for brevity),</p><formula xml:id="formula_25">∂L CCE (x i ) ∂p(j|x i ) = −p(y i |x i ) −1 , j = y i 0, j = y i .<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Derivation of loss layer: MAE</head><p>According to Eq. (3), we have L MAE (x i ; f θ , W) = 2(1 − (p(y i |x i )).</p><p>Therefore, we obtain If j = y i , we have:</p><formula xml:id="formula_27">∂L MAE (x i ) ∂p(j|x i ) = −2, j = y i 0, j = y i .<label>(26</label></formula><formula xml:id="formula_28">∂L CCE (x i ) ∂z iyi = C j=1 ∂L CCE (x i ) ∂p(j|x i ) ∂p(y i |x i ) z iyi = p(y i |x i ) − 1.<label>(27)</label></formula><p>Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters (a) ResNet20: Testing set (higher is better). (b) ResNet20: Noisy subset (lower is better). (c) ResNet20: Clean subset (higher is better).</p><p>(d) ResNet56: Testing set (higher is better). (e) ResNet56: Noisy subset (lower is better). (f) ResNet56: Clean subset (higher is better). If j = y i , it becomes:</p><formula xml:id="formula_29">∂L CCE (x i ) ∂z ij = C j=1 ∂L CCE (x i ) ∂p(j|x i ) ∂p(y i |x i ) z ij = p(j|x i ).<label>(28)</label></formula><p>In summary, ∂L CCE (x i )/∂z i can be represented as:</p><formula xml:id="formula_30">∂L CCE (x i ) ∂z ij = p(y i |x i ) − 1, j = y i p(j|x i ), j = y i .<label>(29)</label></formula><p>∂L MAE (x i )/∂z i : The calculation is analogous with that of ∂L CCE (x i )/∂z i . According to Eq. (26) and Eq. <ref type="formula" target="#formula_23">(22)</ref>, if j = y i :</p><formula xml:id="formula_31">∂L MAE (x i ) ∂z iyi = C j=1 ∂L MAE (x i ) ∂p(j|x i ) ∂p(y i |x i ) z iyi = −2p(y i |x i )(1 − p(y i |x i )).<label>(30)</label></formula><p>otherwise (j = y i ):</p><formula xml:id="formula_32">∂L MAE (x i ) ∂z ij = C j=1 ∂L MAE (x i ) ∂p(j|x i ) ∂p(y i |x i ) z ij = 2p(y i |x i )p(j|x i ).<label>(31)</label></formula><p>In summary, ∂L MAE (x i )/∂z i is:</p><p>∂L MAE (x i ) ∂z ij = 2p(y i |x i )(p(y i |x i ) − 1), j = y i 2p(y i |x i )p(j|x i ), j = y i .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>CIFAR-10 with noise rate r = 40%. The accuracies on testing set, noisy subset and clean subset of training set along with training iterations. The legend on the top left is shared by all subfigures. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Sample's weight along with probability in IMAE with different T (IMAE-T ). The hyper-parameter T controls gradient magnitude's variance, and impact ratio between examples consequently. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The training accuracies of IMAE-T on intact training set. (b) The training accuracies of IMAE-T on corrupted training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>The accuracy on CIFAR-10 training sets along with training iterations. We show the results when training on intact training set and corrupted training set. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>IMAE-16's, IMAE-8's and IMAE-4's accuracies on the clean training set when they are trained on the corrupted training set. The overlap rate between corrupted and intact training sets is only (1 − r) = 60%. Therefore, we can use the original training set as a validation set. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Display of abnormal training examples highlighted by red boxes. The 1st row shows synthetic abnormal examples from corrupted CIFAR-10 (Krizhevsky, 2009). The 2nd and 3rd rows present realistic abnormal examples from video person re-identification benchmark MARS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>CIFAR-10 with noise rate r = 80%. The accuracies on testing set, noisy subset and clean subset of training set along with training iterations. The legend on the top left is shared by all subfigures. Better viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• CCE overfits to noise easily because it emphasises on low-probability examples to which abnormal ones generally belong.</figDesc><table><row><cell>Although CCE's weight variance is not</cell></row><row><cell>large (0.33), its fitting ability benefits from emphasis-</cell></row><row><cell>ing on low-probability examples.</cell></row><row><cell>• MAE is noise-robust by focusing on uncertain</cell></row><row><cell>(medium-probability) examples instead of treating all</cell></row><row><cell>equally. However, MAE generally underfits due to its</cell></row><row><cell>small weights variance (0.09), leading to small impact</cell></row><row><cell>ratio between even far different examples.</cell></row><row><cell>• Our proposed IMAE achieves new state-of-the-art on</cell></row><row><cell>robust training against synthetic label noise and realis-</cell></row><row><cell>tic unknown noise simply by adjusting MAE's weight</cell></row><row><cell>variance, which is inspiring.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) of CCE, MAE, and IMAE on CIFAR-10 (Krizhevsky, 2009). 40% of training examples, i.e., the noisy subset, have wrong labels. We test each model's performance on test set, noisy subset and clean subset of training data. The backbone is ResNet56 owning enough capacity</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Summary of CCE, MAE and IMAE. (x, y) is a training example.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The results on CIFAR-100 using ResNet44. Results from SL and D2L are different due to different optimisation details. In our experiments, we fix the random seed as 123 and do not use any random computational accelerator for the purpose of exact reproducibility. The best results on each block and our IMAE are bolded.</figDesc><table><row><cell></cell><cell cols="2">Method Clean Labels</cell><cell cols="3">Symmetric Noisy Labels r=0.2 r=0.4 r=0.6</cell></row><row><cell></cell><cell>CCE</cell><cell>64.3</cell><cell cols="2">59.3 50.8</cell><cell>25.4</cell></row><row><cell></cell><cell>LS</cell><cell>63.7</cell><cell cols="2">58.8 50.1</cell><cell>24.7</cell></row><row><cell>Results</cell><cell cols="2">Boot-hard 63.3</cell><cell cols="2">57.9 48.2</cell><cell>12.3</cell></row><row><cell>From</cell><cell>Forward</cell><cell>64.0</cell><cell cols="2">59.8 53.1</cell><cell>24.7</cell></row><row><cell>SL</cell><cell>D2L</cell><cell>64.6</cell><cell cols="2">59.2 52.0</cell><cell>35.3</cell></row><row><cell></cell><cell>GCE</cell><cell>64.4</cell><cell cols="2">59.1 53.3</cell><cell>36.2</cell></row><row><cell></cell><cell>SL</cell><cell>66.8</cell><cell cols="2">60.0 53.7</cell><cell>41.5</cell></row><row><cell></cell><cell>CCE</cell><cell>68.2</cell><cell cols="2">52.9 42.9</cell><cell>30.1</cell></row><row><cell></cell><cell cols="2">Boot-hard 68.3</cell><cell cols="2">58.5 44.4</cell><cell>36.7</cell></row><row><cell>Results</cell><cell cols="2">Boot-soft 67.9</cell><cell cols="2">57.3 41.9</cell><cell>32.3</cell></row><row><cell>From</cell><cell>Forward</cell><cell>68.5</cell><cell cols="2">60.3 51.3</cell><cell>41.2</cell></row><row><cell>D2L</cell><cell cols="2">Backward 68.5</cell><cell cols="2">58.7 45.4</cell><cell>34.5</cell></row><row><cell></cell><cell>D2L</cell><cell>68.6</cell><cell cols="2">62.2 52.0</cell><cell>42.3</cell></row><row><cell>Our</cell><cell>CCE</cell><cell>70.0</cell><cell cols="2">60.4 53.2</cell><cell>42.1</cell></row><row><cell>Trained</cell><cell>MAE</cell><cell>8.2</cell><cell>6.4</cell><cell>7.3</cell><cell>5.2</cell></row><row><cell>Results</cell><cell>IMAE</cell><cell>69.2</cell><cell cols="2">63.4 54.7</cell><cell>43.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The results on CIFAR-100 using ResNet44. The best results on each block are bolded.</figDesc><table><row><cell></cell><cell cols="3">Method Asymmetric Noisy Labels</cell></row><row><cell></cell><cell></cell><cell>r=0.2 r=0.3</cell><cell>r=0.4</cell></row><row><cell></cell><cell>CCE</cell><cell>63.0 63.1</cell><cell>61.9</cell></row><row><cell></cell><cell>LS</cell><cell>63.0 62.3</cell><cell>61.6</cell></row><row><cell>Results From</cell><cell cols="2">Bootstrap 63.4 63.2</cell><cell>62.1</cell></row><row><cell>SL</cell><cell cols="2">Forward 64.1 64.0</cell><cell>60.9</cell></row><row><cell>(Wang et al., 2019e)</cell><cell>D2L</cell><cell>62.4 63.2</cell><cell>61.4</cell></row><row><cell></cell><cell>GCE</cell><cell>63.0 63.2</cell><cell>61.7</cell></row><row><cell></cell><cell>SL</cell><cell>65.6 65.1</cell><cell>63.1</cell></row><row><cell>Our trained Results</cell><cell cols="2">CCE MAE IMAE 67.5 65.8 66.4 64.7 7.3 6.3</cell><cell>60.3 7.3 63.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracy (%) on Clothing1M with ResNet50<ref type="bibr" target="#b9">(He et al., 2016)</ref>. The leftmost block's results are from SL<ref type="bibr" target="#b33">(Wang et al., 2019e)</ref> while the middle block's are from Masking<ref type="bibr" target="#b7">(Han et al., 2018)</ref>.</figDesc><table><row><cell cols="5">CCE Boot-hard Forward D2L GCE</cell><cell>SL</cell><cell cols="2">S-adaptation Masking</cell><cell>Joint Optim.</cell><cell cols="3">Our trained results CCE MAE IMAE</cell></row><row><cell>68.8</cell><cell>68.9</cell><cell>69.8</cell><cell>69.5</cell><cell>69.8</cell><cell>71.0</cell><cell>70.3</cell><cell>71.1</cell><cell>72.2</cell><cell>71.7</cell><cell>39.7</cell><cell>73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results (%) of CCE, MAE and IMAE on CIFAR-10 with different noise rates. For classification accuracy on the testing set, we show the best result achieved during training and the final result when training stops, which are indicated by 'Best' and 'Final', respectively. For training accuracy, the results on noisy and clean subsets are displayed. The hybrid accuracy represents the result on the combination of testing set and clean training set. We report training and hybrid accuracies of the final model when training terminates. The ultimate objective is to achieve high hybrid accuracy, since both training and testing data points may occur in a deployed system. The best result on each column block is bolded. '-' indicates there is no noisy subset.</figDesc><table><row><cell>Backbone</cell><cell>Noise rate</cell><cell>Loss</cell><cell cols="2">Testing accuracy Best Final</cell><cell cols="2">Training accuracy: Naive fitting Noisy subset Clean subset</cell><cell>Hybrid accuracy: Meaningful patterns</cell></row><row><cell></cell><cell></cell><cell>CCE</cell><cell>91.5</cell><cell>91.3</cell><cell>-</cell><cell>100</cell><cell>98.5</cell></row><row><cell></cell><cell>0%</cell><cell>MAE</cell><cell>89.3</cell><cell>89.2</cell><cell>-</cell><cell>95.8</cell><cell>94.7</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>91.7</cell><cell>91.4</cell><cell>-</cell><cell>99.8</cell><cell>98.4</cell></row><row><cell>ResNet20</cell><cell>40%</cell><cell>CCE MAE</cell><cell>81.2 76.2</cell><cell>67.0 75.9</cell><cell>34.3 6.8</cell><cell>93.3 84.6</cell><cell>72.6 79.7</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>84.3</cell><cell>84.0</cell><cell>5.5</cell><cell>94.0</cell><cell>88.2</cell></row><row><cell></cell><cell></cell><cell>CCE</cell><cell>43.0</cell><cell>20.3</cell><cell>38.3</cell><cell>57.0</cell><cell>22.0</cell></row><row><cell></cell><cell>80%</cell><cell>MAE</cell><cell>27.7</cell><cell>27.5</cell><cell>9.7</cell><cell>29.4</cell><cell>27.8</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>52.0</cell><cell>41.0</cell><cell>16.8</cell><cell>64.8</cell><cell>41.5</cell></row><row><cell></cell><cell></cell><cell>CCE</cell><cell>92.4</cell><cell>92.2</cell><cell>-</cell><cell>100</cell><cell>98.7</cell></row><row><cell></cell><cell>0%</cell><cell>MAE</cell><cell>89.0</cell><cell>89.0</cell><cell>-</cell><cell>96.1</cell><cell>94.9</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>92.2</cell><cell>92.2</cell><cell>-</cell><cell>99.8</cell><cell>98.5</cell></row><row><cell>ResNet56</cell><cell>40%</cell><cell>CCE MAE</cell><cell>81.6 67.0</cell><cell>63.3 66.9</cell><cell>75.0 8.1</cell><cell>96.2 74.3</cell><cell>63.6 70.2</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>82.2</cell><cell>81.5</cell><cell>6.5</cell><cell>93.1</cell><cell>86.5</cell></row><row><cell></cell><cell></cell><cell>CCE</cell><cell>38.2</cell><cell>16.4</cell><cell>52.5</cell><cell>62.3</cell><cell>18.7</cell></row><row><cell></cell><cell>80%</cell><cell>MAE</cell><cell>15.2</cell><cell>15.1</cell><cell>9.6</cell><cell>15.6</cell><cell>15.1</cell></row><row><cell></cell><cell></cell><cell>IMAE</cell><cell>37.1</cell><cell>34.0</cell><cell>13.0</cell><cell>44.7</cell><cell>34.8</cell></row><row><cell cols="8">(a) ResNet20: Testing set (higher is better). (b) ResNet20: Noisy subset (lower is better). (c) ResNet20: Clean subset (higher is better).</cell></row><row><cell cols="8">(d) ResNet56: Testing set (higher is better). (e) ResNet56: Noisy subset (lower is better). (f) ResNet56: Clean subset (higher is better).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The weight variance (gradient magnitude's variance) of IMAE when T changes.</figDesc><table><row><cell>T</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>1</cell><cell>0.5 0</cell></row><row><cell cols="7">σ IMAE 354.113 4.546 0.299 0.040 0.007 0.002 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The results of algorithms using different stochastic optimisers on CIFAR-10 with 40% class-independent (symmetric) label noise. The trained network is ResNet56<ref type="bibr" target="#b9">(He et al., 2016)</ref>. The key hyper-parameters of all optimisers are shown. Other settings are fixed to be the same as presented in the implementation details of Section 5.3, e.g., weight decay = 0.0001. Since Adam is an adaptive gradient method, we show several variants of it.</figDesc><table><row><cell></cell><cell>SGD (lr: 0.01)</cell><cell>SGD + Momentum (lr: 0.01)</cell><cell>Nesterov (lr: 0.01)</cell><cell>Adam (lr: 0.01, delta: 0.1)</cell><cell>Adam (lr: 0.005, delta: 0.1)</cell><cell>Adam (lr: 0.005, delta: 1)</cell></row><row><cell>CCE</cell><cell>64.3</cell><cell>60.6</cell><cell>56.4</cell><cell>42.5</cell><cell>44.5</cell><cell>50.3</cell></row><row><cell>MAE</cell><cell>39.3</cell><cell>64.7</cell><cell>64.1</cell><cell>68.2</cell><cell>59.9</cell><cell>41.4</cell></row><row><cell>GCE</cell><cell>68.8</cell><cell>80.5</cell><cell>79.7</cell><cell>73.2</cell><cell>70.6</cell><cell>69.3</cell></row><row><cell>IMAE</cell><cell>82.0</cell><cell>83.5</cell><cell>83.7</cell><cell>75.5</cell><cell>76.3</cell><cell>78.6</cell></row><row><cell cols="4">2019b; Movshovitz-Attias et al., 2017; Law et al., 2017),</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">we first L 2 normalise videos' features and then calculate the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">cosine similarity between every two features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The retrieval results of CCE, MAE, GCE and IMAE on MARS with GoogLeNet V2<ref type="bibr" target="#b10">(Ioffe &amp; Szegedy, 2015)</ref>.</figDesc><table><row><cell>Metric</cell><cell cols="4">CCE MAE GCE IMAE</cell></row><row><cell>mAP (%)</cell><cell>58.1</cell><cell>12.0</cell><cell>31.6</cell><cell>70.9</cell></row><row><cell cols="2">CMC-1 (%) 73.8</cell><cell>26.0</cell><cell>51.5</cell><cell>83.5</cell></row><row><cell cols="5">6. The Results of IMAE Using Different</cell></row><row><cell cols="3">Stochastic Optimisers</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>) 7.4. Derivatives w.r.t. z i ∂L CCE (x i )/∂z i : The calculation is based on Eq. (24) and Eq. (22).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The terms, examples' weight or impact, and examples' gradient magnitude w.r.t. logits, are used interchangeably because we define the weight by gradient's magnitude. The impact ratio between two examples is changed only when gradients' magnitude is scaled non-linearly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our purpose is to study the behaviours of CCE, MAE and IMAE on CIFAR-10 instead of pushing its state-of-the-art results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We explore the performance of different losses in real-world applications instead of pushing the state-of-the-art results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We remark that our empirically demonstrated claim-"Gradient Magnitude's Variance Matters"-can be applied for other algorithms as well, for example, CCE. However, it is beyond the scope of this work since we focus on analysing MAE and how to improve MAE here. We will investigate this claim in other loss functions in our future work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>S. A closer look at memorization in</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude&apos;s Variance Matters deep networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Smooth loss functions for deep top-k classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ranked list loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning by online soft mining and class-aware attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Derivative manipulation for general example weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11233</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Id-aware quality for set-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09143</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
