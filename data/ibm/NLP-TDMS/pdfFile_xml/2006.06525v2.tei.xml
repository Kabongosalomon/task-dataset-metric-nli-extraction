<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause trouble in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameterfree module, the WaveBlock, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements on multiple UDA person re-identification tasks. We also prove the generality of the proposed method by applying it to vehicle re-identification and image classification tasks. Our codes and models are available at: AWB.</p><p>Index Terms-Person re-identification, unsupervised domain adaptation, attentive waveblock.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE target of person re-identification (re-ID) is to match images of a person across different camera views. Because of its extensive numbers of applications, person re-ID has attracted attention from both academia and industry. In recent years, with the development of deep learning, supervised re-ID methods, such as <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b1">[2]</ref>, have gained impressive progress. However, there still exist several drawbacks. First, these methods require intensive manual labeling, which is expensive and time-consuming. Second, due to the domain gap, there is a significant performance drop when a model trained on a source domain is tested on a target domain <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Therefore, unsupervised domain adaptation W. Wang is with the School of Mathematical Sciences, Beihang University, Beijing, China.</p><p>F. Zhao, S. Liao, and L. Shao are with the Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates. L. Shao is also with the Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates.</p><p>Corresponding author: Fang Zhao (email: fang.zhao@inceptioniai.org).</p><p>(UDA) was introduced, which aims at learning a model on a labeled source domain and adapting it to an unlabeled target domain. Image-level adaptation, such as <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, uses a generative adversarial network (GAN) <ref type="bibr" target="#b14">[15]</ref> to transfer the image styles of the source domain to a target domain. Feature-level method like <ref type="bibr" target="#b64">[65]</ref> investigates underlying feature invariance. However, the performances of these approaches are still unsatisfactory when compared to their fully-supervised counterparts. Recently, several clustering based methods, such as <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, have been proposed, which employ clustering algorithms to group unannotated target images to generate pseudo-labels for training. Although they achieve state-ofthe-art performance in various UDA tasks, their abilities are hindered by noisy pseudo-labels caused by the imperfect clustering algorithms and the limited feature transferability.</p><p>To address the aforementioned problem, a dual network framework, Mutual Mean-Teaching (MMT) <ref type="bibr" target="#b12">[13]</ref> was proposed, which trains two networks simultaneously and utilizes a temporally averaged model to produce reliable soft labels as supervision signals. Although this design reduces the amplification of training error to some degree, as the two networks converge, as shown in <ref type="figure">Fig. 1</ref>, they unavoidably become more and more similar, which weakens their complementarity and may make them bias towards the same kind of noise. This limits further improvement in performance.</p><p>To overcome the above limitations, we propose a novel module, namely the Attentive WaveBlock (AWB), under the dual network framework. The critical idea behind AWB is to create a difference between features learned by two neural networks to enhance their complementarity. In particular, we first introduce the WaveBlock to modulate feature maps of the two networks with different block-wise waves. Then, an attention mechanism is utilized to force the networks to focus on discriminative features in these regions, which further enlarges the difference between them. Here two kinds of combinations are designed, i.e. pre-attention (Pre-A) and postattention (Post-A), to produce such different and discriminative features. For Pre-A, the attention modules first learn discriminative features, and then WaveBlocks wave regions differently. For Post-A, WaveBlocks first generate different waves, and then the attention modules learn discriminative features on the different waves. In <ref type="figure">Fig. 1</ref>, we visualize the feature attention maps of the three mutual learning methods using a gradientweighted class activation map <ref type="bibr" target="#b32">[33]</ref>  is</p><formula xml:id="formula_0">A − B F = i,j |a ij − b ij | 2 .</formula><p>As shown in <ref type="figure">Fig. 1</ref>, from MMT <ref type="bibr" target="#b12">[13]</ref> to WaveBlock, the difference increases to some degree. Further, from WaveBlock to AWB, the attention mechanism enlarges the difference created before. Our contributions are summarized as follows:</p><p>• We introduce a parameter-free module, the WaveBlock, that can create a difference between features learned by the dual network framework. It enhances the complementarity of the two networks and reduces the possibility that they become biased towards the same kind of noise. • We propose to utilize an attention mechanism to enlarge the difference between networks on the basis of the Wave-Block and design two kinds of combination strategies, i.e. pre-attention and post-attention. • The AWB module significantly improves performances on UDA tasks for person re-ID, with negligible computational increase. Compared with the state-of-the-art methods, we obtain improvements of 9.8%, 5.8%, 6.2%, 6.1%, 6.4% and 10.0% in mAP on Duke-to-Market, Marketto-Duke, Duke-to-MSMT, Market-to-MSMT, MSMT-to-Duke, and MSMT-to-Market re-ID tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised Domain Adaptation for Person Re-ID</head><p>Mainstream algorithms for UDA tasks can be categorized into three classes. The first is image-level methods. They use a GAN to transfer the source domain images to the target-domain style <ref type="bibr" target="#b53">[54]</ref>. For instance, PTGAN <ref type="bibr" target="#b44">[45]</ref> transfers knowledge, while SPGAN <ref type="bibr" target="#b7">[8]</ref> focuses on self-similarity and domain-dissimilarity. However, unfortunately, the performance of these methods lags far behind their fully-supervised counterparts. The second category is feature-level methods. For example, <ref type="bibr" target="#b64">[65]</ref> investigates three types of underlying invariance, i.e. exemplar-invariance, camera-invariance and neighborhoodinvariance. The last category is clustering based adaptation. These methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b11">[12]</ref> follow a similar general pipeline: they first pre-train on the source domain and then transfer the learned parameters to fit the target domain. Due to the imperfect clustering algorithms and big domain variance, the generated pseudo-labels tend to contain noise, which hinders further improvement in performance. Although, MMT <ref type="bibr" target="#b12">[13]</ref> was introduced to alleviate this problem by using a couple of neural networks to generate soft pseudo-labels, as the training process goes on, the two neural networks tend to converge and unavoidably share a high similarity. Therefore, it is necessary to consider how to create different networks and enhance the complementarity. This is the starting point of our AWB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Attention has been widely used to enhance representation learning in the fields of image classification <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b49">[50]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b9">[10]</ref> and so on. For instance, Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b17">[18]</ref> recalibrates channelwise feature responses and convolutional block attention module (CBAM) <ref type="bibr" target="#b45">[46]</ref> further uses channel attention and spatial attention to explore "what" and "where" to focus. By stacking attention modules which can generate module-adaptation and attention-aware features, Residual Attention Network <ref type="bibr" target="#b39">[40]</ref> is built. Non-local block <ref type="bibr" target="#b43">[44]</ref> explores the relationship between different positions on feature maps and exploits global features. In the person re-ID community, fully-supervised state-of-the-arts algorithms, such as ConsAtt <ref type="bibr" target="#b67">[68]</ref>, SCAL <ref type="bibr" target="#b1">[2]</ref>, SONA <ref type="bibr" target="#b48">[49]</ref>, and ABD-Net <ref type="bibr" target="#b3">[4]</ref>, on several datasets (Market-1501 <ref type="bibr" target="#b59">[60]</ref>, DukeMTMC <ref type="bibr" target="#b61">[62]</ref>, CUHK03 <ref type="bibr" target="#b21">[22]</ref>, MSMT17 <ref type="bibr" target="#b44">[45]</ref>) adopt an attention scheme. However, nearly all aforementioned works utilize attention mechanism to discover discriminative or critical features to boost the performance. In our work, beyond the stated functions, we find attention mechanism can enlarge the difference created by WaveBlocks. Therefore, by integrating attention mechanism, the improved performance comes from more complementary and more discriminative features extracted by two neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Drop-series</head><p>Dropout <ref type="bibr" target="#b34">[35]</ref> was proposed as a regularization method to prevent overfitting problem by dropping units in fully connected layers. Instead of dropping discrete units, DropBlock <ref type="bibr" target="#b13">[14]</ref> drops units in a contiguous region of a feature map. Batch DropBlock Network (BDB) <ref type="bibr" target="#b5">[6]</ref> uses a global branch and a feature dropping branch to keep the global salient representations and reinforce the attentive feature learning of local regions. Wu <ref type="bibr" target="#b47">[48]</ref> uses multiple dropping branches on the basis of BDB <ref type="bibr" target="#b5">[6]</ref> to further boost the performance. Different from Dropout <ref type="bibr" target="#b34">[35]</ref>, the proposed WaveBlock modulates a continuous region of a feature map like DropBlock <ref type="bibr" target="#b13">[14]</ref>. However, unlike DropBlock <ref type="bibr" target="#b13">[14]</ref> which may drop some discriminant information randomly, the proposed WaveBlock modulates a given feature map with different waves. This design preserves the original information to some degree. Comparing with BDB <ref type="bibr" target="#b5">[6]</ref>, which increases the computing burden by introducing another branch, the proposed WaveBlock is totally parameter-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we first simply review the Mutual Mean-Teaching (MMT) framework, then introduce the proposed WaveBlock module. Finally, two different strategies for combining attention mechanism with WaveBlock are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MMT framework Revisit</head><p>Briefly, the MMT framework includes two identical networks with different initializations. Its pipeline is as follows: first, the two networks are pre-trained on the source domain to obtain initialized parameters. Then, in each epoch, offline hard pseudo-labels are generated using a clustering algorithm. In each iteration of a given epoch, refined soft pseudo-labels are produced by the two networks. The hard pseudo-labels and refined soft pseudo-labels generated by one network are then used together to supervise the learning process of the other network. Finally, again in each iteration, the temporally averaged models are updated and used for prediction. For more details, please refer to <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WaveBlock</head><p>In order to enhance the complementarity of the two networks, we first introduce the WaveBlock module to create a difference between features learned by the networks, which is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Instead of dropping blocks as in <ref type="bibr" target="#b13">[14]</ref> which may lose discriminant information, WaveBlocks modulate a given feature map with different block-wise waves, so that differences are created between dual networks, and meanwhile the original information is preserved to some extent.</p><p>Given a feature map F ∈ R C×H×W , where C is the number of channels, H and W are spatial height and width, respectively, a waving width rate r w , and a waving height rate r h , we first generate a random integer with uniform distribution:</p><formula xml:id="formula_1">X ∼ U (0, [H · (1 − r w )]),<label>(1)</label></formula><p>where [·] is the rounding function. Then, the WaveBlock modulated feature map is defined as F * ∈ R C×H×W :</p><formula xml:id="formula_2">F * ijk = F ijk , X ≤ j &lt; X + [H · r w ] , r h · F ijk , otherwise.<label>(2)</label></formula><p>where i, j, and k respectively represent the coordinates of the dimension, height, and width of the feature map. This design modulates a given feature map with block-wise waves and meanwhile original information is kept to some degree. When applying WaveBlocks to the feature maps F 1 , F 2 of two networks, respectively, the difference between the networks can be created by waving differently on blocks of feature maps. Let F * 1 , F * 2 denote the output feature maps of WaveBlock and X 1 , X 2 indicate the waving random integers generated on the two networks; we will calculate the probability that the same wave is generated for both. For simplicity, it is assumed that F 1 and F 2 have the same size.</p><p>In order to enable F * 1 = F * 2 , we should make X 1 = X 2 . Since</p><formula xml:id="formula_3">P (X 1 = X 2 ) = [H · (1 − r w )] [H · (1 − r w )] 2 = 1 [H · (1 − r w )] ,<label>(3)</label></formula><p>we have</p><formula xml:id="formula_4">P (F * 1 = F * 2 ) = P (X 1 = X 2 ) = 1 [H · (1 − r w )]</formula><p>. <ref type="formula">(4)</ref> If multiple GPUs are used for training, X will be generated independently in each GPU. In practice, r w is set as 0.3 experimentally and four GPUs are used. Then, on feature maps with H = 32, we have</p><formula xml:id="formula_5">P (F * 1 = F * 2 ) = 1 [H · (1 − r w )] 4 = 4.27 · 10 −6 .<label>(5)</label></formula><p>Because the probability is too small for the waves of the two networks to be the same, we may say that there is always a difference created between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attentive WaveBlock</head><p>To enlarge the difference created by WaveBlocks and find more discriminative and more complementary features, the attention mechanism is integrated with the WaveBlock module in this section. Two kinds of combination strategies are designed, including pre-attention (Pre-A) and post-attention (Post-A). The overview of MMT <ref type="bibr" target="#b12">[13]</ref> integrated with AWB is shown in <ref type="figure">Fig. 3</ref>.   <ref type="figure">Fig. 3</ref>. The overview of our complementarity-enhanced mutual networks. Through the proposed AWB modules, the two networks learn different and discriminative features. The noise in pseudo-labels is suppressed to some degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Attention Mechanism:</head><p>To show that the proposed Wave-Block can be combined to general attention methods, two kinds of attention mechanisms are tried here. The first one is the convolutional block attention module (CBAM) <ref type="bibr" target="#b45">[46]</ref>. Given a feature map F ∈ R C×H×W , CBAM exerts a channel attention map M c and a spatial attention map M s on F sequentially:</p><formula xml:id="formula_6">K 1 = M c (F ) ⊗ F,<label>(6)</label></formula><formula xml:id="formula_7">K 2 = M s (K 1 ) ⊗ K 1 ,<label>(7)</label></formula><p>where ⊗ denotes element-wise multiplication. In CBAM, the channel attention exploits the inter-channel relationship of features, while the spatial attention focuses on "where" an informative part is located.</p><p>In the original paper of CBAM <ref type="bibr" target="#b45">[46]</ref>, CBAM is integrated into a block of ResNet <ref type="bibr" target="#b15">[16]</ref>. However, due to the depth of ResNet <ref type="bibr" target="#b15">[16]</ref>, the computing burden increases to some degree. Therefore, we improve the original CBAM to arrange it between sequential stages of ResNet <ref type="bibr" target="#b15">[16]</ref>. In each improved CBAM module, the original feature map F is added to the modified one K 2 to obtain the final one K 3 , which aims to avoid the information loss.</p><p>The second attention mechanism is the Non-local block <ref type="bibr" target="#b43">[44]</ref>. Here, its simplified version is adopted. Let F ∈ R C×H×W denote a feature map for Non-local block and θ denote a 1×1 convolution. Through θ, the number of channels of F are reduced from C to C/2, i.e. θ (F ) ∈ R C 2 ×H×W . Similarly, another 1×1 convolution φ also reduces the number of channels from C to C/2, i.e. φ (F ) ∈ R C 2 ×H×W . Then we collapse the spatial dimension of θ (F ) and</p><formula xml:id="formula_8">φ (F ) into a single dimension, i.e. θ (F ) ∈ R C 2 ×HW , φ (F ) ∈ R C 2 ×HW .</formula><p>We obtain our matrix J ∈ R HW ×HW :</p><formula xml:id="formula_9">J = (θ (F )) T · φ (F ) .<label>(8)</label></formula><p>Next, we adopt 1 H×W as the scaling factor for J, without using sof tmax. In the other branch, F is fed into a function g, which is a 1×1 convolution followed by a batch normalization layer. Similarly, we collapse the spatial dimension of g(F ) into a single dimension and further apply a transpose to get g (F ) ∈ R HW × C 2 . Finally, we multiply J with g (F ), transpose and reshape its dimensions to C 2 × H × W , and use another 1 × 1 convolution h to restore the channel dimension to C. The result is denoted as I. Also, the final feature map is obtained by the sum of I and F .</p><p>2) Pre-Attention: As illustrated in <ref type="figure">Fig. 4(a)</ref>, to combine the attention module with the WaveBlock, we first try to arrange it before the WaveBlock, which is called the Pre-attention (Pre-A) strategy. In this way, the attention modules first learn discriminative features, and then WaveBlocks wave regions differently to produce different and discriminative features. Given a feature map F ∈ R C×H×W , WaveBlock is applied to either of the two attention modules mentioned before and obtain:</p><formula xml:id="formula_10">F * = W aveBlock (M s (M c (F ) ⊗ F ) ⊗ (M c (F ) ⊗ F ) + F ) ,<label>(9)</label></formula><p>or</p><formula xml:id="formula_11">F * = W aveBlock h (θ (F )) T · φ (F ) · g (F ) + F .</formula><p>(10) Here, the attention modules are used to enlarge the difference of the backward gradients generated by the WaveBlock. Although the WaveBlock is able to make the two networks work on different regions of feature maps, some features learned from non-discriminative regions, such as backgrounds, may still be similar. By combining the attention modules with the WaveBlock, the two networks focus on different and discriminative regions, such as the human body, and thus can learn more different features. The advantage of Pre-A is that the attention weights can be computed by using the complete feature maps. This is more beneficial to CBAM because the convolution used to compute its spatial attention will not be affected near the border of waved regions.</p><p>3) Post-Attention: The second combination strategy is shown in <ref type="figure">Fig. 4(b)</ref>. We arrange the attention mechanism after the WaveBlock, which is named as post-attention (Post-A). Correspondingly, the WaveBlocks first wave regions differently, and then the attention modules learn discriminative features on the waved regions to produce different and discriminative features. Given a feature map F ∈ R C×H×W , after passing through the WaveBlock, either of the two attention modules mentioned before can be applied. This produces:</p><formula xml:id="formula_12">F = W aveBlock (F ) ,<label>(11)</label></formula><formula xml:id="formula_13">F * = M s M c F ⊗ F ⊗ M c F ⊗ F + F ,<label>(12)</label></formula><p>or</p><formula xml:id="formula_14">F * = h θ F T · φ F · g F + F .<label>(13)</label></formula><p>Compared with Pre-A, although the waved regions may affect the computation of the attention weights, directly applying the attention modules on the different waved regions is more efficient for enlarging different features. Post-A is more beneficial to the Non-local block because the non-local operation reduces the impact of waved regions. 2) Vehicle re-ID datasets: To prove the generality of the proposed method, we also accomplish unsupervised domain adaptation task on three vehicle re-ID datasets, including Veri-776 <ref type="bibr" target="#b26">[27]</ref>, VehicleID <ref type="bibr" target="#b25">[26]</ref>, and VehicleX <ref type="bibr" target="#b28">[29]</ref>. Veri-776 <ref type="bibr" target="#b26">[27]</ref> is collected using 20 different cameras. Among them, 37, 746 images of 575 identities are used to train. The query set has 1, 678 images while the gallery set has 11579 images. Vehi-cleID <ref type="bibr" target="#b25">[26]</ref> contains 221, 736 vehicles. For training, 113, 346 images of 13, 164 identities are used. For testing, there are 5, 693 query images and 102, 724 gallery images. VehicleX <ref type="bibr" target="#b28">[29]</ref> is a synthetic dataset generated by Unity engine <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b37">[38]</ref> and translated to real-world style by SPGAN <ref type="bibr" target="#b7">[8]</ref>. The dataset has 192, 150 images of 1, 362 identities for training, and it does not have test part.</p><p>3) Image classification dataset: To further verify the efficacy of the proposed method on large scale datasets, the proposed method is also applied to the classification task on ImageNet <ref type="bibr" target="#b6">[7]</ref>. The dataset contains 1, 000 object classes with about 1.2 million images for training and 50, 000 images for validation.</p><p>4) Evaluation protocol: For re-ID datasets, to evaluate our algorithm, we adopt the mean average precision (mAP) and cumulative matching characteristic (CMC) at rank-1, rank-5, and rank-10. No post-processing, such as re-ranking <ref type="bibr" target="#b62">[63]</ref>, is used and we utilize single-query evaluation protocols. For image classification dataset, top-1 classification accuracy is used as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>We essentially follow the same training settings as MMT <ref type="bibr" target="#b12">[13]</ref>. For the source-domain pre-training, to ensure that the improvement comes from a different mutual training but not an enhanced pre-trained network, no change is made, i.e. ResNet-50 <ref type="bibr" target="#b15">[16]</ref> is used as the backbone network.</p><p>For the first stage of target-domain training, attention modules are trained without WaveBlock engaged. Specifically, two attention modules are plugged after Stage 2 and Stage 3 of the ResNet-50 <ref type="bibr" target="#b15">[16]</ref> backbone with random initialization. The two modules are trained for 10 epochs with other parameters frozen. For the second stage target-domain training, WaveBlocks are added into two networks. Specifically, the attention modules are integrated with WaveBlocks after Stage 2 and Stage 3 of the ResNet-50 <ref type="bibr" target="#b15">[16]</ref> backbone to form AWB. For CBAM, the Pre-A design is used and for Non-local, the Post-A design is utilized. Because we successfully enhance the complementarity and make it some more difficult for the two neural networks biased towards the same kind of noise, the training process can last for more epochs. We train for 80 epochs with all parameters engaged. When clustering, we select the optimal k values of k-means following <ref type="bibr" target="#b12">[13]</ref>, i.e. 500 for Duke-to-Market, 700 for Market-to-Duke, 1500 for Duketo-MSMT and Market-to-MSMT. Similarly, we also conduct experiments the codes provided by MMT <ref type="bibr" target="#b12">[13]</ref> on MSMTto-Market and MSMT-to-Duke, respectively. The selected k values of k-means are 500 and 700, respectively. For vehicle re-ID, because the experiment results are not reported in MMT <ref type="bibr" target="#b12">[13]</ref>, we run its provided codes to get results. The adopted clustering method is DBSCAN <ref type="bibr" target="#b8">[9]</ref>. For testing, the WaveBlock is not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-Arts</head><p>To prove the superiority of the AWB under the MMT <ref type="bibr" target="#b12">[13]</ref> framework, we compare the proposed model with state-of-the- art methods on six person re-ID domain adaptations tasks.</p><p>The comparison results are shown in <ref type="table" target="#tab_3">Table I</ref>. In terms of mAP, we gain a 9.8%, 5.8%, 6.2%, 6.1%, 6.4% and 10.0% improvement on Duke-to-Market, Market-to-Duke, Duke-to-MSMT, Market-to-MSMT, MSMT-to-Duke, and MSMT-to-Market, respectively. As for rank-1, 5.8%, 5.8%, 10.9%, 8.6%, 6.3%, and 6.7% improvements are obtained, respectively. We attribute the improvement in performance to two aspects. On one hand, the WaveBlocks enhance the complementarity and thus the two networks will not be misled by the same kind of noise to some extent when compared with MMT <ref type="bibr" target="#b12">[13]</ref>. On the other hand, the attention modules in AWB learn discriminative and more complementary information which is essential to the performance improvement. In fact, although domain adaptive person re-ID has been explored in many papers, the same experiment setting for vehicle re-ID has not attracted much attention until now. Therefore, we also evaluate it on the vehicle re-ID and image classification tasks. For comparison, we implement the state-of-the-arts algorithm MMT <ref type="bibr" target="#b12">[13]</ref> on three vehicle re-ID datasets, i.e. VehicleID <ref type="bibr" target="#b25">[26]</ref>, VeRi-776 <ref type="bibr" target="#b26">[27]</ref>, and VehicleX <ref type="bibr" target="#b28">[29]</ref>. Two tasks, VehicleID-to-VeRi-776 and VehicleX-to-VeRi-776, are explored. The results are shown in <ref type="table" target="#tab_3">Table II</ref>. When compared with MMT <ref type="bibr" target="#b12">[13]</ref>, as for mAP, 1.0% and 2.0% improvements are achieved, respectively. As for rank-1, we gain a 2.8% and 4.7% improvement, respectively.</p><p>To prove the generality of the proposed method, we also apply the proposed AWB to image classification task on ImageNet <ref type="bibr" target="#b6">[7]</ref>. The selected baseline is Deep Mutual Learning (DML) <ref type="bibr" target="#b58">[59]</ref>. DML <ref type="bibr" target="#b58">[59]</ref> designs two neural networks to learn collaboratively and teach each other. For more details, please refer to <ref type="bibr" target="#b58">[59]</ref>. The intuition is the same, i.e. using WaveBlock to  <ref type="bibr" target="#b12">[13]</ref>. THE RESULTS ARE REPORTED ON VEHICLEID <ref type="bibr" target="#b25">[26]</ref>, VERI-776 <ref type="bibr" target="#b26">[27]</ref>, AND VEHICLEX <ref type="bibr" target="#b28">[29]</ref>. (*) IMPLIES THE IMPLEMENTATION IS BASED ON THE AUTHORS' CODES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>VehicleID-to-VeRi-776 VehicleX-to-VeRi-776 mAP rank-1 rank-5 rank-10 mAP rank-1 rank-5 rank-10 UDAP <ref type="bibr" target="#b33">[34]</ref> 35  enhance the complementarity of the two neural networks, and therefore the co-teaching process will be better. We reproduce the experiment results in DML <ref type="bibr" target="#b58">[59]</ref> by using four GPUs. The selected backbones are MobileNet <ref type="bibr" target="#b16">[17]</ref>. WaveBlocks are arranged after the feature extraction layers. The backbones are trained for 10 epochs before plugging WaveBlocks. The experimental results are shown in <ref type="table" target="#tab_3">Table IV</ref>. As for the top-1 accuracy, WaveBlock improves the performance without any parameters increasing. If using AWB with I-CBAM or Nonlocal, the top-1 accuracy can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Analysis and Ablation Studies</head><p>To prove the efficacy of each component in the AWB, we conduct parameter analysis and ablation experiments on DukeMTMC to Market-1501 and Market-1501 to DukeMTMC tasks. The experimental results and analyses are reported below.</p><p>Selection for the waving width rate r w and waving height rate r h .</p><p>In the proposed WaveBlock, the waving height rate and waving width rate are of great importance. The mAP and rank-1 performance of the different combinations of them are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The waving height rate is set as 0.5, 1, 1.5, 2.5, and 3, while setting the waving width rate as 0.1, 0.2, 0.3, 0.4, 0.5, and 0.6. Apparently, when the waving height rate is 1, whichever the waving width rate is chose, the experiment setting is same as MMT <ref type="bibr" target="#b12">[13]</ref>. For Duke-to-Market, the best performance is achieved when the waving height rate is 1.5 and waving width rate is 0.3. Under this setting, the mAP is 76.3% and the rank-1 is 90.9%. For Market-to-Duke, when the waving height rate equals to 1.5 and the waving width rate equals to 0.2, the highest performance, i.e. 68.6% mAP and 82.4% rank-1, is achieved. It can be found that the optimal waving height rate is 1.5 and the optimal waving width rate is 0.2 to 0.3. Further, when the waving height rate is larger than 1, although there is difference in performance, WaveBlocks improve the baseline continuously without any parameters increasing. In conclusion, the performance of the proposed method is not very sensitive for the waving height rate and waving width rate.</p><p>Effectiveness of the WaveBlock Design.</p><p>To illustrate the effectiveness of the WaveBlock design, the WaveBlock is replaced with the feature dropping block in <ref type="bibr" target="#b5">[6]</ref>. Also, to avoid disturbance, no attention mechanism is used. The experiment results are reported in <ref type="table" target="#tab_3">Table III</ref> as "WaveBlock" and "DropBlock", respectively. Compared to WaveBlock, for Duke-to-Market, the mAP decreases by 24.5% and the rank-1 decreases by 21.0%; for Market-to-Duke, the mAP decreases by 12.5% and the rank-1 decreases by 12.5%. The reason is that DropBlock drops some discriminative and important features, which prevents the two neural networks from fitting training data well. In contrast, the proposed WaveBlock modulates a given feature map with preserved original feature to some degree.  Comparison between the original CBAM and improved CBAM.</p><p>The original CBAM (O-CBAM) and improved CBAM (I-CBAM) are compared from two aspects. Firstly, we compare the parameter numbers of the model. For MMT <ref type="bibr" target="#b12">[13]</ref>, its backbone ResNet-50 has 23.51 million parameters. When the backbone is integrated with O-CBAM, it has 26.04 million parameters. If I-CBAM is used to replace O-CBAM, the parameter numbers decrease to 23.68 million. I-CBAM only has 0.7% more parameters than backbone while O-CBAM increases the parameters by 10.8%. In conclusion, I-CBAM achieves a truly negligible increase in parameters. From the performance aspect, the experimental results are shown in <ref type="table" target="#tab_3">Table III</ref>  Effectiveness of AWB.</p><p>In this part, we try to prove the effectiveness of the attention mechanism in the AWB. Further, two combination strategies for two kinds of attention mechanisms are compared. The experimental results are displayed in <ref type="table" target="#tab_3">Table III</ref> as I-CBAM and Non-local, respectively. As can be observed, for I-CBAM, Pre-A combination strategy is better than Post-A. It is because the border of the waved feature maps may affect the convolution computing for spatial attention, and the Pre-A strategy avoids this problem. For Non-local block, the performances of both combination strategies are better than adding Non-local block directly. Specifically, the Post-A strategy is much better because directly applying attention modules on waved feature maps is more efficient to produce different and discriminative features and non-local operation reduces the impact of waved regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Discussion</head><p>Effectiveness of WaveBlocks for creating a difference.</p><p>To prove the effectiveness of the created difference between features learned by two networks, the same shape of Wave-Blocks is adopted for both networks, i.e. when generating the waving random integer, X 1 is always equal to X 2 . For Duke-to-Market and Market-to-Duke, the mAP drops by 4.1% and 2.2%, respectively. Therefore, even without the attention mechanism, the difference created still enhances the complementarity of two neural networks. Further, although using the same shape of WaveBlocks performs poorer, the performance is still improved minutely because in each iteration, the waving random integer is generated independently which forces a network to learn from different areas. In conclusion, it is necessary to introduce the WaveBlocks with different shapes to create a difference between features learned by two networks. Stable performance improvement with different backbones.</p><p>To prove that the proposed method is a plug-and-play method, we try some other backbones besides ResNet-50 <ref type="bibr" target="#b15">[16]</ref>. The selected backbones are WideResNet-50 <ref type="bibr" target="#b54">[55]</ref> and DenseNet-121 <ref type="bibr" target="#b18">[19]</ref>. Similar with the modification for ResNet-50 <ref type="bibr" target="#b15">[16]</ref>, i.e. the last spatial down-sampling operation is removed, we also modify WideResNet-50 <ref type="bibr" target="#b54">[55]</ref> and DenseNet-121 <ref type="bibr" target="#b18">[19]</ref> to obtain a higher resolution. Specifically, the modification for WideResNet-50 <ref type="bibr" target="#b54">[55]</ref> is same as ResNet-50 <ref type="bibr" target="#b15">[16]</ref> and the average pooling operation in the last transition layer of DenseNet-121 <ref type="bibr" target="#b18">[19]</ref> is removed.</p><p>For WideResNet-50 <ref type="bibr" target="#b54">[55]</ref>, we plug the proposed WaveBlocks or AWBs after the stage 2 and 3. For DenseNet-121 <ref type="bibr" target="#b18">[19]</ref>, they are arranged after the Dense Block 2 and 3. The experiment results are shown in <ref type="table" target="#tab_7">Table V</ref>. In Duke-to-Market task, the mAP increases by 16.2% with WideResNet-50 <ref type="bibr" target="#b54">[55]</ref> and increases by 9.2% with DenseNet-121 <ref type="bibr" target="#b18">[19]</ref>. Also, in Market-to-Duke task, the mAP increases by 7.1% and 7.0% with two backbones, respectively. Further, we achieve higher mAP and rank-1 performance in two tasks by using DenseNet-121 <ref type="bibr" target="#b18">[19]</ref> than ResNet-50 <ref type="bibr" target="#b18">[19]</ref> as our backbone.</p><p>Stable performance improvement with different k values.</p><p>Actually, the AWB can improve performance with different k values stably. Similar with MMT [13], we have tried three different k, i.e. 500, 700, and 900, for Duke-to-Market and Market-to-Duke tasks, respectively. The experiment results are shown in <ref type="table" target="#tab_3">Table VI</ref>. No matter which strategy (Pre-A (I-CBAM) or Post-A (Non-local)) is chosen, the performance is improved significantly when compared with MMT <ref type="bibr" target="#b12">[13]</ref>. These experiment results prove the generality of the proposed method.</p><p>Quantification of the created difference. The differences created by WaveBlocks and enlarged by attention mechanism are quantified in this part. We adopt the strategy of Post-A with Non-local. The difference is quantified by calculating the Frobenius norm between two gradient-weighted class activation maps <ref type="bibr" target="#b32">[33]</ref> of the same input after Stage 3 or the proposed modules, as illustrated in the introduction section. Further, the differences in Frobenius norm for all images are averaged to obtain final quantified differences. As shown in <ref type="table" target="#tab_3">Table VII</ref>, the quantified difference of WaveBlock is larger than MMT's. The quantified difference is further enlarged by integrating attention mechanism with WaveBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a parameter-free module, the WaveBlock, is first proposed. Then, we design two kinds of combination strategies, i.e. pre-attention and post-attention, to integrate the proposed WaveBlock with the attention mechanism. We use the WaveBlock to create a difference between features learned by two networks under the framework of MMT. An attention mechanism is also utilized to enlarge the difference and learn different and discriminative features on the basis of WaveBlock. By plugging the proposed AWB into the MMT, the complementarity of the two networks is enhanced and the possibility of their being biased towards the same kind of noise is decreased. Extensive experiments show that the proposed AWB under the MMT framework outperforms the state-of-theart unsupervised domain adaptation person re-identification methods by a large margin. Further, the generality of the proposed method is proved by applying it a vehicle reidentification and image classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the WaveBlock module, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Specifically, a block is randomly selected and kept the same, while feature values of other blocks are multiplied rw times to form a wave.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Net</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The mAP and rank-1 improvement under different experiment settings. Different lines represent different waving height rates. When waving height rate is larger than 1, WaveBocks improve the performance continuously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>as O-CBAM and I-CBAM respectively. When only CBAM is used, I-CBAM achieves competitive mAP and rank-1 performance with O-CBAM both in Duke-to-Market and Market-to-Duke tasks. In the Post-A strategy, we observe performance degradation for O-CBAM in two tasks. Meanwhile, both the Post-A and Pre-A strategies with I-CBAM outperform I-CBAM and the series of O-CBAM. Because WaveBlocks enhance the complementarity of two networks, Post-A and Pre-A perform better. The positions of I-CBAM and WaveBlocks are the same while the ones of O-CBAM and WaveBlocks are different, therefore the former is more effective for focusing on different and discriminative features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The gradient-weighted class activation maps of MMT<ref type="bibr" target="#b12">[13]</ref>, WaveBlock, and AWB. The differences in Frobenius norm between two maps for the three methods are 1.58, 2.04 and 4.83, respectively.</figDesc><table><row><cell>(a) Original image</cell><cell>(b) MMT [13]</cell><cell>(c) WaveBlock</cell><cell>(d) AWB</cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>arXiv:2006.06525v2 [cs.CV] 2 Dec 2020</cell><cell></cell><cell></cell><cell></cell></row></table><note>and compute the difference in Frobenius norm between two maps A and B, which</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Two different combination strategies for the attention module and WaveBlock. The benefit of Pre-A is the attention modules can be calculated using complete features. The advantage of Post-A is that directly applying attention module on waved features is more efficient to enlarge the difference created. Market-1501<ref type="bibr" target="#b59">[60]</ref> is obtained using six different cameras.The dataset has 1, 501 labeled persons in 32, 668 images. For training, there are 12, 936 images of 751 identities. For testing, the query has 3, 368 images and gallery has 19, 732 images. DukeMTMC-reID [62] contains 1, 404 persons from eight cameras. Among them, 16, 522 images of 702 identities are used for training. For testing, there are 2, 228 queries, and 17, 661 gallery images. MSMT17 [45] is the most challenging and largest re-ID dataset. It consists of 126, 441 bounding boxes of 4, 101 identities taken by 15 cameras. There are 32, 621 images for training while the query has 11, 659 images and the gallery has 82, 161 images.</figDesc><table><row><cell>Stage N</cell><cell>Attention</cell><cell>WaveBlock</cell><cell>Stage N+1</cell></row><row><cell></cell><cell cols="2">(a) Pre-attention</cell><cell></cell></row><row><cell>Stage N</cell><cell>WaveBlock</cell><cell>Attention</cell><cell>Stage N+1</cell></row><row><cell></cell><cell cols="2">(b) Post-attention</cell><cell></cell></row><row><cell>Fig. 4. IV. EXPERIMENT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A. Datasets and Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1) Person re-ID datasets:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I COMPARISON</head><label>I</label><figDesc>BETWEEN THE PROPOSED METHOD AND STATE-OF-THE-ART ALGORITHMS. THE RESULTS ARE REPORTED ON MARKET-1501 [60], DUKEMTMC [62] AND MSMT17 [45]. (*) IMPLIES THE IMPLEMENTATION IS BASED ON THE CODES PROVIDED BY THE ORIGINAL PAPER.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">Duke-to-Market rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell cols="2">Market-to-Duke rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell>PUL [11]</cell><cell>20.5</cell><cell>45.5</cell><cell>60.7</cell><cell>66.7</cell><cell>16.4</cell><cell>30.0</cell><cell>43.4</cell><cell>48.5</cell></row><row><cell>SPGAN [8]</cell><cell>22.8</cell><cell>51.5</cell><cell>70.1</cell><cell>76.8</cell><cell>22.3</cell><cell>41.1</cell><cell>56.6</cell><cell>63.0</cell></row><row><cell>TJ-AIDL [43]</cell><cell>26.5</cell><cell>58.2</cell><cell>74.8</cell><cell>81.1</cell><cell>23.0</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell></row><row><cell>CFSM [1]</cell><cell>28.3</cell><cell>61.2</cell><cell>−</cell><cell>−</cell><cell>27.3</cell><cell>49.8</cell><cell>−</cell><cell>−</cell></row><row><cell>UCDA [31]</cell><cell>30.9</cell><cell>60.4</cell><cell>−</cell><cell>−</cell><cell>31.0</cell><cell>47.7</cell><cell>−</cell><cell>−</cell></row><row><cell>HHL [64]</cell><cell>31.4</cell><cell>62.2</cell><cell>78.8</cell><cell>84.0</cell><cell>27.2</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell></row><row><cell>BUC [25]</cell><cell>38.3</cell><cell>66.2</cell><cell>79.6</cell><cell>84.5</cell><cell>27.5</cell><cell>47.4</cell><cell>62.6</cell><cell>68.4</cell></row><row><cell>ARN [24]</cell><cell>39.4</cell><cell>70.3</cell><cell>80.4</cell><cell>86.3</cell><cell>33.4</cell><cell>60.2</cell><cell>73.9</cell><cell>79.5</cell></row><row><cell>CDS [47]</cell><cell>39.9</cell><cell>71.6</cell><cell>81.2</cell><cell>84.7</cell><cell>42.7</cell><cell>67.2</cell><cell>75.9</cell><cell>79.4</cell></row><row><cell>ECN [65]</cell><cell>43.0</cell><cell>75.1</cell><cell>87.6</cell><cell>91.6</cell><cell>40.4</cell><cell>63.3</cell><cell>75.8</cell><cell>80.4</cell></row><row><cell>PDA-Net [23]</cell><cell>47.6</cell><cell>75.2</cell><cell>86.3</cell><cell>90.2</cell><cell>45.1</cell><cell>63.2</cell><cell>77.0</cell><cell>82.5</cell></row><row><cell>UDAP [34]</cell><cell>53.7</cell><cell>75.8</cell><cell>89.5</cell><cell>93.2</cell><cell>49.0</cell><cell>68.4</cell><cell>80.1</cell><cell>83.5</cell></row><row><cell>CR-GAN [5]</cell><cell>54.0</cell><cell>77.7</cell><cell>89.7</cell><cell>92.7</cell><cell>48.6</cell><cell>68.9</cell><cell>80.2</cell><cell>84.7</cell></row><row><cell>PCB-PAST [58]</cell><cell>54.6</cell><cell>78.4</cell><cell>−</cell><cell>−</cell><cell>54.3</cell><cell>72.4</cell><cell>−</cell><cell>−</cell></row><row><cell>SSG [12]</cell><cell>58.3</cell><cell>80.0</cell><cell>90.0</cell><cell>92.4</cell><cell>53.4</cell><cell>73.0</cell><cell>80.6</cell><cell>83.2</cell></row><row><cell>pMR-SADA[41]</cell><cell>59.8</cell><cell>83.0</cell><cell>91.8</cell><cell>94.1</cell><cell>55.8</cell><cell>74.5</cell><cell>85.3</cell><cell>88.7</cell></row><row><cell>MMCL [39]</cell><cell>60.4</cell><cell>84.4</cell><cell>92.8</cell><cell>95.0</cell><cell>51.4</cell><cell>72.4</cell><cell>82.9</cell><cell>85.0</cell></row><row><cell>ACT [51]</cell><cell>60.6</cell><cell>80.5</cell><cell>−</cell><cell>−</cell><cell>54.5</cell><cell>72.4</cell><cell>−</cell><cell>−</cell></row><row><cell>SNR [20]</cell><cell>61.7</cell><cell>82.8</cell><cell>−</cell><cell>−</cell><cell>58.1</cell><cell>76.3</cell><cell>−</cell><cell>−</cell></row><row><cell>ECN++ [66]</cell><cell>63.8</cell><cell>84.1</cell><cell>92.8</cell><cell>95.4</cell><cell>54.4</cell><cell>74.0</cell><cell>83.7</cell><cell>87.4</cell></row><row><cell>AD-cluster [56]</cell><cell>68.3</cell><cell>86.7</cell><cell>94.4</cell><cell>96.5</cell><cell>54.1</cell><cell>72.6</cell><cell>82.5</cell><cell>85.5</cell></row><row><cell>MMT [13]</cell><cell>71.2</cell><cell>87.7</cell><cell>94.9</cell><cell>96.9</cell><cell>65.1</cell><cell>78.0</cell><cell>88.8</cell><cell>92.5</cell></row><row><cell>AWB (Pre-A with I-CBAM)</cell><cell>78.8</cell><cell>92.2</cell><cell>97.1</cell><cell>98.1</cell><cell>70.0</cell><cell>82.9</cell><cell>91.4</cell><cell>93.9</cell></row><row><cell>AWB (Post-A with Non-local)</cell><cell>81.0</cell><cell>93.5</cell><cell>97.4</cell><cell>98.3</cell><cell>70.9</cell><cell>83.8</cell><cell>92.3</cell><cell>94.0</cell></row><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">Duke-to-MSMT rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell cols="2">Market-to-MSMT rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell>RPTGAN [45]</cell><cell>3.3</cell><cell>11.8</cell><cell>−</cell><cell>27.4</cell><cell>2.9</cell><cell>10.2</cell><cell>−</cell><cell>24.4</cell></row><row><cell>ECN [65]</cell><cell>10.2</cell><cell>30.2</cell><cell>41.5</cell><cell>46.8</cell><cell>8.5</cell><cell>25.3</cell><cell>36.3</cell><cell>42.1</cell></row><row><cell>SSG [12]</cell><cell>13.3</cell><cell>32.2</cell><cell>−</cell><cell>51.2</cell><cell>13.2</cell><cell>31.6</cell><cell>−</cell><cell>49.6</cell></row><row><cell>ECN++ [66]</cell><cell>16.0</cell><cell>42.5</cell><cell>55.9</cell><cell>61.5</cell><cell>15.2</cell><cell>40.4</cell><cell>53.1</cell><cell>58.7</cell></row><row><cell>MMCL [39]</cell><cell>16.2</cell><cell>43.6</cell><cell>54.3</cell><cell>58.9</cell><cell>15.1</cell><cell>40.8</cell><cell>51.8</cell><cell>56.7</cell></row><row><cell>MMT [13]</cell><cell>23.3</cell><cell>50.1</cell><cell>63.9</cell><cell>69.8</cell><cell>22.9</cell><cell>49.2</cell><cell>63.1</cell><cell>68.8</cell></row><row><cell>AWB (Pre-A with I-CBAM)</cell><cell>29.5</cell><cell>61.0</cell><cell>73.5</cell><cell>77.9</cell><cell>27.3</cell><cell>57.8</cell><cell>70.7</cell><cell>75.7</cell></row><row><cell>AWB (Post-A with Non-local)</cell><cell>28.1</cell><cell>56.8</cell><cell>70.1</cell><cell>75.2</cell><cell>29.0</cell><cell>57.3</cell><cell>70.7</cell><cell>75.9</cell></row><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">MSMT-to-Duke rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell cols="2">MSMT-to-Market rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell>PAUL [52]</cell><cell>53.2</cell><cell>72.0</cell><cell>82.7</cell><cell>86.0</cell><cell>40.1</cell><cell>68.5</cell><cell>82.4</cell><cell>87.4</cell></row><row><cell>MMT* [13]</cell><cell>63.2</cell><cell>76.5</cell><cell>88.0</cell><cell>91.4</cell><cell>69.4</cell><cell>85.9</cell><cell>94.4</cell><cell>96.2</cell></row><row><cell>AWB (Pre-A with I-CBAM)</cell><cell>68.6</cell><cell>82.8</cell><cell>91.4</cell><cell>93.1</cell><cell>77.1</cell><cell>91.2</cell><cell>96.7</cell><cell>97.8</cell></row><row><cell>AWB (Post-A with Non-local)</cell><cell>69.6</cell><cell>81.7</cell><cell>90.5</cell><cell>93.4</cell><cell>79.4</cell><cell>92.6</cell><cell>97.1</cell><cell>98.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISON</head><label>II</label><figDesc>BETWEEN THE PROPOSED METHOD AND MMT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>TABLE III THEABLATION STUDIES ABOUT EACH COMPONENTS IN OUR PROPOSED METHODS. "O-CBAM" DENOTES THE ORIGINAL CBAM IS USED WHILE "I-CBAM" DENOTES THE IMPROVED CBAM IS USED.</figDesc><table><row><cell></cell><cell>.8</cell><cell>76.9</cell><cell>85.8</cell><cell>89.0</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>MMT* [13]</cell><cell>36.1</cell><cell>76.3</cell><cell>82.8</cell><cell>87.4</cell><cell>35.2</cell><cell>75.9</cell><cell>82.2</cell><cell>86.9</cell></row><row><cell>AWB (Pre-A with I-CBAM)</cell><cell>37.1</cell><cell>79.1</cell><cell>84.6</cell><cell>88.5</cell><cell>36.7</cell><cell>80.6</cell><cell>86.0</cell><cell>89.7</cell></row><row><cell>AWB (Post-A with Non-local)</cell><cell>36.9</cell><cell>78.2</cell><cell>84.9</cell><cell>88.3</cell><cell>37.2</cell><cell>79.9</cell><cell>85.2</cell><cell>89.2</cell></row><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">Duke-to-Market rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell cols="2">Market-to-Duke rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell>WaveBlock</cell><cell>76.3</cell><cell>90.9</cell><cell>96.7</cell><cell>97.7</cell><cell>68.6</cell><cell>82.4</cell><cell>91.4</cell><cell>94.0</cell></row><row><cell>DropBlock</cell><cell>51.8</cell><cell>69.9</cell><cell>87.3</cell><cell>92.2</cell><cell>56.1</cell><cell>69.9</cell><cell>83.5</cell><cell>88.1</cell></row><row><cell>O-CBAM</cell><cell>76.2</cell><cell>90.6</cell><cell>97.0</cell><cell>98.0</cell><cell>67.2</cell><cell>80.0</cell><cell>90.7</cell><cell>93.5</cell></row><row><cell>Post-A (O-CBAM)</cell><cell>75.1</cell><cell>89.3</cell><cell>96.0</cell><cell>97.3</cell><cell>64.4</cell><cell>77.7</cell><cell>89.4</cell><cell>92.2</cell></row><row><cell>Pre-A (O-CBAM)</cell><cell>78.7</cell><cell>92.5</cell><cell>97.1</cell><cell>98.2</cell><cell>69.0</cell><cell>82.4</cell><cell>90.9</cell><cell>93.5</cell></row><row><cell>I-CBAM</cell><cell>75.4</cell><cell>90.1</cell><cell>96.1</cell><cell>97.4</cell><cell>67.8</cell><cell>80.2</cell><cell>90.3</cell><cell>93.3</cell></row><row><cell>Post-A (I-CBAM)</cell><cell>77.3</cell><cell>90.8</cell><cell>96.7</cell><cell>97.9</cell><cell>68.7</cell><cell>81.4</cell><cell>90.9</cell><cell>93.2</cell></row><row><cell>Pre-A (I-CBAM)</cell><cell>78.8</cell><cell>92.2</cell><cell>97.1</cell><cell>98.1</cell><cell>70.0</cell><cell>82.9</cell><cell>91.4</cell><cell>93.9</cell></row><row><cell>Non-local</cell><cell>79.0</cell><cell>92.5</cell><cell>97.2</cell><cell>98.5</cell><cell>69.6</cell><cell>82.4</cell><cell>91.0</cell><cell>93.8</cell></row><row><cell>Post-A (Non-local)</cell><cell>81.0</cell><cell>93.5</cell><cell>97.4</cell><cell>98.3</cell><cell>70.9</cell><cell>83.8</cell><cell>92.3</cell><cell>94.0</cell></row><row><cell>Pre-A (Non-local)</cell><cell>79.5</cell><cell>93.1</cell><cell>97.2</cell><cell>98.4</cell><cell>70.1</cell><cell>82.5</cell><cell>91.2</cell><cell>93.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V THE</head><label>V</label><figDesc>PERFORMANCE OF THE PROPOSED AWB WITH DIFFERENT BACKBONES. AWB IS A PLUG-AND-PLAY METHOD, WHICH OUTPERFORMS MMT [13]TABLE VI THE PERFORMANCE OF THE PROPOSED AWB UNDER DIFFERENT k VALUES. AWB OUTPERFORMS MMT [13] CONTINUOUSLY.TABLE VII THE AVERAGE DIFFERENCES OF TWO NETWORKS IN FROBENIUS NORM. "ATTENTION" OR "WAVEBLOCK" DENOTES ONLY ATTENTION MECHANISM OR WAVEBLOCK IS USED. "AWB" DENOTES THE COMBINATION OF ATTENTION MECHANISM AND WAVEBLOCK.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CONTINUOUSLY.</cell><cell></cell></row><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell>mAP</cell><cell cols="3">Duke-to-Market rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell>Market-to-Duke rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell></cell><cell></cell><cell cols="3">MMT [13]</cell><cell>57.5</cell><cell>78.5</cell><cell cols="2">91.5</cell><cell>94.5</cell><cell>60.1</cell><cell>74.8</cell><cell>86.8</cell><cell>90.4</cell></row><row><cell cols="2">WideResNet-50 [55]</cell><cell cols="3">WaveBlock Pre-A (I-CBAM)</cell><cell>63.8 66.5</cell><cell>84.8 86.0</cell><cell cols="2">94.1 94.7</cell><cell>96.2 96.7</cell><cell>62.9 66.4</cell><cell>77.6 80.9</cell><cell>87.7 89.6</cell><cell>91.1 92.1</cell></row><row><cell></cell><cell></cell><cell cols="3">Post-A (Non-local)</cell><cell>73.7</cell><cell>89.8</cell><cell cols="2">95.9</cell><cell>97.3</cell><cell>67.2</cell><cell>79.9</cell><cell>90.2</cell><cell>92.8</cell></row><row><cell></cell><cell></cell><cell cols="3">MMT [13]</cell><cell>73.7</cell><cell>89.2</cell><cell cols="2">96.1</cell><cell>97.4</cell><cell>65.5</cell><cell>78.1</cell><cell>89.5</cell><cell>92.9</cell></row><row><cell cols="2">DenseNet-121 [19]</cell><cell cols="3">WaveBlock Pre-A (I-CBAM)</cell><cell>77.7 78.4</cell><cell>91.1 91.4</cell><cell cols="2">96.7 96.8</cell><cell>97.8 97.8</cell><cell>69.7 70.1</cell><cell>81.3 82.0</cell><cell>90.8 91.1</cell><cell>93.1 92.9</cell></row><row><cell></cell><cell></cell><cell cols="3">Post-A (Non-local)</cell><cell>82.9</cell><cell>92.5</cell><cell cols="2">97.5</cell><cell>98.6</cell><cell>72.5</cell><cell>83.6</cell><cell>91.9</cell><cell>93.9</cell></row><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>mAP</cell><cell cols="3">Duke-to-Market rank-1 rank-5</cell><cell>rank-10</cell><cell>mAP</cell><cell>Market-to-Duke rank-1 rank-5</cell><cell>rank-10</cell></row><row><cell></cell><cell cols="2">MMT [13]</cell><cell></cell><cell>71.2</cell><cell>87.7</cell><cell cols="2">94.9</cell><cell>96.9</cell><cell>63.1</cell><cell>76.8</cell><cell>88.0</cell><cell>92.2</cell></row><row><cell>500</cell><cell cols="2">Pre-A (I-CBAM)</cell><cell></cell><cell>78.8</cell><cell>92.2</cell><cell cols="2">97.1</cell><cell>98.1</cell><cell>66.3</cell><cell>80.2</cell><cell>89.9</cell><cell>92.3</cell></row><row><cell></cell><cell cols="3">Post-A (Non-local)</cell><cell>81.0</cell><cell>93.5</cell><cell cols="2">97.4</cell><cell>98.3</cell><cell>67.5</cell><cell>80.9</cell><cell>90.6</cell><cell>93.0</cell></row><row><cell></cell><cell cols="2">MMT [13]</cell><cell></cell><cell>69.0</cell><cell>86.8</cell><cell cols="2">94.6</cell><cell>96.9</cell><cell>65.1</cell><cell>78.0</cell><cell>88.8</cell><cell>92.5</cell></row><row><cell>700</cell><cell cols="2">Pre-A (I-CBAM)</cell><cell></cell><cell>76.8</cell><cell>91.5</cell><cell cols="2">97.1</cell><cell>98.2</cell><cell>70.0</cell><cell>82.9</cell><cell>91.4</cell><cell>93.9</cell></row><row><cell></cell><cell cols="3">Post-A (Non-local)</cell><cell>78.8</cell><cell>93.0</cell><cell cols="2">97.2</cell><cell>98.3</cell><cell>70.9</cell><cell>83.8</cell><cell>92.3</cell><cell>94.0</cell></row><row><cell></cell><cell cols="2">MMT [13]</cell><cell></cell><cell>66.2</cell><cell>86.8</cell><cell cols="2">94.9</cell><cell>96.6</cell><cell>63.1</cell><cell>77.4</cell><cell>88.1</cell><cell>92.5</cell></row><row><cell>900</cell><cell cols="2">Pre-A (I-CBAM)</cell><cell></cell><cell>75.0</cell><cell>91.9</cell><cell cols="2">97.1</cell><cell>98.1</cell><cell>68.2</cell><cell>81.6</cell><cell>91.2</cell><cell>93.7</cell></row><row><cell></cell><cell cols="3">Post-A (Non-local)</cell><cell>73.8</cell><cell>91.4</cell><cell cols="2">96.5</cell><cell>97.7</cell><cell>69.5</cell><cell>81.8</cell><cell>91.3</cell><cell>93.5</cell></row><row><cell>Method</cell><cell cols="2">MMT [13]</cell><cell cols="4">Duke-to-Market Attention WaveBlock</cell><cell>AWB</cell><cell cols="2">MMT [13]</cell><cell>Market-to-Duke Attention WaveBlock</cell><cell>AWB</cell></row><row><cell>Difference</cell><cell></cell><cell>3.14</cell><cell></cell><cell>2.94</cell><cell>3.31</cell><cell></cell><cell>4.74</cell><cell cols="2">2.69</cell><cell>2.57</cell><cell>2.79</cell><cell>2.99</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Informatization Office of Beihang University for the supply of High Performance Computing Platform. We also would like to thank Anna Hennig who helped proofreading the paper. Wenhao Wang wants to thank Jin Fan for his generous computer technique support. This work is also supported by Inception Institute of Artificial Intelligence and School of Mathematical Sciences of Beihang University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disjoint label space transfer learning with common factorised space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-guided context rendering for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved selfsimilarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kdd</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation in person re-id via k-reciprocal clustering and large-scale heterogeneous environment synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Marchwica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting><address><addrLine>Snowmass Village, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2634" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ci-Siang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptation and re-identification network: An unsupervised deep transfer learning approach to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep learning-based approach to progressive vehicle re-identification for urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="869" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The 4th ai city challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranamesh</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="626" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoreid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive reidentification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107173</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratnesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10981" to="10990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10568" to="10577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Clustering and dynamic sampling based unsupervised domain adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Diversity-achieving slow-dropblock network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04414</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan Ning Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised image classification via attention mechanism and generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulmotaleb</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11373</biblScope>
			<biblScope unit="page">113731</biblScope>
		</imprint>
	</monogr>
	<note>ICGIP 2019</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Asymmetric co-teaching for unsupervised cross domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01349</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Patchbased discriminative feature learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Simulating content consistent vehicle datasets with attribute descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9021" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8222" to="8231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Omniscale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Discriminative feature learning with consistent attention regularization for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8040" to="8049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
