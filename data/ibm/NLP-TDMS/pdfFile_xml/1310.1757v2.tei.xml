<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep and Tractable Density Estimator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
							<email>b.uria@ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Ac</forename><surname>Murray@ed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle@usherbrooke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ca</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Département d&apos;informatique</orgName>
								<orgName type="institution">Université de Sherbrooke</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep and Tractable Density Estimator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: NADE and RNADE</head><p>Autoregressive methods use the product rule to factorize the probability density function of a D-dimensional vectorvalued random variable x as a product of one-dimensional</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In probabilistic approaches to machine learning, large collections of variables are described by a joint probability distribution. There is considerable interest in flexible model distributions that can fit and generalize from training data in a variety of applications. To draw inferences from these models, we often condition on a subset of observed variables, and report the probabilities of settings of another subset of variables, marginalizing out any unobserved nuisance variables. The solutions to these inference tasks often cannot be computed exactly, and require iterative approximations such as Monte Carlo or variational methods (e.g., Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&amp;CP volume 32. Copyright 2014 by the author(s). <ref type="bibr" target="#b4">Bishop, 2006)</ref>. Models for which inference is tractable would be preferable.</p><p>NADE <ref type="bibr" target="#b8">(Larochelle &amp; Murray, 2011)</ref>, and its real-valued variant RNADE <ref type="bibr" target="#b15">(Uria et al., 2013)</ref>, have been shown to be state of the art joint density models for a variety of realworld datasets, as measured by their predictive likelihood. These models predict each variable sequentially in an arbitrary order, fixed at training time. Variables at the beginning of the order can be set to observed values, i.e., conditioned on. Variables at the end of the ordering are not required to make predictions; marginalizing these variables requires simply ignoring them. However, marginalizing over and conditioning on any arbitrary subsets of variables will not be easy in general.</p><p>In this work, we present a procedure for training a factorial number of NADE models simultaneously; one for each possible ordering of the variables. The parameters of these models are shared, and we optimize the mean cost over all orderings using a stochastic gradient technique. After fitting the shared parameters, we can extract, in constant time, the NADE model with the variable ordering that is most convenient for any given inference task. While the different NADE models might not be consistent in their probability estimates, this property is actually something we can leverage to our advantage, by generating ensembles of NADE models "on the fly" (i.e., without explicitly training any such ensemble) which are even better estimators than any single NADE. In addition, our procedure is able to train a deep version of NADE incurring an extra computational expense only linear in the number of layers. conditional distributions:</p><formula xml:id="formula_0">p(x) = D d=1 p(x o d | x o &lt;d ),<label>(1)</label></formula><p>where o is a D-tuple in the set of permutations of (1, . . . , D) that serves as an ordering of the elements in x, x o d denotes the element of x indexed by the d-th element in o, and x o &lt;d the elements of x indexed by the first d − 1 elements in o. This factorisation of the pdf assumes no conditional independences. The only element constraining the modelling ability of an autoregressive model is the family of distributions chosen for each of the conditionals.</p><p>In the case of binary data, autoregressive models based on logistic regressors and neural networks have been proposed <ref type="bibr" target="#b5">(Frey, 1998;</ref><ref type="bibr" target="#b2">Bengio &amp; Bengio, 2000)</ref>. The neural autoregressive density estimator (NADE) <ref type="bibr" target="#b8">(Larochelle &amp; Murray, 2011)</ref>, inspired by a mean-field approximation to the conditionals of Equation (1) of a restricted Boltzmann machine (RBM), uses a set of one-hidden-layer neural networks with tied parameters to calculate each conditional:</p><formula xml:id="formula_1">p(x o d = 1 | x o &lt;d ) = sigm(V ·,o d h d + b o d ) (2) h d = sigm(W ·,o &lt;d x o &lt;d + c), (3)</formula><p>where H is the number of hidden units, and</p><formula xml:id="formula_2">V ∈ R H×D , b ∈ R D , W ∈ R H×D , c ∈ R H are the parameters of the NADE model.</formula><p>A NADE can be trained by regularized gradient descent on the negative log-likelihood given the training dataset X.</p><p>In NADE the activation of the hidden units in (3) can be computed recursively:</p><formula xml:id="formula_3">h d = sigm(a d ) where a 1 = c (4) a d+1 = a d + x o d W ·,o d .<label>(5)</label></formula><p>This relationship between activations allows faster training and evaluation of a NADE model, O(DH), than autoregressive models based on untied neural networks, O(D 2 H).</p><p>NADE has recently been extended to allow density estimation of real-valued vectors <ref type="bibr" target="#b15">(Uria et al., 2013)</ref> by using mixture density networks or MDNs <ref type="bibr" target="#b3">(Bishop, 1994)</ref> for each of the conditionals in Equation (1). The networks' hidden layers use the same parameter sharing as before, with activations computed as in (5). NADE and RNADE have been shown to offer better modelling performance than mixture models and untied neural networks in a range of datasets. Compared to binary RBMs with hundreds of hidden units, NADEs usually have slightly worse modelling performance, but they have three desirable properties that the former lack: 1) an easy training procedure by gradient descent on the negative likelihood of a training dataset, 2) a tractable expression for the density of a datapoint, 3) a direct ancestral sampling procedure, rather than requiring Markov chain Monte Carlo methods.</p><p>Inference under a NADE is easy as long as the variables to condition on are at the beginning of its ordering, and the ones to marginalise over are at the end. To infer the density of x oa...o b while conditioning on x o1...oa−1 , and marginalising over x o b+1...D , we simply write</p><formula xml:id="formula_4">p(x o a...b | x o1...a−1 ) = b d=a p(x o d | x o &lt;d ),<label>(6)</label></formula><p>where each one-dimensional conditional is directly available from the model. However, as in most models, arbitrary probabilistic queries require approximate inference methods.</p><p>A disadvantage of NADE compared to other neural network models is that an efficient deep formulation (e.g., <ref type="bibr" target="#b1">Bengio, 2009</ref>) is not available. While extending NADE's definition to multiple hidden layers is trivial (we simply introduce regular feed-forward layers between the computation of Equation 3 and of Equation 2), we lack a recursive expression like Equations 4 and 5 for the added layers. Thus, when NADE has more than one hidden layer, each additional hidden layer must be computed separately for each input dimension, yielding a complexity cubic on the size of the layers O(DH 2 L), where L represents the number of layers. This scaling seemingly made a deep NADE impractical, except for datasets of low dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training a factorial number of NADEs</head><p>Looking at the simplicity of inference in Equation <ref type="formula" target="#formula_4">(6)</ref>, a naive approach that could exploit this property for any inference task would be to train as many NADE models as there are possible orderings of the input variables. Obviously, this approach, requiring O(D!) time and memory, is not viable. However, we show here that through some careful parameter tying between models, we can derive an efficient stochastic procedure for training all models, minimizing the mean of their negative log-likelihood objectives.</p><p>Consider for now a parameter tying strategy that simply uses the same weight matrices and bias parameters across all NADE models (we will refine this proposal later). We will now write p(x | θ, o) as the joint distribution of the NADE model that uses ordering o and p(x</p><formula xml:id="formula_5">(n) o d | x (n) o &lt;d , θ, o &lt;d , o d )</formula><p>as its associated conditionals, which are computed as specified in Equations (2) and (3), or their straightforward extension in the deep network case. Thus we explicitly treat the ordering o as a random variable. Notice that the d th conditional only depends on the first d elements of the ordering, and is thus exactly the same across NADE models sharing their first d elements in o. During training we will attempt to minimise the expected (over variable orderings) negative log-likelihood of the model for the training data:</p><formula xml:id="formula_6">J OA (θ) = E o∈D! − log p(X | θ, o) (7) ∝ E o∈D! E x (n) ∈X − log p(x (n) | θ, o),<label>(8)</label></formula><p>where D! is the set of all orderings (i.e. permutations of D elements). This objective does not correspond to a mixture model, in which case the expectation over orderings would be inside the log operation.</p><p>Using NADE's autoregressive expression for the density of a datapoint, (8) can be rewritten as:</p><formula xml:id="formula_7">J OA (θ) = E o∈D! E x (n) ∈X D d=1 − log p(x (n) o d | x (n) o &lt;d , θ, o).</formula><p>(9) Where d indexes the elements in the order, o, of the dimensions. By moving the expectation over orders inside the sum over the elements of the order, the order can be split in three parts: o &lt;d standing for the index of the d−1 first dimensions in the ordering; o d the index of the d-th dimension in the ordering, and o &gt;d standing for the indices of the remaining dimensions in the ordering. Therefore, the loss function can be rewritten as:</p><formula xml:id="formula_8">J OA (θ) = E x (n) ∈X D d=1 E o &lt;d E o d E o &gt;d − log p(x (n) o d | x (n) o &lt;d , θ, o &lt;d , o d )<label>(10)</label></formula><p>the value of each term does not depend on o &gt;d . Therefore, it can be simplified as:</p><formula xml:id="formula_9">J OA (θ) = E x (n) ∈X D d=1 E o &lt;d E o d − log p(x (n) o d | x (n) o &lt;d , θ, o &lt;d , o d )</formula><p>(11) In practice, this loss function (11) will have a very high number of terms and will have to be approximated by sampling x (n) , d, and o &lt;d . The innermost expectation over values of o d can be calculated cheaply for a NADE given that the hidden unit states h d are shared for all possible o d . Therefore, assuming all orderings are equally probable, we will estimate J OA (θ) by:</p><formula xml:id="formula_10">J OA (θ) = D D − d + 1 o d − log p(x (n) o d | x (n) o &lt;d , θ, o &lt;d , o d )<label>(12)</label></formula><p>which provides an unbiased estimator of (8). Thus training can be done by descent on the stochastic gradient of J OA (θ).</p><p>An implementation of this order-agnostic training procedure corresponds to an artificial neural network with D inputs and D outputs (or an MDN in the real-valued case), where the input values in o ≥d have been set to zero and gradients are backpropagated only from the outputs in o ≥d , and rescaled by D D−d+1 . The end result is a stochastic training update costing O(DH + H 2 L), as in regular multilayer neural networks. At test time, we unfortunately cannot avoid a complexity of O(DH 2 L) and perform D passes through the neural network to obtain all D conditionals for some given ordering. However, this is still tractable, unlike computing probabilities in a restricted Boltzmann machine or a deep belief network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improved parameter sharing using input masks</head><p>While the parameter tying proposed so far is simple, in practice it leads to poor performance. One issue is that the values of the hidden units, computed using (3), are the same when a dimension is in x o &gt;d (a value to be predicted) and when the value of that dimension is zero and conditioned on. When training just one NADE with a fixed o, each output unit knows which inputs feed into it, but in the multiple ordering case that information is lost when the input is zero.</p><p>In order to make this distinction possible, we augment the parameter sharing scheme by appending to the inputs a binary mask vector m o &lt;d ∈ {0, 1} D indicating which dimensions are present in the input. That is, the i-th element of m o &lt;d is 1 if i ∈ o &lt;d and 0 otherwise. One interpretation of this scheme is that the bias vector c of the first hidden layer is now dependent on the ordering o and the value of d, thus slightly relaxing the strength of parameter sharing between the NADE models. We've found in practice that this adjustment is crucial to obtain good estimation performance. Some results showing the difference in statistical performance with and without training masks can be seen in <ref type="table" target="#tab_1">Table 2</ref> as part of our experimental analysis (see Section 6 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">On the fly generation of NADE ensembles</head><p>Our order-agnostic training procedure can be thought of as producing a set of parameters that can be used by a factorial number of NADEs, one per ordering of the input variables. These different NADEs will not, in general, agree on the probability of a given datapoint. While this disagreement might look unappealing at first, we can actually use this source of variability to our advantage, and obtain better estimates than possible with a set of consistent models.</p><p>A NADE with a given input ordering corresponds to a different hypothesis space than other NADEs with different ordering. In other words, each NADE with a different ordering is a model in its own right, with slightly different inductive bias, despite the parameter sharing.</p><p>A reliable approach to improve on some given estimator is to instead construct an ensemble of multiple, strong but different estimators, e.g. with bagging <ref type="bibr" target="#b11">(Ormoneit &amp; Tresp, 1996)</ref> or stacking <ref type="bibr" target="#b13">(Smyth &amp; Wolpert, 1999)</ref>. Our training procedure suggest a straightforward way of generating ensembles of NADE models: generate a set of uniformly distributed orderings {o (k) } K k=1 over the input variables and use the average probability 1 K K k=1 p(x|θ, o (k) ) as our estimator. Ensemble averaging increases the computational cost of density estimation linearly with the size of the ensemble, while the complexity of sampling doesn't change (we pick an ordering o (k) at random from the ensemble and sample from the corresponding NADE). Importantly, the computational cost of training remains the same, unlike ensemble methods such as bagging. Moreover, an adequate number of components can be chosen after training, and can even be adapted to the available computational budget on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>As mentioned previously, autoregressive density/distribution estimation has been explored before by others. For the binary data case, <ref type="bibr" target="#b5">Frey (1998)</ref> considered the use of logistic regression conditional models, while <ref type="bibr" target="#b2">Bengio &amp; Bengio (2000)</ref> proposed a single layer neural network architecture, with a parameter sharing scheme different from the one in the NADE model <ref type="bibr" target="#b8">(Larochelle &amp; Murray, 2011)</ref>. In all these cases however, a single (usually random) input ordering was chosen and maintained during training. <ref type="bibr" target="#b7">Gregor &amp; LeCun (2011)</ref> proposed training a variant of the NADE architecture under stochastically generated random orderings. Like us, they observed much worse performance than when choosing a single variable ordering, which motivates our proposed parameter sharing scheme relying on input masks. Gregor &amp; LeCun generated a single ordering for each training update, and conditioned on contexts of all possible sizes to compute the log-probability of an example and its gradients. Our stochastic approach uses only a single conditioning configuration for each update, but computes the average log-probability for the next dimension under all possible future orderings. This change allowed us to generalize NADE to deep architectures with an acceptable computational cost. <ref type="bibr" target="#b6">Goodfellow et al. (2013)</ref> introduced a procedure to train deep Boltzmann machines by maximizing a variational approximation of their generalised pseudo likelihood. This results in a training procedure similar to the one presented in this work, where a subset of the dimension is predicted given the value of the rest.</p><p>Our algorithm also bears similarity with denoising autoencoders <ref type="bibr" target="#b16">(Vincent et al., 2008)</ref> trained using so-called "masking noise". There are two crucial differences however. The first is that our procedure corresponds to training on the average reconstruction of only the inputs that are missing from the input layer. The second is that, unlike denoising autoencoders, the NADE models that we train can be used as tractable density estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>We performed experiments on several binary and real-valued datasets to asses the performance of NADEs trained using our order-agnostic procedure. We report the average test log-likelihood of each model, that is, the average log-density of datapoints in a held-out test set. In the case of NADEs trained in an order-agnostic way, we need to choose an ordering of the variables so that one may calculate the density of the test datapoints. We report the average of the average test log-likelihoods using ten different orderings chosen at random. Note that this is different from an ensemble, where the probabilities are averaged before calculating its logarithm. To reduce clutter, we have not reported the standard deviation across orderings. In all cases, this standard deviation has magnitude smaller than the log-likelihood's standard error due to the finite size of our test sets. These standard errors are also small enough not to alter the ranking of the different models. In the case of ensembles of NADEs the standard deviation due to different sets of orderings is, as expected, even smaller. Every results table is partitioned in two halves, the top half contains baselines and the bottom half results obtained using our training procedure. In every table the log-likelihood of the best model, and the log-likelihood of the best ensemble are shown in bold.</p><p>Training configuration details common to all datasets (except where specified later on) follow. We trained all orderagnostic NADEs and RNADEs using minibatch stochastic gradient descent on J OA , (11). The initial learning rate, which was chosen independently for each dataset, was reduced linearly to reach zero after the last iteration. For the purpose of consistency, we used rectified linear units <ref type="bibr" target="#b10">(Nair &amp; Hinton, 2010)</ref> in all experiments. We found that this type of unit allow us to use higher learning rates and made training converge faster. We used Nesterov's accelerated gradient <ref type="bibr" target="#b14">(Sutskever, 2013)</ref> with momentum value 0.9. No weight decay was applied. To avoid overfitting, we earlystopped training by estimating the log-likelihood on a validation dataset after each training iteration using the J OA estimator, (12). For models with several hidden layers, each hidden layer was pretrained using the same hyperparameter values but only for 20 iterations, see recursive procedure in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Binary datasets</head><p>We start by measuring the statistical performance of a NADE trained using our order-agnostic procedure on eight binary UCI datasets <ref type="bibr" target="#b0">(Bache &amp; Lichman, 2013)</ref>. if n = 1 then end if 12: end procedure Experimental configuration details follow. We fixed the number of units per hidden layer to 500, following <ref type="bibr" target="#b8">Larochelle &amp; Murray (2011)</ref>. We used minibatches of size 100. Training was run for 100 iterations, each consisting of 1000 weight updates. The initial learning rate was cross-validated for each of the datasets among values {0.016, 0.004, 0.001, 0.00025, 0.0000675}. <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results are shown on</head><p>We compare our method to mixtures of multivariate Bernoullis with their number of components cross-validated among {32, 64, 128, 256, 512, 1024}, tractable RBMs of 23 hidden units, fully visible sigmoidal Bayes networks (FVSBN), and NADEs trained using a fixed ordering of the variables. All baseline results are taken from <ref type="bibr" target="#b8">Larochelle &amp; Murray (2011)</ref> and details can be found there. NADEs trained in an order-agnostic manner obtain performances close to those of NADEs trained on a fixed ordering. The use of several hidden layers offers no advantage on these datasets. However, ensembles of NADEs obtain higher test log-likelihoods on all datasets.</p><p>We also present results on binarized-MNIST <ref type="bibr" target="#b12">(Salakhutdinov &amp; Murray, 2008)</ref>, a binary dataset of 28 by 28 pixel images of handwritten digits. Unlike classification, density estimation on this dataset remains a challenging task.</p><p>Experimental configuration details follow. Training was run for 200 iterations each consisting of 1000 parameter updates, using minibatches of size 1000. The initial learning rate was set to 0.001 and chosen manually by optimizing the validation-set log-likelihood on preliminary runs.</p><p>Results for MNIST are shown in <ref type="table" target="#tab_1">Table 2</ref>. We compare our method with mixtures of multivariate Bernoulli distributions with 10 and 500 components, fixed-ordering NADEs, RBMs (500 hidden units), and two-hidden-layer DBNs (500 and 2000 hidden units on each layer) whose performance was estimated by <ref type="bibr" target="#b12">Salakhutdinov &amp; Murray (2008)</ref>; <ref type="bibr" target="#b9">Murray &amp; Salakhutdinov (2009)</ref>. In order to provide a more direct comparison to our results, we also report the performance of NADEs trained using a fixed variable-ordering, minibatch stochastic gradient descent and sigmoid or rectified linear units. We found the type of hidden-unit did not affect statistical performance, while our minibatch SGD implementation seems to obtain slightly higher log-likelihoods than previously reported.</p><p>One and two hidden-layer NADEs trained by minimizing J OA obtain marginally lower (worse) test-likelihoods than a NADE trained for a fixed ordering of the inputs, but still perform much better than mixtures of multivariate Bernoullis and very close to the estimated performance of RBMs. More than two hidden layers are not beneficial on this dataset. Ensembles of NADEs obtained by using NADEs with different variable orderings but trained simultaneously with our order-agnostic procedure obtain better statistical performance than NADEs trained using a fixed ordering. These EoNADEs can also surpass the estimated performance of RBMs with the same number of hidden units, and even approach the estimated performance of a (larger) 2-hiddenlayer deep belief network. A more detailed account of the statistical performance of EoNADEs can be seen in <ref type="figure">Figure 1.</ref> We also report the performance on NADE trained by  <ref type="figure">Figure 1</ref>. Test-set average log-likelihood per datapoint for RNADEs trained with our new procedure on binarized images of digits. minimizing J OA but without input masks. Input masks are necessary for obtaining competitive results.</p><p>Samples from a 2 hidden layer (500 hidden units per layer) NADE trained using the order-agnostic method are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Most of the samples can be identified as digits. <ref type="figure" target="#fig_3">Figure 4</ref> shows some receptive fields from the model's first hidden layer (i.e. columns of W ). Most of the receptive fields resemble pen strokes. We also show their associated receptive fields on the input masks . These can be thought of as biases that activate or deactivate a hidden unit. Most of them will activate the unit when the input mask contains a Having at our disposal a NADE for each possible ordering of the inputs makes it easy to perform any inference task. In <ref type="figure" target="#fig_2">Figure 3</ref> we show examples of marginalization and imputation tasks. Arbitrarily chosen regions of digits in the MNIST test-set are to be marginalized or sampled from. An RBM or a DBN would require an exponential number of operations to calculate either the marginal density or the density of the complete images. A NADE trained on a fixed ordering of the variables would be able to easily calculate the densities of the complete images, but would require approximate inference to calculate the marginal densities. Both an RBM and a fixed-order NADE require MCMC methods in order to sample the hollowed regions. However, with our order-agnostic training procedure we can easily calculate the marginal densities and sample the hollowed regions in constant time just by constructing a NADE with a convenient ordering of the pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Real-valued datasets</head><p>We also compared the performance of RNADEs trained with our order-agnostic procedure to RNADEs trained for a fixed ordering. We start by comparing the performance on three low-dimensional UCI datasets <ref type="bibr" target="#b0">(Bache &amp; Lichman, 2013)</ref> of heterogeneous data, namely: red wine, white wine and parkinsons. We dropped the other two datasets tested -61. <ref type="bibr">21 -36.33 -84.40 -46.22 -96.68 -66.26 -86.37 -73.31 -93.35 -79.40 -45.84 -41.88</ref>   by <ref type="bibr" target="#b15">Uria et al. (2013)</ref>, because some of their dimensions only take a finite number of values even if those are realvalued. We report the test-log-likelihood on 10 folds of the dataset, each with 90% of the data used for training and 10% for testing. All experiments use normalized data. Each dimension is normalized separately by subtracting its training-set average and dividing by its standard deviation.</p><p>Experimental details follow. Learning rate and weight decay rates were chosen by per-fold cross-validation; using grid search. One ninth of the training set examples were used for validation purposes. Once the hyperparameter values had been chosen, a final experiment was run using all the training data. In order to prevent overfitting, training was stopped when observing a training likelihood higher than the one obtained at the optimal stopping point in the corresponding validation run. All RNADEs trained had a mixture of 20 Gaussian components for output, and were trained by stochastic gradient descent on J OA . We fixed the number of hidden units to 50, following <ref type="bibr" target="#b15">Uria et al. (2013)</ref>. The learning rate was chosen among {0.02, 0.005, 0.002, 0.0005} and the weight decay rate among {0.02, 0.002, 0}.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. RNADEs trained using our procedure obtain results close to those of RNADEs trained for a fixed ordering on the red wine and white wine datasets. On the Parkinsons dataset, RNADEs trained for a fixed ordering perform better. Ensembles of RNADEs obtained better statistical performance on the three datasets.</p><p>We also measured the performance of our new training procedure on 8 by 8 patches of natural images in the BSDS300 dataset. We compare the performance of RNADEs with different number of hidden layers trained with our procedure against a one-hidden layer RNADE trained for a fixed ordering <ref type="bibr" target="#b15">(Uria et al., 2013)</ref>, and with mixtures of Gaussians, which remain the state of the art in this problem <ref type="bibr" target="#b17">(Zoran &amp; Weiss, 2012)</ref>. We adopted the setup described by <ref type="bibr" target="#b15">Uria et al. (2013)</ref>. The average intensity of each patch was subtracted from each pixel's value. After this, all datapoints lay on a 63dimensional subspace, for this reason only 63 pixels were modelled, discarding the bottom-right pixel.</p><p>Experimental details follow. The dataset's 200 training image set was partitioned into a training set and a validation set of 180 and 20 images respectively. Hyperparameters were chosen by preliminary manual search on the model likelihood for the validation dataset. We used a mixture of 10 Gaussian components for the output distribution of each pixel. All hidden layers were fixed to a size of 1000 units. The minibatch size was set to 1000. Training was run for 2000 iterations, each consisting of 1000 weight updates. The initial learning rate was set to 0.001. Pretraining of hidden layers was done for 50 iterations.</p><p>The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. RNADEs with less than 3 hidden layers trained using our order-agnostic procedure obtained lower statistical performance than a fixed-ordering NADE and a mixture of Gaussians. However RNADEs with more than 3 layers are able to beat both baselines and obtain what are, to the extent of our knowledge, the best results  157.0 ever reported on this task. Ensembles of RNADEs also show an improvement in statistical performance compared to the use of single RNADEs.</p><p>No signs of overfitting were observed. Even when using 6 hidden layers, the cost on the validation dataset never started increasing steadily during training. Therefore it may be possible to obtain even better results using more hidden layers or more hidden units per layer. Samples from the 6 hidden layers NADE trained in an order-agnostic manner are shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have introduced a new training procedure that simultaneously fits a NADE for each possible ordering of the dimensions. In addition, this new training procedure is able to train deep versions of NADE with a linear increase in computation, and construct ensembles of NADEs on the fly without incurring any extra training computational cost. NADEs trained with our procedure outperform mixture models in all datasets we have investigated. However, for most datasets several hidden layers are required to surpass or equal the performance of NADEs trained for a fixed ordering of the variables. Nonetheless, our method allows fast and exact marginalization and sampling, unlike the rest of the methods compared.</p><p>Models trained using our order-agnostic procedure obtained what are, to the best of our knowledge, the best statistical performances ever reported on the BSDS300 8×8-imagepatches datasets. The use of ensembles of NADEs, which we can obtain at no extra training cost and have a mild effect on test-time cost, improved statistical performance on most datasets analyzed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Top: 50 examples from binarized-MNIST ordered by decreasing likelihood under a 2-hidden-layer NADE. Bottom: 50 samples from a 2-hidden-layer NADE, also ordered by decreasing likelihood under the model. region of unknown values (zeros in the input mask) flanked by a region of known values (ones in the input mask).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example of marginalization and sampling. First column shows five examples from the test set of the MNIST dataset. The second column shows the density of these examples when a random 10 by 10 pixel region is marginalized. The right-most five columns show samples for the hollowed region. Both tasks can be done easily with a NADE where the pixels to marginalize are at the end of the ordering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Top:50 receptive fields (columns of W ) with the biggest L2 norm. Bottom: Associated receptive fields to the input masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Top: 50 examples of 8 × 8 patches in the BSDS300 dataset ordered by decreasing likelihood under a 6-hidden-layer NADE. Bottom: 50 samples from a 6-hidden-layer NADE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average test-set log-likelihood per datapoint (in nats) of different models on eight binary datasets from the UCI repository. Baseline results were taken from<ref type="bibr" target="#b8">Larochelle &amp; Murray (2011)</ref>.</figDesc><table><row><cell>Model</cell><cell>Adult</cell><cell>Connect4</cell><cell>DNA</cell><cell cols="3">Mushrooms NIPS-0-12 Ocr-letters</cell><cell>RCV1</cell><cell>Web</cell></row><row><cell cols="2">MoBernoullis RBM FVSBN NADE (fixed order) NADE 1hl NADE 2hl NADE 3hl EoNADE 1hl (2 ord) −13.35 −20.44 −16.26 −13.17 −13.19 −13.51 −13.53 −13.54 EoNADE 1hl (16 ord) −13.19</cell><cell>−23.41 −22.66 −12.39 −11.99 −13.04 −12.99 −13.08 −12.81 −12.58</cell><cell>−98.19 −96.74 −83.64 −84.81 −84.28 −84.30 −84.37 −83.52 −82.31</cell><cell>−14.46 −15.15 −10.27 −9.81 −10.06 −10.05 −10.10 −9.88 −9.68</cell><cell>−290.02 −277.37 −276.88 −273.08 −275.20 −274.69 −274.86 −274.12 −272.38</cell><cell>−40.56 −43.05 −39.30 −27.22 −29.05 −28.92 −28.89 −28.36 −27.31</cell><cell>−47.59 −48.88 −49.84 −46.66 −28.39 −30.16 −29.38 −29.35 −46.79 −28.30 −46.71 −28.28 −46.76 −28.29 −46.50 −28.11 −46.12 −27.87</cell></row><row><cell cols="5">Algorithm 1 Pretraining of a NADE with n hidden layers</cell><cell></cell><cell></cell></row><row><cell>on dataset X.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1: procedure PRETRAIN(n, X)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average test-set log-likelihood per datapoint of different models on 28×28 binarized images of digits taken from MNIST.</figDesc><table><row><cell cols="3">Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test LogL</cell></row><row><cell cols="7">MoBernoullis K=10 MoBernoullis K=500 RBM (500 h, 25 CD steps) approx. DBN 2hl approx. NADE 1hl (fixed order) NADE 1hl (fixed order, RLU, minibatch) NADE 1hl (fixed order, sigm, minibatch) NADE 1hl (no input masks) NADE 2hl (no input masks) NADE 1hl NADE 2hl NADE 3hl NADE 4hl EoNADE 1hl (2 orderings) EoNADE 1hl (128 orderings) EoNADE 2hl (2 orderings) EoNADE 2hl (128 orderings)</cell><cell>−168.95 −137.64 −86.34 −84.55 −88.86 −88.33 −88.35 −99.37 −95.33 −92.17 −89.17 −89.38 −89.60 −90.69 −87.71 −87.96 −85.10</cell></row><row><cell></cell><cell>−84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>−85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>−86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test loglikelihood (nats)</cell><cell>−90 −89 −88 −87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2hl-DBN</cell></row><row><cell></cell><cell>−91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RBM NADE (fixed order)</cell></row><row><cell></cell><cell>−92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1hl-NADE 2hl-NADE</cell></row><row><cell></cell><cell>−93</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8 Models averaged 16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average test log-likelihood for different models on three real-valued UCI datasets. Baselines are taken from<ref type="bibr" target="#b15">(Uria et al., 2013)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="3">Red wine White wine Parkinsons</cell></row><row><cell>Gaussian MFA RNADE (fixed) RNADE 1hl RNADE 2hl RNADE 3hl RNADE 1hl 2 ord. RNADE 2hl 2 ord. RNADE 3hl 2 ord. RNADE 1hl 16 ord. RNADE 2hl 16 ord. RNADE 3hl 16 ord.</cell><cell>−13.18 −10.19 −9.36 −9.49 −9.63 −9.54 −9.07 −9.13 −8.93 −8.95 −8.98 −8.76</cell><cell>−13.20 −10.73 −10.23 −10.35 −10.23 −10.21 −10.03 −9.84 −9.79 −9.94 −9.69 −9.67</cell><cell>−10.85 −1.99 −0.90 −2.67 −2.19 −2.13 −1.97 −1.42 −1.39 −1.73 −1.16 −1.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Average test-set log-likelihood for several models trained on 8 by 8 pixel patches of natural images taken from the BSDS300 dataset. Note that because these are log probability densities they are positive, higher is better.</figDesc><table><row><cell>Model</cell><cell>Test LogL</cell></row><row><cell>MoG K = 200 (Zoran &amp; Weiss, 2012)</cell><cell>152.8</cell></row><row><cell>RNADE 1hl (fixed order)</cell><cell>152.1</cell></row><row><cell>RNADE 1hl</cell><cell>143.2</cell></row><row><cell>RNADE 2hl</cell><cell>149.2</cell></row><row><cell>RNADE 3hl</cell><cell>152.0</cell></row><row><cell>RNADE 4hl</cell><cell>153.6</cell></row><row><cell>RNADE 5hl</cell><cell>154.7</cell></row><row><cell>RNADE 6hl</cell><cell>155.2</cell></row><row><cell>EoRNADE 6hl 2 ord.</cell><cell>156.0</cell></row><row><cell>EoRNADE 6hl 32 ord.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank John Bridle and Steve Renals for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling high-dimensional discrete data with multi-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 12 (NIPS 1999)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="400" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Neural Computing Research Group, Aston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>ISBN 0387310738</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graphical models for machine learning and digital communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-prediction deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1108.1169</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Neural Autoregressive Distribution Estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR W&amp;CP</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating probabilities under high-dimensional latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS 2008)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved Gaussian mixture density estimates using Bayesian penalty terms and network averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8 (NIPS 1995)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="542" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linearly combining density estimators via stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="59" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RNADE: The real-valued neural autoregressive density-estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0186</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 26)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural images, Gaussian mixtures and dead leaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25 (NIPS 2012)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1745" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
