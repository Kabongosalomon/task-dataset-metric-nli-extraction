<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentive Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">NC State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">NC State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">NC State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attentive Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between the two schema and present Attentive Normalization (AN). Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weightedsum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging channelwise feature attention. In experiments, we test the proposed AN using four representative neural architectures in the ImageNet-1000 classification benchmark and the MS-COCO 2017 object detection and instance segmentation benchmark. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5% and 2.7%, and absolute increase up to 1.8% and 2.2% for bounding box and mask AP in MS-COCO respectively. We observe that the proposed AN provides a strong alternative to the widely used Squeeze-and-Excitation (SE) module. The source codes are publicly available at the ImageNet Classification Repo and the MS-COCO Detection and Segmentation Repo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pioneered by Batch Normalization (BN) <ref type="bibr" target="#b18">[19]</ref>, feature normalization has become ubiquitous in the development of deep learning. Feature normalization consists of two components: feature standardization and channel-wise affine transformation. The latter is introduced to provide the capability of undoing the standardization (by design), and can be treated as feature re-calibration in general. Many variants of BN have been proposed for practical deployment in terms of variations of training and testing settings with remarkable progress obtained. They can be roughly divided into two categories: i) Generalizing feature standardization. Different methods are proposed for computing the mean and standard deviation or for modeling/whitening the data distribution in general, within a min-batch. They include Batch Renormalization <ref type="bibr" target="#b17">[18]</ref>, Decorrelated BN <ref type="bibr" target="#b15">[16]</ref>, Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref>, Instance Normalization (IN) <ref type="bibr" target="#b41">[42]</ref>, Instance-level Meta Normalization <ref type="bibr" target="#b19">[20]</ref>, Group Normalization (GN) <ref type="bibr" target="#b46">[47]</ref>, Mixture Normalization <ref type="bibr" target="#b20">[21]</ref> and Mode Normalization <ref type="bibr" target="#b4">[5]</ref>. Switchable Normalization (SN) <ref type="bibr" target="#b27">[28]</ref> and its sparse variant (SSN) <ref type="bibr" target="#b38">[39]</ref> learn to switch between different vanilla schema. These methods adopt the vanilla channel-wise affine transformation after standardization, and are often proposed for discriminative learning tasks. ii) Generalizing feature re-calibration. Instead of treating the affine transformation parameters directly as model parameters, different types of task-induced conditions (e.g., class labels in conditional image synthesis using generative adversarial networks) are leveraged and encoded as latent vectors, which are then used to learn the affine transformation parameters, including different conditional BNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2]</ref>, style-adaptive IN <ref type="bibr" target="#b21">[22]</ref> or layout-adaptive IN <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref>. These methods have been mainly proposed in generative learning tasks, except for the recently proposed Instance-level Meta Normalization <ref type="bibr" target="#b19">[20]</ref> in discriminative learning tasks.</p><p>In the meanwhile, feature attention has also become an indispensable mechanism for improving task performance in deep learning. For computer vision, spatial attention is inherently captured by convolution operations within short-range context, and by non-local extensions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b16">17]</ref> for long-range context. Channel-wise attention is relatively less exploited. The squeeze-and-excitation (SE) unit <ref type="bibr" target="#b12">[13]</ref> is one of the most popular designs, which learn instance-specific channel-wise attention weights to re-calibrate an input feature map. Unlike the affine transformation parameters in feature normalization, the attention weights for re-calibrating an feature map are often directly learned from the input feature map in the spirit of self-attention, and often instance-specific or pixel-specific.</p><p>Although both feature normalization and feature attention have become ubiquitous in state-of-the-art DNNs, they are usually studied as separate mod-ules. Therefore, in this paper we address the following problem: How to learn to re-calibrate feature maps in a way of harnessing the best of feature normalization and feature attention in a single light-weight module? And, we present Attentive Normalization (AN): <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the proposed AN. The basic idea is straightforward. Conceptually, the affine transformation component in feature normalization (Section 3.1) and the re-scaling computation in feature attention play the same role in learning-to-re-calibrate an input feature map, thus providing the foundation for integration (Section 3.2). More specifically, consider a feature normalization backbone such as BN or GN, our proposed AN keeps the block-wise standardization component unchanged. Unlike the vanilla feature normalization in which the affine transformation parameters (γ's and β's) are often frozen in testing, we want the affine transformation parameters to be adaptive and dynamic in both training and testing, controlled directly by the input feature map. The intuition behind doing so is that it will be more flexible in accounting for different statistical discrepancies between training and testing in general, and between different sub-populations caused by underlying inter-/intra-class variations in the data.</p><p>To achieve the dynamic and adaptive control of affine transformation parameters, the proposed AN utilizes a simple design (Section 3). It learns a mixture of K affine transformations and exploits feature attention mechanism to learn the instance-specific weights for the K components. The final affine transformation used to re-calibrate an input feature map is the weighted sum of the learned K affine transformations. We propose a general formulation for the proposed AN and study how to learn the weights in an efficient and effective way (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature Normalization. There are two types of normalization schema, feature normalization (including raw data) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref> and weight normalization <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15]</ref>. Unlike the former, the latter is to normalize model parameters to decouple the magnitudes of parameter vectors from their directions. We focus on feature normalization in this paper.</p><p>Different feature normalization schema differ in how the mean and variance are computed. BN <ref type="bibr" target="#b18">[19]</ref> computes the channel-wise mean and variance in the entire min-batch which is driven by improving training efficiency and model generalizability. BN has been deeply analyzed in terms of how it helps optimization <ref type="bibr" target="#b37">[38]</ref>. DecorBN <ref type="bibr" target="#b15">[16]</ref> utilizes a whitening operation (ZCA) to go beyond the centering and scaling in the vanilla BN. BatchReNorm <ref type="bibr" target="#b17">[18]</ref> introduces extra parameters to control the pooled mean and variance to reduce BN's dependency on the batch size. IN <ref type="bibr" target="#b41">[42]</ref> focuses on channel-wise and instance-specific statistics which stems from the task of artistic image style transfer. LN <ref type="bibr" target="#b0">[1]</ref> computes the instance-specific mean and variance from all channels which is designed to help optimization in recurrent neural networks (RNNs). GN <ref type="bibr" target="#b46">[47]</ref> stands in the sweet spot between LN and IN focusing on instance-specific and channel-groupwise statistics, especially when only small batches are applicable in practice. In practice, synchronized BN <ref type="bibr" target="#b31">[32]</ref> across multiple GPUs becomes increasingly favorable against GN in some applications. SN <ref type="bibr" target="#b27">[28]</ref> leaves the design choices of fea-ture normalization schema to the learning system itself by computing weighted sum integration of BN, LN, IN and/or GN via softmax, showing more flexible applicability, followed by SSN <ref type="bibr" target="#b38">[39]</ref> which learns to make exclusive selection. Instead of computing one mode (mean and variance), MixtureNorm <ref type="bibr" target="#b20">[21]</ref> introduces a mixture of Gaussian densities to approximate the data distribution in a mini-batch. ModeNorm <ref type="bibr" target="#b4">[5]</ref> utilizes a general form of multiple-mode computation. Unlike those methods, the proposed AN focuses on generalizing the affine transformation component. Related to our work, Instance-level Meta normalization(ILM) <ref type="bibr" target="#b19">[20]</ref> first utilizes an encoder-decoder sub-network to learn affine transformation parameters and then add them together to the model's affine transformation parameters. Unlike ILM, the proposed AN utilizes a mixture of affine transformations and leverages feature attention to learn the instancespecific attention weights.</p><p>On the other hand, conditional feature normalization schema <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31</ref>] <ref type="bibr" target="#b39">[40]</ref> have been developed and shown remarkable progress in conditional and unconditional image synthesis. Conditional BN learns condition-specific affine transformations in terms of conditions such as class labels, image style, label maps and geometric layouts. Unlike those methods, the proposed AN learns selfattention data-driven weights for mixture components of affine transformations.</p><p>Feature Attention. Similar to feature normalization, feature attention is also an important building block in the development of deep learning. Residual Attention Network <ref type="bibr" target="#b43">[44]</ref> uses a trunk-and-mask joint spatial and channel attention module in an encoder-decoder style for improving performance. To reduce the computational cost, channel and spatial attention are separately applied in <ref type="bibr" target="#b45">[46]</ref>. The SE module <ref type="bibr" target="#b12">[13]</ref> further simplifies the attention mechanism by developing a light-weight channel-wise attention method. The proposed AN leverages the idea of SE in learning attention weights, but formulates the idea in a novel way.</p><p>Our Contributions. This paper makes three main contributions: (i) It presents Attentive Normalization which harnesses the best of feature normalization and feature attention (channel-wise). To our knowledge, AN is the first work that studies self-attention based conditional and adaptive feature normalization in visual recognition tasks. (ii) It presents a lightweight integration method for deploying AN in different widely used building blocks of ResNets, DenseNets, MobileNetsV2 and AOGNets. (iii) It obtains consistently better results than the vanilla feature normalization backbones by a large margin across different neural architectures in two large-scale benchmarks, ImageNet-1000 and MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Attentive Normalization</head><p>In this section, we present details of the proposed attentive normalization. Consider a DNN for 2D images, denote by x a feature map with axes in the convention order of (N, C, H, W ) (i.e., batch, channel, height and width). x is represented by a 4D tensor. Let i = (i N , i C , i H , i W ) be the address index in the 4D tensor. x i represents the feature response at a position i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background on Feature Normalization</head><p>Existing feature normalization schema often consist of two components ( <ref type="figure" target="#fig_0">Fig. 1):</ref> i) Block-wise Standardization. Denote by B j a block (slice) in a given 4-D tensor x. For example, for BN, we have j = 1, · · · , C and B j = {x i |∀i, i C = j}. We first compute the empirical mean and standard deviation in B j , denoted by µ j and σ j respectively:</p><formula xml:id="formula_0">µ j = 1 M x∈Bj x, σ j = 1 M x∈Bj (x − µ j ) 2 + ,</formula><p>where M = |B j | and is a small positive constant to ensure σ j &gt; 0 for the sake of numeric stability. Then, let j i be the index of the block that the position i belongs to, and we standardize the feature response by,</p><formula xml:id="formula_1">x i = 1 σ ji (x i − µ ji )<label>(1)</label></formula><p>ii) Channel-wise Affine Transformation. Denote by γ c and β c the scalar coefficient (re-scaling) and offset (re-shifting) parameter respectively for the c-th channel. The re-calibrated feature response at a position i is then computed by,</p><formula xml:id="formula_2">x i = γ i C ·x i + β i C ,<label>(2)</label></formula><p>where γ c 's and β c 's are shared by all the instances in a min-batch across the spatial domain. They are usually frozen in testing and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Background on Feature Attention</head><p>We focus on channel-wise attention and briefly review the Squeeze-Excitation (SE) module <ref type="bibr" target="#b12">[13]</ref>. SE usually takes the feature normalization result (Eqn. 2) as its input (the bottom-right of <ref type="figure" target="#fig_0">Fig. 1</ref>), and learns channel-wise attention weights:</p><p>i) The squeeze module encodes the inter-dependencies between feature channels in a low dimensional latent space with the reduction rate r (e.g., r = 16),</p><formula xml:id="formula_3">S(x; θ S ) = v, v ∈ R N × C r ×1×1 ,<label>(3)</label></formula><p>which is implemented by a sub-network consisting of a global average pooling layer (AvgPool), a fully-connected (FC) layer and rectified linear unit (ReLU) <ref type="bibr" target="#b22">[23]</ref>. θ S collects all the model parameters.</p><p>ii) The excitation module computes the channel-wise attention weights, denoted by λ, by decoding the learned latent representations v,</p><formula xml:id="formula_4">E(v; θ E ) = λ, λ ∈ R N ×C×1×1 ,<label>(4)</label></formula><p>which is implemented by a sub-network consisting of a FC layer and a sigmoid layer. θ E collects all model parameters. Then, the input,x is re-calibrated by,</p><formula xml:id="formula_5">x SE i = λ i N ,i C ·x i = (λ i N ,i C · γ i C ) ·x i + λ i N ,i C · β i C ,<label>(5)</label></formula><p>where the second step is obtained by plugging in Eqn. 2. It is thus straightforward to see the foundation facilitating the integration between feature normalization and channel-wise feature attention. However, the SE module often entails a significant number of extra parameters (e.g., ∼2.5M extra parameters for ResNet50 <ref type="bibr" target="#b9">[10]</ref> which originally consists of ∼25M parameters, resulting in 10% increase). We aim to design more parsimonious integration that can further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attentive Normalization</head><p>Our goal is to generalize Eqn. 2 in re-calibrating feature responses to enable dynamic and adaptive control in both training and testing. On the other hand, our goal is to simplify Eqn. 5 into a single light-weight module, rather than, for example, the two-module setup using BN+SE. In general, we have,</p><formula xml:id="formula_6">x AN i = Γ (x; θ Γ ) i ·x i + B(x; θ B ) i ,<label>(6)</label></formula><p>where both Γ (x; θ Γ ) and B(x; θ B ) are functions of the entire input feature map (without standardization 1 ) with parameters θ Γ and θ B respectively. They both compute 4D tensors of the size same as the input feature map and can be parameterized by some attention guided light-weight DNNs. The subscript in Γ (x; θ Γ ) i and B(x; θ B ) i represents the learned re-calibration weights at a position i.</p><p>In this paper, we focus on learning instance-specific channel-wise affine transformations. To that end, we have three components as follows.</p><p>i) Learning a Mixture of K Channel-wise Affine Transformations. Denote by γ k,c and β k,c the re-scaling and re-shifting (scalar) parameters respectively for the c-th channel in the k-th mixture component. They are model parameters learned end-to-end via back-propagation.</p><p>ii) Learning Attention Weights for the K Mixture Components. Denote by λ n,k the instance-specific mixture component weight (n ∈ [1, N ] and k ∈ [1, K]), and by λ the N × K weight matrix. λ is learned via some attention-guided function from the entire input feature map,</p><formula xml:id="formula_7">λ = A(x; θ λ ),<label>(7)</label></formula><p>where θ λ collects all the parameters. iii) Computing the Final Affine Transformation. With the learned γ k,c , β k,c and λ, the re-calibrated feature response is computed by,</p><formula xml:id="formula_8">x AN i = K k=1 λ i N ,k [γ k,i C ·x i + β k,i C ],<label>(8)</label></formula><p>where λ i N ,k is shared by the re-scaling parameter and the re-shifting parameter for simplicity. Since the attention weights λ are adaptive and dynamic in both training and testing, the proposed AN realizes adaptive and dynamic feature re-calibration. Compared to the general form (Eqn. 6), we have,</p><formula xml:id="formula_9">Γ (x) i = K k=1 λ i N ,k · γ k,i C , B(x) i = K k=1 λ i N ,k · β k,i C .<label>(9)</label></formula><p>Based on the formulation, there are a few advantages of the proposed AN in training, fine-tuning and testing a DNN:</p><p>-The channel-wise affine transformation parameters, γ k,i C 's and β k,i C 's, are shared across spatial dimensions and by data instances, which can learn population-level knowledge in a more fine-grained manner than a single affine transformation in the vanilla feature normalization. The latter is particularly useful for testing samples slightly "drifted" from training population, that is to improve generalizability. Their weighted sum encodes more direct and "actionable" information for re-calibrating standardized features (Eqn. 8) without being delayed until back-propagation updates as done in the vanilla feature normalization. -In fine-tuning, especially between different tasks (e.g., from image classification to object detection), γ k,i C 's and β k,i C 's are usually frozen as done in the vanilla feature normalization. They carry information from a source task. But, θ λ (Eqn. 7) are allowed to be fine-tuned, thus potentially better realizing transfer learning for a target task. This is a desirable property since we can decouple training correlation between tasks. For example, when GN <ref type="bibr" target="#b46">[47]</ref> is applied in object detection in MS-COCO, it is fine-tuned from a feature backbone with GN trained in ImageNet, instead of the one with BN that usually has better performance in ImageNet. As we shall show in experiments, the proposed AN facilitates a smoother transition. We can use the proposed AN (with BN) as the normalization backbone in pre-training in ImageNet, and then use AN (with GN) as the normalization backbone for the head classifiers in MS-COCO with significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Learning Attention Weights</head><p>We present a simple method for computing the attention weights A(x; θ λ ) (Eqn. 7). Our goal is to learn a weight coefficient for each component from each individual instance in a mini-batch (i.e, a N × K matrix). The question of interest is how to characterize the underlying importance of a channel c from its realization across the spatial dimensions (H, W ) in an instance, such that we will learn a more informative instancespecific weight coefficient for a channel c in re-calibrating the feature map x.</p><p>In realizing Eqn. 7, the proposed method is similar in spirit to the squeeze module in SENets <ref type="bibr" target="#b12">[13]</ref> to maintain light-weight implementation. To show the difference, let's first rewrite the vanilla squeeze module (Eqn. <ref type="bibr" target="#b2">3</ref></p><formula xml:id="formula_10">), v = S(x; θ S ) = ReLU (f c(AvgP ool(x); θ S )) ,<label>(10)</label></formula><p>where the mean of a channel c (via global average pooling, AvgP ool(·)) is used to characterize its underlying importance. We generalize this assumption by taking into account both mean and standard deviation empirically computed for a channel c, denoted by µ c and σ c respectively. More specifically, we compare three different designs using:</p><p>i) The mean µ c only as done in SENets.</p><p>ii) The concatenation of the mean and standard deviation, (µ c , σ c ).</p><p>iii) The coefficient of variation or the relative standard deviation (RSD), σc µc . RSD measures the dispersion of an underlying distribution (i.e., the extent to which the distribution is stretched or squeezed) which intuitively conveys more information in learning attention weights for re-calibration.</p><p>RSD is indeed observed to work better in our experiments 2 . Eqn. 7 is then expanded with two choices,</p><formula xml:id="formula_11">Choice 1: A 1 (x; θ λ ) = Act(f c(RSD(x); θ λ )),<label>(11)</label></formula><formula xml:id="formula_12">Choice 2: A 2 (x; θ λ ) = Act(BN (f c(RSD(x); θ f c ); θ BN )),</formula><p>where Act(·) represents a non-linear activation function for which we compare three designs:</p><p>i) The vanilla ReLU (·) as used in the squeeze module of SENets.</p><p>ii) The vanilla sigmoid(·) as used in the excitation module of SENets.</p><p>iii) The channel-wise sof tmax(·).</p><p>iv) The piece-wise linear hard analog of the sigmoid function, so-called hsigmoid function <ref type="bibr" target="#b11">[12]</ref>, hsigmoid(a) = min(max(a + 3.0, 0), 6.0)/6.0.</p><p>The hsigmoid(·) is observed to work better in our experiments. In the Choice 2 (Eqn. 11), we apply the vanilla BN <ref type="bibr" target="#b18">[19]</ref> after the FC layer, which normalizes the learned attention weights across all the instances in a mini-batch with the hope of balancing the instance-specific attention weights better. The Choice 2 improves performance in our experiments in ImageNet.</p><p>In AN, we have another hyper-parameter, K. For stage-wise building block based neural architectures such as the four neural architectures tested in our experiments, we use different K's for different stages with smaller values for early stages. For example, for the 4-stage setting, we typically use K = 10, 10, 20, 20 for the four stages respectively based on our ablation study. The underlying assumption is that early stages often learn low-to-middle level features which are considered to be shared more between different categories, while later stages learn more category-specific features which may entail larger mixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first show the ablation study verifying the design choices in the proposed AN. Then, we present detailed comparisons and analyses.</p><p>Data and Evaluation Metric. We use two benchmarks, the ImageNet-1000 classification benchmark (ILSVRC2012) <ref type="bibr" target="#b34">[35]</ref> and the MS-COCO object detection and instance segmentation benchmark <ref type="bibr" target="#b25">[26]</ref>. The ImageNet-1000 benchmark consists of about 1.28 million images for training, and 50, 000 for validation, from 1, 000 classes. We apply a single-crop with size 224 × 224 in evaluation. Following the common protocol, we report the top-1 and top-5 classification error rates tested using a single model on the validation set. For the MS-COCO benchmark, For the residual block and its variants, the proposed AN is used to replace the vanilla BN(s) followed the last 3 × 3 convolution in different blocks. This potentially enables jointly integrating local spatial attention (conveyed by the 3 × 3 convolution) in learning the instance-specific attention weights, which is also observed helpful in <ref type="bibr" target="#b29">[30]</ref> and is shown beneficial for the SE module itself in our experiments ( <ref type="table" target="#tab_2">Table 3</ref>). For the dense block, we replace the second vanilla BN (after the 1 × 1 convolution applied to the concatenated features) with our AN.</p><p>there are 80 categories of objects. We use train2017 in training and evaluate the trained models using val2107. We report the standard COCO metrics of Average Precision (AP) at different intersection-over-union (IoU) thresholds, e.g., AP 50 and AP 75 , for bounding box detection (AP bb IoU ) and instance segmentation (AP m IoU ), and the mean AP over IoU=0.5 : 0.05 : 0.75, AP bb and AP m for bounding box detection and instance segmentation respectively.</p><p>Neural Architectures and Vanilla Feature Normalization Backbones. We use four representative neural architectures: (i) ResNets [10] (ResNet50 and ResNet101), which are the most widely used architectures in practice, (ii) DenseNets <ref type="bibr" target="#b13">[14]</ref>, which are popular alternatives to ResNets, (iii) MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>. MobileNets are popular architectures under mobile settings and MobileNetV2 uses inverted residuals and linear Bottlenecks, and (iv) AOGNets <ref type="bibr" target="#b23">[24]</ref>, which are grammar-guided networks and represent an interesting direction of network architecture engineering with better performance than ResNets and DenseNets. So, the improvement by our AN will be both broadly useful for existing ResNets, DenseNets and MobileNets based deployment in practice and potentially insightful for on-going and future development of more advanced and more powerful DNNs in the community.</p><p>In classification, we use BN <ref type="bibr" target="#b18">[19]</ref> as the feature normalization backbone for our proposed AN, denoted by AN (w/ BN). We compare with the vanilla BN, GN <ref type="bibr" target="#b46">[47]</ref> and SN <ref type="bibr" target="#b27">[28]</ref>. In object detection and instance segmentation, we use the Mask-RCNN framework <ref type="bibr" target="#b7">[8]</ref> and its cascade variant <ref type="bibr" target="#b2">[3]</ref> in the MMDetection code platform <ref type="bibr" target="#b3">[4]</ref>. We fine-tune feature backbones pretrained on the ImageNet-1000 dataset. We also test the proposed AN using GN as the feature normalization backbone, denoted by AN (w/ GN) in the head classifier of Mask-RCNN.</p><p>Where to Apply AN? <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates the integration of our proposed AN in different building blocks. At the first thought, it is straightforward to replace all vanilla feature normalization modules (e.g., BN) in a DNN. It may not be necessary to do so, similar in spirit to the SE-residual block which re-calibrates the residual part once in a building block. As we shall see, our ablation study supports the design choice shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Initialization of our AN. The initialization of γ k,c 's and β k,c 's (Eqn. 8) is based on, γ k,c = 1.0 + N (0, 1) × 0.1 and β k,c = N (0, 1) × 0.1, where N (0, 1) represents the standard Gaussian distribution. This type of initialization is also adopted for conditional BN used in the BigGAN <ref type="bibr" target="#b1">[2]</ref>.  <ref type="table">Table 1</ref>: Ablation study on different design choices in AN with BN as feature normalization backbone using ResNet50+Bottleneck in ImageNet-1000. * means AN is applied to all the BNs of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>We compare different design choices in our proposed AN using ResNet50 in ImageNet-1000. <ref type="table">Table 1</ref> summarizes the results. There are four categories of design choices: The first three are related to the realization of learning the attention weights (Eqn. 7): three types of inputs, two architectural choices and four activation function choices. The last one refers to the number K of components in the mixture of affine transformation which is used for each of the four stages in ResNet50 and we empirically select three options for simplicity. All the models are trained using the same settings (the vanilla setup in Section 4.2).</p><p>The best combination is RSD + A 2 (·) + hsigmoid + K = . During our development, we first observed the best combination based on our intuitive reasoning and small experiments (a few epochs) in the process, and then design this ablation study to verify the design choices. Based on the observed best combination, we further verify that replacing all vanilla BNs is not helpful (the last row in <ref type="table">Table 1</ref>). One explanation is that we may not need to re-calibrate the features using our AN (as well as other channel-wise feature attention methods) for both before and after a 1×1 convolution, since channel-wise re-calibration can be tackled by the 1 × 1 convolution kernel and the vanilla feature normalization themselves in training. The ablation study is in support of the intuitions and design choices discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Classification in ImageNet-1000</head><p>Common Training Settings. We use 8 GPUs (NVIDIA V100) to train models  <ref type="table">Table 2</ref>: Comparisons between BN and our AN (w/ BN) in terms of the top-1 and top-5 error rates (%) in the ImageNet-1000 validation set using the vanilla setup and the state-of-the-art setup. † means the model is not trained by us. All other models are trained from scratch under the same settings.</p><p>using the same settings for apple-toapple comparisons. The method proposed in <ref type="bibr" target="#b8">[9]</ref> is used to initialize all convolutions for all models. The batch size is 128 per GPU. with FP16 optimization used in training to reduce the training time. The mean and standard deviation for block-wise standardization are computed within each GPU. The initial learning rate is 0.4, and the cosine learning rate scheduler <ref type="bibr" target="#b26">[27]</ref> is used with 5 warm-up epochs and weight decay 1 × 10 −4 and momentum 0.9. For AN, the best practice observed in our ablation study <ref type="table">(Table 1)</ref> is used. AN is not used in the stem layer in all the models. In addition to the common settings, we have two different setups in experimental comparisons:</p><p>i) The Vanilla Setup. We adopt the basic data augmentation scheme (random crop and horizontal flip) in training as done in <ref type="bibr" target="#b9">[10]</ref>. We train the models for 120 epochs. All ResNets <ref type="bibr" target="#b9">[10]</ref> use the vanilla stem layer with 7 × 7 convolution. The Mo-bileNetsV2 uses 3 × 3 convolution in the stem layer. The AOGNets use two consecutive 3 × 3 convolution in the stem layer. All the γ and β parameters of the feature normalization backbones are initialized to 1 and 0 respectively.</p><p>ii) The State-of-the-Art Setup. There are different aspects in the vanilla setup which have better variants developed with better performance shown <ref type="bibr" target="#b10">[11]</ref>. We want to address whether the improvement by our proposed AN are truly fundamental or will disappear with more advanced tips and tricks added in training ConvNets. First, on top of the basic data augmentation, we also use label smoothing <ref type="bibr" target="#b40">[41]</ref> (with rate 0.1) and the mixup (with rate 0.2) <ref type="bibr" target="#b47">[48]</ref>. We increase the total number of epochs to 200. We use the same stem layer with two consecutive 3 × 3 convolution for all models. For ResNets, we add the zero γ initialization trick, which uses 0 to initialize the last normalization layer to make the initial state of a residual block to be identity.</p><p>Results Summary. <ref type="table">Table 2</ref> shows the comparison results for the two setups respectively. Our proposed AN consistently obtains the best top-1 and top-5 accuracy results with more than 0.5% absolute top-1 accuracy increase (up to 2.7%) in all models without bells and whistles. The improvement is often obtained with negligible extra parameters (e.g., 0.06M parameter increase in MobileNetV2 for 2.02% absolute top-1 accuracy increase, and 0.2M parameter increase in ResNet50 with 1.42% absolute top-1 accuracy increase) at almost no extra computational cost (up to the precision used in measuring FLOPs). With ResNet50, our AN also outperforms GN <ref type="bibr" target="#b46">[47]</ref> and SN [28] by 1.93% and 0.83% in top-1 accuracy respectively. For GN, it is known that it works (slightly) worse than BN under the normal (big) mini-batch setting <ref type="bibr" target="#b46">[47]</ref>. For SN,  our result shows that it is more beneficial to improve the re-calibration component than to learn-to-switch between different feature normalization schema. We observe that the proposed AN is more effective for small ConvNets in terms of performance gain. Intuitively, this makes sense. Small ConvNets usually learn less expressive features. With the mixture of affine transformations and the instance-specific channel-wise feature re-calibration, the proposed AN offers the flexibility of clustering intra-class data better while separating interclass data better in training.</p><p>Comparisons with the SE module. Our proposed AN provides a strong alternative to the widely used SE module. <ref type="table" target="#tab_2">Table 3</ref> shows the comparisons. We observe that applying SE after the second BN in the bottleneck in ResNet50 is also beneficial with better performance and smaller number of extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Detection and Segmentation in COCO</head><p>In object detection and segmentation, high-resolution input images are beneficial and often entailed for detecting medium to small objects, but limit the batch-size in training (often 1 or 2 images per GPU). GN <ref type="bibr" target="#b46">[47]</ref> and SN <ref type="bibr" target="#b27">[28]</ref> have shown significant progress in handling the applicability discrepancies of feature normalization schema from ImageNet to MS-COCO. We test our AN in MS-COCO following the standard protocol, as done in GN <ref type="bibr" target="#b46">[47]</ref>. We build on the MMDetection code platform <ref type="bibr" target="#b3">[4]</ref>. We observe further performance improvement.</p><p>We first summarize the details of implementation. Following the terminologies used in MMDetection <ref type="bibr" target="#b3">[4]</ref>, there are four modular components in the R-CNN detection framework <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8]</ref>: i) Feature Backbones. We use the pre-trained net-   <ref type="table">Table 2</ref> (with the vanilla setup) for fair comparisons in detection, since we compare with some models which are not trained by us from scratch and use the feature backbones pre-trained in a way similar to our vanilla setup and with on par top-1 accuracy. In fine-tuning a network with AN (w/ BN) pre-trained in ImageNet such as ResNet50+AN (w/ BN) in <ref type="table">Table 2</ref>, we freeze the stem layer and the first stage as commonly done in practice. For the remaining stages, we freeze the standardization component only (the learned mixture of affine transformations and the learned running mean and standard deviation), but allow the attention weight sub-network to be fine-tuned. ii) Neck Backbones: We test the feature pyramid network (FPN) <ref type="bibr" target="#b24">[25]</ref> which is widely used in practice. iii) Head Classifiers. We test two setups: (a) The vanilla setup as done in GN <ref type="bibr" target="#b46">[47]</ref> and SN <ref type="bibr" target="#b27">[28]</ref>. In this setup, we further have two settings: with vs without feature normalization in the bounding box head classifier. The former is denoted by "-" in <ref type="table" target="#tab_4">Table 4</ref>, and the latter is denoted by the corresponding type of feature normalization scheme in <ref type="table" target="#tab_4">Table 4</ref> (e.g., GN, SN and AN (w/ GN)). We experiment on using AN (w/ GN) in the bounding box head classifier and keeping GN in the mask head unchanged for simplicity. Adding AN (w/ GN) in the mask head classifier may further help improve the performance. When adding AN (w/ GN) in the bounding box head, we adopt the same design choices except for "Choice 1, A 1 (·)" (Eqn. 11) used in learning attention weights. (b) The state-of-the-art setup which is based on the cascade generalization of head classifiers <ref type="bibr" target="#b2">[3]</ref> and does not include feature normalization scheme, also denoted by "-" in <ref type="table">Table 5</ref>. iv) RoI Operations. We test the RoIAlign operation <ref type="bibr" target="#b7">[8]</ref>.  <ref type="table">Table 5</ref>: Results in MS-COCO using the cascade variant <ref type="bibr" target="#b2">[3]</ref> of Mask R-CNN.</p><p>Result Summary. The results are summarized in <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table">Table 5</ref>. Compared with the vanilla BN that are frozen in fine-tuning, our AN (w/ BN) improves performance by a large margin in terms of both bounding box AP and mask AP (1.8% &amp; 1.6% for MobileNetV2, 1.6% &amp; 1.2% for ResNet50, 1.7% &amp; 1.4% for ResNet101, 1.3% &amp; 1.4% for AOGNet12M and 0.7% &amp; 0.5% for AOGNet40M). It shows the advantages of the self-attention based dynamic and adaptive control of the mixture of affine transformations (although they themselves are frozen) in fine-tuning.</p><p>With the AN further integrated in the bounding box head classifier of Mask-RCNN and trained from scratch, we also obtain better performance than GN and SN. Compared with the vanilla GN <ref type="bibr" target="#b46">[47]</ref>, our AN (w/ GN) improves bounding box and mask AP by 1.3% and 1.7% for ResNet50, and 1.4% and 2.2% for ResNet101. Compared with SN <ref type="bibr" target="#b27">[28]</ref> which outperforms the vanilla GN in ResNet50, our AN (w/ GN) is also better by 0.6% bounding box AP and 0.9% mask AP increase respectively. Slightly less improvements are observed with AOGNets. Similar in spirit to the ImageNet experiments, we want to verify whether the advantages of our AN will disappear if we use state-of-the-art designs for head classifiers of R-CNN such as the widely used cascade R-CNN <ref type="bibr" target="#b2">[3]</ref>. <ref type="table">Table 5</ref> shows that similar improvements are obtained with ResNet101 and AOGNet40M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents Attentive Normalization (AN) that aims to harness the best of feature normalization and feature attention in a single lightweight module. AN learns a mixture of affine transformations and uses the weighted sum via a selfattention module for re-calibrating standardized features in a dynamic and adaptive way. AN provides a strong alternative to the Squeeze-and-Excitation (SE) module. In experiments, AN is tested with BN and GN as the feature normalization backbones. AN is tested in both ImageNet-1000 and MS-COCO using four representative networks (ResNets, DenseNets, MobileNetsV2 and AOGNets). It consistently obtains better performance, often by a large margin, than the vanilla feature normalization schema and some state-of-the-art variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>T.AFig. 1 :</head><label>1</label><figDesc>Wu is the corresponding author. arXiv:1908.01259v3 [cs.CV] 25 Mar 2021 Mixture of Channel-wise Affine Transformations &gt; . ? ,: = EFFG &amp; ',),*,+ . ? ,: Illustration of the proposed Attentive Normalization (AN). AN aims to harness the best of a base feature normalization (e.g., BN or GN) and channelwise feature attention in a single light-weight module. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>λ i N ,k 's are instance specific and learned from features that are not standardized. Combining them with γ k,i C 's and β k,i C 's (Eqn. 8) enables AN paying attention to both the population (what the common and useful information are) and the individuals (what the specific yet critical information are).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of integrating the proposed AN in different building blocks. The first two show the vanilla residual block and the SE-residual block. The remaining four are: the Basicblock and Bottleneck design of a residual block, the inverted residual block (used in MobileNetV2), and the DenseBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Design</head><label></label><figDesc>Choices in AN (w/ BN) #Params FLOPS top-1 top-5 mean + A2(·) + hsigmoid + K = 10 10 20 20 25.76M 4.09G 21.85 5.92 (mean,std) + A2(·) + hsigmoid + K =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Comparisons between SE and</cell></row><row><cell>our AN (w/ BN) in terms of the top-</cell></row><row><cell>1 and top-5 error rates (%) in the</cell></row><row><cell>ImageNet-1000 validation set using the</cell></row><row><cell>vanilla setup. By "(All)", it means SE</cell></row><row><cell>or AN is used for all the three BNs in a</cell></row><row><cell>bottleneck block.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>72M 34.2↓(1.8) 54.6↓(2.4) 37.1↓(1.8) 30.9↓(1.6) 51.1↓(2.7) 32.6↓(1.</figDesc><table><row><cell cols="2">Architecture Backbone</cell><cell>Head</cell><cell cols="2">#Params AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell>MobileNetV2</cell><cell>BN AN (w/ BN)</cell><cell>--</cell><cell cols="7">22.9) 22.78M 36.0 57.0 38.9 32.5 53.8 34.5</cell></row><row><cell></cell><cell>BN</cell><cell>-</cell><cell cols="7">45.71M 39.2↓(1.6) 60.0↓(2.1) 43.1↓(1.4) 35.2↓(1.2) 56.7↓(2.2) 37.6↓(1.1)</cell></row><row><cell></cell><cell cols="2">BN + SE(BN3) -</cell><cell cols="7">48.23M 40.1↓(0.7) 61.2↓(0.9) 43.8↓(0.7) 35.9↓(0.5) 57.9↓(1.0) 38.1↓(0.6)</cell></row><row><cell>ResNet50</cell><cell cols="2">BN + SE(BN2) -</cell><cell cols="7">46.34M 40.1↓(0.7) 61.2↓(0.9) 43.8↓(0.7) 35.9↓(0.5) 57.9↓(1.0) 38.4↓(0.3)</cell></row><row><cell></cell><cell>AN (w/ BN)</cell><cell>-</cell><cell>45.91M</cell><cell>40.8</cell><cell>62.1</cell><cell>44.5</cell><cell>36.4</cell><cell>58.9</cell><cell>38.7</cell></row><row><cell></cell><cell>† GN</cell><cell>GN [47]</cell><cell cols="7">45.72M 40.3↓(1.3) 61.0↓(1.0) 44.0↓(1.7) 35.7↓(1.7) 57.9↓(1.6) 37.7↓(2.2)</cell></row><row><cell></cell><cell>† SN</cell><cell>SN [28]</cell><cell>-</cell><cell cols="6">41.0↓(0.6) 62.3↓(−0.3) 45.1↓(0.6) 36.5↓(0.9) 58.9↓(0.6) 38.7↓(1.2)</cell></row><row><cell></cell><cell>AN (w/ BN)</cell><cell cols="2">AN (w/ GN) 45.96M</cell><cell>41.6</cell><cell>62.0</cell><cell>45.7</cell><cell>37.4</cell><cell>59.5</cell><cell>39.9</cell></row><row><cell></cell><cell>BN</cell><cell>-</cell><cell cols="7">64.70M 41.4↓(1.7) 62.0↓(2.1) 45.5↓(1.8) 36.8↓(1.4) 59.0↓(2.0) 39.1↓(1.6)</cell></row><row><cell>ResNet101</cell><cell>AN (w/ BN)  † GN</cell><cell>-GN [47]</cell><cell cols="7">65.15M 64.71M 41.8↓(1.4) 62.5↓(1.5) 45.4↓(1.9) 36.8↓(2.0) 59.2↓(2.1) 39.0↓(2.6) 43.1 64.1 47.3 38.2 61.0 40.7</cell></row><row><cell></cell><cell>AN (w/ BN)</cell><cell cols="2">AN (w/ GN) 65.20M</cell><cell>43.2</cell><cell>64.0</cell><cell>47.3</cell><cell>38.8</cell><cell>61.3</cell><cell>41.6</cell></row><row><cell></cell><cell>BN</cell><cell>-</cell><cell cols="7">33.09M 40.7↓(1.3) 61.4↓(1.7) 44.6↓(1.5) 36.4↓(1.4) 58.4↓(1.7) 38.8↓(1.6)</cell></row><row><cell>AOGNet12M</cell><cell>AN (w/ BN)</cell><cell>-</cell><cell cols="7">33.21M 42.0↓(1.0) 63.1↓(1.1) 46.1↓(0.7) 37.8↓(0.9) 60.1↓(1.0) 40.4↓(1.3)</cell></row><row><cell></cell><cell>AN (w/ BN)</cell><cell cols="2">AN (w/ GN) 33.26M</cell><cell>43.0</cell><cell>64.2</cell><cell>46.8</cell><cell>38.7</cell><cell>61.1</cell><cell>41.7</cell></row><row><cell></cell><cell>BN</cell><cell>-</cell><cell cols="7">60.73M 43.4↓(0.7) 64.2↓(0.9) 47.5↓(0.7) 38.5↓(0.5) 61.0↓(1.0) 41.4↓(0.4)</cell></row><row><cell>AOGNet40M</cell><cell>AN (w/ BN)</cell><cell>-</cell><cell cols="7">60.97M 44.1↓(0.8) 65.1↓(1.1) 48.2↓(0.9) 39.0↓(1.2) 62.0↓(1.2) 41.8↓(1.5)</cell></row><row><cell></cell><cell>AN (w/ BN)</cell><cell cols="2">AN (w/ GN) 61.02M</cell><cell>44.9</cell><cell>66.2</cell><cell>49.1</cell><cell>40.2</cell><cell>63.2</cell><cell>43.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Detection and segmentation results in MS-COCO val2017<ref type="bibr" target="#b25">[26]</ref>. All models use 2x lr scheduling (180k iterations). BN means BN is frozen in finetuning for object detection. † means that models are not trained by us. All other models are trained from scratch under the same settings. The numbers show sequential improvement in the two AOGNet models indicating the importance of adding our AN in the backbone and the head respectively.</figDesc><table /><note>works in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>32M 44.4 ↓(1.4) 62.5 ↓(1.8) 48.4 ↓(1.4) 38.2 ↓(1.4) 59.7 ↓(2.0) 41.3 ↓(1.4) 35M 45.6 ↓(0.9) 63.9 ↓(1.1) 49.7 ↓(1.1) 39.3 ↓(0.7) 61.2 ↓(1.1) 42.7 ↓(0.4)</figDesc><table><row><cell cols="2">Architecture Backbone</cell><cell>Head #Params AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell></row><row><cell>ResNet101</cell><cell cols="2">BN 96.AN (w/ BN) --96.77M 45.8</cell><cell>64.3</cell><cell>49.8</cell><cell>39.6</cell><cell>61.7</cell><cell>42.7</cell></row><row><cell>AOGNet40M</cell><cell cols="2">BN 92.AN (w/ BN) --92.58M 46.5</cell><cell>65.0</cell><cell>50.8</cell><cell>40.0</cell><cell>62.3</cell><cell>43.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We tried the variant of learning Γ () and B() from the standardized features and observed it works worse, so we ignore it in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In implementation, we use the reverse µc σc+ for numeric stability, which is equivalent to the original formulation when combing with the f c layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by NSF IIS-1909644, ARO Grant W911NF1810295, NSF IIS-1822477 and NSF IUSE-2013451. The views presented in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00644</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.html9" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mode normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyN-M2Rctm1" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1606.00704</idno>
		<ptr target="http://arxiv.org/abs/1606.00704" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.322</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.3229" />
		<title level="m">Mask R-CNN. In: IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015" />
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Computer</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015-12-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 5</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1812.01187</idno>
		<ptr target="http://arxiv.org/abs/1812.0118711" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1905.02244</idno>
		<ptr target="http://arxiv.org/abs/1905.022448" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
		<ptr target="http://arxiv.org/abs/1709.015072,4" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/170723" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3271" to="3278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decorrelated batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ccnet: Crisscross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1811.11721</idno>
		<ptr target="http://arxiv.org/abs/1811.117212" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batchnormalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf1,3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>Blei, D., Bach, F.</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance-level meta normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_Instance-Level_Meta_Normalization_CVPR_2019_paper.html1" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training faster by separating modes of variation in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2895781</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.28957811" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aognets: Compositional grammatical architectures for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.106</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.10613" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<ptr target="http://arxiv.org/abs/1608.0398311" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno>abs/1806.10779</idno>
		<ptr target="http://arxiv.org/abs/1806.107792" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<title level="m">cgans with projection discriminator</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Switchable whitening for deep representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00195</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.001959" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning visual reasoning without strong priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1707.03017</idno>
		<ptr target="http://arxiv.org/abs/1707.03017" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y8" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="2488" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ssn: Learning sparse switchable normalization via sparsestmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1903.03793</idno>
		<ptr target="http://arxiv.org/abs/1903.037932" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<ptr target="http://arxiv.org/abs/1512.0056711" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<ptr target="http://arxiv.org/abs/1607.08022" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7237-modulating-early-visual-processing-by-language2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.683</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.6834" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6450" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html2" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_14" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01261-8_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01261-8_11" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb11" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
