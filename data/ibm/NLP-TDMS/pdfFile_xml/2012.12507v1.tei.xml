<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blur More To Deblur Better: Multi-Blur2Deblur For Efficient Video Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Se</roleName><forename type="first">Young</forename><surname>Chun</surname></persName>
							<email>sychun@unist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">UNIST</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Blur More To Deblur Better: Multi-Blur2Deblur For Efficient Video Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the key components for video deblurring is how to exploit neighboring frames. Recent state-of-the-art methods either used aligned adjacent frames to the center frame or propagated the information on past frames to the current frame recurrently. Here we propose multi-blur-to-deblur (MB2D), a novel concept to exploit neighboring frames for efficient video deblurring. Firstly, inspired by unsharp masking, we argue that using more blurred images with long exposures as additional inputs significantly improves performance. Secondly, we propose multi-blurring recurrent neural network (MBRNN) that can synthesize more blurred images from neighboring frames, yielding substantially improved performance with existing video deblurring methods. Lastly, we propose multi-scale deblurring with connecting recurrent feature map from MBRNN (MSDR) to achieve state-of-the-art performance on the popular GoPro and Su datasets in fast and memory efficient ways.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video deblurring is a highly ill-posed inverse problem that aims to recover the sharp latent image from blurred video frames. The solution for this is getting more and more important due to massive amount of video data from handheld devices such as smart phones. There are a number of factors to non-uniformly blur videos such as camera shake, object motion, and depth variation that particularly make this inverse problem quite challenging. Video deblurring is a long-standing computer vision topic. Most non-deep learning works investigated how to estimate blur kernels and/or latent frames using neighboring video frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref>. Among them, multi-image blind deblurring methods have been developed to incorporate observations from multiple blurred images that share the common underlying latent image with different blurs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Recent deep learning-based approaches for video deblurring have investigated the ways of utilizing neighbor- <ref type="bibr" target="#b0">1</ref> Equal contribution, * Corresponding author <ref type="figure">Figure 1</ref>. Video deblurring results of Kim <ref type="bibr" target="#b8">[9]</ref>, STFAN <ref type="bibr" target="#b29">[30]</ref>, Pan <ref type="bibr" target="#b14">[15]</ref>, and our MB2D (Ours) on the GoPro dataset <ref type="bibr" target="#b12">[13]</ref>.</p><p>ing blurred video frames largely either by using temporally aligned adjacent frames to the reference frame with deep neural networks (DNNs) or by propagating the information about past frames to the reference frame recurrently with recurrent neural network (RNN). One group of works exploits adjacent video frames by warping them to the center frame at image and/or feature levels so that sharp pixel information from other frames can be utilized for deblurring <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref>. Due to reduced variations between video frames by alignment, DNNs seem to work more efficiently with aligned frames than with unaltered frames, yielding state-of-the-art (SOTA) video deblurring performance. However, there are a couple of disadvantages for temporal alignment; Alignment in video deblurring itself is also ill-posed and challenging. Thus, potential errors in alignment may cause undesirable artifacts in deblurring. Alignment often requires heavy computation cost and large memory for warping operation. The other group of works utilizes RNNs to sequentially restore the sharp image from video frames using the features from previous steps <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. Thanks to no alignment operation, these methods have low computation cost, but yielded slightly lower deblurring performance than SOTA methods that were developed around the same time.</p><p>Here, we propose multi-blur-to-deblur (MB2D), a novel concept on how to exploit neighboring frames for efficient video deblurring as an alternative to achieve both SOTA performance and fast computation with small memory.</p><p>First of all, we propose the fundamental argument for our MB2D: using more blurred images with long exposures as additional inputs to video deblurring significantly improves performance. This argument was inspired by classical unsharp masking <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> that enhances high-frequency components via a more blurred input image. We conjecture that if more blurred images with long exposures for the same reference frame are available, they could encode more information on motions that can potentially improve deblurring performance. Thus, our proposed MB2D consists of two steps: more-blurring (MB) and then deblurring (D).</p><p>Secondly, for the MB step of our MB2D, we propose multi-blurring recurrent neural network (MBRNN) that synthesizes more blurred images (not available during testing) from neighboring blurred frames. <ref type="figure">Figure 2</ref> illustrates (a) video deblurring with alignments <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref> and (b) our MB2D with MBRNN. While the former warps neighboring frames to the reference frame, the latter (MBRNN) progressively synthesizes more blurred images by appending small amounts of motion blurs predicted from neighboring frames. Our MBRNN is fast and memory-efficient due to no alignment, while yields substantially improved performance with existing video deblurring algorithms.</p><p>Lastly, for the D step of our MB2D, we propose multiscale deblurring with connecting recurrent feature map (CRFM) from MBRNN (MSDR) to achieve SOTA performance for video deblurring on the GoPro and Su datasets in fast and memory efficient ways. Progressively generated recurrent feature maps further improved the performance of video deblurring with the synthesized more blurred images, and thus our proposed MSDR with CRFM performed favorably against existing SOTA methods with relatively small parameters and fast computation as depicted in <ref type="figure">Figure 1</ref>.</p><p>The main contributions of this work are summarized as: • For the first time, we show that using long exposure images along with the input with regular exposure substantially improves deblurring performance. Then, we propose MB2D, a novel approach with multi-blur and deblur steps.</p><p>• We propose MBRNN that progressively synthesizes more blurred images from the input and its neighboring frames. MBRNN improved the performance of existing methods.</p><p>• We propose MSDR for video deblurring that exploits CRFM from MBRNN so that the multi-blur and deblur steps are connected at feature levels.</p><p>• Our proposed MB2D with MBRNN and MSDR outperformed other SOTA methods on the GoPro <ref type="bibr" target="#b12">[13]</ref> and Su <ref type="bibr" target="#b19">[20]</ref> datasets quickly and parameter-efficiently. <ref type="figure">Figure 2</ref>. Two pre-processings for video deblurring: (a) Temporal Alignment <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref> that warps neighboring frames to the reference frame (e.g., optical flow) (b) our Multi-Blurring that progressively synthesizes more blurred images with long exposures corresponding to the reference frame using adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Non-DNN multi-images / video deblurring Blind deconvolution of motion blur is challenging, but multiple blur images often make this inverse problem less ill-posed. Multi-image deblurring has been investigated such as multichannel deconvolution <ref type="bibr" target="#b30">[31]</ref>, iterative kernel estimation <ref type="bibr" target="#b1">[2]</ref>, multi-channel blind deconvolution with augmented Lagrangian optimization <ref type="bibr" target="#b18">[19]</ref>, homography estimation for image registration <ref type="bibr" target="#b2">[3]</ref>, and multi-image registration <ref type="bibr" target="#b24">[25]</ref>. There have been some works on video deblurring without using DNNs such as patch-based restoration of blurry areas by detecting sharp areas that share the same contents from nearby frames <ref type="bibr" target="#b3">[4]</ref>, simultaneous estimation of the latent image and optical flow based on locally approximating the pixel-wise varying kernels with bi-directional optical flow <ref type="bibr" target="#b7">[8]</ref> and local deblurring through weighted Fourier accumulation after warping adjacent frames to the reference by optical flow consistency <ref type="bibr" target="#b4">[5]</ref>.</p><p>DNN video deblurring with alignment There have been a number of methods for video deblurring to use neighboring frames with temporal alignment. Su et al. proposed to align adjacent frames to the center frame, decreasing the spatial variance of video frames <ref type="bibr" target="#b19">[20]</ref>. EDVR employed deformable CNN to warp the information on adjacent frames to the center frame at feature level <ref type="bibr" target="#b23">[24]</ref>. Zhou et al. proposed STFAN to implicitly estimate the dynamic alignment filters to transform the features in the spatiotemporal filter adaptive network <ref type="bibr" target="#b29">[30]</ref>. Pan et al. developed the temporal sharpness prior that explores the sharpness pixel and constraints the deep CNN model for generating aligned intermediate latent frames with optical flow <ref type="bibr" target="#b14">[15]</ref>.</p><p>RNN video deblurring Kim et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method to recurrently estimate the latent image by developing the RNN with the concatenation of multi-frame features. Nah et al. <ref type="bibr" target="#b13">[14]</ref> developed the RNN to exploit the intra-frame and inter-frame iterations with hidden-state update via reusing RNN cell parameters for video deblurring. Zhong et al. <ref type="bibr" target="#b28">[29]</ref> proposed RNN based method to globally aggregate the spatio-temporal correlation from the spatial high-level features of video frames generated by RNN cell.</p><p>DNN single image deblurring Nah et al. <ref type="bibr" target="#b12">[13]</ref> proposed single image deblurring with multi-scale (MS) approach, Tao et al. <ref type="bibr" target="#b22">[23]</ref> proposed to share models of each stage in the MS approach, and Gao et al. <ref type="bibr" target="#b5">[6]</ref> developed a partial sharing method considering different level of blurs for each stage in the MS approach. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> and Suin et al. <ref type="bibr" target="#b20">[21]</ref> independently proposed to adjust the receptive field using multipatch approaches with coarse-to-fine structures. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> proposed a data augmentation method by predicting a real blur image using GAN from a single sharp image to reduce the gap between synthetic blur and real blur. Park et al. <ref type="bibr" target="#b15">[16]</ref> introduced a progressive deblurring method that recurrently estimates the intermediate and final latent images by exploiting time-resolved deblurring data augmentation.</p><p>Our MB2D consists of the MB step and the D step. MBRNN utilized RNN, not for deblurring <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, but for blurring more. Unlike <ref type="bibr" target="#b27">[28]</ref>, this blurring step is for encoding the information of neighboring frames for the same reference. MSDR employed the MS approach <ref type="bibr" target="#b12">[13]</ref> for deblurring, but with a novel CRFM from MBRNN so that two steps are connected at both image and feature levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Blurred Images with Long Exposures</head><p>This section investigates the fundamental assumption of our proposed MB2D: Using more blurred images with long exposures as additional inputs to video deblurring significantly improves performance. More blurred images with long exposures are not available during testing, but it is possible to synthesize them during training with video deblur datasets from high-speed cameras <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. While Park et al. <ref type="bibr" target="#b15">[16]</ref> generated blurred images with short exposures, we propose to synthesize blurred images with long exposures.</p><p>Dynamic scenes blur dataset Photons are accumulated on a sensor during exposure time, yielding real blurred images. Simulating this, the blurred image B is generated by the integration of successive sharp images S from highspeed cameras <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> as follows: where n denotes the number of sharp frames (odd, proportional to exposure time for the blurred image B n t ), S[i] denotes the ith acquired sharp video frame from a high-speed camera, g is a camera response function and t is an index for the generated blurred image (integer). A typical video deblurring problem is to predict S[nt] or B 1 t (sharp ground truth) from B n t and its neighboring blurred frames B n t−1 and B n t+1 . Note that as n increases, B n t becomes more blurred.</p><formula xml:id="formula_0">B n t = g   1 n n/2 i=− n/2 S [nt + i]  <label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ideal More Blurred Images</head><p>The unsharp masking <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> is a classical image sharpening technique to enhance high-frequency components in an image by utilizing the difference between the input image and its blurred (or unsharp) image using Gaussian filtering (we denote this more blurred image). This difference emphasizes (or masks) high-frequency components such as edges that were degraded by Gaussian filtering (or more blurring). In this work, we extend this idea of unsharp masking to the case of video deblurring.</p><p>Instead of Gaussian blurring, we propose to generate more blurred images for video deblurring using sharp images with <ref type="bibr" target="#b0">(1)</ref>. For the blurred image B n t , the more blurred video frames for the same reference frame S[nt] will be B n+2 t , B n+4 t and B n+6 t that have long exposures to encode more motion information over wide acquisition times as compared to B n t . As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, the difference between the input image and a more blurred image encodes the information for sharp region with small motion and blurred region with large motion effectively.</p><p>Note that more blurred images can be available during training, but not during testing. Thus, more blurred images are ideal and must be predicted during testing. In addition, for other unsharp masking operations (e.g., scaling), we propose to resort to the power of DNNs (see next Section). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">More Blurred Images for Deblurring</head><p>We empirically validated our conjecture on more blurred images with deblurring. We performed a single image deblurring with the original input image B 11 t and/or with ideal more blurred images B 13 t , B 15 t and so on from the GoPro dataset <ref type="bibr" target="#b12">[13]</ref>. We denote the set of input images</p><formula xml:id="formula_1">B 11 t , B 13 t , . . . , B 19 t by B {11,13,...,19} t</formula><p>. A modified U-Net <ref type="bibr" target="#b17">[18]</ref> was trained with the input blurred image and/or ideal more blurred images for the same ground truth sharp frame. Single image deblurring results with the ideal more blurred image sets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Set 1 corresponds to a usual single image deblurring and Sets 2 to 5 correspond to our image deblurring with ideal more blurred images with long exposures.</p><p>As compared to the original performance with the input image B 11 t , deblurring with additional more blurred images yielded significantly improved performance. As the more blurred images with longer exposures are used for deblurring, the better performances in PSNR were observed until n = 17 where dramatic performance boost of 3.33 dB was achieved as compared to the original deblurring with the input with n = 11. However, after n = 17, more information from more blurred images did not help to improve deblurring performance. We chose to use 3 more blurred images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Blurring Recurrent Neural Network</head><p>Ideal more blurred images significantly improved the performance of deblurring as investigated in <ref type="table" target="#tab_0">Table 1</ref>, but they are available only during training, not during testing. We conjecture that these more blurred images corresponding to the n + 6 high-speed video frames can be predicted from the input image B n t and its neighboring images B n t−1 , B n t+1 that are corresponding to 3n high-speed video frames. Thus, the goal of our proposed MBRNN is to progressively predict the multi-blurring images B {n+2,n+4,n+6} t (available during training) from B n {t−1,t,t+1} (available during training and testing). Our MBRNN is responsible for the MB step of our proposed MB2D.</p><p>MBRNN model: A modified U-Net <ref type="bibr" target="#b17">[18]</ref> was used as the baseline network for our proposed MBRNN for essentially progressive blurring that is similar to the network in <ref type="bibr" target="#b15">[16]</ref> for progressive deblurring. Thus, our proposed MBRNN to yield more blurred images is modeled as:</p><formula xml:id="formula_2">{B n+2k t , F k } = Net B (B n {t−1,t,t+1} ,B n+2k−2 t , F k−1 ) (2)</formula><p>where Net B is the baseline network,B n+2k t is the estimated more blurred image at the kth iteration (k = 1, 2, 3), and F i is connecting recurrent feature map from the encoder of MBRNN and to the decoder of MBRNN. <ref type="figure" target="#fig_1">Figure 4</ref>(a) illustrates how the model of MBRNN works to progressively append small amount of motion blurs to the center (or reference) frame by exploiting the estimated more blurred image and intermediate feature map from the previous iteration. <ref type="figure" target="#fig_3">Figure 5</ref> shows examples of estimated more blurred images from MBRNN such that small motion blurs (b, c, d) are progressively added to the center frame (a), emphasizing fast moving objects. This phenomenon is also observed quantitatively as decreased spectral densities (e) that is the exact opposite to progressive deblurring in <ref type="bibr" target="#b15">[16]</ref>.</p><p>MBRNN loss: We trained our proposed MBRNN using a simple L1 function: Given N multi-blurring images B n+2k t as ground truth that were synthesized from highspeed video frames, the loss function for MBRNN is:</p><formula xml:id="formula_3">L = N t=1 3 k=1 B n+2k t − B n+2k t 1<label>(3)</label></formula><p>where t is an index andB n+2i t is the output of MBRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Scale Video Deblurring with Connecting Recurrent Feature Map</head><p>Our proposed MB2D consists of the MB step and the D step for more blurring and deblurring, respectively. It is possible to use MBRNN (MB step) and existing video deblurring methods (D step) for improved performance (see next Section for results). While this approach connects the MB and D steps at image level, we propose to connect these two steps at feature levels for further performance boost.</p><p>MSDR model: <ref type="figure" target="#fig_1">Figure 4</ref>(b) illustrates the schematic of the D step using the predicted more blurred images and connecting recurrent feature map from MBRNN. Our proposed Multi-Scale video Deblurring network with connecting Recurrent feature map (MSDR) exploits the information from MBRNN, the predicted more blurred im-agesB {n+2,n+4,n+6} t and the connecting recurrent feature map F (s) , in the multi-scale framework for deblurring with down-sampling. Our proposed MSDR with connecting recurrent feature map is modeled as:  Connecting recurrent feature map: As illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, the recurrent feature maps F are extracted from the decoders of MBRNN at every iteration and then concatenated before feeding into our MSDR. We call the concatenated feature maps "connecting recurrent feature map (CRFM)" that bridges between MBRNN and MSDR at feature levels for the best possible deblurring performance of our proposed MB2D concept. It seems that CRFM carries more information on blurs around the edges of moving objects than the more blurred images have, leading to the further performance improvement of video deblurring (see 0.64 dB improvement in PSNR via our proposed CRFM as shown in <ref type="table">Table 2</ref>).</p><formula xml:id="formula_4">I (s) = Net D (B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets GoPro dataset <ref type="bibr" target="#b12">[13]</ref> consists of 34,874 sharp images (33 videos) with the size of 1280×720 and with 22 videos for training and 11 videos for testing. By the integration of successive sharp images from high-speed cameras, Nah <ref type="bibr" target="#b12">[13]</ref> synthesized 2,103 blurred images for training and 1,111 blurred images for testing with sharp ground truth images. We further generated and added more blurred images to the GoPro dataset (called M-GoPro) for training.</p><p>Su dataset <ref type="bibr" target="#b19">[20]</ref> consists of 6,708 images (71 videos) with the size of 1,920×1080 or 1,280×720 captured by several high-speed camera devices where 61 videos are for training and 10 videos are for testing. Following the method of synthesizing motion blurs with interpolated frames using optical flow <ref type="bibr" target="#b19">[20]</ref>, we further generated and added more blurred images to the Su dataset (called M-Su) for training.</p><p>Implementation details All implementations were done with PyTorch and the Adam optimizer with learning rate 2 × 10 −4 , β 1 = 0.9, β 2 = 0.999, = 10 −8 , and batch size 16 was used as an optimization algorithm for training. Data augmentation techniques for training were used such as random cropping with 256×256 size, random vertical / horizontal flips, and 90 • rotation. For Tables 1 to 5, the total number of iterations was 10 5 and for <ref type="table" target="#tab_5">Table 6</ref>, it was 10 6 . For <ref type="table">Table 7</ref>, the trained network for <ref type="table" target="#tab_5">Table 6</ref> with additional finetuning of 3×10 5 was used. All experiments were conducted on a NVIDIA Titan V. Run time was measured with batch size 1 without data-loading time. MBRNN was trained first and then MSDR was trained with the pre-trained MBRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Studies for Our MB2D</head><p>We performed the ablation studies to demonstrate the effectiveness of the proposed MB2D approach for several components: the number of input frames (NIF), preprocessing (Pre-proc) such as optical flow (OF) or our MBRNN, and connecting recurrent feature map (CRFM). We chose the modified U-Net as a deblurring network without multi-scale (MS) or multi-temporal (MT) frameworks. The results are summarized in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref>(a),(b) correspond to the cases of single image deblurring (with 1 input frame) and of video deblurring (with <ref type="table">Table 2</ref>. Ablation studies with the number of input frames (NIF) (1 or 3 frames), with pre-processing (Preproc) with optical flow (OF) or our MBRNN (MB), and with our CRFM. 3 input frames), respectively, showing the improved performance of (b) over (a) by using more information from neighboring frames. <ref type="table">Table 2</ref>(b),(c),(d) correspond to different pre-processing methods for video deblurring: Using optical flow for aligned adjacent frames (c) with significantly increased memory requirement (12.0M) improved the deblurring performance by 0.08dB over (b) without preprocessing. However, our proposed MBRNN itself (d) improved the performance by 0.82dB with modestly increased memory requirement (5.2M). <ref type="table">Table 2</ref>(e) shows additional performance improvement by 0.57dB over (d) when using CRFM from MBRNN for deblurring without increasing network parameters much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIF</head><p>We also performed additional studies on the number of network parameters for (b). As observed in <ref type="table">Table 2</ref>(f),(g), increasing the number of network parameters improved deblurring performance, but our proposed MB2D (especially with CRFM) yielded better performances than the deblurring networks with similar numbers of parameters. Thus, our proposed MB2D yielded improved performance not simply because of increased network size, but because of our proposed approach with more blurred images.</p><p>Our MB2D concept can be used with various single image / video deblurring approaches such as One-Stage (OS) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref>, Multi-Temporal (MT) <ref type="bibr" target="#b15">[16]</ref> and Multi-Scale (MS) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b5">6]</ref> methods where the MS and MT approaches achieve high performance with small number of parameters due to parameter sharing over scales or iterations. We performed another ablation study on them with our proposed MBRNN pre-processing and the results are summarized in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="table" target="#tab_2">Table 3</ref>(h),(k),(n) show the results of the baseline methods for OS, MT and MS approaches with the same DNN. In this case, MT approach yielded the best performance among all three approaches by up to 0.66dB. However, with our proposed MBRNN, MS approach yielded the best performance among all approaches by up to 1.08dB as shown in <ref type="table" target="#tab_2">Table 3</ref>(i),(l),(o). Further performance improvements were observed with our proposed CRFM from MBRNN and MS  approach with them yielded the best performance among all methods by up to 1.54dB. As argued in <ref type="bibr" target="#b15">[16]</ref>, MS approach seems to destroy high frequency details for good deblurring by using down-sampled images / features, but it seems that our proposed MBRNN and CRFM compensate for the degraded details to yield excellent performance for video deblurring. Thus, we chose MS for the D step of our MB2D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Role of Neighboring Frames for MBRNN</head><p>To validate the necessity of adjacent frames to provide motion information for generating more blurred images, we performed an ablation study with 1 frame (without neighboring frames) and 3 frames (with neighboring frames) for MBRNN. Note that a single frame is generated with n highspeed video frames while 3 frames cover 3n high-speed video frames that is usually wider than the more blurred images from n + 2 to n + 6 high-speed video frames. The performance results are summarized in <ref type="table" target="#tab_3">Table 4</ref>, showing significant performance differences between the case with a single frame for MBRNN and the case of using neighboring frames. The largest difference was observed for the more blurred image with n + 6 high-speed video frames and the smallest difference was still significant by 1.9dB. Note that MBRNN is computationally efficient (0.02sec) as compared to optical flow (0.076sec) <ref type="bibr" target="#b21">[22]</ref> that we used for Su <ref type="bibr" target="#b19">[20]</ref> instead of a MATLAB implementation for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multi-Blurring Step for Existing Methods</head><p>Our proposed MB2D is an alternative approach to exploit neighboring frames and its MBRNN can be applied to other existing video deblurring methods with mild modifications such as multi-channel inputs with the predicted <ref type="table">Table 5</ref>. Performance comparisons of existing methods (Tao <ref type="bibr" target="#b22">[23]</ref>, Zhang <ref type="bibr" target="#b25">[26]</ref>, Su <ref type="bibr" target="#b19">[20]</ref>, Pan <ref type="bibr" target="#b14">[15]</ref>) and their MB2D versions where * suffix refers to our MB2D version of the existing methods (e.g., Tao * for Tao <ref type="bibr" target="#b22">[23]</ref>   <ref type="bibr" target="#b19">[20]</ref> and Pan <ref type="bibr" target="#b14">[15]</ref>. We implemented our MB2D versions of them, called Tao * , Zhang * , Su * and Pan * , respectively. Note that for Tao <ref type="bibr" target="#b22">[23]</ref>, Zhang <ref type="bibr" target="#b25">[26]</ref>, single image deblurring methods were converted into video deblurring methods with our MB2D by adding MBRNN. For Su <ref type="bibr" target="#b19">[20]</ref>, Pan <ref type="bibr" target="#b14">[15]</ref>, we replaced the optical flow network (PWC-Net <ref type="bibr" target="#b21">[22]</ref>) with our MBRNN, resulting in decreased network parameter size by 6.8M. <ref type="table">Table 5</ref> shows that our proposed MB2D substantially improved the deblurring performance with existing methods by at least 0.52dB up to 1.6dB for all 4 existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Benchmark Results</head><p>We performed our proposed MB2D with MBRNN and MSDR on two popular benchmark video deblurring datasets: GoPro test dataset <ref type="bibr" target="#b12">[13]</ref> and Su test dataset <ref type="bibr" target="#b19">[20]</ref>. We use the reported quantitative performances in PSNR (dB) and SSIM of the SOTA methods for Pan <ref type="bibr" target="#b14">[15]</ref>, Zhong <ref type="bibr" target="#b28">[29]</ref> and Nah <ref type="bibr" target="#b13">[14]</ref>. Parameter size and run time results were measured using publicly available codes. <ref type="table" target="#tab_5">Tables 6 and 7</ref> show that our proposed MB2D significantly outperformed all existing SOTA methods for video deblurring in terms of PSNR and SSIM on the GoPro and Su test datasets, respectively. Note that compared to the most recent SOTA method, Pan <ref type="bibr" target="#b14">[15]</ref>, our proposed MB2D with MBRNN and MSDR yielded substantially outperformed it <ref type="figure">Figure 6</ref>. Qualitative results evaluated on the GoPro test dataset <ref type="bibr" target="#b12">[13]</ref> for (e) our proposed MB2D method and other SOTA methods: (b) Kim <ref type="bibr" target="#b8">[9]</ref>, (c) STFAN <ref type="bibr" target="#b29">[30]</ref>, and (d) Pan <ref type="bibr" target="#b14">[15]</ref> for the given input blurred image (a) and its sharp ground truth image (f). Our proposed MB2D yielded visually excellent deblurring results for fast moving persons and objects. <ref type="figure">Figure 7</ref>. Qualitative results evaluated on the Su test dataset <ref type="bibr" target="#b19">[20]</ref> for (e) our proposed MB2D method and other SOTA methods: (b) Kim <ref type="bibr" target="#b8">[9]</ref>, (c) STFAN <ref type="bibr" target="#b29">[30]</ref>, and (d) Pan <ref type="bibr" target="#b14">[15]</ref> for the given input blurred image (a) and its sharp ground truth image (f). Our proposed MB2D yielded visually excellent deblurring results for fast moving persons and objects. with more than 3 times less network parameters (5.42M vs. <ref type="bibr">16.19M</ref>) and more than 6 times faster computation time (0.27 second vs. 1.73 second). <ref type="figure">Figures 6 and 7</ref> show the examples of video deblurring using a few SOTA methods or using our proposed MB2D on the GoPro and Su test datasets, respectively. Our proposed MB2D seems to better deblur for fast moving objects or people than other existing methods such as Kim <ref type="bibr" target="#b8">[9]</ref>, STFAN <ref type="bibr" target="#b29">[30]</ref>, and Pan <ref type="bibr" target="#b14">[15]</ref>, due to our efficient more blurring based neighboring video frames exploitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel MB2D to exploit neighboring frames for efficient video deblurring. We showed that using more blurred images as additional inputs significantly improves performance, then proposed MBRNN that can synthesize them from neighboring frames, yielding substantially improved performance with existing deblurring methods, and finally proposed MSDR with a novel CRFM to outperform SOTAs in fast and memory-efficient ways.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Difference maps between (a) the input blurred image (B 11 ) and (b) a more blurred image (B 15 ). More blurred image extracts high frequency information around edges for sharp regions by small motions (c) and blurred regions by large motions (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>An overview of our MB2D that consists of (a) MB (multi-blurring) and (b) D (deblurring) steps. (a) For given video frames, our MBRNN (denoted by NetB, parameter shared) progressively generates more blurred images by taking previous output and recurrent feature maps. (b) MSDR (denoted by NetD, parameter shared) uses "coarse-to-fine" framework with estimated more blurred images from (a) and the input frame. Connecting recurrent feature map from (a) is also used to restore the latent sharp image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>illustrates our proposed MB2D framework that contains the MB step with MBRNN and the D step with MSDR. Our proposed MBRNN in Figure 4(a) aims to progressively predict more blurred imagesB {n+2,n+4,n+6} t from the input blurred frame and its neighboring video frames B n {t−1,t,t+1} where B n {t−1,t,t+1} denotes the set of B n t−1 , B n t , B n t+1 . Our proposed MSDR in Figure 4(b) uses the input blurred image B n t , the predicted more blurred im-agesB {n+2,n+4,n+6} t as well as the connecting recurrent feature map (CRFM) from MBRNN for multi-scale based video deblurring of estimating the sharp latent image B 1 t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(b-d) Examples of multi-blurrings from MBRNN. ∆ is the absolute difference map between the input center frame (a) and an estimated more blurred image, showing progressively estimated differences by appending small motion blurs to the center frame. (e) Spectral densities of MBRNN outputs, showing progressive degradations of frequency components for more blurred images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(Î (s+1) ), F (s) ) (4) where s is a down-scaling index (s = 1, 2, 3 for ×1, ×2, ×4 downsamplings, respectively),B m,(s) t is the set ofB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>index s,Î (s) is a restored sharp image with the scale index s, F (s) denotes the downsampled connecting recurrent feature map from MBRNN, and Net D is the modified baseline network from Net B for for multiscale framework (see supplemental for details), and U p is a bilinear up-sampling. Our MSDR iteratively restores the latent sharp image from down-scales to the original scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Deblurring results with the input and/or ideal more blurred images on the GoPro validation dataset. Sets 1, . . . , 5 denote B ) 29.14 31.82 32.39 32.47 32.39</figDesc><table><row><cell>{11} t</cell><cell>, B t {11,13}</cell><cell cols="2">, ..., B t {11,13,...,19}</cell><cell>, respectively.</cell><cell></cell></row><row><cell></cell><cell>Set</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="2">PSNR(dB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4. Multi-Blurring To Deblurring (MB2D)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Performance comparisons for existing video deblurring</cell></row><row><cell cols="5">approaches (one-stage (OS), multi-temporal (MT), multi-scale</cell></row><row><cell cols="4">(MS)) with/without MBRNN (MB) and CRFM.</cell><cell></cell></row><row><cell></cell><cell cols="4">MB CRFM PSNR SSIM Param(M)</cell></row><row><cell>(h) OS</cell><cell>X</cell><cell>-</cell><cell>29.29 0.894</cell><cell>2.6</cell></row><row><cell>(i) OS</cell><cell>O</cell><cell>X</cell><cell>30.37 0.911</cell><cell>5.2</cell></row><row><cell>(j) OS</cell><cell>O</cell><cell>O</cell><cell>30.94 0.922</cell><cell>5.4</cell></row><row><cell>(k) MT</cell><cell>X</cell><cell>-</cell><cell>29.95 0.906</cell><cell>2.8</cell></row><row><cell>(l) MT</cell><cell>O</cell><cell>X</cell><cell>30.59 0.914</cell><cell>5.4</cell></row><row><cell>(m) MT</cell><cell>O</cell><cell>O</cell><cell>30.83 0.920</cell><cell>5.6</cell></row><row><cell>(n) MS</cell><cell>X</cell><cell>-</cell><cell>29.65 0.901</cell><cell>2.6</cell></row><row><cell>(o) MS</cell><cell>O</cell><cell>X</cell><cell>30.62 0.916</cell><cell>5.2</cell></row><row><cell>(p) MS</cell><cell>O</cell><cell>O</cell><cell>31.19 0.926</cell><cell>5.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study with different input frames to MBRNN.</figDesc><table><row><cell></cell><cell>1 frame</cell><cell>3 frames</cell></row><row><cell cols="3">Output PSNR SSIM PSNR SSIM</cell></row><row><cell>B n+2 t</cell><cell cols="2">44.73 0.994 46.63 0.995</cell></row><row><cell>B n+4 t</cell><cell cols="2">40.89 0.987 43.86 0.992</cell></row><row><cell>B n+6 t</cell><cell cols="2">38.54 0.980 42.63 0.990</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). OF is optical flow and MB is our MBRNN.</figDesc><table><row><cell cols="4">Approach Preproc PSNR SSIM Param(M)</cell></row><row><cell>Tao [23]</cell><cell>-</cell><cell>29.52 0.899</cell><cell>6.9</cell></row><row><cell>Tao  *</cell><cell>MB</cell><cell>31.12 0.925</cell><cell>9.7</cell></row><row><cell>Zhang [26]</cell><cell>-</cell><cell>29.91 0.905</cell><cell>5.4</cell></row><row><cell>Zhang  *</cell><cell>MB</cell><cell>30.85 0.920</cell><cell>8.7</cell></row><row><cell>Su [20]</cell><cell>OF</cell><cell>30.11 0.911</cell><cell>25.4</cell></row><row><cell>Su  *</cell><cell>MB</cell><cell>30.63 0.915</cell><cell>18.6</cell></row><row><cell>Pan [15]</cell><cell>OF</cell><cell>30.40 0.908</cell><cell>16.19</cell></row><row><cell>Pan  *</cell><cell>MB</cell><cell>30.97 0.923</cell><cell>9.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Benchmark qualitative results on the GoPro test dataset<ref type="bibr" target="#b12">[13]</ref> for PSNR (dB), SSIM, parameter size (Param in Million) and run time (second).</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR SSIM Param(M) Time</cell></row><row><cell>Kim [9]</cell><cell>26.82 0.825</cell><cell>0.92</cell><cell>0.13</cell></row><row><cell>Su [20]</cell><cell>27.31 0.826</cell><cell>16.67</cell><cell>2.38</cell></row><row><cell>EDVR [15]</cell><cell>26.83 0.843</cell><cell>-</cell><cell>-</cell></row><row><cell>Nah [14]</cell><cell>29.97 0.895</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">STFAN [30] 28.59 0.861</cell><cell>5.37</cell><cell>0.15</cell></row><row><cell>Zhong [29]</cell><cell>31.07 0.902</cell><cell>1.76</cell><cell>0.54</cell></row><row><cell>Pan [15]</cell><cell>31.67 0.928</cell><cell>16.19</cell><cell>1.73</cell></row><row><cell>Ours</cell><cell>32.16 0.953</cell><cell>5.42</cell><cell>0.27</cell></row><row><cell cols="4">Table 7. Benchmark qualitative results on the Su test dataset [20]</cell></row><row><cell cols="4">for PSNR (dB), SSIM, parameter size (Param in Million) and run</cell></row><row><cell>time (second).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">PSNR SSIM Param(M) Time</cell></row><row><cell>Kim [9]</cell><cell>29.95 0.869</cell><cell>0.92</cell><cell>0.13</cell></row><row><cell>Su [20]</cell><cell>30.01 0.888</cell><cell>16.67</cell><cell>2.38</cell></row><row><cell>EDVR [15]</cell><cell>28.51 0.864</cell><cell>-</cell><cell>-</cell></row><row><cell>Nah [14]</cell><cell>30.80 0.899</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">STFAN [30] 31.15 0.905</cell><cell>5.37</cell><cell>0.15</cell></row><row><cell>Pan [15]</cell><cell>32.13 0.927</cell><cell>16.19</cell><cell>1.73</cell></row><row><cell>Ours</cell><cell>32.34 0.947</cell><cell>5.42</cell><cell>0.27</cell></row><row><cell cols="4">more blurred images and with CRFM. Here, we investi-</cell></row><row><cell cols="4">gate the feasibility of using our proposed MB2D (espe-</cell></row><row><cell cols="4">cially MBRNN) along with other existing SOTA meth-</cell></row><row><cell cols="2">ods: Tao [23], Zhang [26], Su</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Douglas-Rachford networks: Learning both the image prior and data fidelity terms for blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raied</forename><surname>Aljadaany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10235" to="10244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust dual motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Registration based non-uniform motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGF</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2183" to="2192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video deblurring for hand-held cameras using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hand-held video deblurring via efficient fourier aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gyro-based multi-image deconvolution for removing handshake blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3366" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized video deblurring for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5426" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4038" to="4047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fundamentals of digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Prentice-Hall, Inc</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeblurGAN: blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeblurGAN-v2: deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with intra-frame iterations for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8102" to="8111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3043" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>with incremental temporal training. ECCV, 2020. 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image enhancement via adaptive unsharp masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Polesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V John</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust multichannel blind deconvolution via fast alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Sroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1687" to="1700" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1279" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3606" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EDVR: video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-shot imaging: joint alignment, deblurring and resolution-enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2925" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiimage blind deblurring using a coupled adaptive sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1051" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2737" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal recurrent neural network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatio-temporal filter adaptive network for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deconvolving PSFs for a better motion deblurring using multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Filipšroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="636" to="647" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
