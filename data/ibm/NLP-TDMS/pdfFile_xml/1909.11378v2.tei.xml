<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
							<email>longyin.wen@jd.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD Digits</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
							<email>yanjun@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>zhaochen@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff5">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual categorization (FGVC) is an important but challenging task due to high intra-class variances and low inter-class variances caused by deformation, occlusion, illumination, etc. An attention convolutional binary neural tree architecture is presented to address those problems for weakly supervised FGVC. Specifically, we incorporate convolutional operations along edges of the tree structure, and use the routing functions in each node to determine the root-to-leaf computational paths within the tree. The final decision is computed as the summation of the predictions from leaf nodes. The deep convolutional operations learn to capture the representations of objects, and the tree structure characterizes the coarse-to-fine hierarchical feature learning process. In addition, we use the attention transformer module to enforce the network to capture discriminative features. Several experiments on the CUB-200-2011, Stanford Cars and Aircraft datasets demonstrate that our method performs favorably against the state-ofthe-arts. Code can be found at https://isrc.iscas. ac.cn/gitlab/research/acnet. * Contributed equally. † Corresponding author: Libo Zhang (libo@iscas.ac.cn). <ref type="figure">Figure 1</ref>: Exemplars of fine-gained visual categorization. FGVC is challenging due to two reasons: (a) high intra-class variances: the birds belonging to the same category usually present significant different appearance, such as illumination variations (the 1st column), view-point changes (the 2nd column), clutter background (the 3rd column) and occlusion (the 4th column); (b) low interclass variances: the birds in different columns belong to different categories, but share similar appearance in the same rows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-Grained Visual Categorization (FGVC) aims to distinguish subordinate objects categories, such as different species of birds <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b54">52]</ref>, and flowers <ref type="bibr" target="#b0">[1]</ref>. The high intraclass and low inter-class visual variances caused by deformation, occlusion, and illumination, make FGVC to be a highly challenging task.</p><p>Recently, the FGVC task is dominated by the convolutional neural network (CNN) due to its amazing classification performance. Some methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> focus on extracting discriminative subtle parts for accurate results. How-ever, it is difficult for a single CNN model to describe the differences between subordinate classes (see <ref type="figure">Figure 1</ref>). In <ref type="bibr" target="#b33">[34]</ref>, the object-part attention model is proposed for FGVC, which uses both object and part attentions to exploit the subtle and local differences to distinguish subcategories. It demonstrates the effectiveness of using multiple deep models concentrating on different object regions in FGVC.</p><p>Inspired by <ref type="bibr" target="#b41">[41]</ref>, we design an attention convolutional binary neural tree architecture (ACNet) for weakly supervised FGVC. It incorporates convolutional operations along the edges of the tree structure, and use the routing functions in each node to determine the root-to-leaf computational paths within the tree as deep neural networks. This designed architecture makes our method inherits the representation learning ability of the deep convolutional model, and the coarse-to-fine hierarchical feature learning process. In this way, different branches in the tree structure focus on different local object regions for classification. The final decision is computed as the summation of the predictions from all leaf nodes. Meanwhile, we use the attention transformer to enforce the tree network to capture discriminative features for accurate results. The negative log-likelihood loss is adopted to train the entire network in an end-to-end fashion by stochastic gradient descent with back-propagation.</p><p>Notably, in contrast to the work in <ref type="bibr" target="#b41">[41]</ref> adaptively growing the tree structure in learning process, our method uses a complete binary tree structure with the pre-specified depth and the soft decision scheme to learn discriminative features in each root-to-leaf path, which avoids the pruning error and reduces the training time. In addition, the attention transformer module is used to further help our network to achieve better performance. Several experiments are conducted on the CUB-200-2011 <ref type="bibr" target="#b42">[42]</ref>, Stanford Cars <ref type="bibr" target="#b24">[25]</ref>, and Aircraft <ref type="bibr" target="#b31">[32]</ref> datasets, demonstrating the favorable performance of the proposed method compared to the state-ofthe-art methods. We also carried out the ablation study to comprehensively understand the influences of different components in the proposed method.</p><p>The main contributions of this paper are summarized as follows. <ref type="formula" target="#formula_4">(1)</ref> We propose a new attention convolutional binary neural tree architecture for FGVC. <ref type="bibr" target="#b1">(2)</ref> We introduce the attention transformer to facilitate coarse-to-fine hierarchical feature learning in the tree network. (3) Extensive experiments on three challenging datasets (i.e., CUB-200-2011, Stanford Cars, and Aircraft) show the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Deep supervised methods. Some algorithms <ref type="bibr" target="#b53">[51,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">50]</ref> use object annotations or even dense part/keypoint annotations to guide the training of deep CNN model for FGVC. Zhang et al. <ref type="bibr" target="#b53">[51]</ref> propose to learn two detectors, i.e., the whole object detector and the part detector, to predict the fine-grained categories based on the pose-normalized representation. Liu et al. <ref type="bibr" target="#b30">[31]</ref> propose a fully convolutional attention networks that glimpses local discriminative regions to adapte to different fine-grained domains. The method in <ref type="bibr" target="#b17">[18]</ref> construct the part-stacked CNN architecture, which explicitly explains the fine-grained recognition process by modeling subtle differences from object parts. In <ref type="bibr" target="#b51">[50]</ref>, the proposed network consists of detection and classification sub-networks. The detection sub-network is used to generate small semantic part candidates for detection; while the classification sub-network can extract features from parts detected by the detection sub-network. However, these methods rely on labor-intensive part annotations, which limits their applications in real scenarios.</p><p>Deep weakly supervised method. To that end, more recent methods <ref type="bibr" target="#b54">[52,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46]</ref> only require image-level annotations. Zheng et al. <ref type="bibr" target="#b54">[52]</ref> introduce a multi-attention CNN model, where part generation and feature learning process reinforce each other for accurate results. Fu et al. <ref type="bibr" target="#b11">[12]</ref> develop a recurrent attention module to recursively learn discriminative region attention and region-based feature representation at multiple scales in a mutually reinforced way. Recently, Sun et al. <ref type="bibr" target="#b38">[38]</ref> regulate multiple object parts among different input images by using multiple attention region features of each input image. In <ref type="bibr" target="#b46">[46]</ref>, a bank of convolutional filters is learned to capture class-specific discriminative patches, through a novel asymmetric multi-stream architecture with convolutional filter supervision. However, the aforementioned methods merely integrate the attention mechanism in a single network, affecting their performance.</p><p>Decision tree. Decision tree is an effective algorithm for classification task. It selects the appropriate directions based on the characteristic of feature. The inherent ability of interpretability makes it as promising direction to pose insight into internal mechanism in deep learning. Xiao <ref type="bibr" target="#b49">[48]</ref> propose the principle of fully functioned neural graph and design neural decision tree model for categorization task. Frosst and Hinton <ref type="bibr" target="#b10">[11]</ref> develop a deep neural decision tree model to understand decision mechanism for particular test case in a learned network. Tanno et al. <ref type="bibr" target="#b41">[41]</ref> propose the adaptive neural trees that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree. In our work, we integrate the decision tree with neural network to implement sub-branch selection and representation learning simultaneously.</p><p>Attention mechanism. Attention mechanism has played an important role in deep learning to mimic human visual mechanism. In <ref type="bibr" target="#b50">[49]</ref>, the attention is used to make sure the student model focuses on the discriminative regions as teacher model does. In <ref type="bibr" target="#b20">[21]</ref>, the cascade attention mechanism is proposed to guide the different layers of CNN and concatenate them to gain discriminative representation as the input of final linear classifier. Hu et al. <ref type="bibr" target="#b15">[16]</ref> apply the attention mechanism from aspect of channels and allocate the different weights according to the contribution of each channel. The CBAM module in <ref type="bibr" target="#b47">[47]</ref> combines space region attentions with feature map attentions. In contrast to the aforementioned methods, we apply the attention mechanism on each branch of the tree architecture to sake the discriminative regions for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attention Convolutional Binary Neural Tree</head><p>Our ACNet model aims to classify each object sample in X to sub-categories, i.e., assign each sample in X with the category label Y , which consists of four modules, i.e., the backbone network, the branch routing, the attention transformer, and the label prediction modules, shown in <ref type="figure" target="#fig_0">Figure  2</ref>. We define the ACNet as a pair (T, O), where T defines the topology of the tree, and O denotes the set of operations along the edges of T. Notably, we use the full binary tree T = {V, E}, where V = {v 1 , · · · , v n } is the set of nodes, n is the total number of nodes, and E = {e 1 , · · · , e k } is the set of edges between nodes, k is the total number of edges. Since we use the full binary tree T, we have n = 2 h − 1 and k = 2 h − 2, where h is the height of T. Each node in T is formed by a routing module determining the sending path of samples, and the attention transformers are used as the operations along the edges.</p><p>Meanwhile, we use the asymmetrical architecture in the fully binary tree T, i.e., two attention transformers are used in the left edge, and one attention transformer is used in the right edge. In this way, the network is able to capture the different scales of features for accurate results. The detail architecture of our ACNet model is described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>Backbone network module. Since the discriminative regions in fine-grained categories are highly localized <ref type="bibr" target="#b46">[46]</ref>, we need to use a relatively small receptive field of the extracted features by constraining the size and stride of the convolutional filters and pooling kernels. The truncated network is used as the backbone network module to extract features, which is pre-trained on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b34">[35]</ref>. Similar to <ref type="bibr" target="#b38">[38]</ref>, we use the input image size 448 × 448 instead of the default 224 × 224. Notably, ACNet can also work on other pre-trained networks, such as ResNet <ref type="bibr" target="#b14">[15]</ref> and Inception V2 <ref type="bibr" target="#b18">[19]</ref>. In practice, we use VGG-16 <ref type="bibr" target="#b37">[37]</ref> (retaining the layers from conv1 1 to conv4 3) and ResNet-50 <ref type="bibr" target="#b14">[15]</ref> (retaining the layers from res 1 to res 4) networks as the backbone in this work. Branch routing module. As described above, we use the branch routing module to determine which child (i.e., left or right child) the samples would be sent to. Specifically, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b), the i-th routing module R k i (·) at the k-th layer uses one convolutional layer with the kernel size 1 × 1, followed by a global context block <ref type="bibr" target="#b3">[4]</ref>. The global context block is an improvement of the simplified non-local (NL) block <ref type="bibr" target="#b44">[44]</ref> and Squeeze-Excitation (SE) block <ref type="bibr" target="#b15">[16]</ref>, which shares the same implementation with the simplified NL block on the context modeling and fusion steps, and shares the transform step with the SE block. In this way, the context information is integrated to better describe the objects. After that, we use the global average pooling <ref type="bibr" target="#b26">[27]</ref>, element-wise square-root and L2 normalization <ref type="bibr" target="#b27">[28]</ref>, and a fully connected layer with the sigmoid activation function to produce a scalar value in [0, 1] indicating the probability of samples being sent to the left or right sub-branches. Let φ k i (x j ) denote the output probability of the j-th sample x j ∈ X being sent to the right sub-branch produced by the branch routing module</p><formula xml:id="formula_0">R k i (x j ), where φ k i (x j ) ∈ [0, 1], i = 1, · · · , 2 k−1 .</formula><p>Thus, we have the probability of the sample x j ∈ X being sent to the left sub-branch to be</p><formula xml:id="formula_1">1 − φ k i (x j ). If the probability φ k i (x j )</formula><p>is larger than 0.5, we prefer the left path instead of the right one; otherwise, the left branch dominates the final decision. Attention transformer. The attention transformer module is used to enforce the network to capture discriminative features, see <ref type="figure" target="#fig_1">Figure 3</ref>. According to the fact that the empirical receptive field is much smaller than theoretical receptive field in deep networks <ref type="bibr" target="#b29">[30]</ref>, the discriminative representation should be formed by larger receptive field in new-added layers of our proposed tree structure. To this end, we intergate the Atrous Spatial Pyramid Pooling (ASPP) module <ref type="bibr" target="#b4">[5]</ref> into the attention transformer. Specifically, ASPP module provides different feature maps with each characterized by a different scale/receptive field and an attention module. Then, multi-scale feature maps are generated by four parallel dilated convolutions with different dilated rates, i.e., 1, 6, 12, 18. Following the parallel dilated convolution layers, the concatenated feature maps are fused by one convolutional layer with kernel 1 × 1 and stride 1. Following the ASPP module, we insert an attention module, which generates a channel attention map with the size R C ×1×1 using a batch normalization (BN) layer <ref type="bibr" target="#b18">[19]</ref>, a global average pooling (GAP) layer, a fully connected (FC) layer and ReLU activation function, and a FC layer and sigmoid function. In this way, the network is guided to focus on meaningful features for accurate results. Label prediction. For each leaf node in our ACNet model, we use the label prediction module P i (i.e., i = 1, · · · , 2 h−1 ) to predict the subordinate category of the object x j , see <ref type="figure" target="#fig_0">Figure 2</ref>. Let r k i (x j ) to be the accumulated probability of the object x j passing from the root node to the i-th node at the k-th layer. For example, if the root to the node R k i (·) path on the tree is R 1 1 , R 2 1 , · · · , R k 1 , i.e., the object x j is always sent to the left child, we have</p><formula xml:id="formula_2">r k i (x j ) = k i=1 φ i 1 (x j )</formula><p>. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the la-  bel prediction module is formed by a batch normalization layer, a convolutional layer with kernel size 1 × 1, a maxpooling layer, a sqrt and L2 normalization layer, and a fully connected layer. Then, the final prediction C(x j ) of the j-th object x j is computed as the summation of all leaf predictions multiplied with the accumulated probability generated by the passing branch routing modules, i.e.,</p><formula xml:id="formula_3">C(x j ) = 2 h−1 i=1 P i (x j )r h i (x j ).</formula><p>We would like to emphasize that C(x j ) 1 = 1, i.e., the summation of confidences of x j belonging to all subordinate classes equal to 1,</p><formula xml:id="formula_4">C(x j ) 1 = 2 h−1 i=1 P i (x j )r h i (x j ) 1 = 1,<label>(1)</label></formula><p>where r h i (x j ) is the accumulated probability of the i-th node at the leaf layer. We present a short description to prove that C(x j ) 1 = 1 as follows.</p><p>Proof. Let r k i (·) be the accumulated probability of the i-th branch routing module R k i (·) at the k-th layer. Thus, the accumulated probabilities of the left and right children corresponding to R k i (·) are r k+1 2i−1 (·) and r k+1 2i (·), respectively. At first, we demonstrate that the summation of the accumulated probabilities r k+1 2i−1 (·) and r k+1 2i (·) is equal to the accumulated probability of their parent r k i (x j ). That is,</p><formula xml:id="formula_5">r k+1 2i−1 (x j ) + r k+1 2i (x j ) = φ k+1 2i−1 (x j ) · r k i (x j ) + φ k+1 2i (x j ) · r k i (x j ) = φ k+1 2i−1 (x j ) · r k i (x j ) + 1 − φ k+1 2i−1 (x j ) · r k i (x j ) = r k i (x j ).<label>(2)</label></formula><p>Meanwhile, due to the fully binary tree T in ACNet, we have</p><formula xml:id="formula_6">2 h−1 i=1 r h i (x j ) = 2 h−2 i=1 r h 2i−1 (x j ) + r h 2i (x j )</formula><p>. We can further get</p><formula xml:id="formula_7">2 h−1 i=1 r h i (x j ) = 2 h−2 i=1 r h−1 i (x j )</formula><p>. This process is carried out iteratively, and we have</p><formula xml:id="formula_8">2 h−1 i=1 r h i (x j ) = · · · = r 1 1 (x j ) = 1.</formula><p>In addition, since the category prediction P i (x i ) is generated by the softmax layer (see <ref type="figure" target="#fig_0">Figure 2</ref>), we have P i (x j ) 1 = 1. Thus,</p><formula xml:id="formula_9">C(x j ) 1 = 2 h−1 i=1 P i (x j )r h i (x j ) 1 = 2 h−1 i=1 P i (x j ) 1 r h i (x j ) = 1.<label>(3)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, when occlusion happens, AC-Net can still localize some discriminative object parts and context information of the bird. Although high intra-class visual variances always happen in FGVC, ACNet uses a coarse-to-fine hierarchical feature learning process to exploit the discriminative feature for classification. In this way, different branches in the tree structure focus on different fine-grained object regions for accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Data augmentation. In the training phase, we use the cropping and flipping operations to augment data to construct a robust model to adapt to variations of objects. That is, we first rescale the original images such that their shorter side is 512 pixels. After that, we randomly crop the patches with the size 448 × 448, and randomly flip them to generate the training samples.</p><p>Loss function. The loss function for our ACNet is formed by two parts, i.e., the loss for the predictions of leaf nodes, and the loss for the final prediction, computed by the summation over all predictions from the leaf nodes. That is,</p><formula xml:id="formula_10">L = L C(x j ), y * + 2 h−1 i=1 L P i (x j ), y * ,<label>(4)</label></formula><p>where h is the height of the tree T, L C(x j ), y * is the negative logarithmic likelihood loss of the final prediction C(x j ) and the ground truth label y * , and L P i (x j ), y * is the negative logarithmic likelihood loss of the i-th leaf prediction and the ground truth label y * .</p><p>Optimization. The backbone network in our ACNet method is pre-trained on the ImageNet dataset. Besides, the "xavier" method <ref type="bibr" target="#b13">[14]</ref> is used to randomly initialize the parameters of the additional convolutional layers. The entire training process is formed by two stages.</p><p>• For the first stage, the parameters in the truncated VGG-16 network are fixed, and other parameters are trained with 60 epochs. The batch size is set to 24 in training with the initial learning rate 1.0. The learning rate is gradually divided by 4 at the 10-th, 20-th, 30-th, and 40-th epochs.</p><p>• In the second stage, we fine-tune the entire network for 200 epochs. We use the batch size 16 in training with the initial learning rate 0.001. The learning rate is gradually divided by 10 at the 30-th, 40-th, and 50-th epochs.</p><p>We use the SGD algorithm to train the network with 0.9 momentum, and 0.000005 weight decay in the first stage and 0.0005 weight decay in the second stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Several experiments on three FGVC datasets, i.e., CUB-200-2011 <ref type="bibr" target="#b42">[42]</ref>, Stanford Cars <ref type="bibr" target="#b24">[25]</ref>, and Aircraft <ref type="bibr" target="#b31">[32]</ref>, are conducted to demonstrate the effectiveness of the proposed method. Our method is implemented in the Caffe library <ref type="bibr" target="#b21">[22]</ref>. All models are trained on a workstation with a 3.26 GHz Intel processor, 32 GB memory, and one Nvidia V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on the CUB-200-2011 Dataset</head><p>The Caltech-UCSD birds dataset (CUB-200-2011) <ref type="bibr" target="#b42">[42]</ref> consists of 11, 788 annotated images in 200 subordinate categories, including 5, 994 images for training and 5, 794 images for testing. The fine-grained classification results are shown in <ref type="table" target="#tab_0">Table 1</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the best supervised method 1 , i.e., PN-CNN <ref type="bibr" target="#b1">[2]</ref> using both the object and part level annotations produces 85.4% top-1 accuracy on the CUB-200-2011 dataset. Without part-level anno-  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b55">53]</ref>, our method achieves the best results with 87.8% and 88.1% top-1 accuracy with different backbones. This is attributed to the designed attention transformer module and the coarse-to-fine hierarchical feature learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the Stanford Cars Dataset</head><p>The Stanford Cars dataset <ref type="bibr" target="#b24">[25]</ref> contains 16, 185 images from 196 classes, which is formed by 8, 144 images for training and 8, 041 images for testing. The subordinate categories are determined by the Make, Model, and Year of cars. As shown in <ref type="table" target="#tab_1">Table 2</ref>, previous methods using part-level annotations (i.e., FCAN <ref type="bibr" target="#b30">[31]</ref> and PA-CNN <ref type="bibr" target="#b23">[24]</ref>) only produces less than 93.0% top-1 accuracy. The recent weakly supervised method WS-DAN <ref type="bibr" target="#b16">[17]</ref> employs the complex Inception V3 backbone <ref type="bibr" target="#b39">[39]</ref> and designs the attention-guided data augmentation strategy to exploit discriminative object parts, achieving 93.0% top-1 accuracy. Without using any fancy data augmentation strategy, our method achieves the best top-1 accuracy, i.e., 94.3% with the VGG-16 backbone and 94.6% with the ResNet-50 backbone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on the Aircraft Dataset</head><p>The Aircraft dataset <ref type="bibr" target="#b31">[32]</ref> is a fine-grained dataset of 100 different aircraft variants formed by 10, 000 annotated images, which is divided into two subsets, i.e., the training set with 6, 667 images and the testing set with 3, 333 images. Specifically, the category labels are determined by the Model, Variant, Family and Manufacturer of airplanes. The evaluation results are presented in <ref type="table" target="#tab_2">Table 3</ref>. Our AC-Net method outperforms the most compared methods, especially with the same VGG-16 backbone. Besides, our model performs on par with the state-of-the-art method DCL <ref type="bibr" target="#b6">[7]</ref>, i.e., 91.2% vs. 91.5% top-1 accuracy for the VGG-16 backbone and 93.0% vs. 92.4% top-1 accuracy for the ResNet-50 backbone. The operations along different root-to-leaf paths in our tree architecture T focus on exploiting discriminative features on different object regions, which help each other to achieve the best performance in FGVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We study the influence of some important parameters and different components of ACNet on the CUB-200-2011 dataset <ref type="bibr" target="#b42">[42]</ref>. Notably, we employ the VGG-16 backbone in the experiment. The Grad-CAM method <ref type="bibr" target="#b36">[36]</ref> is used to generate the heatmaps to visualize the responses of branch routing and leaf nodes.</p><p>Effectiveness of the tree architecture T. To validate the effectiveness of the tree architecture design, we construct two variants, i.e., VGG and w/ Tree, of our ACNet method. Specifically, we construct the VGG method by only using the VGG-16 backbone network for classification, and further integrate the tree architecture to form the w/ Tree method. The evaluation results are reported in <ref type="figure" target="#fig_4">Figure 6</ref>. We find that using the tree architecture significantly improves the accuracy, i.e., 3.025% improvements in top-1 accuracy, which demonstrates the effectiveness of the designed tree architecture T in our ACNet method.</p><p>Height of the tree T. To explore the effect of the height of the tree T, we construct four variants with different heights of tree in <ref type="table" target="#tab_3">Table 4</ref>. Notably, the tree T is degenerated to a single node when the height of the tree is set to 1, i.e., only the backbone VGG-16 network is used in classification. As shown in <ref type="table" target="#tab_3">Table 4</ref>, we find that our ACNet achieves the best performance (i.e., 87.8% top-1 accuracy) with the height of tree equals to 3. If we set h ≤ 2, there are limited number of parameters in our ACNet model, which are not enough to represent the significant variations of the subordinate categories. However, if we set h = 4, too many parameters with limited number of training data cause overfitting of our AC-Net model, resulting in 2.3% drop in the top-1 accuracy. To verify our hypothesis, we visualize the responses of all leaf nodes in ACNet with the height of 4 in <ref type="figure" target="#fig_3">Figure 5</ref>. We find that some leaf nodes focus on almost the same regions (see the 3rd and 4th columns).</p><p>Effectiveness of leaf nodes. To analyze the effectiveness of the individual leaf node, we calculate the accuracy of individual leaf predictions with height of 3, respectively. The accuracies of four individual leaf nodes are 85.8%, 86.2%, 86.7%, and 87.0% on CUB-200-2011 respectively. It shows that all leaf nodes are informative and fusion of them can  produce more accurate result (i.e., 87.8%). As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we observe that different leaf nodes concentrate on different regions of images. For example, the leaf node corresponding to the first column focuses more on the background region, the leaf node corresponding to the second column focuses more on the head region, and the other two leaf nodes are more interested in the patches of wings and tail. The different leaf nodes help each other to construct more effective model for accurate results.</p><p>Asymmetrical architecture of the tree T. To explore the architecture design in T, we construct two variants, i.e., one uses the symmetry architecture, and another one uses the asymmetrical architecture, and set the height of the tree T to be 3. The evaluation results are reported in <ref type="table" target="#tab_4">Table 5</ref>. It can be seen that the proposed method produces 86.2% top-1 accuracy using the symmetrical architecture. If we use the asymmetrical architecture, the top-1 accuracy is improved 1.6% to 87.8%. We speculate that the asymmetrical architecture is able to fuse various features with different receptive fields for better performance.</p><p>Effectiveness of the attention transformer module. We construct a variant "w/ Tree-Attn", of the proposed ACNet model, to validate the effectiveness of the attention transformer module in <ref type="figure" target="#fig_4">Figure 6</ref>. Specifically, we add the attention block in the transformer module in the "w/ Tree"    method to construct the "w/ Tree-Attn" method. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the "w/ Tree-Attn" method performs consistently better than the "w/ Tree" method, producing higher top-1 accuracy with different number of channels, i.e., improving 0.4% top-1 accuracy in average, which demonstrates that the attention mechanism is effective for FGVC.</p><p>To further investigate the effect of ASPP module in our proposed model, we also conduct the "w/ Tree-ASPP" method, a variant of proposed ACNet model, where the only difference lies on between one convolution layer or ASPP module in the attention transformer module. As illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>, the attention transformer with ASPP module achieves better accuracy than the one with only one convolution layer. It indicates that the ASPP module improves the global performance by parallel dilated convolution layers with different dilated rates. Specifically, the "w/ Tree-ASPP" method improves 0.5% top-1 accuracy in average. We can conclusion that multi-scale embedding and different dilated convolutions in ASPP module can facilitate helping the proposed tree network to obtain robust performance.</p><p>Components in the branch routing module. We analyze the effectiveness of the global context block <ref type="bibr" target="#b3">[4]</ref> in the branch routing module in <ref type="figure" target="#fig_4">Figure 6</ref>. Our ACNet method produces the best results with different number of channels in the branch routing module; while the top-1 accuracy drops 0.275% in average after removing the global context block. Meanwhile, we also study the effectiveness of the pooling strategy in the branch routing module in <ref type="table" target="#tab_5">Table 6</ref>. We observe that using the global max-pooling (GMP) instead of the global average pooling (GAP) leads to 0.6% top-1 accuracy drop on the CUB-200-2011 dataset. We speculate that the GAP operation encourages the filter to focus on high average response regions instead of the only maximal ones, which is able to integrate more context information for better performance.</p><p>Coarse-to-fine hierarchical feature learning process. The branch routing modules focus on different semantic regions (e.g., different object parts) or context information (e.g., background) at different levels, e.g., R 1 1 , R 2 1 , and R 2 2 in <ref type="figure" target="#fig_0">Figure 2</ref>. As the example Bobolink shown in <ref type="figure" target="#fig_5">Figure 7</ref>, the R 1 1 module focuses on the whole bird region at level-1; the R 2 1 and R 2 2 modules focus on the wing and head regions of the bird at level-2. As shown in the first row in <ref type="figure" target="#fig_3">Figure  5</ref>, the four leaf nodes focus on several fine-grained object parts at level-3, e.g., different parts of the head region. In this way, our ACNet uses the coarse-to-fine hierarchical feature learning process to exploit discriminative features for more accurate results. This phenomenon demonstrates that our hierarchical feature extraction process in the tree T architecture gradually enforces our model to focus on more discriminative detail regions of object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present an attention convolutional binary neural tree (ACNet) for weakly supervised FGVC. Specifically, different root-to-leaf paths in the tree network focus on different discriminative regions using the attention transformer inserted into the convolutional operations along edges. The final decision is produced by max-voting the predictions from leaf nodes. The experiments on several challenging datasets show the effectiveness of ACNet. We present how we design the tree structure using coarse-tofine hierarchical feature learning process in detail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overview of our ACNet model, formed by (a) the backbone network module, (b) the branch routing module, (c) the attention transformer module, and (d) the label prediction module. We show an example image in Fish Crow. Best visualization in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the attention transformer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the responses in different leaf nodes in our ACNet method. Each column presents a response heatmap of each leaf node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Responses of all leaf nodes in the tree with height of 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Effect of the various components in the proposed ACNet method on the CUB-200-2011 dataset<ref type="bibr" target="#b42">[42]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the responses in different branch routing modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The fine-grained classification results on the CUB-200-2011 dataset<ref type="bibr" target="#b42">[42]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone Annotation Top-1 Acc. (%)</cell></row><row><cell>FCAN [31]</cell><cell>ResNet-50</cell><cell></cell><cell>84.7</cell></row><row><cell>B-CNN [29]</cell><cell>VGG-16</cell><cell></cell><cell>85.1</cell></row><row><cell>SPDA-CNN [50]</cell><cell>CaffeNet</cell><cell></cell><cell>85.1</cell></row><row><cell>PN-CNN [2]</cell><cell>Alex-Net</cell><cell></cell><cell>85.4</cell></row><row><cell>STN [20]</cell><cell>Inception</cell><cell>×</cell><cell>84.1</cell></row><row><cell>B-CNN [29]</cell><cell>VGG-16</cell><cell>×</cell><cell>84.0</cell></row><row><cell>CBP [13]</cell><cell>VGG-16</cell><cell>×</cell><cell>84.0</cell></row><row><cell>LRBP [23]</cell><cell>VGG-16</cell><cell>×</cell><cell>84.2</cell></row><row><cell>FCAN [31]</cell><cell>ResNet-50</cell><cell>×</cell><cell>84.3</cell></row><row><cell>RA-CNN [12]</cell><cell>VGG-19</cell><cell>×</cell><cell>85.3</cell></row><row><cell>HIHCA [3]</cell><cell>VGG-16</cell><cell>×</cell><cell>85.3</cell></row><row><cell cols="2">Improved B-CNN [28] VGG-16</cell><cell>×</cell><cell>85.8</cell></row><row><cell>BoostCNN [33]</cell><cell>VGG-16</cell><cell>×</cell><cell>86.2</cell></row><row><cell>KP [8]</cell><cell>VGG-16</cell><cell>×</cell><cell>86.2</cell></row><row><cell>MA-CNN [52]</cell><cell>VGG-19</cell><cell>×</cell><cell>86.5</cell></row><row><cell>MAMC [38]</cell><cell>ResNet-101</cell><cell>×</cell><cell>86.5</cell></row><row><cell>MaxEnt [10]</cell><cell>DenseNet-161</cell><cell>×</cell><cell>86.5</cell></row><row><cell>HBPASM [40]</cell><cell>Resnet-34</cell><cell>×</cell><cell>86.8</cell></row><row><cell>DCL [7]</cell><cell>VGG-16</cell><cell>×</cell><cell>86.9</cell></row><row><cell>KERL w/ HR [6]</cell><cell>VGG-16</cell><cell>×</cell><cell>87.0</cell></row><row><cell>TASN [53]</cell><cell>VGG-19</cell><cell>×</cell><cell>87.1</cell></row><row><cell>DFL-CNN [46]</cell><cell>ResNet-50</cell><cell>×</cell><cell>87.4</cell></row><row><cell>DCL [7]</cell><cell>ResNet-50</cell><cell>×</cell><cell>87.8</cell></row><row><cell>TASN [53]</cell><cell>ResNet-50</cell><cell>×</cell><cell>87.9</cell></row><row><cell>Ours</cell><cell>VGG-16</cell><cell>×</cell><cell>87.8</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>×</cell><cell>88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The fine-grained classification results on the Stanford Cars dataset<ref type="bibr" target="#b24">[25]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone Annotation Top-1 Acc. (%)</cell></row><row><cell>FCAN [31]</cell><cell>ResNet-50</cell><cell></cell><cell>91.3</cell></row><row><cell>PA-CNN [24]</cell><cell>VGG-19</cell><cell></cell><cell>92.6</cell></row><row><cell>FCAN [31]</cell><cell>ResNet-50</cell><cell>×</cell><cell>89.1</cell></row><row><cell>B-CNN [29]</cell><cell>VGG-16</cell><cell>×</cell><cell>90.6</cell></row><row><cell>LRBP [23]</cell><cell>VGG-16</cell><cell>×</cell><cell>90.9</cell></row><row><cell>HIHCA [3]</cell><cell>VGG-16</cell><cell>×</cell><cell>91.7</cell></row><row><cell cols="2">Improved B-CNN [28] VGG-16</cell><cell>×</cell><cell>92.0</cell></row><row><cell>BoostCNN [33]</cell><cell>VGG-16</cell><cell>×</cell><cell>92.1</cell></row><row><cell>KP [8]</cell><cell>VGG-16</cell><cell>×</cell><cell>92.4</cell></row><row><cell>RA-CNN [12]</cell><cell>VGG-19</cell><cell>×</cell><cell>92.5</cell></row><row><cell>MA-CNN [52]</cell><cell>VGG-19</cell><cell>×</cell><cell>92.8</cell></row><row><cell>MAMC [38]</cell><cell>ResNet-101</cell><cell>×</cell><cell>93.0</cell></row><row><cell>MaxEnt [10]</cell><cell>DenseNet-161</cell><cell>×</cell><cell>93.0</cell></row><row><cell>WS-DAN [17]</cell><cell>Inception v3</cell><cell>×</cell><cell>93.0</cell></row><row><cell>DFL-CNN [46]</cell><cell>ResNet-50</cell><cell>×</cell><cell>93.1</cell></row><row><cell>HBPASM [40]</cell><cell>Resnet-34</cell><cell>×</cell><cell>93.8</cell></row><row><cell>TASN [53]</cell><cell>VGG-19</cell><cell>×</cell><cell>93.2</cell></row><row><cell>TASN [53]</cell><cell>ResNet-50</cell><cell>×</cell><cell>93.8</cell></row><row><cell>DCL [7]</cell><cell>VGG-16</cell><cell>×</cell><cell>94.1</cell></row><row><cell>DCL [7]</cell><cell>ResNet-50</cell><cell>×</cell><cell>94.5</cell></row><row><cell>Ours</cell><cell>VGG-16</cell><cell>×</cell><cell>94.3</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>×</cell><cell>94.6</cell></row><row><cell cols="4">tation, MAMC [38] produces 86.5% top-1 accuracy using</cell></row><row><cell cols="4">two attention branches to learn discriminative features in</cell></row><row><cell cols="4">different regions. KERL w/ HR [6] designs a single deep</cell></row><row><cell cols="4">gated graph neural network to learn discriminative features,</cell></row><row><cell cols="4">achieving better performance, i.e., 87.0% top-1 accuracy.</cell></row><row><cell cols="4">Compared to the state-of-the-art weakly supervised meth-</cell></row><row><cell>ods</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The fine-grained classification results on the Aircraft dataset<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone Annotation Top-1 Acc. (%)</cell></row><row><cell>MG-CNN [43]</cell><cell>ResNet-50</cell><cell></cell><cell>86.6</cell></row><row><cell>MDTP [45]</cell><cell>VGG-16</cell><cell></cell><cell>88.4</cell></row><row><cell>RA-CNN [12]</cell><cell>VGG-19</cell><cell>×</cell><cell>88.2</cell></row><row><cell>MA-CNN [52]</cell><cell>VGG-19</cell><cell>×</cell><cell>89.9</cell></row><row><cell>B-CNN [29]</cell><cell>VGG-16</cell><cell>×</cell><cell>86.9</cell></row><row><cell>KP [8]</cell><cell>VGG-16</cell><cell>×</cell><cell>86.9</cell></row><row><cell>LRBP [23]</cell><cell>VGG-16</cell><cell>×</cell><cell>87.3</cell></row><row><cell>HIHCA [3]</cell><cell>VGG-16</cell><cell>×</cell><cell>88.3</cell></row><row><cell cols="2">Improved B-CNN [28] VGG-16</cell><cell>×</cell><cell>88.5</cell></row><row><cell>BoostCNN [33]</cell><cell>VGG-16</cell><cell>×</cell><cell>88.5</cell></row><row><cell cols="2">PC-DenseNet-161 [9] DenseNet-161</cell><cell>×</cell><cell>89.2</cell></row><row><cell>MaxEnt [10]</cell><cell>DenseNet-161</cell><cell>×</cell><cell>89.7</cell></row><row><cell>HBPASM [40]</cell><cell>Resnet-34</cell><cell>×</cell><cell>91.3</cell></row><row><cell>DFL-CNN [46]</cell><cell>ResNet-50</cell><cell>×</cell><cell>91.7</cell></row><row><cell>DCL [7]</cell><cell>VGG-16</cell><cell>×</cell><cell>91.2</cell></row><row><cell>DCL [7]</cell><cell>ResNet-50</cell><cell>×</cell><cell>93.0</cell></row><row><cell>Ours</cell><cell>VGG-16</cell><cell>×</cell><cell>91.5</cell></row><row><cell>Ours</cell><cell>ResNet-50</cell><cell>×</cell><cell>92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of the height of the tree T.</figDesc><table><row><cell cols="2">Height of T Top-1 Acc. (%)</cell></row><row><cell>1</cell><cell>82.2</cell></row><row><cell>2</cell><cell>86.0</cell></row><row><cell>3</cell><cell>87.8</cell></row><row><cell>4</cell><cell>85.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of tree architecture.</figDesc><table><row><cell>Mode</cell><cell cols="2">Level Top-1 Acc. (%)</cell></row><row><cell>symmetry</cell><cell>3</cell><cell>86.2</cell></row><row><cell>asymmetry</cell><cell>3</cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison between GMP and GAP.</figDesc><table><row><cell>Pooling</cell><cell>Top-1 Acc. (%)</cell></row><row><cell>GMP</cell><cell>87.2</cell></row><row><cell>GAP</cell><cell>87.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notably, the supervised method requires object or part level annotations, demanding significant human effort. Thus, most of recent methods focus on the weakly supervised methods, pushing the state-of-the-art weakly supervised methods surpassing the performance of previous supervised methods.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation for large-scale subcategory flower recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets. CoRR, abs/1406.2952</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge-embedded representation learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairwise confusion for finegrained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling a neural network into a soft decision tree. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1711.09784</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Compact bilinear pooling. CoRR, abs/1511.06062</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Partstacked CNN for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1804.02391</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding. CoRR, abs/1408</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>abs/1611.05109</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-grained classification of cervical cells using morphological and appearance based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1810.06058</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved bilinear pooling with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roy Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better. CoRR, abs/1506.04579</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1603.06765</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft. CoRR, abs/1306</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5151</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Boosted convolutional neural networks. In BMVC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Object-part attention driven discriminative localization for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
		<idno>abs/1704.01740</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<publisher>Andrej Karpathy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="834" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-grained classification via hierarchical bilinear pooling with aggregated slack mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meilian</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>Access, 7</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<title level="m">Adaptive neural trees. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6166" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mining discriminative triplets of patches for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1605.01130</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a CNN for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Joon-Young Lee, and In So Kweon. CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1807.06521</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">NDT: neual decision tree towards fully functioned neural graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<idno>abs/1712.05934</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Dimitris N</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SPDA-CNN: unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1903.06150</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
