<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STATE-OF-THE-ART SPEECH RECOGNITION USING MULTI-STREAM SELF-ATTENTION WITH DILATED 1D CONVOLUTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu</forename><forename type="middle">J</forename><surname>Han</surname></persName>
							<email>khan@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Prieto</surname></persName>
							<email>rprieto@asapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixing</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ASAPP Inc. Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STATE-OF-THE-ART SPEECH RECOGNITION USING MULTI-STREAM SELF-ATTENTION WITH DILATED 1D CONVOLUTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention has been a huge success for many downstream tasks in NLP, which led to exploration of applying selfattention to speech problems as well. The efficacy of selfattention in speech applications, however, seems not fully blown yet since it is challenging to handle highly correlated speech frames in the context of self-attention. In this paper we propose a new neural network model architecture, namely multi-stream self-attention, to address the issue thus make the self-attention mechanism more effective for speech recognition. The proposed model architecture consists of parallel streams of self-attention encoders, and each stream has layers of 1D convolutions with dilated kernels whose dilation rates are unique given stream, followed by a self-attention layer. The self-attention mechanism in each stream pays attention to only one resolution of input speech frames and the attentive computation can be more efficient. In a later stage, outputs from all the streams are concatenated then linearly projected to the final embedding. By stacking the proposed multi-stream self-attention encoder blocks and rescoring the resultant lattices with neural network language models, we achieve the word error rate of 2.2% on the test-clean dataset of the LibriSpeech corpus, the best number reported thus far on the dataset.</p><p>Index Terms-Speech recognition, multi-stream selfattention, dilated 1D convolution, neural network language model, word error rate</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Self-attention is the core component of the neural network architectures recently proposed in NLP <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> to achieve the state-of-the art performances in a number of downstream tasks. Transformer <ref type="bibr" target="#b0">[1]</ref> successfully replaced recurrent neural networks such as LSTMs with sinusoidal positional encoding and the self-attention mechanism to be context-aware on input word embeddings. BERT <ref type="bibr" target="#b1">[2]</ref> took the benefit from the success of Transformer to extend it to the autoencoding based pretraining model, which can be fine-tuned to reach the stateof-the-art performances for various downstream tasks. XLNet <ref type="bibr" target="#b2">[3]</ref>, as the very latest state-of-the-art pretraining model, outperformed BERT in a number of downstream tasks from question answering to document ranking, thanks to model training with targets being aware and relative positional encoding like its ancestor of Transformer-XL <ref type="bibr" target="#b3">[4]</ref>.</p><p>With the huge success in NLP, self-attention has been actively investigated for speech recognition as well <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, time-restricted self-attention was introduced with a one-hot vector representation being exploited as relative positional encoding for given restricted contexts of speech frames. In <ref type="bibr" target="#b5">[6]</ref>, the well-known Listen, Attend and Spell (LAS) ASR model <ref type="bibr" target="#b10">[11]</ref> employed the multi-head approach to the attention mechanism to further improve its already state-of-the-art accuracy on the large-scale voice search data. In <ref type="bibr" target="#b6">[7]</ref>, several approaches were explored for better application of self-attention to speech recognition in the LAS framework, e.g., speech frame handling strategies or attention biasing to restrict the locality of the self-attention mechanism. In <ref type="bibr" target="#b7">[8]</ref>, the CTC loss <ref type="bibr" target="#b11">[12]</ref> was applied to optimize the Transformer encoder structure for ASR. In <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, the entire encoder-decoder structure of the original Transformer <ref type="bibr" target="#b0">[1]</ref> was examined in the context of Mandarin Chinese speech recognition tasks.</p><p>The challenge in terms of applying self-attention to speech recognition is that individual speech frames are not like lexical units such as words. Speech frames do not convey distinct meanings or perform unique functions, which makes it hard for the self-attention mechanism to compute proper attentive weights on speech frames. Considering that adjacent speech frames could form a chunk to represent more meaningful units like phonemes, some sort of pre-processing mechanisms such as convolutions to capture an embedding for a group of nearby speech frames would be helpful for self-attention. In addition, a multi-resolution approach could be beneficial as well since boundaries for such meaningful chunks of speech frames are dependent of many factors, e.g., the type of phonemes (vowel vs. consonant) and the way they are pronounced, affected by gender, speaker, co-articulation and so on. Based on this reasoning, in this paper, we propose a new neural network model architecture for better self- attention, namely multi-stream self-attention. The proposed architecture consists of parallel streams of self-attention. In each stream, input speech frames are processed with a distinct resolution by multiple layers of 1D convolutions with a unique dilation rate, and the convoluted embeddings are fed to a subsequent multi-head self-attention layer. In a later stage, the attentive embeddings from all the streams are concatenated then linearly projected to the final embedding. We achieve the state-of-the-art performances on the LibriSpeech corpus <ref type="bibr" target="#b12">[13]</ref> by stacking up these multi-stream self-attention blocks and rescoring the resultant lattices with powerful neural language models. Our WERs on the dev-clean and test-clean sets of 1.8% and 2.2%, respectively, are the best reported numbers thus far on the datasets as far as we know.</p><p>This paper is organized in the following structure. In Section 2, we provide the details of the proposed multi-stream self-attention model architecture. In Section 3, we present the experimental setups and discuss the validity of the proposed ideas using the ablation tests. In addition, we compare our ASR system based on the multi-stream self-attention models with the other state-of-the-art systems. In Section 4, we conclude the paper with the summary of our contributions as well as the future directions. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the proposed multi-stream self-attention block, each stream of which consists of multiple layers of 1D convolutions, one layer of multi-head self-attention, and a feed forward layer sandwiched by two layer normalizations 1 <ref type="bibr" target="#b13">[14]</ref>. The embeddings from all the streams are projected to the final embedding in a later stage. In this section, we detail each component of the multi-stream self-attention architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTI-STREAM SELF-ATTENTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">1D convolutions with factorization (1D Conv-F)</head><p>Time delay neural networks (TDNNs) have been one of the most popular neural network models for speech recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. It was introduced to capture the long range temporal dependencies of acoustic events in speech signals <ref type="bibr" target="#b14">[15]</ref> by exploiting a modular and incremental design from subcomponents. The modified version was recently proposed for better efficiency using the layer-wise subsampling methods <ref type="bibr" target="#b15">[16]</ref>.</p><p>A TDNN is basically a 1D convolution. We use this convolution layer and its kernels with various dilation rates to control the resolution of input speech frames being processed in the parallel streams. In each stream, layers of 1D convolutions with a a unique dilation rate process speech frames in the specified resolution. This can reduce burden on the selfattention mechanism to enable the attentive computation to focus on only one resolution of speech frames in the given stream. Examples of the 1D convolutions which the 3 × 1 kernels with the dilation rates of 1, 2, and 3 are applied to are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In order to make the convolution layers more efficient, we utilize the factorized TDNN <ref type="bibr" target="#b16">[17]</ref>. Singular Value Decomposition (SVD) has been a popular choice to factorize a learned weight matrix into two low-rank factors and reduce the model complexity of neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. The factorized TDNN or factorized 1D convolution (1D Conv-F) layers also utilizes SVD to factorize a 1D convolution parameter matrix into two low-rank matrices. The kernel for each factorized 1D convolution is set to 2 × 1. One of the two factorized convolutions is constrained by the semi-orthogonal condition during training. Consider U as one of the factors in the original parameter matrix W after SVD. The semi-orthogonal constraint puts the condition to minimize a function f :</p><formula xml:id="formula_0">f = Trace QQ T<label>(1)</label></formula><p>where Q = P −I. P is defined by U U T and I is the identity matrix. This way of factorization with the semi-orthogonal constraint not only leads to less model complexity overall, but also results in even better modeling power. After the factorization, the rectified linear unit (ReLu) <ref type="bibr" target="#b20">[21]</ref>, batch normalization <ref type="bibr" target="#b21">[22]</ref>, and dropout <ref type="bibr" target="#b22">[23]</ref> are followed by a skip connection <ref type="bibr" target="#b23">[24]</ref> between the scaled input embedding and the output of the dropout layer. The scale value is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-attention with factorized feed-forward (FF)</head><p>In a given stream of s, we can formulate the time-restricted self-attention mechanism <ref type="bibr" target="#b4">[5]</ref> in a mathematical manner as follows. We define an input embedding matrix to the stream s, X s ∈ R N ×d model , where N is the total number of input embeddings restricted by the left and right context and d model is the dimension of embeddings used inside the self-attention mechanism. Note that downsampling is applied to the input embeddings and the sampling rate is matched to the specified dilation rate (r s ) of the 1D Conv-F layers in the stream. For the projected query, key and value matrices, Q s i , K s i and V s i in the stream s, the output for the i th head is computed as follows:</p><formula xml:id="formula_1">Head s i = Softmax Q s i K s i T √ d k V s i (2) where Q s i = X s W s,Q i and W s,Q i ∈ R d model ×dq , K s i = X s W s,K i and W s,K i ∈ R d model ×d k , V s i = X s W s,K i and W s,V i ∈ R d model ×dv , d q , d k ,</formula><p>and d v are the dimensions of query, key and value embeddings, respectively. The multihead outputs are concatenated and linearly projected, then layer normalization is applied to the projected embedding that is skip-connected with the input embedding:</p><formula xml:id="formula_2">MultiHeadProj s = Concat Head s 1 , . . . , Head s n s h W s,O<label>(3)</label></formula><formula xml:id="formula_3">MidLayer s = LayerNorm (MultiHeadProj s + X s )<label>(4)</label></formula><p>where n s h is the number of heads in the stream s and W s,O i ∈ R (n h ×dv)×d model . We set n s h = n h /S, given n h is a fixed value for the total number of multi-heads across the selfattention components of the whole streams.</p><p>The multiple streams in the proposed architecture could increase the model complexity significantly as opposed to a single stream approach. To retain the model complexity to a reasonable level as well as avoid the loss of modeling power, we use the factorized feed forward networks with a bottleneck layer in between given stream. The semi-orthogonal constraint discussed in Section 2.1 is also applied here to one of the factorized matrices during training. After the skip connection, the encoder output in the stream s can be written as below:</p><formula xml:id="formula_4">Factorized s = Factorized-FF (MidLayer s )<label>(5)</label></formula><formula xml:id="formula_5">Encoder s = LayerNorm (Factorized s + MidLayer s ) (6)</formula><p>Note that the dimension d f f of the embedding layer between the feed forward networks of the original Transformer encoder <ref type="bibr" target="#b0">[1]</ref> is either 1,024 or 2,048. We could reduce the model complexity of the feed forward component by the factor of 8 or 16 if our choice of the bottleneck layer in the factorized version were 128. We can discuss how the factorized feed forward networks with this narrower bottleneck dimension are compared to the original version with no factorization but wider dimension in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Final embedding</head><p>The final embedding layer concatenates the encoder output from each stream and linearly projects the concatenated vector to the final embedding. The ReLu non-linear activation, batch normalization, and dropout follows before feeding out the final embedding as the output:</p><formula xml:id="formula_6">MultiEncProj = Concat Encoder 1 , . . . , Encoder S W O (7) Final = Dropout (BatchNorm (ReLu (MultiEncProj))) (8) where W O ∈ R (S×d model )×d model .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data and experimental setups</head><p>For the experiments being discussed in the paper, we use the LibriSpeech corpus <ref type="bibr" target="#b12">[13]</ref> as the main training and testing datasets. The LibriSpeech corpus is a collection of approximately 1,000hr audiobooks that are a part of the LibriVox project <ref type="bibr" target="#b24">[25]</ref>. Most of the audiobooks come from the Project Gutenberg 2 . The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the 'clean' and 'other' categories, respectively, depending upon how well or challening ASR systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts 3 excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.</p><p>To prepare a lexicon, we selected 522K words among the 977K unique words that occur more than once in the Lib-riSpeech texts. Using the base lexicon of the CMUdict 4 that covered 81K among the selected words of 522K, we trained a G2P model using the Sequitur tool <ref type="bibr" target="#b25">[26]</ref> to cover the out-ofvocabulary words. We used the SRILM tooklit <ref type="bibr" target="#b26">[27]</ref> to train n-gram language models (LMs). The 4-gram LM was trained initially on the entire texts available with the modified Kneser-Ney smoothing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, then pruned to the 3-gram LM. The first-pass decoding was conducted with the 3-gram LM and the resultant lattices were rescored with the 4-gram LM later in the second-pass. The lattices were further rescored <ref type="bibr" target="#b29">[30]</ref> with the neural network LMs of three TDNN layers and two LSTM layers being interleaved that were trained by the Kaldi toolkit <ref type="bibr" target="#b30">[31]</ref>.</p><p>We used the Kaldi toolkit for the acoustic modeling as well, mostly following the LibriSpeech recipe 5 up to the stage of Speaker Adpative Training (SAT) <ref type="bibr" target="#b31">[32]</ref>. We gradually increased the training data size from 100hrs to 960hrs over the course of the GMM training stages, while the neural network training used the entire 960hr data. We first trained GMMs within the framework of 3-state Hidden Markov Models (HMMs). The conventional 39-dimensional MFCC features were spliced over 9 frames and LDA was applied to project the spliced features onto a 40-dimensional sub-space. Further projection was conducted through MLLT for better orthogonality. SAT was applied with feature-space MLLR (fMLLR) to further refine mixture parameters in GMMs. For the neural network acoustic models, we used the 40-dimensional higherresolution MFCCs appended with 100-dimensional i-vectors <ref type="bibr" target="#b32">[33]</ref> and trained the models having the lattice alignments given by the SAT-ed GMMs as soft targets. The LF-MMI objective was used to optimized the parameters with the three regularization methods of cross-entropy, L2 and leaky HMM <ref type="bibr" target="#b33">[34]</ref>. The exponential decrease of learning rates from 10 −3 to 10 −5 was applied to make the entire training procedure stable and have better convergence. The number of nodes in the final layer was determined by the number of tri-phone states in the HMM, which is 6K after the phonetic tree clustering. The trainings were conducted on the Nvidia V100 servers with 8 GPUs.</p><p>The dimensions of query, key, and value embeddings in self-attention are set to d q = d k = 40, and d v = 80, and d model = 256. The bottleneck dimension for the factorized convolutions and the factorized feed-forwards is 128. The number of streams and the number of the 1D convolution layers in each stream is from 1 to 5 and 0 to 7, respectively, depending on the experiment type for the ablation tests. <ref type="bibr" target="#b4">5</ref> https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech/s5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation tests</head><p>This section validates the proposed idea of multi-stream selfattention by conducting the ablation tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Effect of multiple streams</head><p>As we test the validity of multiple streams in the proposed architecture in a fair comparison, we control the number of multi-heads in the self-attention layer in each stream such that n s h = n h /S while fixing n h = 15. For example, if the number of streams is 3 (i.e., S = 3) , then the number of multi-heads in the self-attention layer of each stream would be 15/3 = 5, thus n 1 h = n 2 h = n 3 h = 5. This is to rule out the possibility that any performance improvement would come from the significant increase of model complexity. <ref type="table" target="#tab_0">Table 1</ref> shows the effect of having multiple streams in the proposed architecture. The three entries for the system configurations in the table correspond to S = 1, 3 and 5, respectively, while fixing the dilation rate of 1 across the streams. (For example, 1-1-1 means the three streams with the fixed dilation rate of 1 for the factorized convolutions of all the streams.) It is noticeable that the more streams we had, the better accuracy we would obtain, but without a diverse selection of dilation rates across the streams, the improvement would be limited, which will be shown more clearly in <ref type="table" target="#tab_1">Table  2</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the effect of having diverse dilation rates across the multiple streams of the proposed architecture. The various dilation rates across the streams are shown to help improve WERs by the clear margins. However, just mixing any values would not guarantee the performance improvement. For example, the system configuration of 1-3-5 presents the WER  on the dev-other set worse than the configuration of 1-1-1 does in <ref type="table" target="#tab_0">Table 1</ref>. It seems that a careful mix of the dilation rates would be critical for the proposed model. We found out the best configuration from the 1-2-3-4-5 setup, which has the 5 different dilation rates (by the difference of 1) for the 1D convolutions across the streams, marking 7.69% and 6.75% WERR on dev-clean and dev-other, respectively, as compared to the single stream baseline. This validates the efficacy of the proposed multi-stream strategy of having 1D convolutions with a unique dilation rate in each stream. The proposed architecture seemingly helps the self-attention mechanism better process embeddings in each stream and lead to more accurate results overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Effect of dilation rates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Effect of factorized feed-forward</head><p>The main purpose of having the factorized feed-forward networks in the proposed architecture is to retain the model complexity within a reasonable boundary even with adding more streams. The proposed multi-stream model architecture, otherwise, would increase the model complexity very easily as we add more streams. In <ref type="table" target="#tab_2">Table 3</ref> where we base the same configuration of 1-2-3-4-5 from <ref type="table" target="#tab_1">Table 2</ref>, it is shown that the factorization works as expected. The factorized feed-forward networks not only contribute to the model complexity under 10M parameters but also keep the performance in a similar level with the control group that includes the normal (i.e., without factorization) feed-forward networks with the wider bottleneck layer of 1,024-or 2,048-dimension.  <ref type="table" target="#tab_3">Table 4</ref> highlights the effect of having the 1D convolutions in the proposed multi-stream self-attention model architecture.</p><p>It presents the importance of the 1D convolutions preceding the self-attention mechanism in the multi-stream framework. The 7-layer Conv-F leads to roughly 15% and 20% WERR for the dev-clean and dev-other dataset, respectively, against the case of having no convolutions. The pattern seems that the more convolution layers, the more performance gain would be obtained, but after the 7 layers we didn't observe any significant performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with the other state-of-the-arts</head><p>In this section, we configure our best model for the Lib-riSpeech speech recognition task by stacking the proposed multi-stream self-attention blocks. The chosen configuration of the best ASR system for us is The total number of parameters for this setup is 23M. The lattice-rescored result of this system with the 4-gram LM is presented in the first row in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>As for the neural network language models, we trained the models of 3 TDNN layers and 2 LSTM layers (unidirectional) being interleaved. The averaged (relative) error rate reduction by the best neural network language model (with the dimention of 4,096) against the 4-gram LM case ranges from 15% to 20%. The embedding dimension seems to matter in terms of performance improvement. We did not try the bigger dimensions than 4,096 due to the prohibitive  <ref type="table" target="#tab_5">Table 6</ref> shows some state-of-the-art system performances. The hybrid DNN/HMM approach was used for the different neural network acoustic models of CNN-biLSTM, pyramidal Feedforward Sequential Memory Network (FSMN), and BiL-STM, respectively, in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. The Transformer LM was exploited as the best rescoring LM in <ref type="bibr" target="#b36">[37]</ref>. In <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">39]</ref> the end-to-end framework was considered. Time Depth Separable (TDS) convolutions were introduced in <ref type="bibr" target="#b37">[38]</ref> while the cut out of spectrograms was applied on the fly during training to enhance the noise robustness. In <ref type="bibr" target="#b40">[40]</ref>, the full Transformer model was studied comparatively in various speech tasks. As compared to the other state-of-the-art system performances, it is shown that we achieve the best performances on both of the dev-clean and test-clean set, while on the dev-other and testother set the lowest WERs are presented in <ref type="bibr" target="#b36">[37]</ref>. The WERs of 1.8% and 2.2% on the datasets by the proposed system are the best numbers thus far reported in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We proposed the multi-stream self-attention model architecture preceded by the layers of the factorized 1D convolutions with the unique dilation rate in each stream. This architecture allows input speech frames to be efficiently processed and effectively self-attended. We validated the proposed ideas by performing the ablation tests, and also configured the state-of-the-art ASR system by stacking the multistream self-attention model blocks with the strong neural network language models. The WERs on the dev-clean and testclean set of 1.8% and 2.2% are the best reported numbers found in the literature.</p><p>Note that the proposed system has only 23M parameters. The other systems in <ref type="table" target="#tab_5">Table 6</ref> have much higher model complexity, for example, 100M for CNN-biLSTM <ref type="bibr" target="#b34">[35]</ref> or 200M for LAS in <ref type="bibr" target="#b39">[39]</ref>. This could make our system more practical and appealing to speech engineers or practitioners who would like to deploy ASR models as a service on devices with the limited computing power. Of course, the practicality of the proposed model on on-device ASR would rely upon the usage of much lighter LM models.</p><p>We plan to further explore the self-attention mechanism to make it more suitable for speech applications. To enhance the noise robustness especially on the dev-other and test-other data of the LibriSpeech corpus, we will continue to work on the data augmentation side. In this paper, we used speed perturbation as the data augmentation strategy and it is worth considering other types of augmentation such as data dropout like the cut out method used in <ref type="bibr" target="#b39">[39]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Multi-stream self-attention block. r S : dilation rate for the 1D convolutions in the stream S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of the 1D convolutions with the 3×1 kernels and the different dilation rates (r). (a): r = 1, (b): r = 2, (c): r = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>• 3</head><label>3</label><figDesc>layers of multi-stream self-attention blocks • 5 streams of self-attention encoders in each block • 7 layers of 1D Conv-F's in each stream • Dilation configuration of 1-2-3-4-5 to 1D CONV-F's across streams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of multiple streams in WER (%) and WERR (%) on dev-clean and dev-other. WER: word error rate, WERR: WER reduction (relative). Lattice-rescored with the 4-gram LM.</figDesc><table><row><cell>Stream Config.</cell><cell cols="2">dev-clean</cell><cell cols="2">dev-other</cell></row><row><cell></cell><cell>WER</cell><cell>WERR</cell><cell>WER</cell><cell>WERR</cell></row><row><cell>Single (1)</cell><cell>4.16</cell><cell>-</cell><cell>10.66</cell><cell>-</cell></row><row><cell>1-1-1</cell><cell>3.99</cell><cell>4.09</cell><cell>10.14</cell><cell>4.88</cell></row><row><cell>1-1-1-1-1</cell><cell>3.95</cell><cell>5.05</cell><cell>10.02</cell><cell>6.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of dilation rates in WER (%) and WERR (%) on dev-clean and dev-other. Lattice-rescored with the 4-gram LM.</figDesc><table><row><cell>Stream Config.</cell><cell cols="2">dev-clean</cell><cell cols="2">dev-other</cell></row><row><cell></cell><cell>WER</cell><cell>WERR</cell><cell>WER</cell><cell>WERR</cell></row><row><cell>Single (1)</cell><cell>4.16</cell><cell>-</cell><cell>10.66</cell><cell>-</cell></row><row><cell>1-2-3</cell><cell>3.95</cell><cell>5.05</cell><cell>10.05</cell><cell>5.72</cell></row><row><cell>1-3-5</cell><cell>3.99</cell><cell>4.09</cell><cell>10.27</cell><cell>3.66</cell></row><row><cell>Single (1)</cell><cell>4.16</cell><cell>-</cell><cell>10.66</cell><cell>-</cell></row><row><cell>1-2-3-4-5</cell><cell>3.84</cell><cell>7.69</cell><cell>9.94</cell><cell>6.75</cell></row><row><cell>1-3-5-7-9</cell><cell>3.85</cell><cell>7.45</cell><cell>10.03</cell><cell>5.91</cell></row><row><cell>3-3-3-3-3</cell><cell>3.84</cell><cell>7.69</cell><cell>10.22</cell><cell>4.13</cell></row><row><cell>5-5-5-5-5</cell><cell>3.89</cell><cell>6.49</cell><cell>10.30</cell><cell>3.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of factorization in the feed-forward networks of the proposed multi-stream architecture in terms of model comlexity and WER (%) on dev-clean and dev-other. Latticerescored with the 4-gram LM.</figDesc><table><row><cell>Stream Config.</cell><cell>Param.</cell><cell cols="2">dev-clean dev-other</cell></row><row><cell>Factorized-FF w/ 128-dim</cell><cell>8M</cell><cell>3.84</cell><cell>9.94</cell></row><row><cell>FF w/ 1,024-dim</cell><cell>10.5M</cell><cell>3.80</cell><cell>9.91</cell></row><row><cell>FF w/ 2,048-dim</cell><cell>13M</cell><cell>3.83</cell><cell>9.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect</figDesc><table><row><cell cols="5">of 1D convolutions in WER (%) and WERR</cell></row><row><cell cols="5">(%) on dev-clean and dev-other. Lattice-rescored with the 4-</cell></row><row><cell>gram LM.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stream Config.</cell><cell cols="2">dev-clean</cell><cell cols="2">dev-other</cell></row><row><cell></cell><cell>WER</cell><cell>WERR</cell><cell>WER</cell><cell>WERR</cell></row><row><cell>No Conv-F</cell><cell>4.36</cell><cell>-7.13</cell><cell>11.86</cell><cell>-10.43</cell></row><row><cell>1 Conv-F</cell><cell>4.07</cell><cell>-</cell><cell>10.74</cell><cell>-</cell></row><row><cell>3 Conv-F</cell><cell>3.84</cell><cell>5.65</cell><cell>9.94</cell><cell>7.45</cell></row><row><cell>5 Conv-F</cell><cell>3.76</cell><cell>7.62</cell><cell>9.75</cell><cell>9.22</cell></row><row><cell>7 Conv-F</cell><cell>3.73</cell><cell>8.35</cell><cell>9.49</cell><cell>11.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the best configured model in terms of LMs in WER (%). 3-T/2-L: 3-layer TDNNs &amp; 2-layer LSTMs being interleaved. The numbers in the parentheses indicate the size of neurons.</figDesc><table><row><cell>System</cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell></row><row><cell></cell><cell>clean</cell><cell>other</cell><cell>clean</cell><cell>other</cell></row><row><cell>4-gram</cell><cell>2.65</cell><cell>8.21</cell><cell>2.93</cell><cell>8.32</cell></row><row><cell>3-T/2-L (1,024)</cell><cell>2.51</cell><cell>7.13</cell><cell>2.69</cell><cell>7.45</cell></row><row><cell>3-T/2-L (2,048)</cell><cell>2.23</cell><cell>6.82</cell><cell>2.39</cell><cell>7.01</cell></row><row><cell>3-T/2-L (4,096)</cell><cell>2.14</cell><cell>6.51</cell><cell>2.21</cell><cell>6.73</cell></row><row><cell>4-LSTM (2,048)</cell><cell>1.84</cell><cell>5.75</cell><cell>2.20</cell><cell>5.82</cell></row><row><cell cols="2">3.2.4. Effect of 1-D convolutions</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the other state-of-the-art systems in WER (%).</figDesc><table><row><cell>System</cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell></row><row><cell></cell><cell>clean</cell><cell>other</cell><cell>clean</cell><cell>other</cell></row><row><cell>Han, et al. [35]</cell><cell>3.1</cell><cell>8.3</cell><cell>3.5</cell><cell>8.6</cell></row><row><cell>Hannun, et al. [38]</cell><cell>3.0</cell><cell>8.9</cell><cell>3.3</cell><cell>9.8</cell></row><row><cell>Yang, et al. [36]</cell><cell>2.6</cell><cell>7.5</cell><cell>3.0</cell><cell>7.5</cell></row><row><cell>Karita, et al. [40]</cell><cell>2.2</cell><cell>5.6</cell><cell>2.6</cell><cell>5.7</cell></row><row><cell>Park, et al. [39]</cell><cell>-</cell><cell>-</cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>Lüscher, et al. [37]</cell><cell>1.9</cell><cell>4.5</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell>Proposed</cell><cell>1.8</cell><cell>5.8</cell><cell>2.2</cell><cell>5.8</cell></row><row><cell cols="5">model training time. The best model in terms of WER im-</cell></row><row><cell cols="5">provement is the one with the 4-layered LSTMs, trained with</cell></row><row><cell cols="2">around 10K word pieces.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Each layer normalization has a skip connection (not depicted in the figure) with the input of its previous layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.gutenberg.org. 3 Available at http://www.openslr.org/11. 4 Available at http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional Transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">XL-Net: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.08237" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1901.02860" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A time-restricted selfattention layer for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-attention acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Intespeech</title>
		<imprint>
			<biblScope unit="page" from="3723" to="3727" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-attention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Speech-Transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Syllable-based sequence-to-sequence speech recognition with the Transformer in Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="791" to="795" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LibrSspeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3743" to="3747" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2365" to="2369" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Mcgraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5970" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Model compression applied to small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaran</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1878" to="1882" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On rectified linear units for speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangye</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3517" to="3521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LibriVox: Free public domain audiobooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jodi</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reference Reviews</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="8" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint-sequence models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SRILM -An extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved backing-off for M-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A pruned RNNLM latticerescoring algorithm for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5929" to="5933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
	<note>Silovsky, Georg Stemmer, and Karel Vesely</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum likelihood linear transformations for HMM-based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Speech and Lang</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="797" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Purely sequencetrained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahrmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Capio 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungsuk</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.00059" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A novel pyramidal-FSMN architecture with lattice-free MMI for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.11352" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for LibriSpeech: Hybrid vs attention -w/o data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Luscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kitza1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.03072" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence speech recognition with time-depth separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<ptr target="http://arxiv.org/abs/1904.02619" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.08779" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A comprative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">Enrique</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenori</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangyou</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.06317" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
