<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holophrasm: a neural Automated Theorem Prover for higher-order logic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P Z</forename><surname>Whalen</surname></persName>
							<email>dwhalen@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Holophrasm: a neural Automated Theorem Prover for higher-order logic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I propose a system for Automated Theorem Proving in higher order logic using deep learning and eschewing hand-constructed features. Holophrasm exploits the formalism of the Metamath language and explores partial proof trees using a neural-network-augmented bandit algorithm and a sequence-to-sequence model for action enumeration. The system proves 14% of its test theorems from Metamath's set.mm module. 1 Here, "holophrasm" is the notion that a complicated idea can be conveyed by a simple theorem-vector.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Formalized mathematics arises from a desire for rigor. A formal proof of a theorem is a proof that is complete: every step follows directly from previous steps and known theorems in an algorithmicallyverifiable manner.</p><p>A number of corpora have been developed for formalized mathematics in various formalisms. Large datasets of formal proofs include Metamath <ref type="bibr" target="#b0">[1]</ref>, the Mizar Mathematical Library <ref type="bibr" target="#b1">[2]</ref>, Flyspeck <ref type="bibr" target="#b2">[3]</ref>, the Archive of Formal Proofs <ref type="bibr" target="#b3">[4]</ref>, the Coq standard library <ref type="bibr" target="#b4">[5]</ref>, and the HOL Light library <ref type="bibr" target="#b5">[6]</ref>. These databases cover wide swaths of mathematics. The Metamath set.mm module, for example, is a collection of theorems and proofs constructing mathematics from ZFC. The module includes a number of important theorems, including Zorn's Lemma from set theory, the theorem of quadratic reciprocity from number theory, and Sylow's theorems from group theory.</p><p>The time cost of formalizing proofs is substantial, and so tools to assist in construction of the formal proofs have arisen. Interactive Theorem Provers automate the technical steps of theorem-proving, leaving the creative steps to the user. Over time, the techniques from Interactive Theorem Provers have been extended to Automated Theorem Provers, complete non-interactive tools for the generation of formal proofs. Rapid advances are being made in Automated Theorem Proving, and recent systems now permit proofs of 40% of the theorems in the Mizer Mathematical Library <ref type="bibr" target="#b6">[7]</ref>. These proof systems generally consist of multiple modules, one of which is Premise Selection: the identification of relevant axioms and theorems. Premise Selection has shown promise as a target for machine-learning techniques <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> and more recently deep learning <ref type="bibr" target="#b10">[11]</ref> -the first application of deep learning to Automated Theorem Proving.</p><p>While most of the current research has focused on the Mizar Mathematical Library, I demonstrate that the tree structure of Metamath proofs is exploitable by modern tree exploration techniques. Holophrasm 1 takes a novel approach to Automated Theorem Proving. The system uses a variant of UCT <ref type="bibr" target="#b11">[12]</ref>, an algorithm for the tree-based multi-armed bandit problem, to search the space of partial proof trees of a Metamath theorem. Recent developments in machine learning have made such searches accessible. Action enumeration is made viable by sequence-to-sequence models <ref type="bibr" target="#b12">[13]</ref>. In parallel, algorithms developed for go-playing AIs describe how neural networks can be used to guide tree exploration <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Those techniques have been adapted here to create a complete, non-interactive system for proving Metamath propositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Metamath Format and Data Set</head><p>The Metamath language is designed for automated theorem verification, utilizing metatheorems and proper substitutions as the standard proof step. The exact specification of the language is given in section 4 of <ref type="bibr" target="#b0">[1]</ref>, but the relevant details are summarized below</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metatheorems</head><p>A theorem in the Metamath database is a proposition if it has a proof or an axiom if it does not. The notion of axiom here is general and includes what are traditionally known as axioms, but also includes definitions and the production rules for expressions.</p><p>We separate the axioms and propositions in set.m by their type, which is "set," "class," "wff," or " ". Axioms of non-" "-type describe the production rules for a context-free grammar with the types as syntactic categories. An expression of the given type is a string of the corresponding syntactic category. These axioms along with the free variables as additional terminal symbols provide a unique parse tree for every expression, and they will be referred to as constructor axioms. Henceforth I will conflate the notion of an expression and its parse tree.</p><p>Propositions of type "set," "class," and "wff," are ignored to maintain uniqueness of the parse trees.</p><p>Axioms and propositions of " " type are assertions that an expression of "wff"-type is true, that the expression can be proved from the axioms and given hypotheses. These theorems will be used as nodes in proof trees.</p><p>A theorem T of " "-type consists of a number of elements • a T , an assertion, which is an expression of "wff"-type. • e T , a set of hypotheses, each an expression of "wff"-type. • f T , a set of free variables that appear in the assertion and hypotheses and a type for each. • d T , a set of unordered pairs of disjoint variables from f T .</p><p>The disjoint variables satisfy (x, x) ∈ d T for all x ∈ f T , and represent pairs of variables which can not share any variables after a proper substitution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proper Substitution</head><p>Consider a context proposition C that is to be proven and an expression a of "wff"-type, either the assertion of C or an intermediate step in the proof of C.</p><p>An application of particular theorem, T , to prove a consists of a set of substitutions, φ, into the free variables f T . In these substitutions, earch variable is replaced by an expression of the same type built out of the constructor axioms extended by additional terminal symbols for the free variables of C. The application requires that the assertion of T after substitution matches a, that is φ(a T ) = a. By performing this process we reduce the problem of proving a to the problem of proving all of the hypotheses φ(e T ). This process is illustrated in figure 2.1.</p><p>The disjointness property adds a restriction on the allowable substitutions: for every pair (x, y) ∈ d T , for every free variable z ∈ φ(x) ∩ f C , and every free variable w ∈ φ(y) ∩ f C it must be the case that (z, w) ∈ d C .</p><p>For a fixed expression, context, and theorem, substitutions that satisfy these properties are called viable. For a fixed expression and context, a theorem is called viable if it permits a viable set of substitutions.</p><p>I divide the variables in f T into two types. Constrained variables are variables that appear in a T . Unconstrained variables are variables that appear in some hypothesis h ∈ e T but not a T . The substitutions in φ are called constrained substitutions or unconstrained substitutions if they apply to con-strained or unconstrained variables respectively. Constrained substitutions are notable in that, given a and T , the constrained substitutions are exactly those fixed by the requirement that φ(a T ) = a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proof Trees</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modus Ponens (ax-mp)</head><p>hypotheses:` ` ` ) ` ) assertion:` `</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Double Modus Ponens (mp2b)</head><formula xml:id="formula_0">hypotheses:`↵ ↵ ↵ ) `↵ ) ` ) ` ) assertion:` ` Proof Modus Ponens ! ! ! ! Modus Ponens ! ↵ ! ↵ ! ! ) ) ↵ ↵ ↵ ) ↵ )</formula><p>hypotheses unconstrained substitution constrained substitution assertion <ref type="figure">Figure 1</ref>: Examples of an axiom, axmp and a proposition, mp2b. The red nodes are expressions, and the blue nodes each describe a theorem and substitutions that will prove the parent.</p><p>The application of a theorem proves an expression, but also provide a set of hypotheses which must be proven in turn. This naturally gives a proof a tree structure. The assertion of the theorem is the root node, and its hypotheses are leaves, because they are assumed to be true without proof. Here I define the notion of a proof tree, but I modify the natural structure slightly to permit compatibility with the notion of a partial proof tree introduced in section 3.1.</p><p>A proof tree of an expression a of "wff"-type in context C is a bipartite tree with two types of nodes, red nodes and blue nodes. Red nodes are labeled by an an expression of "wff"-type, which is an intermediate step in the proof, and the root node is labeled by a. Unless its label is a hypothesis in e C , in which case the node is a leaf, red nodes always have exactly one child, a blue node. Blue nodes are labelled by a pair (T, φ) of a theorem and viable substitutions for that theorem into the parent expression. Blue nodes have one child red node, φ(h), for each hypothesis h ∈ e T . If such a tree exists, it is a proof of a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proof Tree Exploration</head><p>The problem I wish to solve is as follows: given a context theorem C, find a proof tree for that theorem's assertion. The algorithm does so by considering a supertree of potential proofs steps and by using tree exploration techniques to search for the subtree that is a valid proof-tree. The algorithm will refer to three neural networks, payoff, generative, and predictive which are described in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partial proof trees</head><p>A partial proof tree is an extension of the the notion of a proof tree The following changes are made: Red nodes are permitted to have no children even if they are not hypotheses. Red nodes are permitted to have multiple multiple child blue nodes, each a potential approach for proving the expression.</p><p>A red node is said to be proven if any of its children have been proven or if its label is one of the initial hypotheses. A blue node is said to the proven if all of its children have been proven.</p><p>The subtree of a proven red node is necessarily a supertree of a valid proof tree for its expression.</p><p>In particular, the subtree can be pruned by removing all of the children of red nodes except for one proven blue child. Such a pruned subtree must be valid proof tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exploration</head><p>Similarly to UCT <ref type="bibr" target="#b11">[12]</ref>, the algorithm builds a partial proof tree over a series of passes. Each pass traverses the tree downward. At a red node, the traversal chooses either to create a new child or to proceed to the highest valuation child blue node. At a blue node, the traversal proceeds to the worst-performing child. The pass continues until a new child blue node is created, whereupon its red children are created and valued. The process repeats until the root node has been proven.</p><p>In order to to perform this exploration, each node maintains additional state, which is updated whenever the node's children are updated.</p><p>Red nodes, a have an initial payoff, y a which is the output of the payoff network, evaluated as soon as the node is created. They additionally have a total payoff, x a , which is the the sum of the initial payoff of the node and the total payoffs of its children, a visit count, n a , which is 1 plus the sum of the visit counts of its children, and an average payoff, which is x a /n a .</p><p>Blue nodes keep track of their least promising child, which is the unproven child with the lowest average payoff. The total payoff and visit count of a blue node are the corresponding values of its least promising child. Blue nodes also have a value v b , which indicates how likely this substitution is to be applicable and is given by the relevance and generative networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visiting Nodes</head><p>In standard UCT, when the traversal reaches a leaf, the leaf spawns a new child for each available action at that node, but doing so is impractical in this context. In this variant, the number of actions available at red nodes can be infinite, since there are infinitely many unconstrained substitutions that could be made into some theorems. The difficult part of the calculation is determining viable actions rather than calculating payoff. To this end, we attempt to maintain the number of children of a red node, a to na 3 , so that more actions are considered after consecutive visits. When a red node is visited for the first time, the node calculates its initial payoff but does not generate any children. When the node is visited later, it checks if it has sufficiently many children. If so the algorithm visits an extant chid as described in section 3.4. If not, the algorithm creates a new child as described in section 3.5.</p><p>When a blue node is visited, it immediately visits its least promising child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visiting current children of red nodes</head><p>To determine which child is visited from a red node, each extant child b is assigned a priority,</p><formula xml:id="formula_1">x b n b + β v b n b + α log n a n b ,</formula><p>for constants β and α. The highest priority child is then visited.</p><formula xml:id="formula_2">The x b n b + α log na n b</formula><p>arises in the standard UCB algorithm as the upper confidence bound and the correction v b n b encourages exploring propositions with a high probability first <ref type="bibr" target="#b14">[15]</ref>. During the experiment, α and β were assigned the values 1.0 and 0.5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Expansion of red nodes</head><p>The children of a red node are constructed by assigning to them a theorem and substitutions. Each pair b = (T, φ) of theorem T and substitution φ is assigned a value v b = p T pbest theorem p T ,φ pT, best substitution . Here p T is the probability that T is the next theorem to apply, as given by the relevance network. p best theorem is correspondingly the probability of the best theorem. For a fixed T , p T,φ is the probability of those substitutions as given by the generative network. p T,best substitution is correspondingly the probability of the best substitution. Children of the red node are added from this expansion queue in decreasing order of value.</p><p>Evaluation of the the relevance and generative network are performed in a just-in-time manner, evaluating relevance during the second visit to a node and evaluating generative only when a previously unconsidered theorem is due to be added as a child.</p><p>When a blue node is added to the child in this way, the algorithm immediately visits each of the blue node's children once to estimate their payoffs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Other details</head><p>In practice a few additional changes can be made to make the algorithm more efficient.</p><p>Circularity: Any attempt to create a red node with the same expression as one its ancestors fails: the parent blue node is removed from the expansion queue of its parent and the next pair (T, φ) is added instead.</p><p>Node Death: While the theoretical number of actions from a given red node is infinite, in practice the number of actions is limited by the beam search width of the generative network. This may lead to instances where a red node has no children and its expansion queue is empty. Such a red node is called dead. A blue node is said to be dead if any of its children are dead. Dead blue nodes are removed from the graph and their ancestors are subsequently checked for death.</p><p>Multiprocessing: The algorithm can be run efficiently in parallel. When doing so, the different threads traverse the proof tree asyncronously. Following <ref type="bibr" target="#b14">[15]</ref>, the priority function for a blue node b from a red node is modified by replacing the</p><formula xml:id="formula_3">x b n b term with x b n b +γt b , where t b</formula><p>is the number of threads currently exploring a descendent of b and γ is a constant, chosen here to be 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative length limits:</head><p>When evaluating the generative network, outputs with more than 75 tokens in total across all unconstrained substitutions are discarded during the beam search. The beam search returns no substitutions if all items in the beam reached the size limit. If so, a dummy child is added to the red node with a payoff of 0 to discourage further exploration of this node Last step: When a red node is added when proving the context C, the viable theorems are determined. For the viable theorems, T , if there are viable substitutions φ such that φ(e T ) ⊂ e C then that theorem and those substitutions are immediately added as a blue node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Networks</head><p>Three distinct neural networks were used in the algorithm. The payoff network estimates the payoff of red nodes. The relevance network predicts which theorems will be useful at a given step. The generative network generated unconstrained substitutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tokenization</head><p>A token is created for each constructor axiom. A number of dummy variables are created and assigned tokens, the minimum number such that for each theorem the numbers of "set", "wff", "class" free variables are at most the number of dummy variables of the corresponding type. Five special tokens are added, 'EOH' for the end of a hypothesis, 'EOS' for the end of a section, 'START' for the start of sequence generation, 'UV' for an unconstrained variable, and 'TARGET' for a target unconstrained variable.</p><p>The inputs are modified for each iteration by randomly replacing the free variables that appear with distinct dummy variables of the corresponding type. Each expression is tokenized by reading the tokens of the constructor axioms in its parse tree in a depth-first pre-order. Hypotheses are separated by the 'EOH' token. If multiple different components are inputted, such as an assertion and set of hypotheses, they are separated by the 'EOS' token. The other three special tokens are used only by the generative network and are are described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Networks</head><p>The networks all share a number of similar features. In general, the embedding vectors for tokens were inputted into 2 layers of bidirectional GRUs with internal biases for the GRU units and hidden layer dimensions of size 128. GRU weights were permitted to vary between different sections of input and output, but the token embedding vectors were shared. The embedding vectors were augmented with four additional dimensions describing the graph structure of the input, the depth of the node, the degree of the node, the degree of its parent, and its position into the degree of its parent. All fully-connected layers used leaky RELUs with α = 0.01 and had dimension 128 unless otherwise specified. Weights were regularized by their L2-norm with a regularization factor of 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Payoff Network</head><p>The payoff network takes as input an expression, a, and a set of hypotheses e C , which are fed into the GRUs. The network attempts to predict whether the expression is provable from the hypotheses. The outputs of both sides of the bidirectional network are concatenated and fed through two fully connected layers with leaky RELU units and a fully connected layer with a sigmoid to obtain the classification probability, p x .</p><p>The network is trained on known proof steps as positive examples and on incorrect proof steps generated by the relevance and generative networks as negative examples. During training, the cross-entropy loss is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relevance Network</head><p>The relevance network takes as input an expression, a, and a set of hypotheses e C , and attempts to classify the next proposition that will used in the proof of a. The relevance network is designed as two parallel networks. The first parallel branch takes a and e C as inputs and returns a 128dimensional expression-vector v. The second parallel branch is evaluated separately for all theorem T of " "-type, inputs a T and e T , and returns a 128-dimensional theorem-vector w T . The probabilities are computed as the softmax over theorems T , p T = softmax(l T ), where l T = v T W w T for some weight matrix W . This structure permits generalizability to new theorems while simultaneously allowing for the theorem vectors to be precomputed and cached.</p><p>The network is trained using a negative-sampling loss with four negative samples; at each iteration, only five theorems are considered: the correct theorem T C and four incorrect theorems T W,i chosen uniformly at random from the viable theorems for a. The training loss is computed as − log σ(l T C )− i log σ(−l T W,i ) and is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Generative Network</head><p>Given an expression, a, a set of hypotheses, e C , and a theorem, T , to be applied, the generative network uses a sequence-to-sequence model <ref type="bibr" target="#b12">[13]</ref> with an intermediate fully-connected layer to create expressions for the unconstrained substitutions.</p><p>To execute the network, an unconstrained variable in f T is chosen uniformly at random to be the target. A set of substitutions φ is generated as follows.</p><p>For v ∈ f T a constrained variable, φ(v) is the expression needed for φ(a T ) = a. φ additionally maps the target unconstrained variable to the 'TARGET' special token and the other unconstrained variables to the 'UV' special token. The sequence-to-sequence model is used to generate an expression for φ applied to the target variable. φ is updated to include this as a substitution, a new target unconstrained variable is chosen, and the process repeats until all variables have substitutions.</p><p>The network takes as inputs φ(e T ) and e C . A fully-connected layer is applied to the outputs of each direction and the result is used as the initial state of the GRUs for the sequence-to-sequence output. An attentional model is added, following the general model of <ref type="bibr" target="#b15">[16]</ref>. The output sequence is initialized with the 'START' token. The outputs of the last GRU layer are fed through a fullyconnected layer with RELU nonlinearity and then a fully-connected layer with softmax nonlinearly to obtain token probabilities. During training, the total cross-entropy loss of the output tokens is minimized.</p><p>During execution, multiple outputs are given, following the beam search technique of <ref type="bibr" target="#b12">[13]</ref>. The tokens which can be included are restricted by a number of filters. Only constructor axioms defined before the current context may be used. No "wff" or "class" variables may be added unless they appear elsewhere in the context. At most one new such set variable is considered during selection for a given token. Furthermore, no token may be added if doing so would violate the disjoint variable conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>The theorems of the Metamath set.mm module are used as the data set, discarding axioms and keeping only propositions of " "-type. Of these propositions, 21786 were selected as a training set, 2711 as a validation set and 2720 as a test set. The proofs are expanded into a full proof tree, and each step of " "-type was recorded.</p><p>The relevance network was trained and evaluated on all of the proof steps for the propositions. The expansion of the propositions into proof steps provides 1.2M training proof steps, 120k validation proof steps and 158k testing proof steps.</p><p>The generative network was trained on the proof steps where a proposition was applied that had at least one unconstrained variable. This constraint leaves 426k training proof steps, 38k validation proof steps, and 56k testing proof steps.</p><p>Data for the payoff network was generated by including all of the proof steps as positive examples excluding duplicates. Additionally, negative examples were generated by using the trained relevance and generative networks to predict the best two proposition/substitution pairs using the valuation described in section 3.5. The hypotheses generated by applying these propositions were </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head><p>Network weights were initialized with Xavier initialization <ref type="bibr" target="#b16">[17]</ref>. Training for each network was done using stochastic gradient descent with a batch size of 100, with an Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. Learning rates started at 10 −4 , were decayed by a factor of 2 each time the validation loss failed to decrease, and training was ended after the validation loss failed to decrease for three consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance</head><p>The neural networks are trained separately. The relevance network is tested, selecting from all viable propositions. On the test data relevance obtains a 55.3% top-1 accuracy, a 72.8% top-5 accuracy and an 87.4% top-20 accuracy.</p><p>The generative network has a perplexity of 2.08 on the test set, when selecting from 1083 tokens. I also measure the probability that a beam search creates the correct substitutions for all unconstrained variables as one of the results. On the test set, generative achieves an accuracy of 39.1% with a beam width of 1, 51.3% with a beam width of 5, and 57.5% with a beam width of 20.</p><p>The payoff network achieves a classification accuracy of 77.6% on the test set. For comparison, a baseline prediction of negative achieves a 62.1% accuracy.</p><p>The system as a whole was tested on each test theorem by expanding the proof trees for 10000 passes or until 5 minutes had passed. Multiple attempts were made for each proposition with the beam search width set to 1, 5, or 20. Under these parameters, the system finds proofs for 388 of 2720, or 14.3% of the test propositions. The system works particularly well on the initial part of the database, finding proofs for 45.1% of the 457 test propositions in the first 5000 theorems.</p><p>In the cases that a valid proof is generated, the system works quickly. The discovered proofs were created with a median of 17 passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper I have proposed a nonconventional approach to Automated Theorem Proving for higher-order logic, and tested performance on the Metamath set.mm module. While the system does not achieve state-of-the-art performance, it is the first effective complete Automated Theorem Prover to not exploit hand-crafted features.</p><p>Holophrasm takes a unconventional approach to automated theorem proving, attempting to emulate the processes and intuition of human proof exploration. A number of new techniques and novel adaptions of current technologies have been introduced:</p><p>• tree-based bandit algorithms for proof exploration • tree-reduction during exploration passes to permit actions to have multiple subtrees • deep networks for estimating statement provability</p><p>• the theorem-vector encoding for rapid theorem selection</p><p>• sequence-to-sequence models for enumeration from an infinite set of actions While the results of Holophrasm are not directly comparable to current results on the Mizar dataset, the developments show promise as generalizable techniques. They highlight the feasibility of deep learning as an approach to Automated Theorem Proving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>included as negative examples after removing all the the hypotheses that were equivalent to positive examples. There were 587k positive and 960k negative training examples, 69k positive and 113k negative validation examples, and 74k positive and 120k negative test examples.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I am grateful to Yu-hsin Chen, Zach DeVito, and Matthew Fisher for invaluable discussions during the planning stages of this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Metamath: A computer language for pure mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Megill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mizar: the first 30 years. Mechanized mathematics and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Rudnicki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to the Flyspeck project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum für Informatik</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Proving the correctness of Disk Paxos. Archive of Formal Proofs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Jaskelioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Merz</surname></persName>
		</author>
		<idno>2150-914x</idno>
		<ptr target="http://isa-afp.org/entries/DiskPaxos.shtml" />
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
	<note>Formal proof development</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Coq development team. The Coq proof assistant reference manual</title>
		<ptr target="http://coq.inria.fr.Version8.4" />
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">HOL Light: a tutorial introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Formal Methods in Computer-Aided Design</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="265" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MizAR 40 for Mizar 40</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="256" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Premise selection for mathematics by corpus analysis and kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Alama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview and evaluation of premise selection techniques for large theory mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Twan Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Automated Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="378" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mash: machine learning for Sledgehammer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kühlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmin</forename><forename type="middle">Christian</forename><surname>Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interactive Theorem Proving</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DeepMath-Deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04442</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Move evaluation in Go using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6564</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
