<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Track to Detect and Segment: An Online Multi-Object Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suny</forename><surname>Buffalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Track to Detect and Segment: An Online Multi-Object Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most online multi-object trackers perform object detection stand-alone in a neural net without any input from tracking. In this paper, we present a new online joint detection and tracking model, TraDeS (TRAck to DEtect and Segment), exploiting tracking clues to assist detection end-to-end. TraDeS infers object tracking offset by a cost volume, which is used to propagate previous object features for improving current object detection and segmentation. Effectiveness and superiority of TraDeS are shown on 4 datasets, including MOT (2D tracking), nuScenes (3D tracking), MOTS and Youtube-VIS (instance segmentation tracking). Project page: https://jialianwu.com/ projects/TraDeS.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Advanced online multi-object tracking methods follow two major paradigms: tracking-by-detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> and joint detection and tracking <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. The tracking-by-detection (TBD) paradigm treats detection and tracking as two independent tasks ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>). It usually applies an off-the-shelf object detector to produce detections and employs another separate network for data association. The TBD system is inefficient and not optimized end-to-end due to the two-stage processing. To address this problem, recent solutions favor a joint detection and tracking (JDT) paradigm that simultaneously performs detection and tracking in a single forward-pass ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>).</p><p>The JDT methods, however, are confronted with two issues: (i) Although in most JDT works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50]</ref> the backbone network is shared, detection is usually performed standalone without exploring tracking cues. We argue that detection is the cornerstone for a stable and consistent tracklet, and in turn tracking cues shall help detection, especially in tough scenarios like partial occlusion and motion blur. (ii) As studied by <ref type="bibr" target="#b8">[9]</ref> and our experiment (Tab. 1b), common re-ID tracking loss <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51]</ref> is not that compatible with detection loss in jointly training a single backbone network, which could even hurt detection performance to some extent. The reason is that re-ID focuses on intra-class variance, but detection aims to enlarge inter-class difference and minimize intra-class variance.</p><p>In this paper, we propose a new online joint detection and tracking model, coined as TraDeS (TRAck to DEtect and Segment). In TraDeS, each point on the feature map represents either an object center or a background region, similar as in CenterNet <ref type="bibr" target="#b63">[64]</ref>. TraDeS addresses the above two issues by tightly incorporating tracking into detection as well as a dedicatedly designed re-ID learning scheme. Specifically, we propose a cost volume based association (CVA) module and a motion-guided feature warper (MFW) module, respectively. The CVA extracts point-wise re-ID embedding features by the backbone to construct a cost volume that stores matching similarities between the embedding pairs in two frames. Then, we infer the tracking offsets from the cost volume, which are the spatio-temporal displacements of all the points, i.e., potential object centers, in two frames. The tracking offsets together with the embeddings are utilized to conduct a simple two-round long-term data association. Afterwards, the MFW takes the tracking offsets as motion cues to propagate object features from the previous frames to the current one. Finally, the propagated feature and the current feature are aggregated to derive detection and segmentation.</p><p>In the CVA module, the cost volume is employed to su-pervise the re-ID embedding, where different object classes and background regions are implicitly taken into account. This is being said, our re-ID objective involves the inter-class variance. This way not only learns an effective embedding as common re-ID loss <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51]</ref>, but also is well compatible with the detection loss and does not hurt detection performance as shown in Tab. 1b. Moreover, because the tracking offset is predicted based on appearance embedding similarities, it can match an object with very large motion or in low frame rate as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, or even accurately track objects in different datasets with unseen large motion as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Thus, the predicted tracking offset of an object can serve as a robust motion clue to guide our feature propagation in the MFW module. The occluded and blurred objects in the current frame may be legible in early frames, so the propagated features from previous frames may support the current feature to recover potentially missed objects by our MFW module. In summary, we propose a novel online multi-object tracker, TraDeS, that deeply integrates tracking cues to assist detection in an end-to-end framework and in return benefits tracking as shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. TraDeS is a general tracker, which is readily extended to instance segmentation tracking by adding a simple instance segmentation branch. Extensive experiments are conducted on 4 datasets, i.e., MOT, nuScenes, MOTS, and Youtube-VIS datasets, across 3 tasks including 2D object tracking, 3D object tracking, and instance segmentation tracking. TraDeS achieves state-of-theart performance with an efficient inference time as shown in § 5.3. Additionally, thorough ablation studies are performed to demonstrate the effectiveness of our approach as shown in § 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tracking-by-Detection. MOT was dominated by the tracking-by-detection (TBD) paradigm over the past years <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54]</ref>. Within this framework, an off-the-shelf object detector <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16]</ref> is first applied to generate detection boxes for each frame. Then, a separate re-ID model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref> is used to extract appearance features for those detected boxes. To build tracklets, one simple solution is to directly compute appearance and motion affinities with a motion model, e.g., Kalman filter, and then solve data association by a matching algorithm. Some other efforts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19]</ref> formulate data association as a graph optimization problem by treating each detection as a graph node. However, TBD methods conduct detection and tracking separately, hence are usually computationally expensive. Instead, our approach integrates tracking cues into detection and efficiently performs detection and tracking in an end-to-end fashion.</p><p>Joint Detection and Tracking. Recently joint detection and tracking (JDT) paradigm has raised increasing attention due to its efficient and unified framework. One common way <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b60">61]</ref> is to build a tracking-related branch upon an object detector to predict either object tracking offsets or re-ID embeddings for data association. Alternatively, transformer is exploited to match tracklets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26]</ref>. CTracker <ref type="bibr" target="#b28">[29]</ref> constructs tracklets by chaining paired boxes in every two frames. TubeTK <ref type="bibr" target="#b27">[28]</ref> directly predicts a box tube as a tracklet in an offline manner. Most JDT methods, however, are confronted with two issues: First, detection is still separately predicted without the help from tracking. Second, the re-ID loss has a different objective from that of detection loss in joint training. In contrast, our TraDeS tracker addresses these two problems by tightly incorporating tracking cues into detection and designing a novel re-ID embedding learning scheme.</p><p>Tracking-guided Video Object Detection. In video object detection, a few attempts <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b61">62]</ref> exploit tracking results to reweight the detection scores generated by an initial detector. Although these works strive to help detection by tracking, they have two drawbacks: First, tracking is leveraged to help detection only at the post-processing stage. Detections are still predicted by a standalone object detector, so detection and tracking are separately optimized. Thus, the final detection scores may heavily rely on the tracking quality. Second, a hand-crafted reweighting scheme requires manual tune-up for a specific detector and tracker. Our approach differs from these post-processing methods because our detection is learned conditioned on tracking results, without a complex reweighting scheme. Therefore, detection tends to be robust w.r.t. tracking quality.</p><p>Cost Volume. The cost volume technique has been successfully applied in depth estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b17">18]</ref> and optical flow estimation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">53]</ref> for associating pixels between two frames. This motivates us to extend cost volume to a multi-object tracker, which will be demonstrated to be effective in learning re-ID embeddings and inferring tracking offsets in this paper. Our approach may inspire future works using cost volume in tracking or re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>The proposed TraDeS is built upon the point-based object detector CenterNet <ref type="bibr" target="#b63">[64]</ref>. CenterNet takes an image I ∈ R H×W ×3 as input and produces the base feature f = φ(I) via the backbone network φ(·), where f ∈ R H F ×W F ×64 , H F = H 4 , and W F = W 4 . A set of head convolutional branches are then constructed on f to yield a class-wise center heatmap P ∈ R H F ×W F ×N cls and task-specific prediction maps, such as 2D object size map and 3D object size map, etc. N cls is the number of classes. CenterNet detects objects by their center points (peaks on P ) and the corresponding task-specific predictions from the peak positions.</p><p>Similar to <ref type="bibr" target="#b62">[63]</ref>, we build a baseline tracker by adding an extra head branch on CenterNet that predicts a tracking offset map O B ∈ R H F ×W F ×2 for data association. O B computes spatio-temporal displacements from all points at time t to the corresponding points at a previous time t − τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TraDeS Tracker</head><p>Our Idea: Most previous joint detection and tracking methods perform a standalone detection without explicit input from tracking. In contrast, our aim is to integrate tracking cues into detection end-to-end, so as to improve detection for tough scenarios, which in return benefit tracking. To this end, we propose a Cost Volume based Association (CVA: § 4.1) module for learning re-ID embeddings and deriving object motions, and a Motion-guided Feature Warper (MFW: § 4.2) module for leveraging tracking cues from the CVA to propagate and enhance object features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cost Volume based Association</head><p>Cost Volume: Given two base features f t and f t−τ from I t and I t−τ , we extract their re-ID embedding features by the embedding network σ(·), i.e.,</p><formula xml:id="formula_0">e t = σ(f t ) ∈ R H F ×W F ×128 ,</formula><p>where σ(·) consists of three convolution layers. We utilize the extracted embeddings to construct a cost volume which stores dense matching similarities between one point and its corresponding point in two frames. To efficiently compute the cost volume, we first downsample the embeddings by a factor of 2 and obtain e ∈ R H C ×W C ×128 , where H C = H F 2 and W C = W F 2 . Let us denote by C ∈ R H C ×W C ×H C ×W C the 4-dimensional cost volume for I t and I t−τ , which is computed by a single matrix multiplication of e t and e t−τ . Specifically, each element of C is calculated as:</p><formula xml:id="formula_1">C i,j,k,l = e t i,j e t−τ k,l ,<label>(1)</label></formula><p>where C i,j,k,l represents the embedding similarity between point (i, j) at time t and point (k, l) at time t − τ . Here, a point refers to an entry on the feature map f or e .</p><p>Tracking Offset: Based on the cost volume C, we calculate a tracking offset matrix O ∈ R H C ×W C ×2 , which stores the spatio-temporal displacements for all points at time t to their corresponding points at time t − τ . For illustration, we show the estimation procedure for O i,j ∈ R 2 below. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, for an object x centered at point (i, j) at time t, we can fetch from C its corresponding twodimensional cost volume map C i,j ∈ R H C ×W C . C i,j stores the matching similarities among object x and all points at time t − τ . Using C i,j , O i,j ∈ R 2 is estimated by two steps:</p><p>Step (i) C i,j is first max pooled by H C × 1 and 1 × W C kernels, respectively, and then normalized by a softmax function <ref type="bibr" target="#b0">1</ref> </p><formula xml:id="formula_2">, which results in C W i,j ∈ [0, 1] 1×W C and C H i,j ∈ [0, 1] H C ×1 . C W i,j and C H i,j</formula><p>consists of the likelihoods that object x appears on specified horizontal and vertical positions at time t − τ , respectively. For example, C W i,j,l is the likelihood that object x appears at the position ( * , l) at time t − τ .</p><p>Step (ii) Since C W i,j and C H i,j have provided the likelihoods that object x appears on specified positions at t − τ . To obtain the final offsets, we predefine two offset templates for horizontal and vertical directions, respectively, indicating the actual offset values when x appears on those positions. Let M i,j ∈ R 1×W C and V i,j ∈ R H C ×1 denote the horizontal and vertical offset templates for object x, respectively, which are computed by:</p><formula xml:id="formula_3">M i,j,l = (l − j) × s 1 ≤ l ≤ W C V i,j,k = (k − i) × s 1 ≤ k ≤ H C ,<label>(2)</label></formula><p>where s is the feature stride of e w.r.t. the input image, which is 8 in our case. M i,j,l refers to the horizontal offset when object x appears at the position ( * , l) at time t − τ . The final tracking offset can be inferred by the dot product between the likelihoods and actual offset values as:</p><formula xml:id="formula_4">O i,j = [C H i,j V i,j , C W i,j M i,j ] .<label>(3)</label></formula><p>Because O is of H C × W C , we upsample it with a factor of 2 and obtain O C ∈ R H F ×W F ×2 that serves as motion cues for the MFW and is used for our data association.</p><p>Training: Since σ(·) is the only learnable part in the CVA module, the training objective of CVA is to learn an effective re-ID embedding e. To supervise e, we enforce the supervision on the cost volume rather than directly on e like other common re-ID losses. Let us first denote Y ijkl = 1 when an object at location (i, j) at current time t appears at location (k, l) at previous time t − τ ; otherwise Y ijkl = 0. Then, the training loss for CVA is calculated by the logistic regression in the form of the focal loss <ref type="bibr" target="#b21">[22]</ref> as:</p><formula xml:id="formula_5">L CV A = −1 ijkl Y ijkl ijkl      α 1 log(C W i,j,l ) +α 2 log(C H i,j,k ) if Y ijkl = 1 0 otherwise , (4) where α 1 = (1 − C W i,j,l ) β and α 2 = (1 − C H i,j,k ) β .</formula><p>β is the focal loss hyper-parameter. Since C W i,j,l and C H i,j,k are computed by softmax, they involve the embedding similarities not only between points (i, j) and (k, l) but also among point (i, j) and all other points in the previous frame. This is being said, while C W i,j,l and C H i,j,k being optimized to approach 1, it enforces an object to not only approach itself in the previous frame, but also repel other objects and background regions.</p><p>The CVA Characteristics: (i) Common re-ID loss only emphasizes intra-class variance, which may degrade detection performance. In contrast, our L CV A in Eq. 4 not only emphasizes intra-class variance but also forces inter-class difference when learning embedding. We find such a treatment is more compatible with detection loss and learns effective embedding without hurting detection as evidenced in Tab. 1b. (ii) Because the tracking offset is predicted based on appearance embedding similarities, it can track objects under a wide range of motion and low frame rate as shown in <ref type="figure" target="#fig_3">Fig. 3</ref> and <ref type="figure">Fig. 6</ref>, or even accurately track objects in different datasets with unseen large motion in training set as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The predicted tracking offset can therefore serve as a robust motion cue to guide our feature propagation as in Tab. 1c. (iii) Compared to <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25]</ref> and CenterTrack <ref type="bibr" target="#b62">[63]</ref> that only predict either embedding or tracking offset for data association, the CVA produces both embedding and tracking offset that are used for long-term data association ( § 4.3) and serve as motion cues for the MFW ( § 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Motion-guided Feature Warper</head><p>The MFW aims to take the predicted tracking offset O C as motion clues to warp and propagate f t−τ to the current time so as to compensate and enhance f t . To achieve this goal, we perform an efficient temporal propagation via a single deformable convolution <ref type="bibr" target="#b11">[12]</ref>, which has been used for temporally aligning features in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref>. Then, we enhance f t by aggregating the propagated feature.</p><p>Temporal Propagation: To propagate feature maps, the deformable convolution (DCN) takes a spatio-temporal offset map and a previous feature as input and outputs a propagated feature, in which we estimate the input offset based on the O C from the CVA module. Let us denote</p><formula xml:id="formula_6">O D ∈ R H F ×W F ×2K 2</formula><p>as the input two-directional offset for DCN, where K = 3 is the kernel width or height of DCN. To generate O D , we pass O C through a 3 × 3 convolution γ(·). We optionally incorporate the residual feature of f t − f t−τ as the input of γ(·) to provide more motion clues. Since our detection and segmentation are mainly based on object center features, instead of directly warping f t−τ , we propagate a center attentive featuref t−τ ∈ R H F ×W F ×64 from previous time.f t−τ is computed as:</p><formula xml:id="formula_7">f t−τ q = f t−τ q • P t−τ agn , q = 1, 2, ..., 64,<label>(5)</label></formula><p>where q is the channel index, • is the Hadamard product, and P t−τ agn ∈ R H F ×W F ×1 is the class agnostic center heatmap fetched from the P t−τ (as defined in § 3). Then, given O D andf t−τ , the propagated feature is computed via a DCN aŝ</p><formula xml:id="formula_8">f t−τ = DCN (O D ,f t−τ ) ∈ R H F ×W F ×64 .</formula><p>Feature Enhancement: When occlusion or motion blur occurs, objects could be missed by the detector. We propose to enhance f t by aggregating the propagated featuref t−τ , on which the occluded and blurred objects may be visually legible. We denote the enhanced feature asf t−τ , which is calculated by weighted summation as:</p><formula xml:id="formula_9">f t q = w t • f t q + T τ =1 w t−τ •f t−τ q , q = 1, 2, ..., 64, (6) where w t ∈ R H F ×W F ×1 is the adaptive weight at time t and T τ =0 w t−τ i,j = 1.</formula><p>T is the number of previous features used for aggregation. Similar to <ref type="bibr" target="#b23">[24]</ref>, w is predicted by two convolution layers followed by softmax function. We find that in experiment the weighted summation is slightly better than average summation. The enhanced featuref t is then fed into the head networks to produce detection boxes and masks in the current frame. This can potentially recover missed objects and reduce false negatives, enabling complete tracklets and higher MOTA and IDF1 as in Tab. 1a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Tracklet Generation</head><p>The overall architecture of TraDeS is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Based on the enhanced featuref t , TraDeS produces 2D and 3D boxes and instance masks by three different head networks. Afterwards, the generated detection and masks are connected to previous tracklets by our data association.</p><p>Head Networks: Each head network consists of several light-weight convolutions for yielding task-specific predictions. For 2D and 3D detection, we utilize the same head networks as in CenterNet <ref type="bibr" target="#b63">[64]</ref>. For instance segmentation, we refer to the head network in CondInst <ref type="bibr" target="#b38">[39]</ref>, which is an instance segmentation method also based on center points.</p><p>Data Association: Given an enhanced detection or mask d centered at location (i, j), we perform a two-round data association as: DA-Round (i) We first associate it with the closest unmatched detection at time t − 1 within the area centered at (i, j)+O C i,j with radius r, where r is the geometrical average of width and height of the detected box. Here, O C i,j only indicates the object tracking offsets between I t and I t−1 . DA-Round (ii) If d does not match any targets in the first round, we compute cosine similarities of its embedding e t i,j with all unmatched or history tracklet embeddings. d will be assigned to a tracklet if their similarity is the highest and larger than a threshold, e.g., 0.3. DA-Round (ii) is capable of long-term associating. In case d fails to associate with any tacklets in the above two rounds, d starts a new tracklet.</p><p>TraDeS Loss: The overall loss function of TraDeS is defined as L = L CV A + L det + L mask , where L det is the 2D and 3D detection losses as in <ref type="bibr" target="#b63">[64]</ref> and L mask is the instance segmentation loss as in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Implementation Details</head><p>MOT: We conduct 2D object tracking experiments on the MOT16 and MOT17 datasets <ref type="bibr" target="#b26">[27]</ref>, which have the same 7 training sequences and 7 test sequences but slightly different annotations. Frames are labeled at 25-30 FPS. For ablation study, we split the MOT17 training sequences into two halves and use one for training and the other for validation as in <ref type="bibr" target="#b62">[63]</ref>. Metrics: We use common 2D MOT evaluation metrics <ref type="bibr" target="#b1">[2]</ref>: Multiple-Object Tracking Accuracy (MOTA), ID F1 Score (IDF1), the number of False Negatives (FN), False Positives (FP), times a trajectory is Fragmented (Frag), Identity Switches (IDS), and the percentage of Mostly Tracked Trajectories (MT) and Mostly Lost Trajectories (ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>nuScenes:</head><p>We conduct 3D object tracking experiments on the newly released nuScenes <ref type="bibr" target="#b6">[7]</ref>, containing 7 classes, 700 training sequences, 150 validation sequences, and 150 test sequences. Videos are captured by 6 cameras of a moving car in a panoramic view and labeled at 2 FPS. Our TraDeS is a monocular tracker. Metrics: nuScenes designs more robust metrics, AMOTA and AMOTP, which are computed by weighted averages of MOTA and MOTP across score thresholds from 0 to 1. For fair comparison, we also report IDS A that averages IDS in the same way. <ref type="bibr" target="#b40">[41]</ref>, an instance segmentation tracking dataset, is derived from the MOT dataset. MOTS has 4 training sequences and 4 test sequences. Metrics: The evaluation metrics are similar to those on MOT, which however are based on masks. Moreover, the MOTS adopts a Mask-based Soft Multi-Object Tracking Accuracy (sMOTSA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOTS: MOTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-VIS:</head><p>We also conduct instance segmentation tracking on YouTube-VIS <ref type="bibr" target="#b55">[56]</ref>, which contains 2,883 videos labeled at 6 FPS, 131K instance masks, and 40 object classes. Metrics: The YouTube-VIS adopts a mask tracklets based average precision (AP) for evaluation.</p><p>Compared to MOT and MOTS, nuScenes and YouTube-VIS are of low frame rate and large motion, because only key frames are labeled and cameras are moving. In our experiments, only labeled frames are used as input. Implementation Details: We adopt the same experimental settings as CenterTrack <ref type="bibr" target="#b62">[63]</ref>, such as backbone, image sizes, pretraining, score thresholds, etc. Specifically, we adopt the DLA-34 <ref type="bibr" target="#b59">[60]</ref> as the backbone network φ(·). Our method is optimized with 32 batches and learning rate (lr) 1.  <ref type="bibr" target="#b22">[23]</ref> following the static image training scheme in <ref type="bibr" target="#b62">[63]</ref> and then finetuned on YouTube-VIS for 16 epochs where lr drops at epoch 9. Image size is of 352 × 640. We test the runtime on a 2080Ti GPU. In Eq. 6, we set T = 2 by default for MOT and MOTS. We set T = 1 for nuScenes and YouTube-VIS due to their low frame rate characteristic mentioned above. In training, we randomly select T frames out of nearby R t frames, where R t is 10 for MOT and MOTS and 5 for nuScenes and YouTube-VIS. During inference, only previous T consecutive frames are used. Ablation experiments are conducted on the MOT17 dataset. In ablations, all variants without the CVA module perform the DA-Round (i) by predicting a tracking offset O B as in the baseline tracker ( § 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>Effectiveness of TraDeS: As shown in Tab. 1a, we compare our proposed CVA ( § 4.1), MFW ( § 4.2), and TraDeS ( § 4) with our baseline tracker ( § 3) and CenterTrack <ref type="bibr" target="#b62">[63]</ref>. CVA: Compared to the baseline, the CVA achieves better tracking by reducing 60% IDS and improving 7.2 IDF1, validating the effect of our tracking offset, re-ID embedding, and the two-round data association. MFW: For ablation, we directly add the MFW to the baseline tracker. Since the tracking offset O C is unavailable in the baseline, we only use f t − f t−τ  as motion cues to predict the DCN offset O D . Compared to the baseline, the MFW achieves better detection by reducing 1.5% FN, i.e., recovering more missed objects, though FP is slightly increased. Moreover, we observe that the MFW also reduces 43% IDS and improves 6.2 IDF1. It validates that detection is the cornerstone for tracking performance, where improved detection can yield more stable and consistent tracklets. TraDeS: With the help of CVA, TraDeS reduces IDS from 606 to 285. Moreover, in TraDeS, the robust tracking offsets O C from CVA guides the feature propagation in MFW, which significantly decreases FN from 29.5% to 27.8%. Better IDS and missed object recovery (↓FN) together improve our comprehensive tracking performance, achieving 68.2 MOTA and 71.7 IDF1. TraDeS also achieves better results than the recent JDT method CenterTrack <ref type="bibr" target="#b62">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the CVA Module:</head><p>We study the two major characteristics of the proposed CVA module as mentioned in § 4.1. (i): First, we add the re-ID embedding network σ(·) into the baseline tracker, which is supervised by a common re-ID loss, e.g., the cross-entropy loss L CEembed as in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b60">61]</ref>. We denote the learned embedding as CE embedding, which is used to perform our two-round data association. As shown in Tab. 1b, with DA-Round (ii), CE embedding helps baseline improve IDF1 and reduce IDS, as long-term data association is enabled by using the re-ID embedding to match history tracklets. However, we observe that CE embedding cannot improve MOTA as detection performance is degraded (+1.1% FN). Next, we still add σ(·) into the baseline tracker, which however is supervised by our CVA module. Tab. 1b shows that our CVA module not only learns an effective re-ID embedding as CE embedding but also slightly improves detection performance, which clearly leads to a higher MOTA. We argue that this is because common re-ID loss only emphasizes intra-class variance, which may not be compatible with detection loss in joint training as indicted in <ref type="bibr" target="#b8">[9]</ref>. In contrast, our proposed L CV A in Eq. 4 supervises the re-ID embedding via the cost volume and considers both intra-class and inter-class difference. (ii): We visualize the predicted cost volume map C and tracking offset O C in <ref type="figure" target="#fig_3">Fig. 3</ref>. The CVA accurately predicts the tracking offset for an object under low frame rate or large motion. Moreover, O C even accurately tracks objects in a new dataset with unseen large motion in training as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Visualization of O C on more samples are shown in <ref type="figure">Fig. 6</ref>. These examples indicate the CVA is able to predict tracking offsets for objects with a wide range of motion and provide robust motion cues.</p><p>Effectiveness of the MFW Module: DCN: In Tab. 1c, we use different motion clues to predict the DCN input offset O D . We find that the tracking offset O C is the key to reduce FN and recover more missed objects. It validates that the proposed O C is a robust tracking cue for guiding feature propagation and assisting detection. Moreover, we visualize the predicted O D in <ref type="figure">Fig. 5</ref>  <ref type="table">Table 3</ref>. Results of 3D object tracking on the nuScenes dataset. We compare with the state-of-the-art monocular 3D tracking methods. We mainly assess the major classes: car and pedestrian. We also list "All" for reference, which is the average among all the 7 classes. itive performance compared to other state-of-the-art instance segmentation trackers. We observe that TraDeS outperforms the baseline tracker by a large margin on both nuScenes and YouTube-VIS. We argue that this is because the baseline cannot well predict the tracking offset O B with a single image in case these datasets are of low frame rate and large motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work presents a novel online joint detection and tracking model, TraDeS, focusing on exploiting tracking cues to help detection and in return benefit tracking. TraDeS is equipped with two proposed modules, CVA and MFW. The CVA learns a dedicatedly designed re-ID embedding and models object motions via a 4d cost volume. While the MFW takes the motions from CVA as the cues to propagate previous object features to enhance the current detection or segmentation. Exhaustive experiments and ablations on 2D tracking, 3D tracking and instance segmentation tracking validate both effectiveness and superiority of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of different online MOT pipelines. Our method follows the joint detection and tracking (JDT) paradigm. Different from most JDT methods, the proposed TraDeS tracker deeply couples tracking and detection within an end-to-end and unified framework, where the motion clue from tracking is exploited to enhance detection or segmentation (omitted in the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of TraDeS. TraDeS may propagate features from multiple previous frames for object feature enhancement (i.e., T &gt; 1), which is not shown in the above figure for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>25e − 4</head><label>4</label><figDesc>dropping by a factor of 10. For MOT and MOTS, TraDeS is trained for 70 epochs where lr drops at epoch 60 with image size of 544 × 960. For nuScenes, TraDeS is trained for 35 epochs where lr drops at epoch 30 with image size of 448 × 800. For YouTube-VIS, TraDeS is first pretrained on COCO instance segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>CVA workflow visualization: the cost volume map C and tracking offset O C under low frame rate (left) and large motion (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CVA vs. Common embedding: Common embedding loss L CEembed may downgrade detection performance, while our CVA learns an effective embedding without hurting detection. As "Baseline" does not have embedding, it only performs DA-Round(i). Motion cues: In MFW, we evaluate different motion cues as the input of γ(·) to predict the DCN input offset O D . Ablations are based on baseline with CVA. Ablation studies on the MOT17 validation set. MOTA and IDF1 reflect the comprehensive tracking performance, while FN and FP reflect the detection performance. Lower FN means more missed objects are recovered. ↓ denotes lower is better. ↑ denotes higher is better.</figDesc><table><row><cell cols="2">Scheme</cell><cell></cell><cell></cell><cell></cell><cell cols="8">MOTA↑ IDF1↑ IDS↓</cell><cell></cell><cell>FN↓</cell><cell cols="2">FP↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Scheme</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MOTA↑ IDF1↑ IDS↓</cell><cell>FN↓</cell><cell>FP↓</cell></row><row><cell cols="6">CenterTrack[63] 66.1</cell><cell></cell><cell cols="2">64.2</cell><cell></cell><cell></cell><cell cols="6">528 28.4% 4.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Baseline</cell><cell></cell><cell></cell><cell>64.8</cell><cell cols="2">59.5</cell><cell>1055 31.0% 2.3%</cell></row><row><cell cols="4">Baseline Baseline+CVA</cell><cell></cell><cell>64.8 66.5</cell><cell></cell><cell cols="2">59.5 66.7</cell><cell></cell><cell cols="7">1055 31.0% 2.3 % 415 30.6% 2.2%</cell><cell></cell><cell></cell><cell cols="4">w/o DA-Round (ii)</cell><cell cols="6">+CE embedding 63.7 +CVA 65.5</cell><cell cols="2">59.6 60.9</cell><cell>1099 32.1% 2.2% 936 30.6% 2.2%</cell></row><row><cell cols="5">Baseline+MFW TraDeS</cell><cell>66.3 68.2</cell><cell></cell><cell cols="2">65.7 71.7</cell><cell></cell><cell></cell><cell cols="6">606 29.5% 3.0% 285 27.8% 3.5%</cell><cell></cell><cell></cell><cell cols="4">w/ DA-Round (ii)</cell><cell cols="6">+CE embedding 64.5 +CVA 66.5</cell><cell cols="2">64.3 66.7</cell><cell>671 415 30.6% 2.2% 32.1% 2.2%</cell></row><row><cell cols="31">(a) Effectiveness of each proposed module: we evaluate the proposed CVA ( § 4.1), MFW ( § 4.2), and overall TraDeS ( § 4). "Baseline+CVA+MFW" is represented by "TraDeS". (b) Scheme MOTA↑ IDF1↑ IDS↓ FN↓ Baseline+CVA 66.5 66.7 415 30.6% 2.2% FP↓ TraDeS w/ f t − f t−τ only 67.1 68.8 273 29.9% 2.5% f t − f t−τ &amp; O C 68.2 71.7 285 27.8% 3.5% (c) Scheme MOTA↑ IDF1↑ IDS↓ T = 1 67.8 69.0 350 28.2% 3.4% FN↓ FP↓ Time(ms)↓ 46 T = 2 68.2 71.7 285 27.8% 3.5% 57 T = 3 67.5 69.9 283 29.2% 2.8% 70 (d) Number of previous features: We evaluate the MFW when</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">aggregating different numbers of previous features.</cell></row><row><cell>− :</cell><cell>MOT ×</cell><cell>( ×</cell><cell>,</cell><cell>)</cell><cell>:</cell><cell>×</cell><cell cols="2">MOT ×</cell><cell>(</cell><cell>,</cell><cell>)</cell><cell>0</cell><cell>20</cell><cell cols="3">40 : × × 60 max(axis=0) + softmax 80 100 ×</cell><cell>120 max(axis=1) + softmax</cell><cell>0 10 20 30 40 50 60 68</cell><cell>39,108</cell><cell>= − . 39,108 Vertical offset</cell><cell>:</cell><cell cols="2">Youtube-VIS × × ( ,</cell><cell>)</cell><cell>:</cell><cell>(</cell><cell cols="2">Youtube-VIS × × , )</cell><cell>0</cell><cell>10</cell><cell>40 max(axis=0) + softmax 60 50 70 , : × × 20 30 ×</cell><cell>80 max(axis=1) + softmax</cell><cell>14,33 20 0 5 25 40 44 10 15 30 35</cell><cell>14,33 offset = . Vertical</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39,108</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14,33</cell></row><row><cell cols="5">Current Center Previous Center (Ground Truth) (Ground Truth)</cell><cell cols="3">39,108 = [− . , − . ] ⊤ (Prediction)</cell><cell cols="4">39,108 = [− , − ] ⊤ (Ground Truth)</cell><cell></cell><cell></cell><cell></cell><cell>39,108</cell><cell cols="4">= − . Horizontal offset</cell><cell></cell><cell cols="4">Current Center Previous Center (Ground Truth) (Ground Truth)</cell><cell cols="3">14,33 = [ . , 113.5] ⊤ (Prediction)</cell><cell>14,33 = [ , (Ground Truth)</cell><cell>] ⊤</cell><cell>14,33</cell><cell>= Horizontal offset .</cell></row></table><note>,−</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>. The DCN successfully samples the center features at the previous frames even if the car in the middle image has dramatic displacements. Number of Previous Features: As in Eq. 6, the MFW aggregates the current feature with T previous features. We evaluate the MFW with different T as shown in Tab. 1d, and find that we Results of 2D object tracking on the MOT test set under the private detection protocol. "Joint" indicates joint detection and tracking in a single model, i.e., no external detections. " " indicates that Track Re-birth<ref type="bibr" target="#b62">[63]</ref> is used. The top two results in the "Joint" manner without Track Re-birth are highlighted in red and blue, respectively. +D indicates the additional detection time<ref type="bibr" target="#b30">[31]</ref>. Visualized O C on nuScenes. All models are only trained on MOT but tested on nuScenes, where nuScenes has much larger object motions than MOT. TraDeS successfully tracks objects with unseen large motion in training dataset, but baseline and CenterTrack fail.Figure 5. Visualization of DCN input offset O D .The DCN kernel at is translated by and samples the previous feature at . For clear visualization, we only show the sampling center of the DCN kernel as depicted by in I t−τ . The previous image is highlighted by the previous class agnostic heatmap P t−τ agn . achieve the best speed-accuracy trade-off when T = 2. As shown in Tab. 2, we compare the proposed TraDeS tracker with the state-of-the-art 2D trackers on the MOT16 and MOT17 test sets. Our TraDeS tracker outperforms the second best tracker by 2.5 MOTA and 1.8 MOTA on MOT16 and MOT17, respectively, running at 15 FPS. Compared to joint detection and tracking algorithms, we achieve the best results on most metrics, e.g., MOTA, IDF1, MT, FN, etc. nuScenes: As shown in Tab. 3, we compare TraDeS with the state-of-the-art monocular 3D trackers on nuScenes. There exists extreme class imbalance in nuScenes dataset, e.g., car and pedestrian has over 82% data. Since class imbalance is not our focus, we mainly evaluate on major classes: car and pedestrian. Tab. 3 shows that the TraDeS tracker outperforms other monocular trackers by a large margin on all metrics. MOTS: As shown in Tab. 4, we compare TraDeS with the recent instance segmentation tracker TrackR-CNN on the MOTS test set. TrackR-CNN is based on Mask R-CNN [17] and also temporally enhances object features. The TraDeS tracker outperforms TrackR-CNN by a large margin in terms of both accuracy and speed. 100% 101,897GTs ) nuScenes Val set AMOTA↑ AMOTP↓ IDSA ↓ AMOTA↑ AMOTP↓ IDSA ↓ AMOTA↑ AMOTP↓ IDSA ↓ Time AMOTA↑ AMOTP↓ IDSA ↓ AMOTA↑ AMOTP↓ IDSA ↓ Time</figDesc><table><row><cell>MOT16 Test Set</cell></row></table><note>MOT:YouTube-VIS: As shown in Tab. 5, TraDeS notably im- proves AP by 6.2 over the baseline. TraDeS achieves compet-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Publication Year sMOTSA ↑ IDF1 ↑ MOTSA ↑ MOTSP ↑ MODSA ↑ MT↑ ML↓ FP↓ FN↓ IDS ↓ Time Results of instance segmentation tracking on the MOTS test set. Visualization that TraDeS tracks objects on three tasks. Red arrow is the tracking offset O C w.r.t. the previous frame I t−1 . Results of instance segmentation tracking on the YouTube-VIS validation set.</figDesc><table><row><cell cols="4">TrackR-CNN [41] CVPR 2019</cell><cell>40.6</cell><cell>42.4</cell><cell>55.2</cell><cell>76.1</cell><cell>56.9</cell><cell>38.7% 21.6% 1,261 12,641 567 500ms</cell></row><row><cell cols="2">TraDeS (Ours)</cell><cell cols="2">CVPR 2021</cell><cell>50.8</cell><cell>58.7</cell><cell>65.5</cell><cell>79.5</cell><cell>67.0</cell><cell>49.4% 18.3% 1,474 9,169 492 87ms</cell></row><row><cell>Task</cell><cell>Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>One color denotes one identity</cell></row><row><cell>Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D</cell><cell>MOT</cell><cell></cell><cell cols="2">MOT</cell><cell></cell><cell></cell><cell>MOT</cell><cell>MOT</cell></row><row><cell>Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D</cell><cell>nuScenes</cell><cell></cell><cell cols="2">nuScenes</cell><cell></cell><cell></cell><cell>nuScenes</cell><cell>nuScenes</cell></row><row><cell>Mask Tracking</cell><cell>Youtube-VIS</cell><cell></cell><cell cols="3">Youtube-VIS</cell><cell></cell><cell cols="2">Youtube-VIS</cell><cell>Youtube-VIS</cell></row><row><cell></cell><cell>MOTS</cell><cell></cell><cell cols="2">MOTS</cell><cell></cell><cell></cell><cell>MOTS</cell><cell>MOTS</cell></row><row><cell cols="2">Figure 6. Method</cell><cell></cell><cell cols="4">Publication AP AP50 AP75</cell><cell></cell></row><row><cell cols="7">OSMN(mask propagation)[57] CVPR'18 23.4 36.5 25.7</cell><cell></cell></row><row><cell cols="2">FEELVOS[40]</cell><cell></cell><cell cols="4">CVPR'19 26.9 42.0 29.7</cell><cell></cell></row><row><cell cols="3">OSMN(track-by-detect)[57]</cell><cell cols="4">CVPR'18 27.5 45.1 29.1</cell><cell></cell></row><row><cell cols="3">MaskTrack R-CNN[56]</cell><cell cols="4">ICCV'19 30.3 51.1 32.6</cell><cell></cell></row><row><cell cols="2">SipMask[8]</cell><cell></cell><cell cols="4">ECCV'20 32.5 53.0 33.3</cell><cell></cell></row><row><cell cols="2">Our Baseline</cell><cell></cell><cell></cell><cell cols="3">26.4 43.2 26.8</cell><cell></cell></row><row><cell cols="2">TraDeS (Ours)</cell><cell></cell><cell cols="4">CVPR'21 32.6 52.6 32.8</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We add a temperature of 5 into the softmax, such that the softmax output values are more discriminative.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported in part by a gift grant from Horizon Robotics and National Science Foundation Grant CNS1951952. We thank Sijia Chen and Li Huang from Horizon Robotics for helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning temporal pose estimation from sparsely-labeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Brasó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multiimage matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single shot video object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dpsnet: End-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-class multiobject tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enkhbayar</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songguo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><forename type="middle">Young</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Giu</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phill</forename><surname>Kyu Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning spatial fusion for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09516</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Retinatrack: Online single stage joint detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Votel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning multiobject tracking and segmentation from automatic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idoia</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple-object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep affinity network for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huansheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-object tracking using online metric learning with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunming</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining detection and tracking for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Joint object detection and multi-object tracking with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards real-time multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple target tracking based on undirected hierarchical relation hypergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3d multi-object tracking: A baseline and new evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gnn3dmot: Graph neural network for 3d multi-object tracking with 2d-3d multi-feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal-context enhanced detection of heavily occluded pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatialtemporal relation networks for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A unified object motion and affinity model for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01888</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Integrated object detection and tracking with tracklet-conditioned detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11167</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Online multi-target tracking with tensor-based high-order graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
