<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-level Context Gating of Embedded Collective Knowledge for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Asadi-Aghbolaghi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Azad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
						</author>
						<title level="a" type="main">Multi-level Context Gating of Embedded Collective Knowledge for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-BConvLSTM</term>
					<term>Dense Convolution</term>
					<term>Medical Im- age Segmentation</term>
					<term>Squeeze and Excitation</term>
					<term>U-Net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation has been very challenging due to the large variation of anatomy across different cases. Recent advances in deep learning frameworks have exhibited faster and more accurate performance in image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation. In this paper, we propose an extension of U-Net for medical image segmentation, in which we take full advantages of U-Net, Squeeze and Excitation (SE) block, bi-directional ConvLSTM (BConvLSTM), and the mechanism of dense convolutions. (I) We improve the segmentation performance by utilizing SE modules within the U-Net, with a minor effect on model complexity. These blocks adaptively recalibrate the channel-wise feature responses by utilizing a self-gating mechanism of the global information embedding of the feature maps. (II) To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. (III) Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM in all levels of the network to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. The proposed model is evaluated on six datasets DRIVE, ISIC 2017 and 2018, lung segmentation, P H 2 , and cell nuclei segmentation, achieving state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M EDICAL images play a key role in medical treatment and diagnosis. The goal of Computer-Aided Diagnosis (CAD) systems is providing doctors with more precise interpretation of medical images to follow-up of many diseases and have better treatment of a large number of patients. Moreover, accurate and reliable processing of medical images results in reducing the time, cost, and error of human-based processing. A critical step in numerous medical imaging studies is image segmentation. Medical image segmentation is the process of partitioning an image into multiple meaningful regions. Due to the complex geometry and inherent noise value of medical images, segmentation of these images is difficult. Interest in The first two authors contributed equally. This work is partially supported by the Spanish project TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPU used for this research. This work is partially supported by ICREA under the ICREA Academia programme.</p><p>M. Asadi-Aghbolaghi, and M. Fathy are with the School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran, (e-mail:masadi@ipm.ir, mahfathy@ipm.ir).</p><p>R. Azad is with the Department of Computer Engineering, Sharif University of Technology, Tehran, Iran, (e-mail: rezazad68@gmail.com).</p><p>S. Escalera is with the Universitat de Barcelona and Computer Vision Center, Barcelona, Spain. (email: sergio@maia.ub.es) medical image segmentation has grown considerably in the last few years. This is due in part to the large number of application domains, like segmentation of blood vessel, skin cancer, lung, and cell nuclei ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>For instance, segmentation of blood vessels will help to detect and treat many diseases that influence the blood vessels. Width and curves of retinal blood vessel show some symptoms about many diseases. Early diagnosis of many sightthreatening diseases is vital since lots of these diseases like glaucoma, hypertension and diabetic retinopathy cause blindness among working age people. Skin lesion segmentation helps to detect and diagnosis the skin cancer in the early stage. One of the most deadly form of skin cancer is melanoma, which is the result of unusual growth of melanocytes. Dermoscopy, captured by the light magnifying device and immersion fluid, is a non-invasive imaging technique providing with a visualization of the skin surface. The detection of melanoma in dermoscopic images by the dermatologists may be inaccurate or subjective. If melanoma is detected in its early stages, the five-year relative survival rate is 92% <ref type="bibr" target="#b0">[1]</ref>.</p><p>The first vital step of pulmonary image analysis is identifying the boundaries of lung from surrounding thoracic tissue on CT images, called lung segmentation. It can also be applied to lung cancer segmentation. Another application of medical image segmentation is cell nuclei segmentation. All known biological lives include a fundamental unit called cell. By segmentation of nuclei in different situations, we can understand the role and function of the nucleus and the DNA contained in cell in various treatments.</p><p>Deep learning networks achieve outstanding results and use to outperform non-deep state-of-the-art methods in medical imaging. These networks require a large amount of data to train and provide a good generalization behavior given the huge number of network parameters. A critical issue in medical image segmentation is the unavailability of large (and annotated) datasets. In medical image segmentation, per pixel labeling is required instead of image level label. Fully convolutional neural network (FCN) <ref type="bibr" target="#b1">[2]</ref> was one of the first deep networks applied to image segmentation.</p><p>Ronneberger et al. <ref type="bibr" target="#b2">[3]</ref> extended this architecture to U-Net, achieving good segmentation results leveraging the need of a large amount of training data. Their network consists of encoding and decoding paths. In the encoding path a large number of feature maps with reduced dimensionality are extracted. The decoding path is used to produce segmentation maps (with the same size as the input) by performing upconvolutions. Many extensions of U-Net have been proposed so far <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. In some of them, the extracted feature maps in the skip connection are first fed to a processing step (e.g. attention gates <ref type="bibr" target="#b4">[5]</ref>) and then concatenated. The main drawback of these networks is that the processing step is performed individually for the two sets of feature maps, and these features are then simply concatenated.</p><p>In this paper, we propose Multi-level Context Gating U-Net (MCGU-Net) an extended version of the U-Net, by including BConvLSTM <ref type="bibr" target="#b6">[7]</ref> in the skip connection, using SE mechanism in the decoding path, and reusing feature maps with densely convolutions. A VGG backbone is employed in the encoding path to make it possible to use pre-trained weights on large datasets. The feature maps from the corresponding encoding layer have higher resolution while the feature maps extracted from the previous up-convolutional layer contain more semantic information. Instead of a simple concatenation, combining these two kinds of features with non-linear functions in all levels of the network may result in more precise segmentation. Therefore, in this paper we extend the U-Net architecture by adding multi-level BConvLSTM in the skip connection.</p><p>Inspired by the effectiveness of the recently proposed squeeze and excitation modules <ref type="bibr" target="#b7">[8]</ref> on image classification, we modify the U-Net by inserting these blocks in the decoding path. SE modules allow the network to recalibrate the feature map to have more attention on useful channels by assigning different weights to various channels of feature maps based on to their relationship by employing a context gating mechanism. By using global embedding information, these modules help the network to boost informative and meaningful features, while suppressing weak ones. Having a sequence of convolutional layers may help the network to learn more kinds of features; however, in many cases, the network learns redundant features. To mitigate this problem and enhance information flow through the network, we utilize the idea of densely connected convolutions <ref type="bibr" target="#b8">[9]</ref>. In the last layer of the contracting path, convolutional blocks are connected to all subsequent blocks in that layer via channel-wise concatenation. This strategy helps the method to learn a diverse set of features based on the collective knowledge gained by previous layers. Furthermore, we accelerate the convergence speed of the network by employing BN after the up-convolution filters.</p><p>We evaluate the proposed MCGU-Net on four different applications retinal blood vessel segmentation (DRIVE dataset), Skin lesion segmentation (three datasets of P H 2 , ISIC 2017 and 2018), lung nodule segmentation (Lung dataset), and cell nuclei segmentation (Data Science Bowl 2018). The experimental results demonstrate that the proposed network achieves superior performance than state-of-the-art alternatives. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>During the last few years, deep learning-based approaches have outstandingly improved the performance of classical image segmentation strategies. Based on the exploited deep architecture, we divide these approaches into three groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Neural Network (CNN)</head><p>Cui et al. <ref type="bibr" target="#b9">[10]</ref> exploited CNN for automatic segmentation of brain MRI images. The authors first divided the input images into some patches and then utilized these patches for training CNN. To handle an arbitrary number of modalities as the input data, Kleesiek et al. <ref type="bibr" target="#b10">[11]</ref> proposed a 3D CNN for brain lesion segmentation. To process MRI data, the network consists of four channels: non-enhanced and contrast-enhanced T1w, T2w and FLAIR contrasts. Roth et al. <ref type="bibr" target="#b11">[12]</ref> proposed a multi-level deep convolutional networks for pancreas segmentation in abdominal CT scans as a probabilistic bottom-up approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fully Convolutional Network (FCN)</head><p>A problem of the CNN models for segmentation is that the spatial information of the image is lost when the convolutional features are fed into the fc layers. To overcome this problem the FCN was proposed <ref type="bibr" target="#b1">[2]</ref>. This network is trained end-to-end and pixels-to-pixels, in which all fc layers of the CNN architecture are replaced with convolutional and deconvolutional to keep the original spatial resolutions. Zhou et al. <ref type="bibr" target="#b12">[13]</ref> exploited FCN for segmentation of anatomical structures on 3D CT images. An FCN with convolution and de-convolution parts is trained end-to-end, performing voxelwise multiple-class classification to map each voxel in a CT image to an anatomical label. Drozdzal et al. <ref type="bibr" target="#b13">[14]</ref> proposed very deep FCN by using short skip connections. The authors showed that a very deep FCN with both long and short skip connections achieved better result than the original one. Roth et al. <ref type="bibr" target="#b14">[15]</ref> proposed to employ 3D FCN in a cascaded fashion for segmentation of the organs and vessels in CT images.</p><p>U-Net, <ref type="bibr" target="#b2">[3]</ref>, is one of the most popular FCNs for medical image segmentation. It has some advantages than the other segmentation-based networks <ref type="bibr" target="#b3">[4]</ref>. It works well with few training samples and the network is able to utilize the global location and context information at the same time. Milletari et al. <ref type="bibr" target="#b15">[16]</ref> proposed V-Net, a 3D extension version of U-Net to predict segmentation of a given volume at once. V-Net is an end-to-end 3D image segmentation network based on a volumetric (MRI volumes). 3D U-Net <ref type="bibr" target="#b16">[17]</ref> is proposed for processing 3D volumes instead of 2D images. In which, all 2D operations of U-Net are replaced with their 3D counterparts. In <ref type="bibr" target="#b17">[18]</ref>, the authors combine multiple segmentation maps that are created at different scales. Moreover, to forward feature maps from one stage of the network to the other one, elementwise summation is utilized. A dual pathway 3D CNN (with 11 layers) <ref type="bibr" target="#b18">[19]</ref> was proposed for brain lesion segmentation in multi-modal brain MRI. In this model, input images at multiple scales are fed simultaneously to a FCN. <ref type="bibr">Li et al. proposed</ref> High-Res3DNet <ref type="bibr" target="#b19">[20]</ref>, which is a high-resolution, compact convolutional network for volumetric image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recurrent Neural Network (RNN)</head><p>One of the most used neural networks for processing a sequence is RNN, which can take into account the temporal data using recurrent connections in hidden layers. It has been successfully applied for modeling short-and long-temporal sequences. These networks are able to model the global contexts and improve semantic segmentation.Different RNN based deep network have been proposed for semantic segmentation. Pinheiro et al. <ref type="bibr" target="#b20">[21]</ref> proposed a deep network consisting of an RNN that can take into account long range label dependencies in the scenes while limiting the capacity of the model. Visin et al. <ref type="bibr" target="#b21">[22]</ref> proposed ReSeg for semantic segmentation. In that network, the input images are processed with a pre-trained VGG-16 model and its resulting feature maps are then fed into one or more ReNet layers. DeepLab architecture <ref type="bibr" target="#b22">[23]</ref> contains a deep convolutional neural network in which all fully connected layers are replaced by convolutional layers and then the feature resolution is increased through atrous convolutional layers. Alom et al. <ref type="bibr" target="#b3">[4]</ref> proposed Recurrent Convolutional Neural Network (RCNN) and Recurrent Residual Convolutional Neural Network (R2CNN) based on U-Net models for medical image segmentation. Gao <ref type="bibr" target="#b23">[24]</ref> proposed an end to end combination of FCN and RNN with long short-term memory (LSTM) units for 4D segmentation of MRI images.</p><p>He et al. <ref type="bibr" target="#b7">[8]</ref> introduced the Squeeze and Excitation (SE) network for image classification which models the explicit relationship between the channels of a feature map. In these modules, the convolutional features are first passed through a squeeze operation in which global average pooling is exploited to produce channel descriptor. The output of the aggregation is then fed to an excitation operation to generate a set of per-channel modulation weights. These weights are utilized to recalibrate the feature map to emphasize on useful channels.</p><p>In this paper, MCGU-Net is proposed as an extension of U-Net, showing better performance than state-of-the-art alternatives. The BConvLSTM is employed in the skip connection to combine features from contracting and expanding paths to learn more discriminative information. The dense convolutions help the network to learn more diverse features. Moreover, BN, utilized in the network, has a significant effect on the convergence speed of the network. In addition, the SE modules are exploited in the decoding path to extract more useful information by considering the interdependecies between channels of features. It is worth mentioning SE blocks are utilized in our network in a different ways than other approaches <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Zhu et al. <ref type="bibr" target="#b25">[26]</ref> employed SE residual block in the encoding path, and Rundo et al. utilized these blocks before the concatenation of skip connections while these blocks are inserted in the decoding path of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Inspired by U-Net <ref type="bibr" target="#b2">[3]</ref>, BConvLSTM <ref type="bibr" target="#b6">[7]</ref>, SENet <ref type="bibr" target="#b7">[8]</ref> and dense convolutions <ref type="bibr" target="#b8">[9]</ref>, we propose the MCGU-Net ( <ref type="figure" target="#fig_1">Figure  2</ref>). We detail all parts of the network in the next sub sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoding Path</head><p>The U-Net consists of a contracting path to extract hierarchically semantic features from the input and capture context information. To improve the performance of the U-Net we utilize the idea of transfer learning by exploiting a pre-trained CNN of VGG family as the encoder <ref type="bibr" target="#b26">[27]</ref>. To train a complex model with a huge amount of parameters, a large dataset is necessary. However, gathering a vast number of labeled data is very tough. On the other hand, deep learning models are mostly focused on a specific task. To overcome the isolated learning paradigm, the idea of transfer learning has been proposed, which leverage knowledge from pre-trained models and use it to solve new problem, which may have less data. Inspiring by this idea, we design the encoding path like the first four layers of VGG-16 to make it possible to use pretrained models. The first two layers includes two convolutional 3 × 3 filters followed by a 2 × 2 max pooling and ReLU. The number of convolutional filters in the third layer is three with the same filter size followed by the same pooling and ReLU. The number of feature maps are doubled at each step.</p><p>The original U-Net contains a sequence of convolutional layers in the last step of encoding path. Having a sequence of convolutional layers in a network yields the method learn different kinds of features. Nevertheless, the network might learn redundant features in the successive convolutions. To mitigate this problem, densely connected convolutions are proposed <ref type="bibr" target="#b8">[9]</ref>. This helps the network to improve its performance by the idea of "collective knowledge" in which the feature maps are reused through the network. It means feature maps learned from all previous convolutional layers are concatenated with the feature map learned from the current layer and then are forwarded to use as the input to the next convolution.</p><p>The idea of densely connected convolutions has some advantages over the regular one <ref type="bibr" target="#b8">[9]</ref>. First, it helps the network to learn a diverse set of feature maps instead of redundant ones. Moreover, this idea improves the network's representational power by allowing information flow through the network. Furthermore, dense connected convolutions can benefit from all the produced features before it (i.e., collecting knowledge), which prompt the network to avoid the risk of exploding or vanishing gradients. In addition, the gradients are sent to their respective places in the network more quickly in the backward path. We employ this idea in the proposed network. To do that, we introduce one block as two consecutive convolutions. There are a sequence of N blocks in the last convolutional layer of the encoding path. These blocks are densely connected. We consider X i e as the output of the i th convolutional block. The input of the i th (i ∈ {1, ..., N }) convolutional block receives the concatenation of the feature maps of all preceding convolutional blocks as its input, i.e., X 1 e , X 2 e , ..., X i−1 e ∈ R (i−1)F l ×W l ×H l , and the output of the i th block is X i e ∈ R F l ×W l ×H l . In the remaining part of the paper we use simply X e instead of X N e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoding Path</head><p>Each step in the decoding path starts with an up-sampling function over the output of the previous layer.  the representation power of the network, decoding path of the original U-Net is augmented with two important modules of SE block and BConvLSTM. SE yield the network to use global information to selectively empathize informative features and suppress less useful ones. This block receives the output of the up-sampling function, which is a collection of feature maps, and encourages the feature maps to be more informative using a weight for each channel based on the interdependencies between all channels. The output of the SE module is then passed to an up-sampling function. In the standard U-Net, the corresponding feature maps in the contracting path are concatenated with the output of the up-sampling function. In the proposed network, we employ BConvLSTM to combine these two kinds of feature maps. The output of the BConvL-STM is then fed to a set functions including two convolutional functions, one SE module, and another convolutional filter. A diagram illustrating the structure of the combination of these modules in our network is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Assume that the set of extracted feature maps from the previous layer in decoding path is</p><formula xml:id="formula_0">X d ∈ R F l+1 ×W l+1 ×H l+1</formula><p>where F l is the number of feature maps at layer l, and W l × H l is the size of each feature map at layer l. We have</p><formula xml:id="formula_1">F l+1 = 2 * F l , W l+1 = 1 2 * W l , and H l+1 = 1 2 * H l . For simplicity we consider X d ∈ R 2F × W 2 × H 2 .</formula><p>As it can be seen in <ref type="figure" target="#fig_2">Figure 3</ref>, this set of feature maps is first passed through an up-convolutional layer in which an up-sampling function followed by a 2 × 2 convolution are applied, doubling the size of each feature map and halving the number of channels, i.e., producing X up d ∈ R F ×W ×H . In other words, the expanding path increases the size of the feature maps layer by layer to reach the original size of the input image after the final layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Squeeze and Excitation Module:</head><p>Capturing spatial correlations between features has improved the performance of deep networks, like Inception architectures <ref type="bibr" target="#b27">[28]</ref> and spatial attention <ref type="bibr" target="#b28">[29]</ref>. However, the network produces the same attention to the channels when creating the output feature maps. The SE mechanism <ref type="bibr" target="#b7">[8]</ref> is proposed to capture explicit relationship between channels of the convolutional layers by a context gating mechanism, which results in improving the representation </p><formula xml:id="formula_2">z f = F sq (x up f ) = 1 H × W H i W j x up f (i, j)<label>(1)</label></formula><p>where F sq are the spatial squeeze function, x up f (i, j) is a spatial location of the f th channel, and H × W is the size of this channel. In other words, each two-dimensional feature map is compressed by a global average pooling to produce z f . To capture the channel-wise dependencies, the global information embedded in the first step is then fed to the second step, i.e., Excitation. This function must be able to learn non-mutually-exclusive relationship and nonlinear interaction between channels <ref type="bibr" target="#b7">[8]</ref>. As it is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the excitation step consists of two fully connected (FC) layers. The pooled vector is first encoded to shape 1 × 1 × F r , and then encoded again to shape 1 × 1 × F to generate excitation vector as</p><formula xml:id="formula_3">s = F ex (z; W) = σ (W 2 δ(W 1 z))<label>(2)</label></formula><p>where W 1 ∈ R F r ×F is the parameters of the first fc layer, R F × F r , δ is ReLU, and the σ refers to the sigmoid activation. Moreover, r is the reduction ratio. In <ref type="bibr" target="#b7">[8]</ref>, to limit model complexity and aid generalization, a dimensionality-reduction layer with reduction ratio r is used in the first fc layer. In the second fc layer a dimensionality-increasing layer is utilized to set the dimension to the channel one of the transformation output. The output of the SE block is generated asx</p><formula xml:id="formula_4">up f = F scale (x up f , z c ) = s c x up f . In whichX up d = [x up 1 ,x up 2 , ...,x up F ]</formula><p>, and F scale is a channel-wise multiplication between the channel attention, the scale factor s c , and the input feature map.</p><p>2) Batch Normalization: After up-sampling,X up d goes through a BN function and produces X up d . A problem in the intermediate layers in training step is that the distribution of the activations varies. This problem makes the training process very slow since each layer in every training step has to learn to adapt themselves to a new distribution. BN <ref type="bibr" target="#b29">[30]</ref> is utilized to increase the stability of a neural network, which standardizes the inputs to a layer in the network by subtracting the batch mean and dividing by the batch standard deviation. BN affectedly accelerates the speed of training process of a neural network. Moreover, in some cases the performance of the model is improved thanks to the modest regularization effect. More details can be found in <ref type="bibr" target="#b29">[30]</ref>.</p><p>3) Bi-Directional ConvLSTM: The output of the BN step ( X up d ∈ R F l ×W l ×H l ) is now fed to a BConvLSTM layer. The main disadvantage of the standard LSTM is that these networks does not take into account the spatial correlation since these models use full connections in input-to-state and state-to-state transitions. To solve this problem, ConvLSTM <ref type="bibr" target="#b30">[31]</ref> was proposed which exploited convolution operations into input-to-state and state-to-state transitions. It consists of an input gate i t , an output gate o t , a forget gate f t , and a memory cell C t . Input, output and forget gates act as controlling gates to access, update, and clear memory cell. ConvLSTM can be formulated as follows (for convenience we remove the subscript and superscript from the parameters):</p><formula xml:id="formula_5">i t = σ (W xi * X t + W hi * H t−1 + W ci * C t−1 + b i ) f t = σ (W xf * X t + W hf * H t−1 + W cf * C t−1 + b f ) C t = f t • C t−1 + i t tanh (W xc * X t + W hc * H t−1 + b c ) o t = σ (W xo * X t + W ho * H t−1 + W co • C t + b c ) H t = o t • tanh(C t ),<label>(3)</label></formula><p>where * and • denote the convolution and Hadamard functions, respectively. X t is the input tensor (in our case X e and X up d ), H t is the hidden sate tensor, C t is the memory cell tensor, and, W x * and W h * are 2D Convolution kernels corresponding to the input and hidden state, respectively, and b i , b f , b o , and b c are the bias terms.</p><p>In this network, we employ BConvLSTM <ref type="bibr" target="#b6">[7]</ref> to encode X e and X up d . BConvLSTM uses two ConvLSTMs to process the input data into two directions of forward and backward paths, and then makes a decision for the current input by dealing with the data dependencies in both directions. In a standard ConvLSTM, only the dependencies of the forward direction are processed. However, all the information in a sequence should be fully considered, therefore, it might be effective to take into account backward dependencies. It has been proved that analyzing both forward and backward temporal perspectives enhanced the predictive performance <ref type="bibr" target="#b31">[32]</ref>. Each of the forward and backward ConvLSTM can be considered as a standard one. Therefore, we have two sets of parameters for backward and forward states. The output of the BConvLSTM is calculated as</p><formula xml:id="formula_6">Y t = tanh W − → H y * − → H t + W ← − H y ← − H t + b . In which</formula><p>− → H t and ← − H t denote the hidden sate tensors for forward and backward states, respectively, b is the bias term, and Y t ∈ R F l ×W l ×H l indicates the final output considering bidirectional spatio-temporal information. Moreover, tanh is the hyperbolic tangent which is utilized here to combine the output of both forward and backward states through a nonlinear way. We utilize the energy function like the original U-Net to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULT</head><p>We evaluate MCGU-Net on six datasets of: DRIVE, ISIC 2017, ISIC 2018, a lung segmentation benchmark, P H 2 , and a cell nuclei segmentation dataset. The empirical results show that the proposed method outperforms state-of-the-art alternatives for all six datasets. Keras with TenserFlow backend is utilized for implementation. We consider several performance metrics to perform the experimental comparative, including accuracy (AC), sensitivity (SE), specificity (SP), F1-Score, Jaccard similarity (JS), and area under the curve (AUC). We stop the training of the network when the validation loss remains the same in 10 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DRIVE Dataset</head><p>DRIVE <ref type="bibr" target="#b32">[33]</ref> is a dataset for blood vessel segmentation from retina images. It includes 40 color retina images, from which 20 samples are used for training and the remaining 20 samples for testing. The original size of images is 565 × 584 pixels. It is clear that a dataset with this number of samples is not sufficient for training a deep network. Therefore, we use the same strategy as <ref type="bibr" target="#b3">[4]</ref> for training our network. The input images are first randomly divided into a number of patches (64 × 64). In total, around 190, 000 patches are produced from 20 training images, from which 171, 000 patches are used for training, and the remaining 19, 000 patches are used for validation.</p><p>Some precise and promising segmentation results of the proposed network are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. <ref type="table" target="#tab_1">Table I</ref> lists the quantitative results obtained by other methods and the proposed network on DRIVE dataset. We evaluate the network with d = 1 and d = 3 as the number of dense blocks. With d = 1 we have one convolutional block without any dense connection in that layer. With d = 3 we have three convolutional blocks and two dense connections in that layer. It is shown that the MCGU-Net (with both d = 1 and d = 3) outperforms w.r.t. the state-of-the-art alternatives for most of the evaluation metrics.  Moreover, it can be seen that the network with d = 3 works better than d = 3. It is worth mentioning that for this dataset, we achieved better result by training the network from scratch rather than using pre-trained weights.</p><p>To ensure the proper convergence of the proposed network, the training and validation accuracy for DRIVE dataset is shown in <ref type="figure">Figure 5 (a)</ref>. It is shown that the network converges very fast, i.e., after the 30 th epoch. We also can see that in the first 15 epochs the validation accuracy is larger than the training one. This fact is mostly because of the small size of dataset since we use a small set of images as the validation set. Moreover, it might be related to the fact that we evaluate the validation set at the end of epoch. To show the overall performance of the MCGU-Net on DRIVE dataset, ROC curves is shown in <ref type="figure">Figure 6 (a)</ref>. ROC is the plot of the true positive rate (TPR) against the false positive rate (FPR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ISIC 2017 Dataset</head><p>The ISIC 2017 dataset <ref type="bibr" target="#b34">[35]</ref> is taken from the Kaggle competition. We evaluate the proposed method on the provided data for skin lesion segmentation. This dataset provides 2000 skin lesion images as a training set with masks (containing cancer or non-cancer lesions) for segmentation. We use 1250 samples for training, 150 samples as validation data, and the other 600 samples for test. The original size of each sample is 576 × 767. We resize images to 256 × 256. For this dataset, we train the network with pre-trained weights on imageNet.</p><p>Since the input data is RGB images, the pre-trained weights are good initialization for the network. <ref type="figure">Figure 7</ref>(a) shows some segmentation results of the proposed network on ISIC 2017. In <ref type="table" target="#tab_1">Table II,</ref>  than the other approaches. Moreover, the result of MCGU-Net with three dense blocks is a bit higher than with one dense block. The training and validation accuracy of the proposed network for this dataset is shown in <ref type="figure">Figure 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ISIC 2018 Dataset</head><p>This dataset <ref type="bibr" target="#b35">[36]</ref> was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of der- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>F1 SE SP AC PC JS U-net <ref type="bibr" target="#b2">[3]</ref> 0.647 0.708 0.964 0.890 0.779 0.549 Att U-net <ref type="bibr" target="#b4">[5]</ref> 0.665 0.717 0.967 0.897 0.787 0.566 R2U-net <ref type="bibr" target="#b3">[4]</ref> 0.679 0.792 0.928 0.880 0.741 0.581 Att R2U-Net <ref type="bibr" target="#b3">[4]</ref> 0.691 0.726 0.971 0.904 0.822 0.592 MCGU-Net (d=1) 0.889 0.845 0.984 0.952 0.938 0.952 MCGU-Net (d=3) 0.895 0.848 0.986 0.955 0.947 0.955 moscopy images in 2018. It includes 2594 images where like previous approaches <ref type="bibr" target="#b3">[4]</ref>, we used 1815 images for training, 259 for validation and 520 for testing. The original size of each sample is 2016 × 3024. We resize images to 256 × 256. The training data consists of the original images and corresponding ground truth annotations (containing cancer or non-cancer lesions). Like the ISIC 2017 dataset, the proposed network works better with pre-trained weights. For qualitative analysis, <ref type="figure">Figure 7</ref>(b) shows some example outputs of the proposed MCGU-Net on ISIC 2018. <ref type="table" target="#tab_1">Table III</ref> lists the quantitative results obtained by different methods and the proposed network on this dataset. A large improvement is achieved by the MCGU-Net (with both d = 1 and d = 3) w.r.t. state-of-theart alternatives for all of the evaluation metrics. It is clear that the network with d = 3 works better than the one with d = 1. It is worth mentioning that there was a challenge on ISIC dataset and the best result achieved by the participants was JS = 0.802. Compare to this result, there is a good gap between the JS achieved by the MCGU-Net (0.955) and the best result of the ISIC challenge.</p><p>The training and validation accuracy of the proposed network for ISIC dataset is shown in <ref type="figure">Figure 5</ref> (c). The convergence speed of the network for ISIC dataset is fast (after 40 epochs). The validation accuracy over the training process is variable. The reason behind this fact is that the validation set contains some images totally different from the ones in training set, therefore, during the first learning iterations the model has some problems about segmenting those images. To show the overall performance of the MCGU-Net on ISIC dataset, the ROC curves are shown in <ref type="figure">Figure 6</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Lung Segmentation Dataset</head><p>A lung segmentation dataset is introduced in the Lung Nodule Analysis (LUNA) competition at the Kaggle Data Science Bowl in 2017. This dataset consists of 2D and 3D CT images with respective label images for lung segmentation <ref type="bibr">[38]</ref>. We use 70% of the data as the train set and the remaining 30% as the test set. The size of each image is 512 × 512. For this dataset, the MCGU-Net works better with training from scratch since the input data is entirely different from images in ImageNet dataset. Since the lung region in CT images have almost the same Hausdorff value with non-object of interests such as bone and air, it is worth to learn lung region by learning its surrounding tissues. To do that first we extract the surrounding region by applying algorithm 1 and then make a new mask for the training sets. We train the model on these new masks and on the testing phase,and estimate the lung region as a region inside the estimated surrounding tissues. <ref type="figure" target="#fig_7">Figure 8</ref> shows some segmentation outputs of the proposed network for lung dataset. The quantitative results of the proposed MCGU-Net is compared with other methods in <ref type="table" target="#tab_1">Table  IV</ref>. It is clear that the MCGU-Net (with both d = 1 and d = 3) outperforms the other methods. Moreover, the network with dense connections works better. The training and validation accuracy for this dataset is shown in <ref type="figure">Figure 5 (d)</ref>. To show the overall performance of the network on this dataset, ROC curves is shown in <ref type="figure">Figure 6</ref> (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. P H 2 Dataset</head><p>The P H 2 dataset <ref type="bibr" target="#b37">[39]</ref> is a a dermoscopic image database proposed for segmentation and classification. It contains a total number of 200 melanocytic lesions, including 80 common nevi, 80 atypical nevi, and 40 melanomas. The manual segmentations of the skin lesions are availablee. Each input image is a 8-bit RGB color images with a resolution of 768 × 560 pixels. There are not a pre-defined test and train sets for this dataset. We follow the experimental setting used in <ref type="bibr" target="#b38">[40]</ref>. We randomly split the dataset into two sets of 100 images, and then use one set as the test data, 80% of the other set for  the train, and the remained data for the validation. Since the number of data is not enough for training the network, we exploit the learnt weights of ISIC 2017 as the pre-trained model (like <ref type="bibr" target="#b38">[40]</ref>) and then finetune the network with train data. Some segmentation outputs of the proposed network for P H 2 dataset are depicted in <ref type="figure">Figure 7</ref>(c). In <ref type="table" target="#tab_4">Table V</ref>, the results of the proposed network are compared with other state-of-theart approaches. We can see the MCGU-Net results in better performance than other methods. The network has almost the same performance for both d = 1 and d = 3. The reason behind this fact is the small size of training data since the network with d = 1 contains fewer parameters for learning. The training and validation accuracy for this dataset is shown in <ref type="figure">Figure 5</ref> (e). The network converges very fast (20 th epoch) which might be related to the small size of data. The ROC curve is shown in <ref type="figure">Figure 6</ref> (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Cell Nuclei Dataset</head><p>We evaluate the proposed network on the dataset from 2018 Kaggle Data Science Bowl 2018 <ref type="bibr" target="#b43">[44]</ref>. This data is captured with various situations, like different cell type, illumination status, and image size. Moreover, this dataset contains smaller regions inside images for segmentation for which we want to evaluate the performance of the MCGU-Net. It includes a total number of 670 images. We randomly split the data into 70% training, 10% validation, and 20% test data sets. <ref type="figure" target="#fig_9">Figure  9</ref> shows some segmentation outputs of MCGU-Net. In <ref type="table" target="#tab_1">Table  VI</ref>, the performance of the proposed method is compared with other approaches. We can see there is a high gap between the results of the MCGU-Net and other methods. The network works better with d = 3 for this data. The training and validation accuracy for this dataset is shown in <ref type="figure">Figure 5</ref> (f). Since the validation and training data are taken from the same Input GT Estimated  set (and validation set is smaller than train set), the validation accuracy is a bit higher than the training. The ROC curve is shown in <ref type="figure">Figure 6</ref> (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discussion</head><p>The proposed network has some modifications from the original U-Net. We evaluate each modified part of the network to analyze its influence on the result.</p><p>We included BN after each up-convolutional layer to speed up the network learning process. To evaluate the effect of this function, we train the network with and without BN. BN yields the network to converge 2 times faster. BN manages the variations among data by standardizing data through controlling the mean and variance of distributions of inputs which results in a small regularization and reducing generalization error.</p><p>The last convolutional layer of the encoding path is augmented with dense blocks. The results for the network with 1 and 3 dense blocks are reported for all datasets. In Tables I to VI, it can be seen that MCGU-Net with 3 dense block results in better performance. The key idea of dense convolutions is collecting knowledge by sharing feature maps between blocks through direct connection between convolutional block. Consequently, each dense block receives all preceding layers as input, and therefore, produces more diversified and richer features. Thus, it helps the network to increase the representational power of deeper models. We have more feature propagation both in backward and forward paths through dense blocks. The network performs a kind of deep supervision in backward path since dense block receives additional supervision from loss function through shorter connections <ref type="bibr" target="#b8">[9]</ref>. The error signal is propagated to earlier layers more directly, hence, earlier layers can get direct supervision from the final softmax layer, and moreover, it results in decreasing the vanishing-gradient problem. In addition, compared to other deep architectures like residual connections, dense convolutions require fewer parameters while improving the accuracy of the network.</p><p>In the proposed network, we used multi-level BConvLSTMs to combine encoded and decoded features. The encoded features have higher resolution and therefore contain more local  information of the input image, while the decoded features have more semantic information about the input images. The affection of these two features over each other might result in a set of feature maps rich in both local and semantic information. Therefore, instead of a simple concatenation, we utilize BConvLSTM to combine the encoded and decoded features. In BConvLSTM, a set of convolution filters are applied on each kind of features. Therefore each ConvLSTM state, corresponds to one kind of features (e.g. encoded features), ia able to encode relevant information about the other kind of features (e.g. decoded features). The convolutional filters along with the hyperbolic tangent functions help the network to learn complex data structures. <ref type="figure" target="#fig_0">Figure 10</ref> shows the output segmentation mask of the original U-Net and MCGU-Net for two samples of the ISIC 2018 dataset. It shows a more precise and fine segmentation output of the proposed network.</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, we compare the segmentation output of the MCGU-Net with and without SE blocks for two samples of ISIC 2018 dataset, which demonstrates the power of SE features on semantic segmentation. It can be seen that the SE blocks help the network to produce more precise output by a context gating mechanism. To do that, this block exploits the global information embedded features in different channels and assign different channel attentions. The quality of the segmentation output of a network relies on effective feature learning. These findings reveal that the adaptive feature recalibration of SE blocks result in boosting the representational power of deep networks by focusing on informative features and suppressing weak ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed MCGU-Net for medical image segmentation. We showed that by including multi-level BConvLSTM in the skip connection, SE blocks in decoding path, inserting a densely connected convolutional blocks, and also employing SE blocks in decoding path, the network is able to capture more discriminative information which resulted in more precise segmentation results. Moreover, we were able to speed up the network by utilizing BN after the up-convolutional layer. The experimental results on six public benchmark datasets showed high gain in semantic segmentation in relation to stateof-the-art alternatives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Different applications of medical image segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>MCGU-Net with bi-directional ConvLSTM in the skip connections, SE modlues in the decoding path, and densely connected convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>BConvLSTM with SE block in the decoding path of MCGU-Net.power of the network. These modules encode feature maps by assigning a weight for each channel (i.e. channel attention) in the feature map.The SE block includes two parts squeeze and excitation. The first operation is squeeze. The input feature maps to SE block are aggregated to generate channel descriptor by employing global average pooling (GAP) of the whole context of channels. We haveX up d = [x up 1 , x up 2 , ..., x up F ], where x up f ∈ R W ×H , as the input data to the SE block. The spatial squeeze (GAP) is performed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Segmentation result of MCGU-Net on DRIVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>the results of the MCGU-Net on this dataset are compared with the state-ofthe-art approaches. It can be seen that MCGU-Net with both d = 1 and d = 3 achieves better results (except sensitivity) Training and validation accuracy of MCGU-Net for six datasets. ROC diagrams of the proposed MCGU-Net for six dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b). The network converges very fast for this data (after the 30 th epoch). To show the overall performance of the MCGU-Net on ISIC 2017 dataset, the ROC curves are shown in Figure 6 (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Fig. 7 .</head><label>27</label><figDesc>ISIC 2017, (b) ISIC 2018, (c) P H Segmentation result of MCGU-Net on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Segmentation result of MCGU-Net on Lung dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1</head><label>1</label><figDesc>Pre-processing over lung dataset. 1: Input = X and GT M ask M in range = −512 M ax range = +512 2: Output = Surrounding M ask 3: X(X &gt; M ax range) = M ax range X(X &lt; M in range) = M in range {Remove bones and vessels} 4: X = N orm(X) {Normalize X} 5: X = image2binary(X) {Convert to binary} 6: X = X ∪ GT M ask 7: X = M orphology(X) {Remove noise} 8: Surrounding M ask = X − GT M ask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Segmentation result of MCGU-Net on cell nuclei Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Visual effect of BConvLSTM in MCGU-Net . From left column 1 input, 2 GT mask, 3 and 4 are the outputs of network without and with ConvLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Visual effect of SE blocks in MCGU-Net. From left column 1 input, 2 GT mask, 3 and 4 are the outputs of network without and with SE blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON ON DRIVE DATASET.</figDesc><table><row><cell>Methods</cell><cell>F1</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>AUC</cell></row><row><cell>U-net [3]</cell><cell cols="5">0.8142 0.7537 0.9820 0.9531 0.9755</cell></row><row><cell>Deep Model [34]</cell><cell>-</cell><cell cols="4">0.7763 0.9768 0.9495 0.9720</cell></row><row><cell>Att U-net [5]</cell><cell cols="5">0.8155 0.7751 0.9816 0.9556 0.9782</cell></row><row><cell>RU-net [4]</cell><cell cols="5">0.8149 0.7726 0.9820 0.9553 0.9779</cell></row><row><cell>R2U-Net [4]</cell><cell cols="5">0.8171 0.7792 0.9813 0.9556 0.9782</cell></row><row><cell cols="2">MCGU-Net (d=1) 0.8222</cell><cell>0.8012</cell><cell cols="3">0.9784 0.9559 0.9788</cell></row><row><cell cols="3">MCGU-Net (d=3) 0.8224 0.8007</cell><cell cols="3">0.9786 0.9560 0.9789</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON ON ISIC 2017 DATASET.</figDesc><table><row><cell>Methods</cell><cell>F1</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>JS</cell></row><row><cell>U-net [3]</cell><cell>0.8682</cell><cell cols="4">0.9479 0.9263 0.9314 0.9314</cell></row><row><cell>Melanoma det. [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>o.9340</cell><cell>-</cell></row><row><cell>Lesion Analysis [37]</cell><cell>-</cell><cell cols="3">0.8250 0.9750 0.9340</cell><cell>-</cell></row><row><cell>R2U-net [4]</cell><cell>0.8920</cell><cell>0.9414</cell><cell cols="3">0.9425 0.9424 0.9421</cell></row><row><cell>MCGU-Net (d=1)</cell><cell>0.8871</cell><cell>0.8305</cell><cell>0.9888</cell><cell cols="2">0.9555 0.9555</cell></row><row><cell>MCGU-Net (d=3)</cell><cell cols="2">0.8927 0.8502</cell><cell>0.9855</cell><cell cols="2">0.9570 0.9570</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">PERFORMANCE COMPARISON ON ISIC 2018 DATASET.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON ON LUNG DATASET.</figDesc><table><row><cell>Methods</cell><cell>F1</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>JS</cell></row><row><cell>U-net [3]</cell><cell cols="5">0.9658 0.9696 0.9872 0.9872 0.9858</cell></row><row><cell>RU-net [4]</cell><cell cols="5">0.9638 0.9734 0.9866 0.9836 0.9836</cell></row><row><cell>R2U-Net [4]</cell><cell>0.9832</cell><cell>0.9944</cell><cell cols="3">0.9832 0.9918 0.9918</cell></row><row><cell>MCGU-Net (d=1)</cell><cell cols="5">0.9889 0.9901 0.9979 0.9967 0.9967</cell></row><row><cell cols="6">MCGU-Net (d=3) 0.9904 0.9910 0.9982 0.9972 0.9972</cell></row><row><cell>Input GT Estimated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON ON P H 2 DATASET.</figDesc><table><row><cell>Methods</cell><cell>DIC</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>JS</cell></row><row><cell>FCN [41]</cell><cell>0.8903</cell><cell cols="2">0.9030 0.9402</cell><cell>0.9282</cell><cell>0.8022</cell></row><row><cell>U-net [3]</cell><cell>0.8761</cell><cell cols="2">0.8163 0.9776</cell><cell>0.9255</cell><cell>0.7795</cell></row><row><cell>SegNet [42]</cell><cell>0.8936</cell><cell cols="2">0.8653 0.9661</cell><cell>0.9336</cell><cell>0.8077</cell></row><row><cell>FrCN [43]</cell><cell>0.9177</cell><cell>0.9372</cell><cell>0.9565</cell><cell>0.9508</cell><cell>0.8479</cell></row><row><cell>MCGU-Net (d=1)</cell><cell>0.9762</cell><cell cols="2">0.8727 0.9925</cell><cell>0.9536</cell><cell>0.9536</cell></row><row><cell cols="2">MCGU-Net (d=3) 0.9763</cell><cell cols="2">0.8322 0.9714</cell><cell cols="2">0.9537 0.9537</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>COMPARISON ON CELL NUCLEI DATASET.</figDesc><table><row><cell>Methods</cell><cell>F1</cell><cell>DIC</cell><cell>AC</cell><cell>JS</cell></row><row><cell>U-net [3]</cell><cell cols="3">0.8994 0.9077 0.9604</cell><cell>0.8310</cell></row><row><cell>Att U-Net [5]</cell><cell cols="3">0.8899 0.8879 0.9672</cell><cell>0.7984</cell></row><row><cell>FocusNet [45]</cell><cell cols="3">0.8998 0.8996 0.9697</cell><cell>0.8176</cell></row><row><cell>MCGU-Net (d=1)</cell><cell cols="3">0.9295 0.9882 0.9766</cell><cell>0.9766</cell></row><row><cell cols="4">MCGU-Net (d=3) 0.9306 0.9884 0.9771</cell><cell>0.9771</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code is available on https://github.com/rezazad68/BCDU-Net.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ca Cancer J Clin</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Jemal a</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional convlstm u-net with densley connected convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPRW</title>
		<meeting>the CVPRW</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Brain mri segmentation with patch-based cnn approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 35th Chinese Control Conference (CCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7026" to="7031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep mri brain extraction: a 3d convolutional neural network for skull stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="460" to="469" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Threedimensional ct image segmentation by combining 2d fully convolutional network with 3d majority voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An application of cascaded 3d fully convolutional networks for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="90" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d u-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cnn-based segmentation of medical imaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kayalibay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03056</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the compactness, efficiency, and representation of 3d convolutional networks: brain parcellation as a pretext task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="348" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPRW</title>
		<meeting>the IEEE CVPRW</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional structured lstm networks for joint 4d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 15th</title>
		<imprint>
			<biblScope unit="page" from="1104" to="1108" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Use-net: incorporating squeeze-and-excitation blocks into u-net for prostate zonal segmentation of multi-institutional mri datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rundo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Anatomynet: Deep 3d squeeze-and-excitation u-nets for fast and fully automated whole-volume anatomical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">392969</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer learning improves supervised image segmentation across imaging protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Opbroek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Vernooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1018" to="1030" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep bidirectional and unidirectional lstm recurrent neural network for network-wide traffic speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02143</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segmenting retinal blood vessels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 15th</title>
		<imprint>
			<biblScope unit="page" from="168" to="172" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Skin lesion analysis towards melanoma detection using deep learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">556</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ph 2-a dermoscopic image database for research and benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Marcal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 35th EMBC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5437" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An enhanced neural network based on deep metric learning for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Chinese Control And Decision Conference (CCDC)</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1633" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation in dermoscopy images via deep full resolution convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Masni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Antari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">2018 data science bowl, find the nuclei in divergent images to advance medical discovery</title>
		<ptr target="https://www.kaggle.com/c/data-science-bowl" />
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Focusnet: an attention-based fully convolutional network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th ISBI 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="455" to="458" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
