<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quo Vadis, Skeleton Action Recognition?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubh</forename><surname>Maheshwari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><surname>Ravi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
						</author>
						<title level="a" type="main">Quo Vadis, Skeleton Action Recognition?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>human action recognition · human activity recognition · skeleton · 3-D human pose · deep learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding human actions, especially from their 2-D and 3-D joint-based skeleton representations, has received a lot of focus recently. Joint-based representations have a small memory footprint which improves Ravi Kiran Sarvadevabhatla F23, 3rd Floor, KCIS, IIIT Hyderabad, Gachibowli, Hyderabad 50032, INDIA E-mail: ravi.kiran@iiit.ac.in feasibility of on-board processing in compute-restricted environments (e.g. smartphones, cameras on IoT devices). The privacy-friendly nature of the skeleton representation is also an advantageous factor.</p><p>On the flip side, obtaining accurate 3-D skeleton data usually requires specialized capture mechanisms and constraints on the capture environment. Even after the capture hurdle is crossed, the sparsity of skeleton representation relative to denser counterparts (RGB, depth) induces ambiguity and imposes additional challenges. In addition, the lack of large-scale, diverse datasets remained a challenge until the advent of datasets such as NTU-60 <ref type="bibr" target="#b40">[41]</ref> and PKU-MMD <ref type="bibr" target="#b11">[12]</ref>. These datasets have prompted a number of diverse approaches for skeletonbased action recognition <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b27">28]</ref>. The introduction of the even larger NTU-120 dataset <ref type="bibr" target="#b29">[30]</ref> is poised to continue this trend.</p><p>The datasets and capture methods for aforementioned works are confined to controlled, indoor settings. Naturally, this prompts the question regarding the ability to recognize human activities occurring outdoors, 'in the wild' ? Also, in recent times, a number of works on robust estimation of human 3-D pose from RGB data have emerged <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. These prompt yet another question: How well can human actions be recognized in terms of 3-D skeletal pose estimated from RGB videos? To answer these questions, we introduce Skeletics-152 (Sec. 3.1), a carefully curated and 3-D pose-annotated subset of videos sourced from Kinetics-700 <ref type="bibr" target="#b7">[8]</ref>, a large-scale RGB action dataset.</p><p>Actions in NTU-120 and Kinetics datasets retain either full or partial context supplied by object interactions and background. In contrast, out-of-context actions represent an unconventional and challenging frontier for skeleton action recognition. To benchmark performance for such actions, we introduce the skeletal ver-sion of Mimetics <ref type="bibr" target="#b51">[52]</ref>, a subset of Kinetics-400 containing exaggerated, out-of-context human actions (Sec. <ref type="bibr">3.2)</ref>. Additionally, we introduce Metaphorics, a new video dataset with detailed action phrase annotations for videos of the popular social game Dumb Charades and expert dance performances of popular songs (Sec. <ref type="bibr">3.3)</ref>.</p><p>Typically, the introduction of a newer, larger dataset (NTU-120) is marked by a flurry of novel architectures which aim to solve challenging domain tasks. In this paper, we argue that this is also a good opportunity to evaluate approaches originally trained for earlier dataset versions and more generally, re-evaluate the status quo. This argument has already been made successfully for RGB action recognition <ref type="bibr" target="#b44">[45]</ref>. To this end, we benchmark state-of-the-art approaches on the NTU-120 dataset and analyze the results (Sec. 4). Subsequently, we evaluate the performance of top ranked models on our newly introduced datasets -Skeletics-152 (Sec. 5.1), Skeleton-Mimetics (Sec. 5.2), Metaphorics (Sec. 5.3).</p><p>Overall, our work characterizes the strengths and limitations of existing approaches and datasets. It also provides an assessment of top-performing approaches across a spectrum of activity settings and via the introduced datasets, proposes new frontiers for human action recognition.</p><p>Our primary contributions can be summarized as follows:</p><p>-We introduce Skeletics-152, a curated 3-D pose annotated subset of Kinetics-700 for benchmarking skeleton action recognition 'in the wild'(Sec. 3.1). -We introduce Skeleton-Mimetics to recognize skeletonbased out-of-context and exaggerated actions (Sec. 3.2). -We introduce Metaphorics, a new video dataset with phrase annotations for YouTube videos of Dumb Charades and interpretative dance to explore the new frontier of metaphor-style actions (Sec. 3.3). -We benchmark current, past state-of-the-art skeleton action recognition approaches on in-lab datasets (Sec. 4) and also on our newly introduced datasets containing actions happening in-the-wild (Sec. 5). We also summarize trends within and across these various datasets (Sec. 6).</p><p>To enable a rich, interactive exploration of our contributions mentioned above, we have made them available at https://skeleton.iiit.ac.in/. The website features an interactive data analytics dashboard, code and pre-trained models for top-performing skeleton action recognition models and new skeleton action datasets (Skeletics-152, Skeleton-Mimetics, Metaphorics) introduced by us for additional exploration and benefit of the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Skeletal Datasets: Numerous 3-D skeletal datasets <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref> have been proposed over the last decade to further the advances in human action understanding. These datasets are focused on sequence based action detection and involve human subjects performing daily actions captured from multiple viewpoints. Sadeghipour et. al. <ref type="bibr" target="#b38">[39]</ref> introduce 3D Iconic gesture dataset where subjects outline the shape of virtual objects that is captured via Kinect v2 sensor. Recent work by Yan et al. <ref type="bibr" target="#b54">[55]</ref> introduces Skeleton-Kinetics using 2-D Open Pose on the large scale video dataset Kinetics-400. Weinzaepfel et. al. take this idea further and introduce Mimetics <ref type="bibr" target="#b51">[52]</ref> containing a subset of Kinetics-400 with mimed actions.</p><p>Skeleton Action Recognition: An earlier era of works serve to document handcrafted features for skeleton action recognition <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. The recent class of approaches based on deep networks can be broadly categorized into three groups based on input skeleton data representation.</p><p>The first group explicitly consider the sequential nature of actions wherein the temporal dependencies are modelled using an RNN or an LSTM <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59]</ref>. To further discriminate activities based on the joint dependencies, Song et al. <ref type="bibr" target="#b45">[46]</ref> introduce attention mechanisms at multiple levels in the network. Kundu et. al. <ref type="bibr" target="#b23">[24]</ref> learn the action sequence as a trajectory in the pose manifold for the downstream activity classification task. Caetano et al. <ref type="bibr" target="#b3">[4]</ref> use CNN-based feature representation over a temporal window containing skeleton dynamics.</p><p>The second group of works model the input skeleton as a single spatio-temporal unit. In some instances, this unit is a tensor of the form frames × joints × coordinates which is subsequently processed by a CNN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref>. More recently, a series of approaches use graph convolutions to model the (spatio-temporal) unit. Prominent examples include the ST-GCN framework introduced by Yan et. al. <ref type="bibr" target="#b54">[55]</ref> and variants <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b46">47]</ref>. In contrast to the fixed graph in ST-GCN, newer approaches involve adaptation to learn graph topology <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In addition to the groups mentioned above, hybrid approaches also exist. Si et al. <ref type="bibr" target="#b43">[44]</ref> employ an attentionbased graph convolutional LSTM to capture the spatiotemporal co-occurrence relationships. Zhang et. al. <ref type="bibr" target="#b57">[58]</ref> propose a CNN-RNN late-fusion model with learnable view transformation. For a survey of 3-D skeleton action recognition, refer to Presti et al. <ref type="bibr" target="#b35">[36]</ref> and Wang et. al. <ref type="bibr" target="#b50">[51]</ref>. <ref type="figure">Fig. 1</ref>: A pictorial illustration of the landscape for skeleton-based action recognition. Datasets such as NTU-120 characterize actions in controlled lab-like settings. We use state-of-the-art RGB 3-D pose estimation to obtain skeletons and benchmark recognition models 'in the wild' by introducing Skeletics-152 dataset (Sec. 3.1). To explore out-of-context action recognition in the wild, we introduce Skeleton-Mimetics (Sec. 3.2) and benchmark models trained on Skeletics-152. As a novel frontier for action recognition, we introduce Metaphorics (Sec. 3.2) which contains indirectly conveyed metaphor-style actions. Note that all datasets are skeleton-based -RGB background has been included to convey the original context. Skeleton Action Recognition from RGB video based pose: In another class of approaches, human skeletal pose estimated from in-the-wild RGB video frames is used for action recognition. A number of approaches based on 2-D skeleton pose from RGB video exist <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. A recent variation involves a pseudo 3-D pose representation wherein 2-D OpenPose coordinates <ref type="bibr" target="#b6">[7]</ref> in Kinetics-400 <ref type="bibr" target="#b8">[9]</ref> videos are augmented with joint-level confidence scores as the third coordinate <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>As mentioned in the Introduction (Section 1), largescale datasets such as NTU-120 represent lab-style, controlled, indoor settings. In contrast, a much larger variety of human actions characterize in-the-wild RGB videos of human activities. To obtain 3-D skeleton representations from such videos, pose estimation techniques are applied on action sequences from large-scale activity datasets. In this section, we explore three diverse settings with progressively increasing level of complexity and abstractness in terms of human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skeletics-152</head><p>To source in the wild action videos, we use Kinetics-700 <ref type="bibr" target="#b7">[8]</ref> as the starting point. Kinetics-700 is a large-scale video dataset consisting of over 650,000 YouTube video clips spanning over 700 action categories ranging from daily routine activities, sports and other fine-grained actions. However, unlike previously existing dataset with similar preparatory approach (Skeleton-Kinetics-400 <ref type="bibr" target="#b54">[55]</ref>), we carefully omit categories from action settings which are incompatible for pose-based skeleton action recognition. Specifically, -A number of classes (e.g. 'Petting cat', 'Scrubbing face') were removed because most of the videos contain occluded poses which make the 3D pose estimation unviable. -Some classes (e.g. 'Cooking eggs', 'Wrapping presents', 'Clay pottery making') were removed as they were captured from egocentric views. -Some classes (e.g. 'Peeling apples', 'Peeling potatoes', 'Baking cookies') are highly object-centric and hence, irrelevant for skeleton based action recognition. -Classes involving no substantial movement (e.g. 'Staring', 'Attending a conference') cannot be recognised solely based on human pose. -Classes where the labels differ solely due to scene background were removed (e.g. 'Walking through snow' is same as 'Walking').</p><p>After carefully removing such action categories, we use VIBE <ref type="bibr" target="#b21">[22]</ref> on the remaining 274 categories to obtain the corresponding 3-D skeleton sequences. Within these sequences, we removed classes such as 'Playing American football', 'Playing ice hockey', 'Doing aerobics' containing large groups of people performing different activities within a single video. In addition, classes such as 'Somersaulting', 'Springboard diving' were removed since the VIBE model typically reported missing joints. <ref type="figure" target="#fig_0">Figure 2</ref> shows some examples of omitted action classes.</p><p>For the case of multiple (&gt; 2) skeleton detections in single video, we select the top two skeletons appearing in maximum number of frames. For intermediate frames with missing skeletons, we perform bound- In 'Springboard diving', the person performing the diving action is not tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Best-3</head><p>Worst-3</p><p>Best-3 Worst-3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M I M E T I C S</head><p>￼ ￼ ￼ <ref type="figure">Fig. 3</ref>: Sample skeleton sequences from Skeletics-152 and Mimetics-Skeleton. The sequences are chosen from best-3 and worst-3 classes in terms of performance achieved by best models on these datasets (see <ref type="table" target="#tab_7">Tables 5, 7</ref>). The ground-truth phrase is color-coded green. The top-5 predictions by 4s-ShiftGCN are coded pink and those by MS-G3D are coded blue. Refer to Section 5 for details on the evaluation protocol and predictions.</p><p>ing box and joint interpolation. In the end, we obtain Skeletics-152, our curated 3-D skeleton dataset which contains 125,621 sequences spread over 152 classes. Refer to <ref type="figure">Figure 3</ref> (left) for some example sequences from Skeletics-152. It must be noted that Skeletics-152 is different from Skeleton-Kinetics-400 <ref type="bibr" target="#b54">[55]</ref> which contains pseudo 3-D pose [2-D Pose + Joint-level confidence] obtained from the OpenPose <ref type="bibr" target="#b6">[7]</ref> toolbox. Skeleton-Kinetics-400 indiscriminately includes all categories, without any curation. Unlike VIBE-based skeletons in our dataset, the pseudo 3D Skeleton-Kinetics-400 representations fail to capture the actual dynamics of 3D motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skeleton-Mimetics</head><p>Human actions in the wild tend to be contextual. Context such as background, presence of certain objects can be very influential in certain traditional (RGB) action recognition approaches. This can lead to incorrect predictions when such actions are performed in out-ofcontext scenarios <ref type="bibr" target="#b51">[52]</ref>. To this end, Mimetics dataset, consisting of 50 out-of-context action classes and derived from Kinetics-400 was introduced in <ref type="bibr" target="#b51">[52]</ref>. To explore skeleton action recognition for out-of-context, exaggerated action sequences, we introduce Skeleton-Mimetics. This dataset is derived from Skeletics-152, introduced <ref type="figure">Fig. 4</ref>: An illustration of the annotation for a typical Charades episode using the Anvil interface. 'Action', 'Ground Truth', 'Success/Fail', 'Special Actions' are the annotation channels. In the 'Action'channel, 'rabb...(rabbit)'and 'zorro' are guesses that the guessing player makes for the first two actions performed by the actor performs upon being revealed the ground truth phrase 'the vampire diaries'. The segment labelled 'vampire' in the 'Ground Truth' channel is the entire duration for which the actor tried to act out the word 'vampire'. The 'Success/Fail' channel shows the success and failure for corresponding guesses present in the 'Action' channel. Here, 'rabbit' and 'zorro' are both incorrect and hence they are marked as 'F'. The 'Special Action' channel has tabs containing 'TV' and 'wo...(number of words)'. These are helping actions to indicate that the phrase is the name of a TV show and the number of words in the phrase respectively. previously. To create this new dataset, we shortlisted classes with exaggerated movements and gestures. Instead of considering all the videos, we select specific action videos where action is performed by mimicry experts in out-of-context settings and without object interactions. The final dataset consists of 319 skeleton sequences across 23 classes.</p><p>The proposed dataset differs from the existing Mimetics dataset in terms of accurate 3-D poses. Since Skeleton-Mimetics is derived from a curated set of videos specially designed for skeleton action recognition, it eliminates the factor of unusable 3-D poses for action classification. Further, since we wanted to compare the efficacy of Skeletics-152 as a dataset for pre training skeleton ac-tion recognition models, we keep joint positions same for both the datasets. Refer to <ref type="figure">Figure 3</ref> (right) for some example sequences from Skeleton-Mimetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metaphorics</head><p>The datasets encountered so far can be characterized as verb-based actions, since the action class is fundamentally incomplete without the verb or the activity being performed. However, humans also tend to associate actions to non-verb words or objects, Iconic Gestures dataset <ref type="bibr" target="#b38">[39]</ref> being a known example. In general, actions can be more abstract and used to convey metaphorical concepts. One such scenario is the popular social game of Dumb Charades. The game involves interactive and adaptive guessing of a target 'phrase'(usually a movie title) based on actions being performed by an 'actor'. Unlike other datasets, nouns and adjectives can have action depictions. Moreover, the vocabulary is openended, further compounding the action understanding challenge. To study actions arising in this challenging scenario, we introduce Metaphorics, an even complex dataset. The dataset contains videos from two scenarios -dumb charades and interpretive dance.</p><p>Dumb Charades: We first source Dumb Charade game episodes from YouTube. In the game episodes, one person ('actor') acts out a target phrase word by word while the other player tries to guess the target phrase solely from actions performed by the 'actor'. For annotation, we use the popular Anvil tool <ref type="bibr" target="#b20">[21]</ref>. We annotate (i) target phrase (ii) beginning and ending timestamps for each action segment (iii) guess phrase associ-ated with a segments (iv) episode outcome ('correctly guessed','incorrect'). We also annotate certain special actions such as number of words and the current word number for a multi-word target phrase. These special actions also include helping actions which the actor uses to convey some basic information to the guessers such as length of the word ('long','short'). <ref type="figure">Figure 4</ref> provides an illustration of a typical annotation for a Charades video episode.</p><p>To characterize performance of action recognition approaches, we associate each word with its corresponding temporal video segment. After removing instances where the actor is occluded, we obtain 716 segments across 28 game sessions.</p><p>Interpretative dance: We also source YouTube videos containing interpretative dances of popular songs. In these videos, the song is made audible only to the actor who then proceeds to enact real-time actions corresponding to song lyrics. In this case, the guesser needs  <ref type="table">Table 1</ref>: Attributes of different datasets in the skeleton based action recognition domain. Gray-shaded rows correspond to the new datasets introduced in this paper. Prompted means that subjects were instructed what action to perform. N.A means that there is no notion of classes. Not Specified means that the duration of an action is not specified in the respective paper.</p><p>to correctly guess the song title based on the performed lyric-based actions. Unlike Charades, the actor is required to act out the song lyrics in real time which increases the challenge since the actions are more fast paced than Charades.</p><p>As part of the annotation process, we align the lyric subtitle file of original song and the video based on the starting point of the song (in the video). Since the actions are performed in real time, we obtain the action level annotations by aligning the audio file of the video with the original audio file of the song. The temporal extents of the dance are thus annotated into word-level action video segments. We obtain a total of 129 video segments across two full-length music videos.</p><p>In total, our Metaphorics dataset contains 845 video clips. As with other RGB datasets, we obtain corresponding 3-D skeleton sequences using VIBE <ref type="bibr" target="#b21">[22]</ref> -see <ref type="figure" target="#fig_1">Figure 5</ref> for examples. The proposed Metaphorics dataset is very diverse in terms of the action sequences and the labels due to its open-ended vocabulary (see <ref type="figure" target="#fig_2">Figure  6</ref>). Compared to existing datasets, videos tend to be 'bursty' due to the extremely small temporal extents of the actions. The 3D-Iconic dataset <ref type="bibr" target="#b38">[39]</ref> is similar to Metaphorics in the sense that it contains object based gesture actions. However, subjects are prompted to explicitly outline shape of the object categories unlike the unprompted actions seen in Metaphorics.</p><p>To gain an overall perspective about the datasets, both existing ones and those introduced in our work, we summarize their prominent attributes in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Skeleton Action Recognition in the Lab</head><p>In this section, we look at the NTU-120 <ref type="bibr" target="#b29">[30]</ref> dataset which is currently the largest lab styled 3-D skeleton action recognition dataset. It comprises 114,480 25-joint 3-D skeleton annotated videos of 120 human actions, performed by 106 subjects in a controlled indoor setting and captured from 32 different setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Protocol</head><p>Two standard evaluation protocols are typically used for evaluation of multi-subject multi-viewpoint skeleton action recognition approaches. In the Cross Subject protocol, the train and test set are split based on performer id. Under the protocol proposed by Liu et al. <ref type="bibr" target="#b29">[30]</ref> for NTU-120, 53 subject ids out of 106 are allocated for training and the remaining for test. We use data from 11 (20%) randomly selected ids of original training set for validation.</p><p>The other protocol is Cross Setup. By default, action sequences from the 16 even-numbered camera setup ids are used for training and 16 odd setup ids are used for testing. As with cross subject protocol, we retain the original NTU120 test set and use 4 (25%) ids randomly chosen from even setup ids for validation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance with full sequences</head><p>For benchmarking, we selected approaches which report performance on NTU-120 and top 5 approaches with the best performance on NTU-60 <ref type="bibr" target="#b40">[41]</ref>, the precursor to NTU-120. The results on the test set of NTU-120 can be viewed in <ref type="table" target="#tab_2">Table 2</ref>. The results show that 4s-Shift-GCN <ref type="bibr" target="#b9">[10]</ref> and MS-G3D <ref type="bibr" target="#b30">[31]</ref> are the best performers for Cross Setup and Cross Subject respectively. The gray-shaded portion of <ref type="table" target="#tab_2">Table 2</ref> shows the performance of top performing NTU-60 models evaluated on the NTU-120 test set. Note that these models were not originally designed for NTU-120 and were retrained by us from scratch, for benchmarking purposes. From the results, we notice that our version of VA-NN <ref type="bibr" target="#b57">[58]</ref>, retrained with a more powerful backbone (ResNeXt-101) performs competitively with state-of-the-art NTU-120 approaches (MS-G3D and 4s-Shift-GCN). VA-CNN has a relatively simpler architecture and is fast to train , adding to its appeal. More significantly, the results underscore the importance of benchmarking existing approaches on newly introduced datasets while investing effort into creation of novel architectures. <ref type="figure">Figure 7</ref> shows the mean accuracy and associated standard deviation for the top-5 models. The significant magnitude of deviation indicates that additional progress is needed before mean accuracy can be considered a reliable measure of overall performance.</p><p>To obtain a better understanding of performance, we list the 10 best recognized and worst recognized action classes in <ref type="table" target="#tab_4">Table 3</ref> (cross subject) and <ref type="table" target="#tab_5">Table 4</ref> (cross setup). The results show that the best and worst performers largely stay same across all the models. The best performing classes (e.g. 'Arm swings', 'Jump up', 'Walking towards') have distinct actions and involving <ref type="bibr" target="#b0">1</ref> Our version of VA-CNN <ref type="bibr" target="#b56">[57]</ref> with ResNeXt-101 backbone. <ref type="figure">Fig. 7</ref>: Class accuracy plots for NTU-120 with standard deviation <ref type="figure">Fig. 8</ref>: Comparison of top-3 models on the partially observed sequences for NTU-120 (Cross Subject) large joint-level movements. On the other hand, action classes containing subtle actions with fine-grained differences are hardest to recognize and exhibit large intraclass confusion <ref type="figure">(Figure 9</ref>). For instance, 'Make ok sign' and 'Make victory sign' get confused with each other significantly because the actions differ only in terms of hand joint movement which is not captured in adequate detail by the Kinect sensor. In addition, skeletons for classes such as 'Reading'', 'Writing' have very low interclass variability, resulting in poor performance.</p><p>The top 5 models exhibit similarities at the set level for best-10 and worst-10 classes. However, motivated by the variance in actual rank order <ref type="table" target="#tab_4">(Tables 3,4</ref>), we examine performance with an average pooled ensemble of top-5 models. The noticeably improved ensemble performance (bottom row of <ref type="table" target="#tab_2">Table 2</ref>) suggests that the top-5 models span action classes in a complementary manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance on partial sequences</head><p>Action recognition from partially observed sequences has been an active area of research <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b4">[5]</ref> and has many practical applications in the field of video   surveillance and human-computer interaction. The ambiguity induced by partial sequences naturally makes this a challenging problem. To study action recognition in this setting, we benchmark the top-3 models of Table 2 on partially observed skeleton sequences of NTU-120 using the Cross Subject protocol. The increase in accuracy is on expected lines, i.e. actions are generally better recognized when the extent to which they are accessible increases (see <ref type="figure">Figure 8</ref>). Note that 4s-ShiftGCN and MS-G3D outperform the third best performer, VA-CNN, by a noticeable margin. This is likely due to the complementary features which are learnt via the mul-tiple feature stream processing present in 4s-ShiftGCN and MS-G3D models.</p><p>To explore this at class level, we replicate the plot of <ref type="figure">Figure 8</ref>, but now for the top-1 model (MS-G3D) and for individual action categories. The curves for best-5 and worst-5 classes sorted by overall accuracy can be viewed in <ref type="figure" target="#fig_3">Figure 10</ref>. The closer an activity's curve is to the top-left corner, the better is its ability to be recognized early. It is evident from the plot that most of the best-5 classes are not confidently recognizable until 50% of the action sequence is completed. From this viewpoint, it is also interesting to note that some activities (best-3,4) ranked lower than the best-1 but have <ref type="figure">Fig. 9</ref>: The NTU-120 confusion matrix for MS-G3D model sorted by class-wise accuracy shows that the least accurately recognized classes are confused amongst each other (magnified inset).</p><p>an earlier onset of recognition. The temporal effects of intra-class confusion on worst ranked classes can also be observed in this plot.</p><p>To understand recognition onset trends in a more fine-grained manner, we propose Area-under-curve (AUC) as a better alternative, i.e. the normalized area under the curve with % of elapsed sequence on x-axis and % of correctly recognized sequences on the y-axis. The closer a category's AUC to 1, the earlier it can be recognized. Refer to <ref type="figure" target="#fig_4">Figure 11</ref> where best-5 and worst-5 activities by AUC can be seen. Multiple interesting trends can be observed. Firstly, most of the best-5 and worst-5 classes are different from overall accuracy plot counterparts from <ref type="figure" target="#fig_3">Figure 10</ref>. The AUC-wise top performing activities (e.g. 'Shake head', 'Walking', and 'Take off jacket') contain unique action sequences from the beginning and therefore, are more consistently identified with increasing number of frames. Also classes like 'Walking Towards' and 'Walking Apart' which show very minimal intra-class variation and differ only in terms of increasing distance between the subjects are easily differentiable with the increasing frames. The accuracy of 'Point finger' decreases in the first half due to barely discernible joint movement in the initial frames. Among the AUC-wise worst classes ('Snap fingers', 'Make victory sign', 'Make OK sign'), the finger-joint motion is predominant. Since hand joints are not captured via Kinect v2 sensor in NTU-120 dataset, a fundamental bottleneck arises in recognizing these activities regardless of the elapsed time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Skeleton Action Recognition in the Wild</head><p>In the upcoming sections, we describe our experiments using the newly introduced datasets and involving the best performing architectures on NTU-120 dataset discussed in previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Skeletics-152</head><p>The train-test split in Skeletics-152 is created following splits originally provided with Kinetics-700. We use the original validation set of Kinetics-700 as our test set. We randomly split the original Kinect-700 training set into training and validation sets in a 85:15 ratio. To address class imbalance, we employ class-frequency based minibatch resampling and class-based loss weighting.</p><p>We evaluated the best two performers (MS-G3D and 4s-ShiftGCN) from NTU-120 in two training regimes. In one regime, we first extracted VIBE skeletons from RGB videos of NTU-120 and trained the models on these skeletons. This allows us to eliminate the effect of different joint positions in NTU-120 dataset and VIBE     pose estimation. The resulting models were ultimately fine-tuned on the Skeletics-152 data. In the second regime, we trained the models from scratch on Skeletics-152 data. We found that 4s-ShiftGCN provides the best performance ( <ref type="table" target="#tab_7">Table 5</ref>). Pre-training on NTU-120 provides a slight benefit compared to training from scratch. Comparing the performance rates in <ref type="table" target="#tab_2">Tables 2 and 5</ref>, it is evident that skeleton-based action recognition in the wild is significantly more challenging given the inter/intracategory diversity and noise-inducing factors (e.g. occlusion, lighting, uncontrolled background context, interaction with unnecessary objects). In addition, we empirically observed that even the best 3-D pose estimators routinely generate poorly localized joint estimates, impacting performance.  As shown in the <ref type="table" target="#tab_8">Table 6</ref>, the best-5 classes are all exercise based activities which tend to have very low intra class variability. On the other hand, sequences from the worst-5 classes exhibit a lot of diversity and intra-class variability (see <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Skeleton-Mimetics</head><p>Following the procedure of Weinzaepfel et. al. <ref type="bibr" target="#b51">[52]</ref>, we use our proposed Skeleton-Mimetics only for evaluation. Similar to Skeletics-152, we use 4s-ShiftGCN and MS-G3D as the base models. Due to the similarity of challenges faced in recording out-of-context skeleton actions and actions in the wild, we choose Skeletics-152 as the training dataset. The final prediction is obtained by considering the maximum softmax score among the 23 skeleton-mimetics classes out of the 152 in Skeletics. We perform another experiment where instead of training on samples from all the 152 classes of Skeletics-152, samples pertaining only to the 23 skeleton mimetics classes are used.</p><p>The results shown in <ref type="table" target="#tab_10">Table 7</ref> depict that for both the base models, training on the complete Skeletics-152 dataset, improves performance as compared to training  NTU-120 0.14 4s-ShiftGCN <ref type="bibr" target="#b9">[10]</ref> Skeletics-152 0.08 MS-G3D <ref type="bibr" target="#b30">[31]</ref> Skeletics-152 0.10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Metaphorics</head><p>Due to the small number of available action segments, we use the Metaphorics dataset only for evaluation. To begin with, we use the previously determined state of the art skeleton action recognition models -MS-G3D and 4s-ShiftGCN to obtain the class predictions for the action segments. Since the label sets used to train the models and the ones from Metaphorics dataset are different, direct comparison is not possible. Therefore, to perform quantitative evaluation, we propose to compare lexical representations of the predicted label and the ground truth label from the Metaphorics dataset.</p><p>To obtain the lexical representations, we average the word2vec embedding vectors of words that constitute the action description. Subsequently, we compute the cosine distance between these representations. These distances are averaged across the dataset. <ref type="table" target="#tab_13">Table 9</ref> reports the mean performance for two different sets of models -one set trained on NTU-120 and another set trained on Skeletics-152. The rather poor performance can be attributed to the fundamentally different nature of training and evaluation settings (see <ref type="table">Table 1</ref>). In particular, the abstract, non-contextual, gesture-style actions in Metaphorics are qualitatively distinct from the explicit and contextual actions present in training datasets (NTU-120, Skeletics-152).</p><p>To obtain a qualitative perspective, we report top-5 predictions by MS-G3D, 4s-ShiftGCN pre-trained on Skeletics-152 for sample action segments in <ref type="figure" target="#fig_1">Figure 5</ref>. As mentioned before, target phrases in Dumb Charades and interpretative dances are typically enacted indirectly using metaphors. Models trained on other datasets tend to map the skeleton sequence 'literally' to action labels. This explains some of the predictions seen in <ref type="figure" target="#fig_1">Figure 5</ref>. For example, the model predictions for actions shown in the first row are related to the ground-truth tags ('fight', 'swimming'). The predictions for the other example sequences highlight the shortcomings arising from the literal nature of actions in contextual action datasets as mentioned previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we analyze the salient trends for skeleton action recognition approaches across different datasets. The list of best-5 and worst-5 classes for various scenarios (datasets) can be viewed in <ref type="table" target="#tab_12">Table 8</ref>.</p><p>The first two columns correspond to the lab-based indoor datasets -NTU-60 and NTU-120. It is interesting to note that even the worst performing classes of NTU-60 have accuracy in the range 50-70 % while the counterparts in NTU-120 exist in a much lower range (32-60 %). One reason is that the introduction of NTU-120 resulted in an increase of action classes with subtle, finger-level movements which impacts performance as mentioned previously (Section 4.3). Overall, our analysis motivates the need for approaches which can explicitly focus on boosting the performance for classes ranked lowest. Another concurrent requirement arising from our analysis is for skeleton representations which provide finger-level joint information.</p><p>We have already seen that average performance in the wild is relatively lower compared to lab-based settings <ref type="table" target="#tab_2">(Tables 2, 5</ref>). The results from <ref type="table" target="#tab_12">Table 8</ref> for Skeletics-152 reflect this trend as well. Actions in the wild exhibit large intra-class variability which affects even the best-5 classes (cf. best-5 of NTU-120). Actions belonging to the worst-5 classes in Skeletics-152 and Skeleton-Mimetics are characterized either by high intra-class variability or by containing subtle, finger-dominant motions which cannot be captured by existing skeleton representations. Additionally, action sequences in NTU-120 are somewhat choreographed, having a defined starting pose and ending pose, but this is absent in Skeletics.</p><p>In terms of base architectures, MS-G3D provides the best performance across various datasets except for Skeletics-152, where 4s-ShiftGCN is the best performer. A pictorial illustration of performance trends in the top-2 models for selected action classes from Skeletics-152 and Skeleton-Mimetics can be viewed in <ref type="figure">Figure 3</ref>. Interestingly, even for the classes with lowest performance (worst-3), the correct prediction for Skeletics is often in the list of top-5 model predictions. This is similar to the trend already observed for NTU-120 (Section 4).</p><p>Such class-level insights cannot be deduced for the proposed Metaphorics dataset since its label set is openended, i.e. does not contain a fixed set of action categories. We believe this setting represents an open frontier for skeleton action recognition. The performance on skeleton version of Metaphorics provides an opportunity to study the generalization capabilities and limitations of existing approaches which are typically optimized for non-interactive, category-based, closed-world recognition paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have examined multiple existing and upcoming frontiers in the landscape of skeleton-based human action recognition. As an important facet of establishing new frontiers for skeleton action understanding in the wild, we curate and introduce three new datasets -Skeletics-152, Skeleton-Mimetics and Metaphorics. Our experiments and benchmarking reveal the capabilities and shortcomings of state-of-theart recognition models. In addition, the results also highlight the bias induced by processing components (e.g. RGB 3-D pose estimation) and the task paradigm (classification). We hope these findings and the newly introduced datasets will spur the design of better models for 'in the wild' actions, both contextual and noncontextual -see the work of Moon et. al. <ref type="bibr" target="#b32">[33]</ref> as a preliminary representative example.</p><p>As mentioned earlier, our findings can be interactively explored at https://skeleton.iiit.ac.in/. The website features an interactive data analytics dashboard, code and pre-trained models for top-performing skeleton action recognition models and new skeleton action datasets (Skeletics-152, Skeleton-Mimetics, Metaphorics) introduced by us for additional exploration and benefit of the community.</p><p>In our current work, we have not examined approaches which map skeleton actions to lexical phrase representations (cf. class labels) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> in detail. We intend to study this promising frontier in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of classes from Kinetics-700 omitted for skeleton action recognition. In 'Playing American football', multiple people are detected. For 'Playing ice hockey' and 'Somersaulting', pose estimation is not accurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Sample skeleton sequences from our Metaphorics dataset. The ground-truth phrase is color-coded green. The top-5 predictions by 4s-ShiftGCN are coded pink and those by MS-G3D are color-coded blue. Refer to Section 5.3 for details on the evaluation protocol and predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Part of speech distribution across the groundtruth for Metaphorics dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 :</head><label>10</label><figDesc>Early recognition curves for best-5, worst-5 classes of MS-G3D model on NTU-120 Cross Subject with classwise accuracy as the measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 :</head><label>11</label><figDesc>Early recognition curves for best-5, worst-5 classes of MS-G3D model on NTU -120 Cross Subject with AUC as the measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Benchmarking comparison for NTU-120 test</cell></row><row><cell>set (mean accuracy). Gray-shaded lines correspond to</cell></row><row><cell>models which originally reported results on NTU-60 but</cell></row><row><cell>retrained by us on NTU-120 for comparison. The last</cell></row><row><cell>row of the table corresponds to the new state-of-the-</cell></row><row><cell>art which is average pooled ensemble of best-5 models</cell></row><row><cell>above.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>4s-ShiftGCN</cell><cell>MS-G3D</cell><cell>VA-CNN</cell><cell>2s-SDGCN</cell><cell>2s-AGCN</cell></row><row><cell></cell><cell>Put on jacket</cell><cell>Stand up</cell><cell>Walking towards</cell><cell>Walking towards</cell><cell>Walking towards</cell></row><row><cell></cell><cell>Walking towards</cell><cell>Nod head or bow</cell><cell>Falling down</cell><cell>Put on jacket</cell><cell>Put on jacket</cell></row><row><cell></cell><cell>Staggering</cell><cell>Put on jacket</cell><cell>Jump up</cell><cell>Stand up</cell><cell>Stand up</cell></row><row><cell></cell><cell>Jump up</cell><cell>Walking towards</cell><cell>Staggering</cell><cell>Nod head or bow</cell><cell>Hopping</cell></row><row><cell>Best-10</cell><cell>Stand up Walking apart</cell><cell>Arm circles Hopping</cell><cell>Hopping Stand up</cell><cell>Walking apart Hopping</cell><cell>Arm circles Staggering</cell></row><row><cell></cell><cell>Take off jacket</cell><cell>Staggering</cell><cell>Arm swings</cell><cell>Arm circles</cell><cell>Nod head or bow</cell></row><row><cell></cell><cell>Hopping</cell><cell>Arm swings</cell><cell>Walking apart</cell><cell>Jump up</cell><cell>Cheer up</cell></row><row><cell></cell><cell>Nod head or bow</cell><cell>High five</cell><cell>Cross toe touch</cell><cell>Take off jacket</cell><cell>Drink after cheers</cell></row><row><cell></cell><cell>Cross toe touch</cell><cell>Walking apart</cell><cell>Nod head or bow</cell><cell>Sit down</cell><cell>Walking apart</cell></row><row><cell></cell><cell>Staple book</cell><cell>Writing</cell><cell>Make ok sign</cell><cell>Staple book</cell><cell>Cutting paper</cell></row><row><cell></cell><cell>Writing</cell><cell>Staple book</cell><cell>Staple book</cell><cell>Writing</cell><cell>Play magic cube</cell></row><row><cell></cell><cell>Make ok sign</cell><cell>Cutting paper</cell><cell>Writing</cell><cell>Cutting paper</cell><cell>Make ok sign</cell></row><row><cell></cell><cell>Cutting paper</cell><cell>Make ok sign</cell><cell>Counting money</cell><cell>Yawn</cell><cell>Staple book</cell></row><row><cell>Worst-10</cell><cell>Yawn Make victory sign</cell><cell>Counting money Cutting nails</cell><cell>Reading Yawn</cell><cell>Counting money Cutting nails</cell><cell>Make victory sign Counting money</cell></row><row><cell></cell><cell>Wield knife</cell><cell>Yawn</cell><cell>Make victory sign</cell><cell>Make ok sign</cell><cell>Writing</cell></row><row><cell></cell><cell>Counting money</cell><cell>Wield knife</cell><cell>Cutting paper</cell><cell>Make victory sign</cell><cell>Yawn</cell></row><row><cell></cell><cell>Reading</cell><cell>Reading</cell><cell>Play with phone or tablet</cell><cell>Reading</cell><cell>Type on keyboard</cell></row><row><cell></cell><cell>Cutting nails</cell><cell>Make victory sign</cell><cell>Hit with object</cell><cell>Play magic cube</cell><cell>Cutting on nails</cell></row></table><note>Best-10 and Worst-10 classes for models trained on NTU-120 (Cross Subject)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Best-10 and Worst-10 classes for the models trained on NTU120 (Cross Setup)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results on Skeletics-152 test set with mean accuracy as performance measure.</figDesc><table><row><cell></cell><cell>MS-G3D</cell><cell>4s-ShiftGCN</cell></row><row><cell></cell><cell>Mountain climber (exercise)</cell><cell>Mountain climber (exercise)</cell></row><row><cell></cell><cell>Front raises</cell><cell>Clean and jerk</cell></row><row><cell>Best-5</cell><cell>Jumping Jacks</cell><cell>Front raises</cell></row><row><cell></cell><cell>Deadlifting</cell><cell>Lunge</cell></row><row><cell></cell><cell>Lunge</cell><cell>Jumping jacks</cell></row><row><cell></cell><cell>High fiving</cell><cell>Falling off chair</cell></row><row><cell></cell><cell>Cumbia</cell><cell>Cumbia</cell></row><row><cell>Worst-5</cell><cell>Falling off chair</cell><cell>Swinging baseball bat</cell></row><row><cell></cell><cell>Hugging (not baby)</cell><cell>Passing American football (not in game)</cell></row><row><cell></cell><cell>Combing hair</cell><cell>Digging</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Best-5 and Worst-5 classes of all models trained on Skeletics-152 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance summary in terms of mean accuracy for Skeleton Mimetics dataset as the test set. The 25 joints skeletons for both skeletics-152 (train set) and Skeleton-Mimetics (test set) are exctracted using VIBE. Refer to Sec. 5.2 for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>List of Best-5 and Worst-5 classes in terms of accuracy for NTU-60, NTU-120, Skeletics-152 and Skeleton-Mimetics datasets. The model associated with the best peformance is in brackets alongside the dataset name.</figDesc><table><row><cell>Method</cell><cell cols="2">Training Dataset Mean Cosine Similarity</cell></row><row><cell>4s-ShiftGCN [10]</cell><cell>NTU-120</cell><cell>0.04</cell></row><row><cell>MS-G3D [31]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Benchmarking comparison for Metaphorics dataset (mean cosine similarity).on the smaller split containing the 23 skeleton mimetics classes. The results suggest that training on a more larger dataset (classes outside the test set) helps the models to learn better generalisable features which boost the test performance, whereas a smaller dataset (classes specific to the test set) tends to learn features which offers limited generalisability.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We wish to thank the anonymous reviewers for their detailed and constructive feedback. We also wish to thank Kalyan Adithya and Sai Shashank Kalakonda for their efforts in creating the project page. This work is partly supported by MeitY, Government of India.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Actionxpose: A novel 2d multi-view pose-based algorithm for real-time human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Naqvi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-based human action recognition with extreme gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ayumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Student Conference on Research and Development (SCOReD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton image representation for 3d action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action knowledge transfer for action prediction with partial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.330181188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8118" to="8125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognize human activities from partially observed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2013.3438</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>/ CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2658" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1907.06987</idno>
		<ptr target="http://arxiv.org/abs/1907.06987" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pku-mmd: A large scale benchmark for continuous multimodal human action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chunhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yueyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yanghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiaying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACPR.2015.74865692</idno>
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eweiwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human action recognition based on 3d skeleton part-based pose estimation and temporal multi-resolution analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Halim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dartigues-Pallez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riveill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benslimane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghoneim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3041" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11344</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<idno>abs/1704.04516</idno>
		<ptr target="http://arxiv.org/abs/1704.045162" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anvil-a generic annotation tool for multimodal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 1</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning of human actions as trajectories in pose embedding manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno>abs/1812.02592</idno>
		<ptr target="http://arxiv.org/abs/1812.025922" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition using translationscale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1704.05645</idno>
		<ptr target="http://arxiv.org/abs/1704.056452" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno>CoRR abs/1704.07595</idno>
		<ptr target="http://arxiv.org/abs/1704.075952" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/109</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/1092" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2916873</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Integral action: Pose-driven feature integration for robust human action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Documentation mocap database hdm05</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Universität Bonn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CG-2007-2</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI (2020) 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>La Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.61263498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.4119/UNIBI/2683224</idno>
		<ptr target="https://pub.uni-bielefeld.de/record/26832242" />
		<title level="m">3d iconic gesture dataset</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing actions from depth cameras as weakly aligned multi-part bag-of-poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>1, 2, 7, 8</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What actions are needed for understanding human actions in videos?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An endto-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1611.06067</idno>
		<ptr target="http://arxiv.org/abs/1611.060672" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for action recognition with incomplete skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rgb-d-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.04.007.URLhttp:/www.sciencedirect.com/science/article/pii/S10773142183006632</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2018.04.007.URLhttp://www.sciencedirect.com/science/article/pii/S10773142183006632" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="118" to="139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07249</idno>
		<title level="m">Mimetics: Towards understanding human actions out of context</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mimetics: Towards understanding human actions out of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-021-01446-y</idno>
		<idno>10.1007/ s11263-021-01446-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-021-01446-y7" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spatial residual layer and dense connection block enhanced spatial temporal graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A large scale rgb-d dataset for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Understanding Human Activities through 3D Sensors</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno>abs/1603.07772</idno>
		<ptr target="http://arxiv.org/abs/1603.077722" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
