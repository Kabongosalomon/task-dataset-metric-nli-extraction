<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP ENSEMBLES FOR LOW-DATA TRANSFER LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>André</roleName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susano</forename><surname>Pinto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">DEEP ENSEMBLES FOR LOW-DATA TRANSFER LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the low-data regime, it is difficult to train good supervised models from scratch. Instead practitioners turn to pre-trained models, leveraging transfer learning. Ensembling is an empirically and theoretically appealing way to construct powerful predictive models, but the predominant approach of training multiple deep networks with different random initialisations collides with the need for transfer via pre-trained weights. In this work, we study different ways of creating ensembles from pre-trained models. We show that the nature of pre-training itself is a performant source of diversity, and propose a practical algorithm that efficiently identifies a subset of pre-trained models for any downstream dataset. The approach is simple: Use nearest-neighbour accuracy to rank pre-trained models, fine-tune the best ones with a small hyperparameter sweep, and greedily construct an ensemble to minimise validation cross-entropy. When evaluated together with strong baselines on 19 different downstream tasks (the Visual Task Adaptation Benchmark), this achieves state-of-the-art performance at a much lower inference budget, even when selecting from over 2,000 pre-trained models. We also assess our ensembles on ImageNet variants and show improved robustness to distribution shift.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There are many ways to construct models with minimal data. It has been shown that fine-tuning pre-trained deep models is a compellingly simple and performant approach <ref type="bibr">(Dhillon et al., 2020;</ref>, and this is the paradigm our work operates in. It is common to use networks pre-trained on ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009</ref>), but recent works show considerable improvements by careful, task-specific pre-trained model selection <ref type="bibr" target="#b31">(Ngiam et al., 2018;</ref>.</p><p>Ensembling multiple models is a powerful idea that often leads to better predictive performance. Its secret relies on combining different predictions. The source of diversity for deep networks has been studied <ref type="bibr" target="#b12">(Fort et al., 2019;</ref><ref type="bibr" target="#b47">Wenzel et al., 2020)</ref>, though not thoroughly in the low-data regime. Two of the most common approaches involve training independent models from scratch with (a) different random initialisations, (b) different random subsets of the training data. Neither of these are directly applicable downstream with minimal data, as we require a pre-trained initialisation to train competitive models 1 , and data scarcity makes further data fragmentation impractical. We study some ways of encouraging model diversity in a supervised transfer-learning setup, but fundamentally argue that the nature of pre-training is itself an easily accessible and valuable form of diversity.</p><p>Previous works consider the construction of ensembles from a set of candidate models <ref type="bibr" target="#b4">(Caruana et al., 2004)</ref>. Services such as Tensorflow Hub <ref type="bibr">(Google, 2018)</ref> and PyTorch Hub <ref type="bibr">(FAIR, 2019)</ref> contain hundreds of pre-trained models for computer vision; these could all be fine-tuned on a new task to generate candidates. Factoring in the cost of hyperparameter search, this may be prohibitively expensive. We would like to know how suited a pre-trained model is for our given task before training it. This need has given rise to cheap proxy metrics which assess this suitability . We use such metrics -leave-one-out nearest-neighbour (kNN) accuracy, in particular -as a way of selecting a subset of pre-trained models, suitable for creating diverse ensembles of task-specific experts. We show that our approach is capable of quickly narrowing large pools <ref type="bibr">(up to</ref>   <ref type="figure">Figure 1</ref>: Overview of the different ways of constructing diverse ensembles studied in this work.</p><p>We propose an algorithm that exploits diversity in a large pool of pre-trained models, by using leave-one-out k-nearest-neighbour (kNN) accuracy to select a subset to form the ensemble.</p><p>2,000) of candidate pre-trained models down to manageable (15 models) task-specific sets, yielding a practical algorithm in the common context of the availability of many pre-trained models.</p><p>We first experiment with sources of downstream diversity (induced only by hyperparameterisation, augmentation or random data ordering), giving significant performance boosts over single models. Using our algorithm on different pools of candidate pre-trained models, we show that various forms of upstream diversity produce ensembles that are more accurate and robust to domain shift than this. <ref type="figure">Figure 1</ref> illustrates the different approaches studied in our work. Ultimately, this new form of diversity improves on the Visual Task Adaptation Benchmark  SOTA by 1.8%.</p><p>The contributions of this paper can be summarized as follows:</p><p>• We study ensembling in the context of transfer learning in the low data regime &amp; propose a number of ways to induce advantageous ensemble diversity which best leverage pre-trained models.</p><p>• We show that diversity from upstream pre-training achieves better accuracy than that from the downstream fine-tuning stage (+1.2 absolute points on average across the 19 downstream classification VTAB tasks), and that it is more robust to distribution shift (+2.2 absolute average accuracy increase on distribution shifted ImageNet variants).</p><p>• We show that they also surpass the accuracy of large SOTA models (76.2% vs. 77.6%) at a much lower inference cost, and achieve equal performance with less than a sixth of the FLOPS.</p><p>• We extend the work from  and demonstrate the efficacy of kNN accuracy as a cheap proxy metric for selecting a subset of candidate pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CREATING ENSEMBLES FROM PRE-TRAINED MODELS</head><p>We first formally introduce the technical problem we address in this paper. Next we discuss baseline approaches which use a single pre-trained model, and then we present our method that exploits using multiple pre-trained models as a source of diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE LEARNING SETUP: UPSTREAM, MODEL SELECTION, DOWNSTREAM</head><p>Transfer learning studies how models trained in one context boost learning in a different one. The most common approach pre-trains a single model on a large dataset such as ImageNet, to then tune the model weights to a downstream task. Despite algorithmic simplicity, this idea has been very successful. In a downstream low-data scenario, it is more difficult for a one-size-fits-all approach to triumph as specializing the initial representation becomes harder. As in , we explore the scenario where a range of pre-trained models is available, and we can look at the target data to make a decision on which models to fine-tune. However, we generalize and improve it by simultaneously selected several models for fine-tuning, since downstream tasks may benefit from combining expert representations aimed at capturing different aspects of the learning task: for instance, on a natural scenery dataset one could merge different models that focus on animals, plants, food, or buildings. Fine-tuning all pre-trained models to pick the best one is a sensible strategy, but rarely feasible. To keep the algorithms practical, we identify two compute budgets that should be controlled for: The fine-tuning budget, i.e. the total number of models we can fine-tune on a downstream task; and the inference budget, the maximum size of the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BASELINES: DIVERSITY FROM DOWNSTREAM TRAINING</head><p>The baselines we propose leverage transfer learning by requiring a pre-trained model -this is crucial, see Appendix C.1. We use a strong generalist model (BiT-ResNet 50s from , trained on all upstream data) and consider three methods to create a model set for ensemble selection.</p><p>Random Seeds. Fine-tuning a generalist model multiple times with fixed hyperparameters will yield different classifiers, analagous to the DeepEnsembles of <ref type="bibr" target="#b24">Lakshminarayanan et al. (2017)</ref>. Note, here we can only take advantage of randomised data ordering/augmentation, which <ref type="bibr" target="#b12">Fort et al. (2019)</ref> showed, though useful, was not as beneficial as diversity from random initalisation.</p><p>HyperEnsembles. Hyperparameter diversity was recently shown to further improve DeepEnsembles <ref type="bibr" target="#b47">(Wenzel et al., 2020)</ref>. We define a hyperparameter search space, randomly sample as many configurations as we have fine-tuning budget, and fine-tune the generalist on downstream data with each of those configurations. Further details on training are given in Appendix A.2.</p><p>AugEnsembles. We generate a set of models by fine-tuning the generalist on each task with randomly sampled families of augmentation (but fixed hyperparameters). Details are in Appendix A.3. <ref type="bibr" target="#b12">Fort et al. (2019)</ref> explain the strong performance of classical ensembling approaches -independently training randomly initialised deep networks -by showing that each constituent model explores a different mode in the function space. For transfer learning, <ref type="bibr" target="#b30">Neyshabur et al. (2020)</ref> show that with pre-trained weights, fine-tuned models stay in a local 'basin' in the loss landscape. Combining both gives a compelling reasoning for the use of multiple pre-trained networks for transfer with ensembles, as we propose here. Instead of diversity from downstream fine-tuning, we show that in the low data regime, better ensembles can be created using diversity from pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OUR METHOD: DIVERSITY FROM UPSTREAM PRE-TRAINING</head><p>We consider three sources of upstream diversity. First, we consider generalists that were pre-trained with different random seeds on the same architecture and data. Second, we consider experts, specialist models which were pre-trained on different subsets of the large upstream dataset. Lastly, we exploit diversity in scale -pre-trained models with architectures of different sizes. Given a pool of candidate models containing such diversity, we propose the following algorithm ( <ref type="figure">Figure 1</ref>):</p><p>1. Pre-trained model selection. Fine-tuning all experts on the new task would be prohibitively expensive. Following , we rank all the models by their kNN leave-one-out accuracy as a proxy for final fine-tuned accuracy, instead keeping the K best models (rather than 1).</p><p>2. Fine-tuning. We add a fully connected layer to each model's final representation, and then train the whole model by minimising categorical cross-entropy via SGD. Given a pool of K pre-trained models from stage 1, we tune each with 4 learning rate schedules, yielding a total of L = 4K models for the step 3 (Usually K = 15 and L = 60). See Appendix A.1.1 for more details.</p><p>3. Ensemble construction. This is shared among all presented ensembles. We use the greedy algorithm introduced by <ref type="bibr" target="#b4">Caruana et al. (2004)</ref>. At each step, we greedily pick the next model which minimises cross-entropy on the validation set when it is ensembled with already chosen models.</p><p>These steps are independently applied to each task; each step makes use of the downstream dataset, so each dataset gets a tailored set of pre-trained models to create the ensemble pool and therefore very different final ensembles result. We also considered a greedy ensembling algorithm in kNN space which aims to sequentially pick complementary models which will likely ensemble well together (Appendix C.6), but picking top-K was generally better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">COMBINED APPROACHES</head><p>The diversity induced by different upstream models and distinct downstream hyperparameters should be complementary. Given a fine-tuning budget of L, we can set the number of pre-trained models K in advance, providing each of them with a random hyperparameter sweep of size L/K. However, for some tasks it may be more beneficial to have fewer different pre-trained models and a wider sweep, or vice versa. We aim to dynamically set this balance per-dataset using a heuristic based on the kNN accuracies; namely, we keep all pre-trained models within some threshold percentage τ % of the top kNN accuracy, up to a maximum of K = 15. Ideally, this would adaptively discard experts poorly suited to a given task, whose inclusion would likely harm ensemble performance. The saved compute budget is then used to squeeze more performance from available experts by testing more hyperparameters, and hopefully leading to greater useful diversity. We arbitrarily set τ = 2% for our experiments, but this choice could likely be improved upon. Appendix C.5 shows how the number of models picked varies per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">PRE-TRAINING MODELS</head><p>We use BiT ResNets pre-trained on two large upstream datasets with hierarchical label spaces: JFT-300M <ref type="bibr" target="#b41">(Sun et al., 2017)</ref> and ImageNet-21k <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. We consider two types of pretrained models. Generalists are trained on the entire upstream dataset. In particular, we consider 15 JFT ResNet-50 generalists that were pre-trained with different random initalisations. Experts are generated by splitting the hierarchical label spaces into sub-trees and training independent models on the examples in each sub-tree. We pre-train 244 experts from JFT and 50 from ImageNet21k, following the protocol of (Puigcerver et al., 2020) (see Appendix A.1). For low-data downstream tasks, this is by far the most expensive stage of the process. It is however only incurred once, and its cost is amortized as new downstream tasks are served, since any downstream task can reuse them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ENSEMBLE EVALUATION</head><p>Downstream Tasks. We evaluate our models on the Visual Task Adaptation Benchmark : 19 diverse downstream classification tasks, split into 'natural', 'specialised' and 'structured' categories. As we are primarily interested in low-data regimes, the tasks only have 1000 training datapoints (i.e., VTAB 1K ) with a number of classes ranging from 2 to 397. We split data into <ref type="formula">800</ref>  Test Performance. For our final competitive models, we first train all the individual models on the 800 training points. Then, we use the 200 validation data points to find the best ensemble (both running the greedy algorithm and choosing the overall ensemble size). For the resultant ensemble, we retrain constituent models on the full 1000 data points, and evaluate it on the test data.</p><p>Robustness. We train ExpertEnsembles and HyperEnsembles on ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. While ImageNet does not match our low-data regime of interest, previous work and additional datasets allow us to conveniently measure robustness and uncertainty metrics. Thus, alongside reporting the accuracy on the official validation split, we assess models on a number of ImageNetbased robustness benchmarks, aiming to quantify calibration and robustness to distribution shift <ref type="bibr" target="#b10">(Djolonga et al., 2020)</ref>. More details on these variants are available in Appendices B.2 and A.4.</p><p>(a) JFT pre-trained models.</p><p>(b) Imagenet21k pre-trained models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>Unless otherwise specified, all experiments use a fine-tuning budget and inference budget of 60 and 15 models respectively. This was set arbitrarily; we experiment with both budgets to see the effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THE VALUE OF ENSEMBLES</head><p>We first show that ensembles in low-data transfer regimes dramatically beat their single-model counterparts -which are often much larger networks. <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="table" target="#tab_1">Table 1</ref> compare our best ensembles (which all use upstream or combined diversity) on VTAB 1K tasks. Our baselines are BiT models from , which had until now state-of-the-art performance.</p><p>JFT pre-trained models. The most standard approach -fine-tuning a single R50 model trained on all of JFT-leads to an average accuracy of 70.6%. It greatly lags behind compared to the ensemblebased algorithms; in particular, the difference is striking for structured and natural datasets. On average, the ensembles selected between 9 and 10 downstream models -this number greatly varies depending on the task, e.g. 3 were selected for CalTech101 and 14 for Diabetic Retinopathy. Accordingly, capacity-wise, it makes sense to compare the ensembles to larger ResNets. <ref type="table" target="#tab_1">Table 1</ref> shows that the JFT R50 ensembles match or slightly beat the performance of a R152x4. In particular, the ensembles offer a large advantage in settings where tasks diverge from single-object recognition, e.g. in the structured datasets. Even in natural domains, the experts have a better accuracy/FLOP ratio than the R152x4, which has 40× more parameters than a single R50. Even more significant is the difference in inference time, as ensemble predictions can be easily parallelized.   ImageNet21k pre-trained models. The story is fairly similar for the pool of ImageNet21k experts. The ensembles select on average between 7 and 8 models -again, strongly task-dependent. Ensembles' average performance improvement with respect to a single R50 trained on all of ImageNet21k is around 5 absolute points (more than 7%). We also consider a much larger generalist baseline, in this case a R101x3, which has more than 16× as many parameters as a single R50. Still the ensembles far outperform it, especially in structured datasets.</p><p>Overall. A more complete story supporting the ensembles' value is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. The gray dashed line represents the previous Pareto frontier of VTAB 1K average accuracy per FLOP. The ensemble models dominate the state-of-the-art, indicating their efficacy in the low-data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">THE VALUE OF UPSTREAM DIVERSITY</head><p>Results in <ref type="table" target="#tab_2">Table 2</ref> suggest that upstream diversity improves downstream diversity. For both JFT and ImageNet21k pre-training, ensembles benefiting from upstream sources of diversity outperform their downstream-based counterparts. Combining both gives a further small boost for expert ensembles.</p><p>Ablations on JFT pre-trained models are shown in <ref type="table" target="#tab_3">Table 3</ref>; we now discuss the learnings from that.</p><p>Experts help when pre-training is relevant: Results on <ref type="table" target="#tab_2">Table 2</ref> used kNN to pick from a pool of 15 generalists and 244 experts. We break these pools down separately. Experts give a significant boost on Natural datasets; the ensembles take advantage of the relevance of experts pre-trained on the predominately 'natural' slices of the upstream data. For datasets without clear experts, there is less benefit to this approach, and the generalists shine.</p><p>Performance improvements stack with scale: The strong performance of the R101 Expert Ensemble shows performance gains stack somewhat with scale; it improves on R50 Experts by 1.2% absolute points in accuracy, improving in all categories. We explore this more thoroughly in Appendix C.7.</p><p>Combining upstream and downstream diversity helps: As discussed in Section 2.3.1, we combine upstream and downstream diversity. The simple approach of thresholding by kNN accuracy works well for choosing the balance between the two at fixed training budget. For R50s, adding augmentations yields a further 0.8% gain. The number of experts chosen is shown in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">THE VALUE OF NEAREST NEIGHBOUR SELECTION</head><p>kNN may possibly help: The greedy ensemble algorithm is not perfect, and with such a small validation set it is prone to overfit. When all upstream JFT R50 experts are fine-tuned and passed to the greedy algorithm, test performance drops slightly. We further explore this in Appendices C.4, C.9.</p><p>It can compare models of different sizes: Overall, larger models perfom better at transfer . Per-dataset, this is not the case; e.g. we found R34 experts were best on structured tasks. One may expect kNN selection or the greedy algorithm to be biased towards selecting larger architectures. The final ensembles instead use a mix of scales. The R18/R34/R50 experts ensemble improves on just R50s by 0.4%, indicating possible benefits; more discussion is in Appendix C.7.</p><p>Can filter a very large pool of models: When selecting only 15 pre-trained models from over 2000 candidates (different architecture sizes and upstream datasets), the overall VTAB performance (All Experts in <ref type="table" target="#tab_3">Table 3</ref> is similar to only selecting from ResNet-101s. This highlights the remarkable robustness of our model selection. These results are broken down further in Appendix C.8.</p><p>Mirroring , we have shown kNN to be a cheap yet successful way of selecting models. It is not perfect -when combining pools, one would hope for at least a 'best of both' performance. kNN selection wasn't needed for generalists (we had 15 pre-trained models), but when combining the generalists and experts in a pool, specialised/structured performance drops slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EFFECT OF FINE-TUNING BUDGET</head><p>In most experiments, the kNN picks K = 15 experts. With the default 4 hyperparameters, this is a fine-tuning budget of 60 models. This is the number of models trained for a given task, and the majority of compute expenditure incurred by a practitioner, as the kNN selection/ensembling are comparatively cheap. The hyperensemble was run with the same budget. <ref type="figure" target="#fig_2">Figure 3</ref> shows how performance drops with reduced fine-tuning budget. Interestingly, the expert ensembles are actually more robust to a reduced budget, retaining higher performance when training fewer models, indicating the kNN's usefulness as a pre-selection phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ROBUSTNESS TO DISTRIBUTION SHIFT</head><p>Previous work has shown ensembles help with metrics relating to uncertainty and calibration <ref type="bibr" target="#b24">(Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b40">Stickland &amp; Murray, 2020)</ref>. To assess this, we train JFT R50 HyperEnsembles and ExpertEnsembles for ImageNet classification. For the former, we use the BiT generalist; for the latter, we use the 244 experts, applying kNN to 50,000 examples from the training set to select experts. For both we use the validation set for greedy ensembling. Once the ensembles are trained and constructed, we assess them on a suite of datasets aiming to quantify robustness to distribution shift. Each dataset introduces some form of distribution shift (further details in Appendix B.</p><p>2)what we assess is the accuracy on these datasets. <ref type="figure">Figure 4</ref> shows them. The expert ensembles offer a slightly better accuracy on the held out data; more importantly, they perform significantly better under distribution shift, improving over the HyperEnsembles by on average 2.2% across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>We present literature related to the main aspects of this work. As well as previous highlighted novelties, we believe our contribution is distinguished from previous ensembling works by focusing on <ref type="figure">Figure 4</ref>: Expert ensembles retain higher accuracy under domain shift. Aside from the first bar, which shows test accuracy, all other bars correspond to some form of induced distribution shift, either artificially or otherwise. In all but one, we get significant boosts in accuracy compared to the HyperEnsembles.</p><p>diverse datasets with production-scale deep models (instead of demonstrative smaller architectures and datasets), limiting the training data available, and formally assessing distribution shift.</p><p>Transfer Learning. Relating to a long history of research in manifold and representation learning <ref type="bibr" target="#b43">(Tan et al., 2018)</ref>, the transfer of features learnt by deep neural networks aims to reuse the abstract features learnt on a source (upstream) dataset in order to improve performance or data efficiency on a target (downstream) dataset. <ref type="bibr" target="#b3">Bengio (2011)</ref> studied this in the context of unsupervised pre-training, proposing a number of ways to learn and re-use the features. Many works have shown benefits of transfer relating to convergence speed <ref type="bibr" target="#b35">(Raghu et al., 2019)</ref>, generalisation <ref type="bibr" target="#b50">(Yosinski et al., 2014)</ref>, accuracy  and robustness <ref type="bibr" target="#b10">(Djolonga et al., 2020)</ref>, with the latter two showing particular benefits in the low-data regime.</p><p>Ensemble Learning. Known for improving models, ensembling methods have been studied in depth in and out of deep learning academia <ref type="bibr" target="#b38">(Seni &amp; Elder, 2010)</ref>. There are few works which study ensembles in the context of transfer learning <ref type="bibr" target="#b0">(Acharya et al., 2012;</ref><ref type="bibr" target="#b42">Sun et al., 2013)</ref> Deep Ensembles. <ref type="bibr" target="#b24">Lakshminarayanan et al. (2017)</ref> show that a simple approach of adversarially training multiple randomly initialised models from scratch and ensembling them yielded models with strong predictive uncertainty and calibration. <ref type="bibr" target="#b47">Wenzel et al. (2020)</ref> showed that hyperensembles, which vary random initialisations and hyperparameters, outperform these deep ensembles.</p><p>On Constructing Ensembles. A key part of our algorithm is the use of the kNN to narrow down candidate pre-trained models into a relevant subset. <ref type="bibr" target="#b4">Caruana et al. (2004)</ref> was arguably the seminal work studying how to select an optimal ensemble from a set of candidate models. A number of works extend AutoML frameworks <ref type="bibr" target="#b14">(He et al., 2019)</ref> to explicitly optimise both the ensembling method and the members to maximise overall performance <ref type="bibr" target="#b48">(Wistuba et al., 2017;</ref><ref type="bibr" target="#b49">Xavier-Júnior et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have studied simple ways of creating performant ensembles with a limited amount of data. Overall, ensembles dramatically outperform their single-model counterparts. We show that diversity from upstream pre-training results in better ensembles than diversity induced downstream, regardless of whether this upstream diversity comes from pre-training multiple generalist models with different initialisations, using different architectures or specialisation via pre-training on different data. We demonstrate the efficacy of the nearest-neighbours classifier as an easily calculated discriminator between different pre-trained models, and even as a way to decide how many models to try on a downstream task, leading to convenient ways to combine both upstream and downstream diversity.</p><p>These ensembles achieve SOTA performance on the Visual Task Adaptation Benchmark at a significantly smaller inference cost, while also outperforming ensemble approaches relying on downstream diversity. They also exhibit higher robustness to domain shift as assessed by ImageNet variants.</p><p>There are many interesting avenues for future work. All our considered models were pre-trained in a supervised fashion, and this should certainly be extended to include other forms of pre-training. This approach of combining different pre-trained models is highly complimentary with efforts which train ensembles end-to-end with diversity-encouraging losses, such as those in <ref type="bibr" target="#b26">Lee et al. (2015)</ref> and <ref type="bibr" target="#b45">Webb et al. (2019)</ref>. Lastly, works such as Batch Ensembles <ref type="bibr">(Wen et al., 2020)</ref> and Parameter Superposition <ref type="bibr" target="#b6">(Cheung et al., 2019)</ref> systematically decompose network parameters to compactly train ensembles. For pre-trained models with the same architecture, weights could be deconstructed to initialise those methods so as to benefit from transfer learning and make them feasible in the low-data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TRAINING DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 PRETRAINING PRE-TRAINED MODELS</head><p>Architectures: The ResNet architectures used is ResNetV2, with batch norm replaced by group norm and weight standardisation used in the convolutions, as in . We consider 18, 34, 50 and 101 layer models. Datasets: As mentioned, we use JFT-300M and ImageNet21k as our pretraining datasets. We split these datasets into subsets following the protocol set in , where a generalist model is finetuned for 2 epochs on each subset of the pretraining dataset. We repeat this for 18, 34 and 101 layer ResNets on JFT.</p><p>Training: For the vast majority of pre-trained models in this work, models were pre-trained by taking a generalist model and fine-tuning it for 2 epochs on a subset of the pretraining data. We also consider training architectures from scratch on subsets of the pre-training data; these generally performed slightly worse, but are included in the 'All Experts' pool. We pre-train models with the same settings described in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 VTAB: FINETUNING PRE-TRAINED MODELS</head><p>For both generalist and expert models, pre-trained models are trained on a target dataset by adding a new dense layer at the head, with units equal to the number of classes of the output space. All models were trained using SGD with momentum, with the momentum parameter set to 0.99. They were trained on Google Cloud TPUv3s with batch size 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head><p>We use the task-specific preprocessing suggested by BiT-HyperRule ; this dictates resolutions (pre and post crop) and whether to flip left/right. Except for AugEnsembles, we do not use MixUp as they do. For all methods, we train 3 models on each task in order to give rough confidence intervals. As we explore training large numbers of models, it was not computationally feasible to train more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Default Hyperparameter Sweep</head><p>Unless hyperparameters are sampled as in Appendix A.2, models undergo a default sweep of parameters on the target dataset. For ExpertEnsembles, this means each feature extractor was fine-tuned 4 times on the downstream dataset. For AugEnsembles, it means for a given randomly sampled augmentation setting, the generalist model was fine-tuned 4 times on the downstream dataset. This default sweep is the product of two learning rates (0.1 and 0.01) and two schedule lengths (2500 and 10000 steps). For the schedule, we use an exponential step decay schedule; it warms-up linearly for 20% of the schedule, then decays by a factor of 0.1 at each subsequent 20% interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TRAINING HYPERENSEMBLES</head><p>As discussed, our hyper ensembles start with a single pre-trained generalist model -in this case, either a BiT-L or BiT-M ResNet-50x1, which were pretrained on JFT and ImageNet21k respectively. We use the random hyperparameter search space defined in , which we recount here for convenience: <ref type="figure" target="#fig_1">Choice(500, 1k, 2k, 5k, 8k, 16k)</ref> A.3 TRAINING AUGENSEMBLES Similar to the hyper-ensembles, the AugEnsembles start with a single pre-trained generalist model. For all augmentations, the final resolution for each dataset is that defined by the BiT-HyperRule in , and we rescale intensities to have a range of -1.0 to 1.0. Note that augmentations themselves are random from image to image, e.g. the color distortion augmentations will apply different magnitudes of distortions (hence why for a fixed augmentation setting, there is still benefit to diversity). We sample whether or not to include these distortions and the parameters of these distortions themselves (e.g. distortion level). We randomly sample augmentations as so:</p><formula xml:id="formula_0">• Weight decay to init ∼ LogUniform(1×10 −6 , 1×10 −1 ) • Dropout rate ∼ Uniform(0.0, 0.5) • Learning rate ∼ LogUniform(1×10 −4 , 1×10 −1 ) • Schedule length ∼</formula><p>• Choose between inception crop and random crop -If random crop: Sample initial resolution ∼ <ref type="figure" target="#fig_2">Uniform(1.05, 1.30)</ref> × the final resolution</p><p>• If flipping left-right is mandated in the BiT-HyperRule, choose to flip with probability 50%.</p><p>• (For non artificial datasets), apply each of the following color distortions with probability 50%:</p><p>-Random additive brightness modulation. Adds to all channels some δ ∼ Uniform(−δ max , −δ max ). We sampled δ max ∼ Uniform(0.01, 0. Note that we did not apply color distortion to datasets which were artificially generated, e.g. dSprites, as that results in significantly worse performance. That being said, it is not clear what augmentations to use for such datasets. We found the greatest gains from this method to be on specialised datasets. We suspect this is because the default augmentations specified by the BiT Hy-perRule are not well suited for such datasets; in particular, we found PatchCamelyon benefited the most from different augmentations.</p><p>For both augmentation ensembles and hyper ensembles, we checked that the models finetuned on each dataset had a roughly similar distribution of validation accuracies as expert models in order to ascertain these were fair ways of generating models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 IMAGENET: TRAINING MODELS</head><p>Data: We use 96% of the train data for training individual models and the remaining 4% for constructing ensembles. We report numbers on the official validation split.</p><p>Preprocessing: During training we resize to 512 × 512 pixel images, then randomly crop to 480 × 480, and also randomly flip images horizontally. At evaluation time we simply resize images to 480 × 480 pixel. We train five random trials of each approach.</p><p>Training: We train on Cloud TPUv3s with a batch size of 512, using SGD with Momentum as for VTAB models, with momentum parameter set to 0.99. For the ExpertEnsembles, we use the two learning rates from the default hyperparameter sweep above (0.1 and 0.01), but now use two longer schedules (10k and 20k steps). For the HyperEnsembles, we use the same hyperparameter search space defined in Appendix A.2, but we explore longer schedules of length 10k, 15k, 20k, 25k and 30k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EVALUATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 THE VISUAL TASK ADAPTATION BENCHMARK (VTAB)</head><p>The Visual Task Adaptation Benchmark consists of 19 tasks. They are split into three categories:</p><p>• Natural tasks Validation Performance.</p><p>In order to facilitate in-depth study without over-evaluation on the test data, for some plots in the appendix we explore ensemble behaviour on validation data. In this context, given candidate models which have been finetuned on the 800 datapoints, we require data to a) make ensembling decisions (e.g. for the greedy algorithm) and b) evaluate ensemble performance. We partition the 200 validation points into an 80/20 split, using 160 points to make ensembling decisions and 40 for evaluation. We use k-fold cross validation with k = 15, comparing final mean accuracy across iterations and datasets. We generally found that this was representative; i.e. ranking of models according to this setup was the same as for evaluation on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 IMAGENET ROBUSTNESS VARIANTS</head><p>Given ensembles trained on ImageNet, we assess models using the suite of ImageNet variants collected in <ref type="bibr" target="#b10">Djolonga et al. (2020)</ref>. Chiefly speaking, these variants induce some kind of distribution shift on the input images while still retaining the same label space.</p><p>• ImageNet Corrupted (ImageNet-C - <ref type="bibr" target="#b16">(Hendrycks &amp; Dietterich, 2019</ref>)) The original Ima-geNet images, with artificial corruptions such as blur and snow applied.</p><p>• Re-collected data These datasets consist of data that was re-collected and labelled with ImageNet labels. ImageNet trained classifiers usually experience an accuracy decrease on these datasets. • Video datasets These datasets consist of videos where the frames are labelled with Im-ageNet classes. In these dataset, as well as the accuracy of the frames, the pm-k metric is introduced, whereby given an anchor frame, a classifier is only considered correct if it classifies the surrounding 2k + 1 frames correctly.</p><p>-YouTube Bounding Boxes (YTBB - <ref type="bibr" target="#b36">(Real et al., 2017;</ref> -ImageNet Vidrobust <ref type="bibr" target="#b8">(Deng et al., 2009;</ref> C ABLATIONS AND EXPERIMENTAL ANALYSIS C.1 FROM-SCRATCH APPROACHES ARE NOT COMPETITIVE IN THE LOW-DATA REGIME All across the transfer-learning literature there is clear evidence that as the number of available samples for training decreases, the less competitive approaches based on training from scratch become. To demonstrate that also happens when using ensembles we constructed HyperEnsembles starting from a random-initialization and from generalist pre-trained model and evaluate them in VTAK-1K. <ref type="figure" target="#fig_6">Figure 5</ref> shows the result -training from scratch is hopeless. Even while ensembling many models, it can't compete with a single fine-tuned strong generalist. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 NUMBER OF MODELS BEING PICKED</head><p>Due to the greedy algorithm, different numbers of models are chosen for different datasets. We show in <ref type="table" target="#tab_5">Table 4</ref> the number of experts chosen for each; in particular we note that on structured datasets, where there are less relevant models from pre-training, it tends to use a large number of models. We note also that typically there is no significant difference in the number of models being picked by different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 RANDOM VS FILTERING PRE-TRAINED MODELS</head><p>It is possible that the use of experts, or generalists, is simply the transfer learning equivalent of random initialisation -i.e. that diversity in initialisation for the sake of diversity in initialisation is 9.1 3 10 14 1 6 3 11 8 7 11 14 4 11 14 2 13 14 14 14 U R101 Experts 9.7 6 9 14 1 10 5 10 8 8 11 14 6 13 14 2 11 13 14 13 C HyperExperts 9.9 5 13 15 1 4 7 13 12 9 12 15 2 14 15 2 8 11 15 15 C AugExperts 10.3 4 8 15 1 9 7 15 9 9 8 15 5 15 15 4 11 15 15 15 enough to yield performant models. <ref type="figure">Figure 6</ref> shows this is not the case. All categories perform worse, though it is particularly notable in Natural datasets where the ensembles lose the edge of relevant pre-training.</p><p>Following from the results of , it is clear that the choice of expert is important in the low-data regime. The final results are much more significantly influenced by the initalisation, and transfer from pre-training, than when more data is available. Less suited pre-trained models will lead to lower accuracy and ultimately drag down the predictive power of the entire ensemble, hence why when suitability of pre-trained models varies significantly, the kNN becomes much more valuable.</p><p>Figure 6: Comparison of ensembles created from diversity generated via a random selection of pre-trained models vs using kNN to filter pre-trained models. Diverse initialisations alone are not enough; we require individually competitive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 EVALUATING FILTERING WITH kNN</head><p>To evaluate the performance of using kNN as a strategy to narrow down a set of pre-trained models, we computed the ensemble test performance on VTAB 1K using the 244 JFT R50 pre-trained experts with the following strategies:</p><p>Brute + Greedy Ensemble: finetune each of the 244 available pre-trained models using the default sweep (4 hyper-parameters). The 976 obtained models are then used to build an ensemble of 15 models using the greedy algorithm.</p><p>kNN top 15 + Greedy Ensemble: use kNN to select 15 out of the 244 available pre-trained models. The 15 models are finetuned 4 times obtaining 60 models that are then used to build an ensemble of 15 models using the greedy algorithm.</p><p>kNN + Threshold Ensemble: use kNN to select a number (≤ 15) of pre-trained models per task using the threshold approach described in Section 2.3.1. Each selected models is finetuned 4 times with the default sweep and the one with the best validation accuracy is used in the final ensemble.</p><p>The results in <ref type="table" target="#tab_6">Table 5</ref> show that kNN filtering was effective at reducing the number of models without drop in VTAB 1K accuracy. Note also that kNN based approaches were able to reduce the noise from the 244 models so that the final ensemble (computed over the validation score) obtained better test accuracy for the natural datasets where we expect to exist clear experts among the models. This conclusion is made stronger by the fact that the kNN + Threshold Ensemble was also able to beat the brute approach. We presented the use of a simple heuristic to decide how many pre-trained models we should finetune on a given task. The remaining fine-tuning budget can then be used for a hyperparameter sweep or varying augmentations; we showed that there were significant gains to be had from doing this. <ref type="figure" target="#fig_7">Figure 7</ref> shows the number of experts kept for each dataset using a threshold of τ = 98%. We experimented with a variant of the kNN selection which aimed to pick pre-trained models that would ensemble well together (Greedy), as opposed to picking pre-trained models which are independently accurate (top-K). The effect of this for JFT and ImageNet21k ResNet-50 experts is shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The greedy approach does not give any clear benefits, and we found at test time it resulted in a small drop in performance. We suspect this is due to the small amount of data the kNN is making decisions based off.  <ref type="figure" target="#fig_9">Figure 9</ref> shows validation accuracy performance of ensembles of different architecture sizes. Firstly, we note again that to some extent, performance gains stack with architecture size. Secondly we note that the larger architectures are not always better; for instance, the ResNet-34s perform significantly better than others on structured datasets. Lastly, when considering the 'All' line -where the kNN selected from experts pretrained at all four architecture scales -we would hope it performs as well as the best alternative line. To some extent it does, but it does not always get it right.  One may expect that the typically higher performing ResNet-101s  would be heavily favoured by the kNN algorithm; or failing that, after finetuning on the dataset, would be more performant on the validation set and therefore favoured by the greedy algorithm. Interestingly -on both fronts -this is not the case. The approach is often creating ensembles of diverse architectures scales. We discuss further in Appendix C.8, but it is not clear there is always a clear gain from such diversity. In fact, we see here the kNN is not picking as many R34 experts as it should be, despite their clear superiority after fine-tuning; this may be due to comparing features with different dimensions (ResNet-18s and 34s produce 512 dimension features, whereas ResNet-50 and 101 produce 2048 dimensional features), and we suspect using a different distance metric (e.g. cosine distance instead of Euclidean) may have alleviated this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 SUBSET SELECTION FROM A VERY LARGE POOL</head><p>For the majority of our experiments, the kNN picked K = 15 feature extractors from a pool of 244 (for JFT experts) or 50 (for ImageNet21k experts). Intuitively, the larger the pool of candidate feature extractors is, the harder the job for the kNN; and it is clear from the marginal drop in performance when combining the two pools of feature extractors that the task of subset selection will become harder as more candidates are added.</p><p>We consider what happens in the extreme limit -as discussed in Appendix A.1, as well as the 50 ImageNet21k experts, we trained 4 different architectures of ResNet on 244 different slices of JFT with 2 different pretraining methods (fine-tuning from a generalist and training from scratch). This gives us a total of 2002 experts in our pool. <ref type="figure" target="#fig_11">Figure 11</ref> shows the experts picked from this pool. Notably, once again, both the kNN and the greedy algorithm pick a diverse selection of models. There are some peculiarities; e.g. for some datasets, ImageNet21k-trained experts perform better as a kNN classifier, whereas the JFT experts are better after finetuning, raising issues when using the former as a proxy for the latter.  The interesting question is: are we benefiting from diverse architectures? We suspect a-priori that the best-performing models will be the R101 experts trained on JFT. We therefore compare the performance of the diverse expert ensembles with the R101 ones -this is shown in <ref type="figure" target="#fig_1">Figure 12</ref>. It is not immediately clear the existence of any correlation between the diversity of ensembles -in terms of architecture etc. -with the final test performance. That is not to say there is no theoretical benefit to that at all; this is obfuscated by the extra kNN stage, which means there are almost certainly more optimal combinations of different architectures/pretraining styles.</p><p>C.9 AVOIDING OVERFITTING All ensembling approaches attempted in this paper experienced higher drops in performance from validation to the test set. It has been previously noted that, especially with small validation datasets, the ensembling process itself can overfit to the validation data <ref type="bibr" target="#b4">(Caruana et al., 2004)</ref>. We attempted a few methods to alleviate this:</p><p>Bootstrapping data: At each iteration of the greedy algorithm, we randomly bootstrap the 200 validation data points, with some proportion M (such that 200M samples are used to make ensembling decisions). This should ensure the ensembling process is not overfitting to individual data points too much, a phenomenon we observed when constructing greedy ensembles based on accuracy. This helped marginally when selecting ensembles for maximum accuracy, but we did not see any improvement when using this to select ensembles for maximum cross-entropy.</p><p>(a) JFT pre-trained models (xent) (b) Imagenet21k pre-trained models (xent) (c) JFT pre-trained models (acc) (d) Imagenet21k pre-trained models (acc) <ref type="figure" target="#fig_2">Figure 13</ref>: Effect of different greedy ensembling selection strategies. We vary the inference budget to generate ensemble curves. Note that when selecting based on accuracy, the ensembles perform better with smaller ensembles, but that performance doesn't monotonically increase with increasing budget, and overall performance is worse.</p><p>Greedy selection with replacement: <ref type="bibr" target="#b4">Caruana et al. (2004)</ref> reported that allowing the algorithm to re-select models it had already selected reduced overfitting. The extra benefit (in our view, which is not discussed in that work) is that by weighting ensembles according to the number of times they were picked, we can use a weighted average, downweighting less performant models and potentially improving performance. In our set up unfortunately, neither of these held; it did not reduce overfitting, and weighting did not improve performance.</p><p>Ensembling to maximise cross-entropy: We initially constructed ensembles in order to maximise accuracy on the validation set. This resulted in multiple problems; with so few data points, ensembles were making decisions based on single data points (fighting for 0.5% improvement in accuracies on the 200 data point validation set). Furthermore, multiple ensembles generated by the greedy process would give the exact same validation accuracy, resulting in the need for an arbitrary tiebreaker. For all model sets, with upstream or downstream diversity, maximising cross entropy generalised better and avoided the latter problem, and was the most successful approach to avoiding over-fitting. As well as overall improving performance, we found it more reliable -when making ensembling decisions based on cross-entropy, final test performance monotonically increased with the inference budget (ensemble size). It is worth noting however that decisions made based on cross-entropy are worse for single models or smaller ensembles. With a small inference budget, we suggest optimising for accuracy. These phenomena are evident in <ref type="figure" target="#fig_2">Figure 13</ref>.</p><p>Note that <ref type="bibr" target="#b4">Caruana et al. (2004)</ref> suggest bagging of ensembles -i.e. selecting multiple ensembles using different splits of the data, then combining them in one pool. We did not opt for this, as we wanted to have a fixed downstream inference budget, and it is non-trivial to ensure that when combining multiple ensembles of variable size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>training examples and 200 validation examples. See Appendix B.1 for more information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Inference cost vs. VTAB 1K performance. State-of-the-art generalist models of different scales are compared against ensembles with varying inference budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of fine-tuning budget on ensemble VTAB 1K performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.<ref type="bibr" target="#b1">Bachman et al. (2014)</ref> pre-train entire ensembles on the source data and transfer, instead of transferring individual models. Work in the low-data regime is sparser. Using an ensemble of models from multiple training checkpoints,<ref type="bibr" target="#b23">Laine &amp; Aila (2017)</ref> label unlabelled data to then train individual models further, improving data efficiency for CIFAR100/SVHN. For few-shot classification on a new class,<ref type="bibr" target="#b11">Dvornik et al. (2019)</ref> construct ensembles of mean-centroid classifiers from pre-trained ResNet18s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b27">(Li et al., 2004</ref>) · CIFAR100<ref type="bibr" target="#b22">(Krizhevsky, 2009</ref>) · Street View House Numbers(SVHN -Netzer et al. (2011)) · Describable Textures (DTD -<ref type="bibr" target="#b7">Cimpoi et al. (2014)</ref>) · Oxford Flowers<ref type="bibr" target="#b32">(Nilsback &amp; Zisserman, 2006</ref>) · Oxford Pets<ref type="bibr" target="#b33">(Parkhi et al., 2012)</ref> These tasks contain 'classical' natural real-world images obtained with a camera.• Specialised tasks EuroSAT (Helber et al., 2019) · Diabetic Retinopothy (Kaggle &amp; EyePacs, 2015) PatchCamelyon (Veeling et al., 2018) · Remote Sensing Image Scene Classification (RESISC -Cheng et al. (2017)) These are datasets of arguably 'natural' images which were captured with specialised photographic equipment. • Structured datasets DeepMind Lab (Object distance prediction -Zhai et al. (2019)) · SmallNOrb (Azimuth &amp; Elevation prediction -LeCun et al. (2004) CLEVR (Counting &amp; Distance prediction Johnson et al. (2017) · Kitti (Vehicle distance prediction Geiger et al. (2012)) · dSprites (pixel location &amp; orientation prediction -Matthey et al. (2017)) These assess understanding of scene structure in some way, predominately from synthetic environments. Example tasks include 3D depth estimation and counting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-</head><label></label><figDesc>ImageNet Adversarial (ImageNet-A -(Hendrycks et al., 2019)) Collection of new data which ResNet-50 models failed to classify. -ImageNet-R (Hendrycks et al., 2020) Collection of cartoons/art/tatoos/toys etc with original ImageNet labels, which ResNet-50 models failed to classify. -ImageNet-v2 (Recht et al., 2019) Data recollected in a method that closely mimics the original protocol as best as possible. -ObjectNet (Barbu et al., 2019) Collection of data with controls for backgrounds, rotation and imaging viewpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>VTAB 1K validation accuracy of HyperEnsembles trained from scratch and HyperEnsembles trained from a generalist pre-trained model (JFT-R50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Number of pre-trained models picked by thresholding JFT R50 experts with τ = 98% C.6 GREEDY PRE-TRAINED MODEL SELECTION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of greedy kNN and top-K kNN selection. C.7 SELECTING FROM MULTIPLE SCALES OF PRE-TRAINED MODELS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Expert ensembles created from pretraining on JFT subsets at architecture scales. 'All' indicates an ExpertEnsemble where the kNN selected from all architecture sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Experts selected when picking from JFT ResNet experts with 18, 34, 50 and 101 layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Experts selected when picking from all available feature extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Test performance of experts selected from all feature extractors vs. those only selected from R101s. Numbers show the number of unique 'types' of experts picked; e.g. for dtd, the final ensemble contains ResNet-50s trained on ImageNet21k and ResNet-101s trained on JFT. There is no clear correlation between the number of unique expert types (a form of diversity) and the accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy of our best ensembles against reproduced baselines from Kolesnikov et al.</figDesc><table><row><cell>Description</cell><cell>Diversity</cell><cell>VTAB 1K</cell><cell cols="3">Natural Specialised Structured</cell></row><row><cell>JFT BiT-L R50  *  JFT BiT-L R152x4  *</cell><cell>--</cell><cell>70.6 [70.4 -71.0] 76.2 [74.5 -76.7]</cell><cell>77.8 86.0</cell><cell>83.6 87.0</cell><cell>57.9 62.2</cell></row><row><cell>JFT R50 Experts + Generalists</cell><cell>U</cell><cell>76.8 [76.4 -77.0]</cell><cell>82.6</cell><cell>85.8</cell><cell>67.2</cell></row><row><cell>JFT R101 Experts</cell><cell>U</cell><cell>77.6 [77.4 -77.8]</cell><cell>83.6</cell><cell>86.4</cell><cell>68.0</cell></row><row><cell>INet21k BiT-M R50  *  INet21k BiT-M R101x3  *</cell><cell>--</cell><cell>70.0 [69.6 -70.5] 72.7 [72.1 -73.5]</cell><cell>77.0 80.3</cell><cell>84.7 85.7</cell><cell>56.6 59.4</cell></row><row><cell>INet21k R50 Experts</cell><cell>U</cell><cell>75.3 [74.5 -75.6]</cell><cell>79.9</cell><cell>85.7</cell><cell>66.1</cell></row><row><cell>INet21k R50 HyperExperts</cell><cell>C</cell><cell>75.6 [74.8 -75.8]</cell><cell>79.9</cell><cell>85.5</cell><cell>67.0</cell></row></table><note>(2019) [* ]. For each dataset, we take the median of three independent runs. Rows show the average over datasets. Bootstrapped confidence intervals at the 95% level. The source of diversity for ensem- bles is shown: U = upstream (during pre-training) and C = combined (pre-training and fine-tuning).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Upstream diversity gives better ensembles. Test accuracy of different ensembles. For each dataset, we take the median of three independent runs. Rows show the average over datasets. Bootstrapped confidence intervals at the 95% level. The source of diversity is noted as: D = downstream (during fine-tuning), U = upstream (during pre-training) and C = combined (both).</figDesc><table><row><cell>Description</cell><cell>Diversity</cell><cell>VTAB 1K</cell><cell cols="3">Natural Specialised Structured</cell></row><row><cell>JFT R50 Seeds</cell><cell>D</cell><cell>74.9 [74.5 -75.1]</cell><cell>78.5</cell><cell>85.9</cell><cell>66.2</cell></row><row><cell>JFT R50 Augs</cell><cell>D</cell><cell>73.7 [73.4 -75.7]</cell><cell>80.6</cell><cell>86.8</cell><cell>61.2</cell></row><row><cell>JFT R50 Hypers</cell><cell>D</cell><cell>75.6 [75.2 -75.7]</cell><cell>80.1</cell><cell>85.6</cell><cell>66.6</cell></row><row><cell>JFT R50 Generalists+Experts</cell><cell>U</cell><cell>76.8 [76.4 -77.0]</cell><cell>82.6</cell><cell>85.8</cell><cell>67.2</cell></row><row><cell>JFT R50 Hyper(Gen. + Experts)</cell><cell>C</cell><cell>77.1 [76.9 -77.4]</cell><cell>82.4</cell><cell>86</cell><cell>68.1</cell></row><row><cell>JFT R50 Aug(Gen. + Experts)</cell><cell>C</cell><cell>77.6 [76.8 -77.9]</cell><cell>82.7</cell><cell>86.4</cell><cell>68.7</cell></row><row><cell>INet21k R50 Hypers</cell><cell>D</cell><cell>74.2 [73.7 -74.8]</cell><cell>79.6</cell><cell>86.2</cell><cell>63.5</cell></row><row><cell>INet21k R50 Experts</cell><cell>U</cell><cell>75.3 [74.5 -75.6]</cell><cell>79.9</cell><cell>85.7</cell><cell>66.1</cell></row><row><cell>INet21k R50 HyperExperts</cell><cell>C</cell><cell>75.6 [74.8 -75.8]</cell><cell>79.9</cell><cell>85.5</cell><cell>67.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Description</cell><cell>VTAB 1K</cell><cell cols="3">Natural Specialised Structured</cell></row><row><cell>Base R50 Generalists+Experts</cell><cell>76.8 [76.4 -77.0]</cell><cell>82.6</cell><cell>85.8</cell><cell>67.2</cell></row><row><cell>Specialists vs R50 Experts</cell><cell>76.4 [76.1 -76.6]</cell><cell>82.2</cell><cell>85.6</cell><cell>66.8</cell></row><row><cell>generalists R50 Generalists</cell><cell>76.5 [75.9 -76.7]</cell><cell>81.3</cell><cell>86.2</cell><cell>67.7</cell></row><row><cell>Combine scales R18/34/50 Experts</cell><cell>76.8 [76.4 -77.2]</cell><cell>82.2</cell><cell>85.5</cell><cell>67.9</cell></row><row><cell>Stack with scale R101 Experts</cell><cell>77.6 [77.4 -77.8]</cell><cell>83.6</cell><cell>86.4</cell><cell>68.0</cell></row><row><cell cols="2">Massive pool All Experts (JFT/INet21k) 77.6 [77.3 -77.7]</cell><cell>83.6</cell><cell>86.1</cell><cell>68.1</cell></row></table><note>Ablations. Test accuracy of different ensembles. For each dataset, we take the median of three independent runs. Rows show the average over datasets. Bootstrapped confidence intervals at the 95% level. Pre-training done on JFT, except for "All Experts" that also used ImageNet21k.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Median number of experts chosen by each method for each dataset. All are pre-trained on JFT, and all are based on ResNet-50 architectures unless otherwise specified</figDesc><table><row><cell></cell><cell>mean</cell><cell>caltech101</cell><cell>cifar100</cell><cell>dtd</cell><cell>flowers</cell><cell>pets</cell><cell>sun397</cell><cell>svhn</cell><cell>camelyon</cell><cell>eurosat</cell><cell>resisc45</cell><cell>retino</cell><cell>clever.count</cell><cell>clever.closest</cell><cell>dmlab</cell><cell>dsprites.xpos</cell><cell>dsprites.orient</cell><cell>kitti</cell><cell>smallnorb.azmth</cell><cell>smallnorb.elev</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Natural</cell><cell></cell><cell></cell><cell cols="4">Specialised</cell><cell></cell><cell></cell><cell cols="4">Structured</cell><cell></cell></row><row><cell>D SeedEnsemble</cell><cell cols="20">9.4 5 11 15 2 3 4 10 10 8 13 15 4 15 15 3 15 10 11 15</cell></row><row><cell>D AugEnsemble</cell><cell cols="8">8.2 3 7 13 2 8 4 7</cell><cell cols="12">8 5 6 10 3 7 15 10 11 8 15 14</cell></row><row><cell>D HyperEnsemble</cell><cell cols="20">9.1 10 13 12 2 8 10 10 13 8 8 8 8 11 10 4 7 6 14 12</cell></row><row><cell>U Experts</cell><cell cols="8">9.6 4 10 14 1 9 3 11</cell><cell cols="12">8 11 12 14 4 14 14 3 13 14 14 14</cell></row><row><cell>U Generalists</cell><cell cols="20">10.9 5 12 15 1 13 14 13 13 9 7 15 5 12 15 3 14 11 15 15</cell></row><row><cell>U Gen+Experts</cell><cell cols="8">9.3 5 8 14 1 6 4 13</cell><cell cols="12">8 8 11 14 4 14 14 3 9 14 14 14</cell></row><row><cell>U R18/34/50 Exp.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>VTAB 1K test accuracy of different selection and ensemble strategies. Note that Brute + Greedy Ensemble requires finetuning 976 models per task whether the others require 60 or less.</figDesc><table><row><cell></cell><cell cols="4">VTAB 1K Natural Specialised Structured</cell></row><row><cell>Brute + Greedy Ensemble</cell><cell>76.20</cell><cell>81.17</cell><cell>85.46</cell><cell>67.22</cell></row><row><cell>kNN top 15 + Greedy Ensemble</cell><cell>76.41</cell><cell>82.20</cell><cell>85.55</cell><cell>66.79</cell></row><row><cell>kNN + Threshold Ensemble</cell><cell>76.45</cell><cell>82.00</cell><cell>85.90</cell><cell>66.87</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transfer learning with cluster ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreangsu</forename><surname>Acharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Unsupervised and Transfer Learning Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superposition of many models into one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Terekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08558</idno>
		<title level="m">On robustness and transferability of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<ptr target="https://pytorch.org/hub/" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), 2019. FAIR. PyTorch Hub</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep ensembles: A loss landscape perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012. Google. TensorFlow Hub</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">AutoML: A survey of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00709</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
	</analytic>
	<monogr>
		<title level="j">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kaggle diabetic retinopathy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyepacs</forename><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/diabetic-retinopathy-detection/data" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Why M heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">dSprites: Disentanglement testing sprites dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">What is being transferred in transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Susano</forename><surname>Pinto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13239</idno>
		<title level="m">Sylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">YouTube-BoundingBoxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ensemble methods in data mining: improving accuracy through combining predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Seni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do image classifiers generalize across time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Phenomena</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04206</idno>
		<title level="m">Diverse ensembles improve calibration</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transfer learning with part-based ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">To ensemble or not ensemble: When does end-to-end training fail?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan-Andrei</forename><surname>Iliescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Lujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Batchensemble: An alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13570</idno>
		<title level="m">Hyperparameter ensembles for robustness and uncertainty quantification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic Frankensteining: Creating complex ensembles autonomously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A novel evolutionary algorithm for automated machine learning focusing on classifier ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Xavier-Júnior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><forename type="middle">B</forename><surname>Feitosa-Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ludermir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Conference on Intelligent Systems (BRACIS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
