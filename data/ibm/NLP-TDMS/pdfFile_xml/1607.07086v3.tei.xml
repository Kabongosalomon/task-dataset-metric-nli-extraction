<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN ACTOR-CRITIC ALGORITHM FOR SEQUENCE PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AN ACTOR-CRITIC ALGORITHM FOR SEQUENCE PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. * CIFAR Senior Fellow † CIFAR Fellow</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many important applications of machine learning, the task is to develop a system that produces a sequence of discrete tokens given an input. Recent work has shown that recurrent neural networks (RNNs) can deliver excellent performance in many such tasks when trained to predict the next output token given the input and previous tokens. This approach has been applied successfully in machine translation <ref type="bibr" target="#b31">(Sutskever et al., 2014;</ref>, caption generation <ref type="bibr" target="#b17">(Kiros et al., 2014;</ref><ref type="bibr" target="#b10">Donahue et al., 2015;</ref><ref type="bibr" target="#b45">Xu et al., 2015;</ref><ref type="bibr" target="#b15">Karpathy &amp; Fei-Fei, 2015)</ref>, and speech recognition <ref type="bibr" target="#b7">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b4">Chan et al., 2015)</ref>.</p><p>The standard way to train RNNs to generate sequences is to maximize the log-likelihood of the "correct" token given a history of the previous "correct" ones, an approach often called teacher forcing. At evaluation time, the output sequence is often produced by an approximate search for the most likely candidate according to the learned distribution. During this search, the model is conditioned on its own guesses, which may be incorrect and thus lead to a compounding of errors . This can become especially problematic for longer sequences. Due to this discrepancy between training and testing conditions, it has been shown that maximum likelihood training can be suboptimal <ref type="bibr" target="#b26">Ranzato et al., 2015)</ref>. In these works, the authors argue that the network should be trained to continue generating correctly given the outputs already produced by the model, rather than the ground-truth reference outputs from the data. This gives rise to the challenging problem of determining the target for the next network output.  use the token k from the ground-truth answer as the target for the network at step k, whereas <ref type="bibr" target="#b26">Ranzato et al. (2015)</ref> rely on the REINFORCE algorithm <ref type="bibr" target="#b42">(Williams, 1992)</ref> to decide whether or not the tokens In this work, we propose and study an alternative procedure for training sequence prediction networks that aims to directly improve their test time metrics (which are typically not the log-likelihood). In particular, we train an additional network called the critic to output the value of each token, which we define as the expected task-specific score that the network will receive if it outputs the token and continues to sample outputs according to its probability distribution. Furthermore, we show how the predicted values can be used to train the main sequence prediction network, which we refer to as the actor. The theoretical foundation of our method is that, under the assumption that the critic computes exact values, the expression that we use to train the actor is an unbiased estimate of the gradient of the expected task-specific score.</p><p>Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL) <ref type="bibr" target="#b33">(Sutton &amp; Barto, 1998)</ref>, in particular from the actor-critic approach <ref type="bibr" target="#b35">(Sutton, 1984;</ref><ref type="bibr" target="#b34">Sutton et al., 1999;</ref><ref type="bibr" target="#b1">Barto et al., 1983)</ref>. RL studies the problem of acting efficiently based only on weak supervision in the form of a reward given for some of the agent's actions. In our case, the reward is analogous to the task-specific score associated with a prediction. However, the tasks we consider are those of supervised learning, and we make use of this crucial difference by allowing the critic to use the ground-truth answer as an input. In other words, the critic has access to a sequence of expert actions that are known to lead to high (or even optimal) returns. To train the critic, we adapt the temporal difference methods from the RL literature <ref type="bibr" target="#b32">(Sutton, 1988)</ref> to our setup. While RL methods with non-linear function approximators are not new <ref type="bibr" target="#b36">(Tesauro, 1994;</ref><ref type="bibr" target="#b21">Miller et al., 1995)</ref>, they have recently surged in popularity, giving rise to the field of 'deep RL' <ref type="bibr" target="#b22">(Mnih et al., 2015)</ref>. We show that some of the techniques recently developed in deep RL, such as having a target network, may also be beneficial for sequence prediction.</p><p>The contributions of the paper can be summarized as follows: 1) we describe how RL methodology like the actor-critic approach can be applied to supervised learning problems with structured outputs; and 2) we investigate the performance and behavior of the new method on both a synthetic task and a real-world task of machine translation, demonstrating the improvements over maximum-likelihood and REINFORCE brought by the actor-critic training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We consider the problem of learning to produce an output sequence Y = (y 1 , . . . , y T ), y t ∈ A given an input X, where A is the alphabet of output tokens. We will often use notation Y f ...l to refer to subsequences of the form (y f , . . . , y l ). Two sets of input-output pairs (X, Y ) are assumed to be available for both training and testing. The trained predictor h is evaluated by computing the average task-specific score R(Ŷ , Y ) on the test set, whereŶ = h(X) is the prediction. To simplify the formulas we always use T to denote the length of an output sequence, ignoring the fact that the output sequences may have different length.</p><p>Recurrent neural networks A recurrent neural network (RNN) produces a sequence of state vectors (s 1 , . . . , s T ) given a sequence of input vectors (e 1 , . . . , e T ) by starting from an initial s 0 state and applying T times the transition function f : s t = f (s t−1 , e t ). Popular choices for the mapping f are the Long Short-Term Memory <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref> and the Gated Recurrent Units <ref type="bibr" target="#b6">(Cho et al., 2014)</ref>, the latter of which we use for our models.</p><p>To build a probabilistic model for sequence generation with an RNN, one adds a stochastic output layer g (typically a softmax for discrete outputs) that generates outputs y t ∈ A and can feed these outputs back by replacing them with their embedding e(y t ):</p><formula xml:id="formula_0">y t ∼ g(s t−1 )</formula><p>(1) s t = f (s t−1 , e(y t )).</p><p>( <ref type="formula">2)</ref> Thus, the RNN defines a probability distribution p(y t |y 1 , . . . , y t−1 ) of the next output token y t given the previous tokens (y 1 , . . . , y t−1 ). Upon adding a special end-of-sequence token ∅ to the alphabet A, the RNN can define the distribution p(Y ) over all possible sequences as p(Y ) = p(y 1 )p(y 2 |y 1 ) . . . p(y T |y 1 , . . . , y T −1 )p(∅|y 1 , . . . , y T ).</p><p>RNNs for sequence prediction To use RNNs for sequence prediction, they must be augmented to generate Y conditioned on an input X. The simplest way to do this is to start with an initial state s 0 = s 0 (X) <ref type="bibr" target="#b31">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al., 2014)</ref>. Alternatively, one can encode X as a variable-length sequence of vectors (h 1 , . . . , h L ) and condition the RNN on this sequence using an attention mechanism. In our models, the sequence of vectors is produced by either a bidirectional RNN <ref type="bibr" target="#b29">(Schuster &amp; Paliwal, 1997)</ref> or a convolutional encoder <ref type="bibr" target="#b28">(Rush et al., 2015)</ref>.</p><p>We use a soft attention mechanism  that computes a weighted sum of a sequence of vectors. The attention weights determine the relative importance of each vector. More formally, we consider the following equations for RNNs with attention:</p><formula xml:id="formula_1">y t ∼ g(s t−1 , c t−1 ) (3) s t = f (s t−1 , c t−1 , e(y t )) (4) α t = β(s t , (h 1 , . . . , h L )) (5) c t = L j=1 α t,j h j<label>(6)</label></formula><p>where β is the attention mechanism that produces the attention weights α t and c t is the context vector, or 'glimpse', for time step t. The attention weights are computed by an MLP that takes as input the current RNN state and each individual vector to focus on. The weights are typically (as in our work) constrained to be positive and sum to 1 by using the softmax function.</p><p>A conditioned RNN can be trained for sequence prediction by gradient ascent on the log-likelihood log p(Y |X) for the input-output pairs (X, Y ) from the training set. To produce a predictionŶ for a test input sequence X, an approximate beam search for the maximum of p(·|X) is usually conducted. During this search the probabilities p(·|ŷ 1 , . . . ,ŷ t−1 ) are considered, where the previous tokenŝ y 1 , . . . ,ŷ t−1 comprise a candidate beginning of the predictionŶ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value functions</head><p>We view the conditioned RNN as a stochastic policy that generates actions and receives the task score (e.g., BLEU score) as the return. We furthermore consider the case when the return R is partially received at the intermediate steps in the form of rewards r t :</p><formula xml:id="formula_2">R(Ŷ , Y ) = T t=1 r t (ŷ t ;Ŷ 1...t−1 , Y )</formula><p>. This is more general than the case of receiving the full return at the end of the sequence, as we can simply define all rewards other than r T to be zero. Receiving intermediate rewards may ease the learning for the critic, and we use reward shaping as explained in Section 3. Given the policy, possible actions and reward function, the value represents the expected future return as a function of the current state of the system, which in our case is uniquely defined by the sequence of actions taken so far,Ŷ 1...t−1 .</p><p>We define the value of an unfinished predictionŶ 1...t as follows:</p><formula xml:id="formula_3">V (Ŷ 1...t ; X, Y ) = Ê Y t+1...T ∼p(.|Ŷ1...t,X) T τ =t+1 r τ (ŷ τ ;Ŷ 1...τ −1 , Y ).</formula><p>We define the value of a candidate next token a for an unfinished predictionŶ 1...t−1 as the expected future return after generating token a:</p><formula xml:id="formula_4">Q(a;Ŷ 1...t−1 , X, Y ) = Ê Y t+1...T ∼p(.|Ŷ1...t−1a,X) r t (a;Ŷ 1...t−1 , Y ) + T τ =t+1 r τ (ŷ τ ;Ŷ 1...t−1 aŶ t+1...τ , Y ) .</formula><p>We will refer to the candidate next tokens as actions. For notational simplicity, we henceforth drop X and Y from the signature of p, V , Q, R and r t , assuming it is clear from the context which of X and Y is meant. We will also use V without arguments for the expected reward of a random prediction.</p><p>Algorithm 1 Actor-Critic Training for Sequence Prediction</p><p>Require: A criticQ(a;Ŷ 1...t , Y ) and an actor p(a|Ŷ 1...t , X) with weights φ and θ respectively. 1: Initialize delayed actor p and target criticQ with same weights: θ = θ, φ = φ. 2: while Not Converged do 3:</p><p>Receive a random example (X, Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Generate a sequence of actionsŶ from p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute targets for the critic</p><formula xml:id="formula_5">q t = r t (ŷ t ;Ŷ 1...t−1 , Y ) + a∈A p (a|Ŷ 1...t , X)Q (a;Ŷ 1...t , Y ) 6:</formula><p>Update the critic weights φ using the gradient</p><formula xml:id="formula_6">d dφ T t=1 Q (ŷ t ;Ŷ 1...t−1 , Y ) − q t 2 + λ C C t where C t = a Q (a;Ŷ 1...t−1 ) − 1 |A| bQ (b;Ŷ 1...t−1 ) 2 7:</formula><p>Update actor weights θ using the following gradient estimate</p><formula xml:id="formula_7">dV (X, Y ) dθ = T t=1 a∈A dp(a|Ŷ 1...t−1 , X) dθQ (a;Ŷ 1...t−1 , Y ) + λ LL T t=1 dp(y t |Y 1...t−1 , X) dθ 8:</formula><p>Update delayed actor and target critic, with constants γ θ 1, γ φ 1</p><formula xml:id="formula_8">θ = γ θ θ + (1 − γ θ )θ , φ = γ φ φ + (1 − γ φ )φ 9: end while</formula><p>Algorithm 2 Complete Actor-Critic Algorithm for Sequence Prediction 1: Initialize criticQ(a;Ŷ 1...t , Y ) and actor p(a|Ŷ 1...t , X) with random weights φ and θ respectively.</p><p>2: Pre-train the actor to predict y t+1 given Y 1...t by maximizing log p(y t+1 |Y 1...t , X). 3: Pre-train the critic to estimate Q by running Algorithm 1 with fixed actor. 4: Run Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACTOR-CRITIC FOR SEQUENCE PREDICTION</head><p>Let θ be the parameters of the conditioned RNN, which we will also refer to as the actor. Our training algorithm is based on the following way of rewriting the gradient of the expected return dV dθ :</p><formula xml:id="formula_9">dV dθ = Ê Y ∼p(Ŷ |X) T t=1 a∈A dp(a|Ŷ 1...t−1 ) dθ Q(a;Ŷ 1...t−1 ).<label>(7)</label></formula><p>This equality is known in RL under the names policy gradient theorem <ref type="bibr" target="#b34">(Sutton et al., 1999)</ref> and stochastic actor-critic <ref type="bibr" target="#b35">(Sutton, 1984)</ref>. 1 Note that we use the probability rather than the log probability in this formula (which is more typical in RL applications) as we are summing over actions rather than taking an expectation. Intuitively, this equality corresponds to increasing the probability of actions that give high values, and decreasing the probability of actions that give low values. Since this gradient expression is an expectation, it is trivial to build an unbiased estimate for it:</p><formula xml:id="formula_10">dV dθ = M k=1 T t=1 a∈A dp(a|Ŷ k 1...t−1 ) dθ Q(a;Ŷ k 1...t−1 )<label>(8)</label></formula><p>whereŶ k are M random samples from p(Ŷ ). By replacing Q with a parameteric estimateQ one can obtain a biased estimate with relatively low variance. The parameteric estimateQ is called the critic.</p><p>The above formula is similar in spirit to the REINFORCE learning rule that Ranzato et al. <ref type="formula">(2015)</ref> use in the same context:</p><formula xml:id="formula_11">dV dθ = M k=1 T t=1 d log p(ŷ k t |Ŷ k 1...t−1 ) dθ T τ =t r τ (ŷ k τ ;Ŷ k 1...τ −1 ) − b t (X) ,<label>(9)</label></formula><p>where the scalar b t (X) is called baseline or control variate. The difference is that in REINFORCE the inner sum over all actions is replaced by its 1-sample estimate, namely</p><formula xml:id="formula_12">d log p(ŷt|Ŷ1...t−1) dθ Q(ŷ t ;Ŷ 1...t−1 ), where the log probability d log p(ŷt|...) dθ = 1 p(ŷt|...) dp(ŷt|...) dθ</formula><p>is introduced to correct for the sampling ofŷ t . Furthermore, instead of the value Q(ŷ t ;Ŷ 1...t−1 ), REIN-FORCE uses the cumulative reward T τ =t r τ (ŷ τ ;Ŷ 1...τ −1 ) following the actionŷ t , which again can be seen as a 1-sample estimate of Q. Due to these simplifications and the potential high variance in the cumulative reward, the REINFORCE gradient estimator has very high variance. In order to improve upon it, we consider the actor-critic estimate from Equation 8, which has a lower variance at the cost of significant bias, since the critic is not perfect and trained simultaneously with the actor. The success depends on our ability to control the bias by designing the critic network and using an appropriate training criterion for it.</p><p>To implement the critic, we propose to use a separate RNN parameterized by φ. The critic RNN is run in parallel with the actor, consumes the tokensŷ t that the actor outputs and produces the estimateŝ Q(a;Ŷ 1...t ) for all a ∈ A. A key difference between the critic and the actor is that the correct answer Y is given to the critic as an input, similarly to how the actor is conditioned on X. Indeed, the return R(Ŷ , Y ) is a deterministic function of Y , and we argue that using Y to computeQ should be of great help. We can do this because the values are only required during training and we do not use the critic at test time. We also experimented with providing the actor states s t as additional inputs to the critic. See <ref type="figure">Figure 1</ref> for a visual representation of our actor-critic architecture.</p><p>Temporal-difference learning A crucial component of our approach is policy evaluation, that is the training of the critic to produce useful estimates ofQ. With a naïve Monte-Carlo method, one could use the future return T τ =t r τ (ŷ τ ;Ŷ 1...τ −1 ) as a target toQ(ŷ t ;Ŷ 1...t−1 ), and use the critic parameters φ to minimize the square error between these two values. However, like with REINFORCE, using such a target yields to very high variance which quickly grows with the number of steps T . We use a temporal difference (TD) method for policy evaluation <ref type="bibr" target="#b32">(Sutton, 1988)</ref>. Namely, we use the right-hand side q t = r t (ŷ t ;Ŷ 1...t−1 ) + a∈A p(a|Ŷ 1...t )Q(a;Ŷ 1...t ) of the Bellman equation as the target for the left-handQ(ŷ t ;Ŷ 1...t−1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Both the actor and the critic are encoder-decoder networks. The actor receives an input sequence X and produces samplesŶ which are evaluated by the critic. The critic takes in the ground-truth sequence Y as input to the encoder, and takes the input summary (calculated using an attention mechanism) and the actor's predictionŷ t as input at time step t of the decoder. The values Q 1 , Q 2 , · · · , Q T computed by the critic are used to approximate the gradient of the expected returns with respect to the parameters of the actor. This gradient is used to train the actor to optimize these expected task specific returns (e.g., BLEU score). The critic may also receive the hidden state activations of the actor as input.</p><p>Applying deep RL techniques It has been shown in the RL literature that ifQ is non-linear (like in our case), the TD policy evaluation might diverge <ref type="bibr" target="#b38">(Tsitsiklis &amp; Van Roy, 1997)</ref>. Previous work has shown that this problem can be alleviated by using an additional target networkQ to compute q t , which is updated less often and/or more slowly thanQ. Similarly to <ref type="bibr" target="#b18">(Lillicrap et al., 2015)</ref>, we update the parameters φ of the target critic by linearly interpolating them with the parameters of the trained one. Attempts to remove the target network by propagating the gradient through q t resulted in a lower square error (Q(ŷ t ;Ŷ 1...T ) − q t ) 2 , but the resultingQ values proved very unreliable as training signals for the actor.</p><p>The fact that both actor and critic use outputs of each other for training creates a potentially dangerous feedback loop. To address this, we sample predictions from a delayed actor <ref type="bibr" target="#b18">(Lillicrap et al., 2015)</ref>, whose weights are slowly updated to follow the actor that is actually trained.</p><p>Dealing with large action spaces One of the challenges of our work is that the action space is very large (as is typically the case in NLP tasks with large vocabularies). This can be alleviated by putting constraints on the critic values for actions that are rarely sampled. We found experimentally that shrinking the values of these rare actions is necessary for the algorithm to converge. Specifically, we add a term C t for every step t to the critic's optimization objective which drives all value predictions of the critic closer to their mean:</p><formula xml:id="formula_13">C t = a Q (a;Ŷ 1...t−1 ) − 1 |A| bQ (b;Ŷ 1...t−1 ) 2<label>(10)</label></formula><p>This corresponds to penalizing the variance of the outputs of the critic. Without this penalty the values of rare actions can be severely overestimated, which biases the gradient estimates and can cause divergence. A similar trick was used in the context of learning simple algorithms with Q-learning .</p><p>Reward shaping While we are ultimately interested in the maximization of the score of a complete prediction, simply awarding this score at the last step provides a very sparse training signal for the critic. For this reason we use potential-based reward shaping with potentials Φ(Ŷ 1...t ) = R(Ŷ 1...t ) for incomplete sequences and Φ(Ŷ ) = 0 for complete ones <ref type="bibr" target="#b23">(Ng et al., 1999)</ref>. Namely, for a predicted sequenceŶ we compute score values for all prefixes to obtain the sequence of scores (R(Ŷ 1...1 ), R(Ŷ 1...2 ), . . . , R(Ŷ 1...T )). The difference between the consecutive pairs of scores is then used as the reward at each step: r t (ŷ t ;Ŷ 1...t−1 ) = R(Ŷ 1...t ) − R(Ŷ 1...t−1 ). Using the shaped reward r t instead of awarding the whole score R at the last step does not change the optimal policy <ref type="bibr" target="#b23">(Ng et al., 1999)</ref>.</p><p>Putting it all together Algorithm 1 describes the proposed method in detail. We consider adding the weighted log-likelihood gradient to the actor's gradient estimate. This is in line with the prior work by <ref type="bibr" target="#b26">(Ranzato et al., 2015)</ref> and <ref type="bibr" target="#b30">(Shen et al., 2015)</ref>. It is also motivated by our preliminary experiments that showed that using the actor-critic estimate alone can lead to an early determinization of the policy and vanishing gradients (also discussed in Section 6). Starting training with a randomly initialized actor and critic would be problematic, because neither the actor nor the critic would provide adequate training signals for one another. The actor would sample completely random predictions that receive very little reward, thus providing a very weak training signal for the critic. A random critic would be similarly useless for training the actor. Motivated by these considerations, we pre-train the actor using standard log-likelihood training. Furthermore, we pre-train the critic by feeding it samples from the pre-trained actor, while the actor's parameters are frozen. The complete training procedure including pre-training is described by Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>In other recent RL-inspired work on sequence prediction, Ranzato et al. <ref type="formula">(2015)</ref> trained a translation model by gradually transitioning from maximum likelihood learning into optimizing BLEU or ROUGE scores using the REINFORCE algorithm. However, REINFORCE is known to have very high variance and does not exploit the availability of the ground-truth like the critic network does. The approach also relies on a curriculum learning scheme. Standard value-based RL algorithms like SARSA and OLPOMDP have also been applied to structured prediction <ref type="bibr" target="#b20">(Maes et al., 2009)</ref>. Again, these systems do not use the ground-truth for value prediction.</p><p>Imitation learning has also been applied to structured prediction <ref type="bibr" target="#b41">(Vlachos, 2012)</ref>. Methods of this type include the SEARN <ref type="bibr" target="#b9">(Daumé Iii et al., 2009</ref>) and DAGGER <ref type="bibr" target="#b27">(Ross et al., 2010)</ref> algorithms. These methods rely on an expert policy to provide action sequences that the policy learns to imitate. Unfortunately, it's not always easy or even possible to construct an expert policy for a task-specific score. In our approach, the critic plays a role that is similar to the expert policy, but is learned without requiring prior knowledge about the task-specific score. The recently proposed 'scheduled sampling'  can also be seen as imitation learning. In this method, ground-truth tokens are occasionally replaced by samples from the model itself during training. A limitation is that the token k for the ground-truth answer is used as the target at step k, which might not always be the optimal strategy.</p><p>There are also approaches that aim to approximate the gradient of the expected score. One such approach is 'Direct Loss Minimization' <ref type="bibr" target="#b13">(Hazan et al., 2010)</ref> in which the inference procedure is adapted to take both the model likelihood and task-specific score into account. Another popular approach is to replace the domain over which the task score expectation is defined with a small subset of it, as is done in Minimum (Bayes) Risk Training <ref type="bibr" target="#b11">(Goel &amp; Byrne, 2000;</ref><ref type="bibr" target="#b30">Shen et al., 2015;</ref><ref type="bibr" target="#b24">Och, 2003)</ref>. This small subset is typically an n-best list or a sample (like in REINFORCE) that may or may not include the ground-truth as well. None of these methods provide intermediate targets for the actor during training, and <ref type="bibr" target="#b30">Shen et al. (2015)</ref> report that as many as 100 samples were required for the best results.</p><p>Another recently proposed method is to optimize a global sequence cost with respect to the selection and pruning behavior of the beam search procedure itself <ref type="bibr" target="#b43">(Wiseman &amp; Rush, 2016)</ref>. This method follows the more general strategy called 'learning as search optimization' <ref type="bibr" target="#b8">(Daumé III &amp; Marcu, 2005)</ref>. This is an interesting alternative to our approach; however, it is designed specifically for the precise inference procedure involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To validate our approach, we performed two sets of experiments 2 . First, we trained the proposed model to recover strings of natural text from their corrupted versions. Specifically, we consider each character in a natural language corpus and with some probability replace it with a random character. We call this synthetic task spelling correction. A desirable property of this synthetic task is that data is essentially infinite and overfitting is no concern. Our second series of experiments is done on the task of automatic machine translation using different models and datasets.</p><p>In addition to maximum likelihood and actor-critic training we implemented two versions of the REINFORCE gradient estimator. In the first version, we use a linear baseline network that takes the actor states as input, exactly as in <ref type="bibr" target="#b26">(Ranzato et al., 2015)</ref>. We also propose a novel extension of REINFORCE that leverages the extra information available in the ground-truth output Y . Specifically, we use theQ estimates produced by the critic network as the baseline for the REINFORCE algorithm. The motivation behind this approach is that using the ground-truth output should produce a better baseline that lowers the variance of REINFORCE, resulting in higher task-specific scores. We refer to this method as REINFORCE-critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SPELLING CORRECTION</head><p>We use text from the One Billion Word dataset for the spelling correction task <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref>, which has pre-defined training and testing sets. The training data was abundant, and we never used any example twice. We evaluate trained models on a section of the test data that comprises 6075 sentences. To speed up experiments, we clipped all sentences to the first 10 or 30 characters.</p><p>For the spelling correction actor network, we use an RNN with 100 Gated Recurrent Units (GRU) and a bidirectional GRU network for the encoder. We use the same attention mechanism as proposed in , which effectively makes our actor network a smaller version of the model used in that work. For the critic network, we employed a model with the same architecture as the actor.</p><p>We use character error rate (CER) to measure performance on the spelling task, which we define as the ratio between the total of Levenshtein distances between predictions and ground-truth outputs and the total length of the ground-truth outputs. This is a corpus-level metric for which a lower value is better. We use it as the return by negating per-sentence ratios. At the evaluation time greedy search is used to extract predictions from the model.  We use the ADAM optimizer <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> to train all the networks with the parameters recommended in the original paper, with the exception of the scale parameter α. The latter is first set to 10 −3 and then annealed to 10 −4 for log-likelihood training. For the pre-training stage of the actor-critic, we use α = 10 −3 and decrease it to 10 −4 for the joint actor-critic training. We pretrain the actor until its score on the development set stops improving. We pretrain the critic until its TD error stabilizes 3 . We used M = 1 sample for both actor-critic and REIN-FORCE. For exact hyperparameter settings we refer the reader to Appendix A.</p><p>We start REINFORCE training from a pretrained actor, but we do not use the curriculum learning employed in MIXER. The critic is trained in the same way for both REINFORCE and actorcritic, including the pretraining stage. We report results obtained with the reward shaping described in Section 3, as we found that it slightly improves REINFORCE performance. <ref type="table">Table 1</ref> presents our results on the spelling correction task. We observe an improvement in CER over log-likelihood training for all four settings considered. Without simultaneous loglikelihood training, actor-critic training results in a better CER than REINFORCE-critic in three <ref type="table">Table 1</ref>: Character error rate of different methods on the spelling correction task. In the table L is the length of input strings, η is the probability of replacing a character with a random one. LL stands for the log-likelihood training, AC and RF-C and for the actor-critic and the REINFORCE-critic respectively, AC+LL and RF-C+LL for the combinations of AC and RF-C with LL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>Character out of four settings. In the fourth case, actor-critic and REINFORCE-critic have similar performance. Adding the log-likelihood gradient with a cofficient λ LL = 0.1 helps both of the methods, but actor-critic still retains a margin of improvement over REINFORCE-critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MACHINE TRANSLATION</head><p>For our first translation experiment, we use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign <ref type="bibr" target="#b3">(Cettolo et al., 2014)</ref>, as used in <ref type="bibr" target="#b26">Ranzato et al. (2015)</ref>, and closely follow the pre-processing described in that work. The training data comprises about 153,000 German-English sentence pairs. In addition we considered a larger WMT14 English-French dataset <ref type="bibr" target="#b6">Cho et al. (2014)</ref> with more than 12 million examples. For further information about the data we refer the reader to Appendix B.</p><p>The return is defined as a smoothed and rescaled version of the BLEU score. Specifically, we start all n-gram counts from 1 instead of 0, and multiply the resulting score by the length of the ground-truth translation.</p><p>Smoothing is a common practice when sentence-level BLEU score is considered, and it has been used to apply REINFORCE in similar settings <ref type="bibr" target="#b26">(Ranzato et al., 2015)</ref>.</p><p>IWSLT 2014 with a convolutional encoder In our first experiment we use a convolutional encoder in the actor to make our results more comparable with <ref type="bibr" target="#b26">Ranzato et al. (2015)</ref>. For the same reason, we use 256 hidden units in the networks. For the critic, we replaced the convolutional network with a bidirectional GRU network. For training this model we mostly used the same hyperparameter values as in the spelling correction experiments, with a few differences highlighted in Appendix A. For decoding we used greedy search and beam search with a beam size of 10. We found that penalizing candidate sentences that are too short was required to obtain the best results. Similarly to <ref type="bibr" target="#b12">(Hannun et al., 2014)</ref>, we subtracted ρT from the negative log-likelihood of each candidate sentence, where T is the candidate's length, and ρ is a hyperparameter tuned on the validation set.</p><p>The results are summarized in <ref type="table">Table 2</ref>. We report a significant improvement of 2.3 BLEU points over the log-likelihood baseline when greedy search is used for decoding. Surprisingly, the best performing method is REINFORCE with critic, with an additional 0.6 BLEU point advantage over the actor-critic. When beam-search is used, the ranking of the compared approaches is the same, but the margin between the proposed methods and log-likelihood training becomes smaller. The final performances of the actor-critic and the REINFORCE-critic with greedy search are also 0.7 and 1.3 BLEU points respectively better than what <ref type="bibr" target="#b26">Ranzato et al. (2015)</ref> report for their MIXER approach. This comparison should be treated with caution, because our log-likelihood baseline is 1.6 BLEU <ref type="table">Table 3</ref>: Our IWSLT 2014 machine translation results with a bidirectional recurrent encoder compared to the previous work. Please see <ref type="table">Table 1</ref> for an explanation of abbreviations. The asterisk identifies results from <ref type="bibr" target="#b43">(Wiseman &amp; Rush, 2016</ref>   <ref type="table">Table 1</ref> for an explanation of abbreviations. The apostrophy and the asterisk identify results from  and <ref type="bibr" target="#b30">(Shen et al., 2015)</ref> respectively. points stronger than its equivalent from <ref type="bibr" target="#b26">(Ranzato et al., 2015)</ref>. The performance of REINFORCE with a simple baseline matches the score reported for MIXER in <ref type="bibr" target="#b26">Ranzato et al. (2015)</ref>.</p><p>To better understand the IWSLT 2014 results we provide the learning curves for the considered approaches in <ref type="figure" target="#fig_1">Figure 2</ref>. We can clearly see that the training methods that use generated predictions have a strong regularization effect -that is, better progress on the validation set in exchange for slower or negative progress on the training set. The effect is stronger for both REINFORCE varieties, especially for the one without a critic. The actor-critic training does a much better job of fitting the training set than REINFORCE and is the only method except log-likelihood that shows a clear overfitting, which is a healthy behaviour for such a small dataset.</p><p>In addition, we performed an ablation study. We found that using a target network was crucial; while the joint actor-critic training was still progressing with γ θ = 0.1, with γ θ = 1.0 it did not work at all. Similarly important was the value penalty described in Equation <ref type="formula" target="#formula_13">(10)</ref>. We found that good values of the λ coefficient were in the range [10 −3 , 10 −6 ]. Other techniques, such as reward shaping and a delayed actor, brought moderate performance gains. We refer the reader to Appendix A for more details.</p><p>IWSLT 2014 with a bidirectional GRU encoder In order to compare our results with those reported by <ref type="bibr" target="#b43">Wiseman &amp; Rush (2016)</ref> we repeated our IWSLT 2014 investigation with a different encoder, a bidirectional RNN with 256 GRU units. In this round of experiments we also tried to used combined training objectives in the same way as in our spelling correction experiments. The results are summarized in <ref type="table">Table 3</ref>. One can see that the actor-critic training, especially its AC+LL version, yields significant improvements (1.7 with greedy search and 1.0 with beam search) upon the pure log-likelihood training, which are comparable to those brought by Beam Search Optimization (BSO), even though our log-likelihood baseline is much stronger. In this round of experiments actor-critic and REINFORCE-critic performed on par.</p><p>WMT 14 Finally we report our results on a very popular large WMT14 English-French dataset <ref type="bibr" target="#b6">(Cho et al., 2014)</ref> in <ref type="table" target="#tab_2">Table 4</ref>. Our model closely follows the achitecture from , however we achieved a higher baseline performance by annealing the learning rate α and penalizing output sequences that were too short during beam search. The actor-critic training brings a significant 1.5 BLEU improvement with greedy search and a noticeable 0.4 BLEU improvement with beam search. In previous work <ref type="bibr" target="#b30">Shen et al. (2015)</ref> report a higher improvement of 1.4 BLEU with beam search, however they use 100 samples for each training example, whereas we use just one. We note that in this experiment, which is perhaps the most realistic settings, the actor-critic enjoys a significant advantage over the REINFORCE-critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We proposed an actor-critic approach to sequence prediction. Our method takes the task objective into account during training and uses the ground-truth output to aid the critic in its prediction of intermediate targets for the actor. We showed that our method leads to significant improvements over maximum likelihood training on both a synthetic task and a machine translation benchmark. Compared to REINFORCE training on machine translation, actor-critic fits the training data much faster, although in some of our experiments we were able to significantly reduce the gap in the training speed and achieve a better test error using our critic network as the baseline for REINFORCE.</p><p>One interesting observation we made from the machine translation results is that the training methods that use generated predictions have a strong regularization effect. Our understanding is that conditioning on the sampled outputs effectively increases the diversity of training data. This phenomenon makes it harder to judge whether the actor-critic training meets our expectations, because a noisier gradient estimate yielded a better test set performance. We argue that the spelling correction results obtained on a virtually infinite dataset in conjuction with better machine translation performance on the large WMT 14 dataset provide convincing evidence that the actor-training can be effective. In future work we will consider larger machine translation datasets.</p><p>We ran into several optimization issues. The critic would sometimes assign very high values to actions with a very low probability according to the actor. We were able to resolve this by penalizing the critic's variance. Additionally, the actor would sometimes have trouble to adapt to the demands of the critic. We noticed that the action distribution tends to saturate and become deterministic, causing the gradient to vanish. We found that combining an RL training objective with log-likelihood can help, but in general we think this issue deserves further investigation. For example, one can look for suitable training criteria that have a well-behaved gradient even when the policy has little or no stochasticity.</p><p>In a concurrent work <ref type="bibr" target="#b44">Wu et al. (2016)</ref> show that a version of REINFORCE with the baseline computed using multiple samples can improve performance of a very strong machine translation system. This result, and our REINFORCE-critic experiments, suggest that often the variance of REINFORCE can be reduced enough to make its application practical. That said, we would like to emphasize that this paper attacks the problem of gradient estimation from a very different angle as it aims for low-variance but potentially high-bias estimates. The idea of using the ground-truth output that we proposed is an absolutely necessary first step in this direction. Future work could focus on further reducing the bias of the actor-critic estimate, for example, by using a multi-sample training criterion for the critic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETERS</head><p>For machine translation experiments the variance penalty coefficient λ was set to 10 −4 , and the delay coefficients γ θ and γ φ were both set to 10 −4 . For REINFORCE with the critic we did not use a delayed actor, i.e. γ θ was set to 1. For the spelling correction task we used the same γ θ and γ φ but a different λ = 10 −3 . When we used a combined training criterion, the weight of the log-likelihood gradient λ LL was always 0.1. All initial weights were sampled from a centered uniform distribution with width 0.1.</p><p>In some of our experiments we provided the actor states as additional inputs to the critic. Specifically, we did so in our spelling correction experiments and in our WMT 14 machine translation study. All the other results were obtained without this technique.</p><p>For decoding with beam search we substracted the length of a candidate times ρ from the loglikelihood cost. The exact value of ρ was selected on the validation set and was equal to 0.8 for models trained by log-likelihood and REINFORCE and to 1.0 for models trained by actor-critic and REINFORCE-critic.</p><p>For some of the hyperparameters we performed an ablation study. The results are reported in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATA</head><p>For the IWSLT 2014 data the sizes of validation and tests set were 6,969 and 6,750, respectively. We limited the number of words in the English and German vocabularies to the 22,822 and 32,009 most frequent words, respectively, and replaced all other words with a special token. The maximum sentence length in our dataset was 50. For WMT14 we used vocabularies of 30,000 words for both English and French, and the maximum sentence length was also 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C GENERATED Q-VALUES</head><p>In <ref type="table">Table C</ref> we provide an example of value predictions that the critic outputs for candidate next words. One can see that the critic has indeed learnt to assign larger values for the appropriate next words. While the critic does not always produce sensible estimates and can often predict a high return for irrelevant rare words, this is greatly reduced using the variance penalty term from Equation <ref type="formula" target="#formula_13">(10)</ref>.  <ref type="formula" target="#formula_9">(7)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Progress of log-likelihood (LL), RE-INFORCE (RF) and actor-critic (AC) training in terms of BLEU score on the training (train) and validation (valid) datasets. LL* stands for the annealing phase of log-likelihood training. The curves start from the epoch of log-likelihood pretraining from which the parameters were initialized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The best 3 words according to the critic at intermediate steps of generating a translation. The numbers in parentheses are the value predictionsQ. The German original is "über eine davon will ich hier erzählen ." The reference translation is "and there's one I want to talk about"729) want(3.497) going(3.396) tell talk(3.717) you(2.407) to(2.133) you about(1.209) that(0.989) talk(0.924) about about(0.706) .(0.660) right(0.653) here .(0.498) ?(0.291) -(0.285) . .(0.195) there(0.175) know(0.087) ∅ .(0.168) ∅ (-0.093) ?(-0.173) D PROOF OF EQUATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Our WMT 14 machine translation results compared to the previous work. Please see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of an ablation study. We tried varying the actor update speed γ θ , the critic update speed γ φ , the value penalty coefficient λ, whether or not reward shaping is used, whether or not temporal difference (TD) learning is used for the critic. Reported are the best training and validation BLEU score obtained in the course of the first 10 training epochs. Some of the validation scores would still improve with longer training. Greedy search was used for decoding.</figDesc><table><row><cell>γ θ</cell><cell>γ φ</cell><cell>λ</cell><cell cols="4">reward shaping TD train BLEU valid BLEU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −3</cell><cell>yes</cell><cell>yes</cell><cell>33.73</cell><cell>23.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">with different γ φ</cell><cell></cell><cell></cell></row><row><cell cols="2">0.001 0.01</cell><cell>10 −3</cell><cell>yes</cell><cell>yes</cell><cell>33.52</cell><cell>23.03</cell></row><row><cell>0.001</cell><cell>0.1</cell><cell>10 −3</cell><cell>yes</cell><cell>yes</cell><cell>32.63</cell><cell>22.80</cell></row><row><cell>0.001</cell><cell>1</cell><cell>10 −3</cell><cell>yes</cell><cell>yes</cell><cell>9.59</cell><cell>8.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">with different γ θ</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.001</cell><cell>10 −3</cell><cell>yes</cell><cell>yes</cell><cell>32.9</cell><cell>22.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">without reward shaping</cell><cell></cell><cell></cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −3</cell><cell>no</cell><cell>yes</cell><cell>32.74</cell><cell>22.61</cell></row><row><cell></cell><cell></cell><cell cols="4">without temporal difference learning</cell><cell></cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −3</cell><cell>yes</cell><cell>no</cell><cell>23.2</cell><cell>16.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">with different λ</cell><cell></cell><cell></cell></row><row><cell cols="3">0.001 0.001 3  *  10 −3</cell><cell>yes</cell><cell>yes</cell><cell>32.4</cell><cell>22.48</cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −4</cell><cell>yes</cell><cell>yes</cell><cell>34.10</cell><cell>23.15</cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −6</cell><cell>yes</cell><cell>yes</cell><cell>35.00</cell><cell>23.10</cell></row><row><cell cols="2">0.001 0.001</cell><cell>10 −8</cell><cell>yes</cell><cell>yes</cell><cell>33.6</cell><cell>22.72</cell></row><row><cell cols="2">0.001 0.001</cell><cell>0</cell><cell>yes</cell><cell>yes</cell><cell>27.41</cell><cell>20.55</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also provide a simple self-contained proof of Equation(7)in Supplementary Material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The source code is available at https://github.com/rizar/actor-critic-public</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A typical behaviour for TD error was to grow at first and then start decreasing slowly. We found that stopping pretraining shortly after TD error stops growing leads to good results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the developers of Theano <ref type="bibr" target="#b37">(Theano Development Team, 2016)</ref> and Blocks <ref type="bibr" target="#b39">(van Merriënboer et al., 2015)</ref> for their great work. We thank NSERC, Compute Canada, Calcul Quebéc, Canada Research Chairs, CIFAR, CHISTERA project M2CR (PCIN-2015-226)  and Samsung Institute of Advanced Techonology for their financial support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T t=1 a∈A dp(a|Ŷ 1...t−1 ) dθ Q(a;Ŷ 1...t−1 )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR 2015</title>
		<meeting>the ICLR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neuronlike adaptive elements that can solve difficult learning control problems. Systems, Man and Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles W</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03099</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<title level="m">Listen, attend and spell</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition. CoRR, abs/1506.07503</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.07503" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">Daumé</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimum bayes-risk automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="135" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2873</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct loss minimization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1594" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured prediction with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="271" to="301" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural networks for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Werbos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.0686</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press Cambridge</publisher>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Temporal credit assignment in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Td-gammon, a self-teaching backgammon program, achieves master-level play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="219" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation. Automatic Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00619</idno>
		<title level="m">Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An investigation of imitation learning algorithms for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EWRL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02960</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning simple algorithms from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07275</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
