<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning of Musical Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Spijkervet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Ashley</forename><surname>Burgoyne</surname></persName>
						</author>
						<title level="a" type="main">Contrastive Learning of Musical Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While supervised learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations, to form a simple framework for self-supervised learning of raw waveforms of music: CLMR. This approach requires no manual labeling and no preprocessing of music to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagA-Tune and Million Song datasets. A linear classifier fine-tuned on representations from a pretrained CLMR model achieves an average precision of 35.4% on the MagnaTagATune dataset, superseding fully supervised models that currently achieve a score of 34.9%. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that they capture important musical knowledge. Lastly, we show that self-supervised pre-training allows us to learn efficiently on smaller labeled datasets: we still achieve a score of 33.1% despite using only 259 labeled songs during fine-tuning. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Music has been both an important application domain and a source of fresh approaches to machine learning for more than two decades, and in recent years, there has been a focus on how deep learning methods can be adapted to work for musical audio. Supervised, end-to-end learning methods have been widely used in tasks like chord recognition <ref type="bibr" target="#b26">(Korzeniowski &amp; Widmer, 2016;</ref><ref type="bibr" target="#b7">Chen &amp; Su, 2019)</ref>, key detection <ref type="bibr" target="#b27">(Korzeniowski &amp; Widmer, 2017)</ref>, beat tracking <ref type="bibr" target="#b2">(Böck et al., 2016)</ref>, music audio tagging <ref type="bibr">(Pons et al., 2017)</ref> and music recommendation (van den <ref type="bibr" target="#b39">Oord et al., 2013)</ref>. These methods require labeled corpora, which are difficult, expensive and time-consuming to create for music in particular <ref type="bibr" target="#b25">(Koops et al., 2019)</ref>, while raw unlabeled music data is available in vast quantities. Unsupervised alternatives to end-to-end deep learning for music are compelling, especially if these techniques also generalise to smaller datasets.</p><p>Despite the importance of unsupervised learning for raw audio signals, unsupervised learning for musical tasks has yet to see breakthroughs comparable to those in supervised learning. There have been successes with methods like PCA, PMSC's and spherical k-means that rely on a transformation pipeline <ref type="bibr" target="#b17">(Hamel et al., 2011;</ref>, but learning effective representations of raw audio in an unsupervised manner has remained elusive overall.</p><p>Self-supervised representation learning is an upcoming unsupervised learning paradigm in many research domains <ref type="bibr" target="#b13">(Dosovitskiy et al., 2015;</ref><ref type="bibr" target="#b39">van den Oord et al., 2019;</ref><ref type="bibr" target="#b20">Hjelm et al., 2019;</ref><ref type="bibr">Chen et al., 2020a;</ref><ref type="bibr" target="#b16">Grill et al., 2020)</ref>. Without ground truth, there can be no ordinary loss function for training; self-supervised learning trains by way of a proxy loss function instead. One way to preserve the amount of use-arXiv:2103.09410v1 [cs.SD] 17 Mar 2021 ful information during self-supervised learning is to define the proxy loss function with respect to a relatively simple 'pretext' task, with the idea that a representation that is good for the pretext task will also be useful for downstream tasks. Many approaches rely on heuristics to design pretext tasks <ref type="bibr" target="#b12">(Doersch et al., 2015;</ref><ref type="bibr" target="#b40">Zhang et al., 2016)</ref>, e.g., by witholding a pitch transformation <ref type="bibr" target="#b14">(Gfeller et al., 2020)</ref>. Alternatively, contrastive representation learning formulates the proxy loss directly on the learned representations and relies on contrasting multiple, slightly differing versions of any one example by often using negative sampling strategies <ref type="bibr" target="#b38">(Tian et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr">Chen et al., 2020a)</ref> or by bootstrapping the representations <ref type="bibr" target="#b16">(Grill et al., 2020)</ref>.</p><p>In this paper, we combine the insights of a simple contrastive learning framework for images, SimCLR <ref type="bibr">(Chen et al., 2020a)</ref>, with recent advances in representation learning for audio in the time domain, and contribute a pipeline of data augmentations on musical audio, to form a simple framework for self-supervised, contrastive learning of representations of raw waveforms of music. To compare the effectiveness of this simple framework compared to a more complex self-supervised learning objective, we also evaluate representations learned by contrastive predictive coding (CPC) (van den . The self-supervised models are evaluated on the downstream music tagging task, enabling us to evaluate their versatility: music tags describe many characteristics of music, e.g., genre, instrumentation and dynamics. Our key contributions are the following.</p><p>• CLMR achieves strong performance on the music classification task compared to supervised models, despite self-supervised pre-training and fine-tuning on the downstream task using a linear classifier (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>• CLMR learns useful, compact representations from highdimensional, raw signals of musical audio.</p><p>• CLMR enables efficient classification: when fine-tuning a music classification task, we achieve comparable performance using as few as 1% of the labeled data.</p><p>• We show the out-of-domain transferability of representations learned from pre-training CLMR on entirely different corpora of musical audio.</p><p>• CLMR can learn from any dataset of raw music audio, requiring neither transformations nor fine-tuning on the input data; nor do the models require manually annotated labels for pre-training.</p><p>• We provide an ablation study on the effectiveness of individual audio data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>This work builds on SimCLR, a simple contrastive learning framework of visual representations <ref type="bibr">(Chen et al., 2020a)</ref>. Despite a task-agnostic, labelless discriminative pre-training approach, a linear classifier achieved performance comparable to fully supervised models on ImageNet classification. Its learning objective is to maximise the agreement of latent representations of augmented views of the same image using a contrastive loss. In Section 4, we will continue an overview of contrastive learning.</p><p>In CLMR, we adapt this framework to the domain of raw music audio. While most core components of CLMR have appeared in previous work, its ability to model waveforms of music cannot be explained by a single design choice, but by their composition. We will first elaborate the four core components in the following subsections:</p><p>• A stochastic composition of data augmentations that produces two correlated, augmented examples of the same audio fragment, the 'positive pair', denoted as x i and x j .</p><p>• An encoder neural network g enc (·) that encodes the augmented examples to their latent representations.</p><p>• A projector neural network g proj (·) that maps the encoded representations to the latent space where the contrastive loss is formulated.</p><p>• A contrastive loss function, which aims to identify x j from the negative examples in the batch {x k =i } for a given x i .</p><p>The complete framework is visualised in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Augmentations</head><p>We designed a rich chain of audio augmentations for raw audio waveforms of music to make it harder for the model to identify the correct pair of examples. Each consecutive augmentation is stochastically applied on x i and x j <ref type="figure">Figure 3</ref>. During pre-training, a random fragment of size N is selected from a full piece of audio. The independently chosen fragments xi (blue) and xj (yellow) could overlap or be disjoint, which should allow the model to infer both local and global structures.</p><p>independently, i.e., each augmentation has an independent probability p transform of being applied to the audio. The order of augmentations applied to audio is carefully considered, e.g., applying a delay effect after reverberation empirically gives an entirely different result in music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A random fragment of size N is selected from a full piece of audio, without trimming silence (e.g., the intro or outro of a song). The independently chosen fragments for x i and x j could overlap or be very disjoint, allowing the model to infer both local and global structures. This intuition is visualised in <ref type="figure">Figure 3</ref>.</p><p>2. The polarity of the audio signal is inverted, i.e., the amplitude is multiplied by −1.</p><p>3. Additive white Gaussian noise is added with a signal-tonoise ratio of 80 decibels to the original signal.</p><p>4. The gain is reduced between [−6, 0] decibels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>A frequency filter is applied to the signal. A coin flip determines whether it is a low-pass or a high-pass filter. The cut-off frequencies are drawn from uniform distributions on <ref type="bibr">[2200,</ref><ref type="bibr">4000]</ref> or <ref type="bibr">[200,</ref><ref type="bibr">1200]</ref> respectively.</p><p>6. The signal is delayed and added to the original signal with a volume factor of 0.5. The delay is randomly sampled between 200-500ms, in 50ms increments.</p><p>7. The signal is pitch shifted. The pitch transposition interval is drawn from a uniform distribution of semitones between [−5, 5], i.e., a perfect fourth compared to the original signal's scale.</p><p>8. Reverb is added to alter the signal's acoustics. The impulse response's room size, reverbation and damping factor is drawn from a uniform distribution on [0, 100].</p><p>The space of augmentations is not limited to these operations and could be extended to, e.g., randomly applying chorus, distortion and other modulations. Some of these have been shown to improve performance in self-supervised learning for automatic speech recognition in the time-domain as well <ref type="bibr" target="#b34">(Ravanelli et al., 2020;</ref><ref type="bibr" target="#b22">Kharitonov et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Batch Composition</head><p>We sample one song from the batch, augment it into two examples, and treat them as the positive pair. We treated the remaining 2(N − 1) examples in the batch as negative examples, and did not sample the negative examples explicitly. A larger batch size makes the model's objective harder -there are simply more negative samples the anchor sample needs to identify the positve samle from -but it can substantially improve model performance <ref type="bibr">(Chen et al., 2020a)</ref>. This introduces a practical problem for raw audio when training on a GPU, as the input dimensionality of a raw waveform increases for higher sample rates. The batch size can be increased more easily when audio is re-sampled at lower sampling rates.</p><p>Alternatively, multiple GPU's can be used for training, but this introduces another practical problem: batch normalisation is used in the encoder to stabilise training <ref type="bibr" target="#b21">(Ioffe &amp; Szegedy, 2015)</ref> . When training in a parallel manner, the batch normalisation statistics are usually aggregated locally per device. Positive examples are sampled on the same device, leading to potential leakage of batch statistics which improves training loss, but counteracts learning of useful representations. We used global batch normalisation, which aggregates the batch statistics over all devices during parallel training, to alleviate this issue. We leave the effect of different stabilisation strategies, e.g., layer normalisation <ref type="bibr" target="#b19">(Hénaff et al., 2019)</ref>, for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Encoder</head><p>To directly compare a state-of-the-art end-to-end supervised model used in music classification on raw waveforms against a self-supervised model, we use the SampleCNN architecture as our encoder <ref type="bibr" target="#b29">(Lee et al., 2018)</ref>. Similarly, we use a fixed audio input of 59 049 samples with a sample rate of 22 050 Hz. In this configuration, the SampleCNN encoder g enc consists of 9 1D convolution blocks, each with a with a filter size of 3, batch normalisation, ReLU activation and max pooling with pool size 3. The fully connected and dropout layers are removed, which yields a 512-dimensional feature vector for every audio input. The feature vectors from the encoder can be directly used in the learning objective, but formulating the objective on encodings mapped to a different latent space by a parameterised function helps the effectiveness of the representations <ref type="bibr">(Chen et al., 2020a)</ref>. In our experiments, we use a non-linear layer z i = W (2) ReLU(W (1) h i ) with an output dimensionality of 128 as the projection head g proj . There are 2.5 million trainable parameters in total, which is considerably less than the state-of-the-art supervised model as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We used 96 samples per batch and the aforedescribed encoder configuration to directly compare our self-supervised performance with the equally expressive fully supervised method <ref type="bibr" target="#b29">(Lee et al., 2018)</ref>. We ran experiments with batch sizes of 96 on 2× NVIDIA 1080Ti, while for larger batches up to 4 × Titan RTX's were used. With 2 1080Ti's, it takes ∼5 days to train 1 000 epochs on our largest dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Contrastive Loss Function</head><p>In keeping with recent findings on several objective functions in contrastive learning <ref type="bibr">(Chen et al., 2020a)</ref>, the contrastive loss function used in this model is normalised temperature-scaled cross-entropy loss, commonly denoted as NT-Xent loss:</p><formula xml:id="formula_0">i,j = − log exp (sim (z i , z j ) /τ ) 2N k=1 1 [k =i] exp (sim (z i , z k ) /τ )<label>(1)</label></formula><p>Instead of using a scoring function that preserves the mutual information between vectors, the pairwise similarity is measured using cosine similarity, sim(u, v) = u v/ u v . It introduces a new temperature parameter τ to help the model learn from hard negatives. The indicator function</p><formula xml:id="formula_1">1 [k =i] evaluates to 1 iff k = i. This loss is computed for all pairs, both (z i , z j ) and (z j , z i ), for i = j.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Contrastive Predictive Coding</head><p>We adjusted the original CPC encoder g enc (van den Oord et al., 2019) to a deeper architecture for more direct comparison <ref type="bibr" target="#b29">(Lee et al., 2018)</ref>. The encoder g enc consists of 7 layers with 512 filters each, and filter sizes [10, 6, 4, 4, 4, 2, 2] and strides [5, 3, 2, 2, 2, 2, 2]. Instead of relying on max-pooling, the filter sizes and strides are adjusted to parameterise and facilitate downsampling. We also increased the number of prediction steps k to 20, effectively asking the network to predict 100 ms of audio into the future. The batch size is set to 64 from which 15 negative samples in the contrastive loss are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Evaluation</head><p>The evaluation of representations learned by self-supervised models is commonly done with linear evaluation (van den <ref type="bibr" target="#b20">Hjelm et al., 2019;</ref><ref type="bibr">Chen et al., 2020a)</ref>, which measures how linearly separable the relevant classes are under the learned representations. We obtain representations h t for all datapoints from a frozen CLMR network after pre-training has converged, and train a linear classifier using these self-supervised representations on the downstream task of music classification. For CPC, the representations are extracted from the autoregressor, yielding a context vector c k of size (20, 256), which is global-average pooled to obtain a single vector of 512 dimensions. For CLMR, the last 512-dimensional h from the encoder are used instead of z from the projection head. We compute the evaluation metrics on a held-out test set, averaged over three runs on the training set using different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Data Efficient Classification</head><p>When training on a specific task like music classification, only a limited amount of labeled data may be available. In the field of music, data is especially hard and expensive to collect from expert human annotators <ref type="bibr" target="#b25">(Koops et al., 2019)</ref>.</p><p>Self-supervised approaches have demonstrated the ability to use substantially less labeled data when fine-tuning on a specific task <ref type="bibr" target="#b19">(Hénaff et al., 2019;</ref><ref type="bibr">Chen et al., 2020a;</ref><ref type="bibr" target="#b37">b)</ref>. To test the efficient classification capability of the CLMR model, we fine-tune the linear classifier on consecutive subsets of the labels in the train dataset and report its performance. During the task-agnostic, self-supervised pre-training phase, 100% of the data is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Transfer Learning</head><p>To test the out-of-domain generalisability of the learned representations, we pre-trained CLMR on entirely different music datasets. After pre-training, we freeze the weights of the network and subsequently perform the linear evaluation procedure outlined in Section 2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9.">Optimisers</head><p>We use the Adam optimiser <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 0.0003 and β 1 = 0.9 and β 2 = 0.999 during pre-training and employ Kaiming initialisation for all convolutional layers. The temperature parameter τ is set to 0.5, since we observed consistent results regardless of varying batch sizes and temperature τ ∈ {0.1, 0.5, 1.0}.</p><p>For linear evaluation, we use the Adam optimiser with a learning rate of 0.0003, a weight decay of 10 −6 and backpropagation is only done in the fine-tune head. The pretrained encoder is frozen during all evaluation procedures, including the efficient classification and transfer learning experiments. We also employ an early stopping mechanism when the validation scores do not improve for 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluated the quality of our models' representations with music classification experiments. Predicting the top 50 semantic tags in the MagnaTagATune and Million Song datasets <ref type="bibr" target="#b28">(Law et al., 2009;</ref><ref type="bibr" target="#b1">Bertin-Mahieux et al., 2011)</ref> are a popular benchmark for music classification. These semantic tags are annotated by human listeners, and have a varying degree of abstraction and describe many facets of music, including genre, instrumentation and dynamics. It is a multilabel classification task: each track can have multiple tags, of which we use the 50 most frequently occuring to compare our performance against supervised benchmarks. The MagnaTagATune dataset consists of 25k music clips from 6622 unique songs, of which we use 187k fragments of 2.6 seconds for training, and the same train/test split as previous work <ref type="bibr">(Pons et al., 2017;</ref><ref type="bibr" target="#b29">Lee et al., 2018;</ref>. The Million Song Dataset contains a million songs, of which 240k previews of 30 seconds are available and labeled with Last.FM tag annotations. We use a train, validation and test split of 201 680 / 11 774 / 28 435 songs as used in previous work <ref type="bibr">(Pons et al., 2017;</ref><ref type="bibr" target="#b29">Lee et al., 2018)</ref>. This results in 2.2 million music fragments of 2.6 seconds for training, i.e., almost 1600 hours of music. The tags for the Million Song Dataset also contain overlapping genre and more semantic tags, e.g., 'beautiful', 'happy' and 'sad', which are arguably harder to separate during the linear evaluation phase.</p><p>Like the other music classification studies, we use average tag-wise area under the receiver operating characteristic curve (ROC-AUC) and average precision (PR-AUC) scores as evaluation metrics, which are global measures indicating how well the classifier ranks music fragments given a tag. PR-AUC is calculated in addition to ROC-AUC, because ROC-AUC scores can be over-optimistic for imbalanced datasets like MagnaTagATune <ref type="bibr" target="#b8">(Davis &amp; Goadrich, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quantitative Evaluation</head><p>The most important goal set out in this paper, is to evaluate the difference in performance between an otherwise identical, fully supervised network when learning representations using a self-supervised objective. CLMR exceeds the supervised benchmark with a PR-AUC of 35.4%, despite taskagnostic, self-supervised pre-training and a linear classifier for fine-tuning. An additional 0.5% PR-AUC performance gain is added by adding an extra hidden layer to the classifier. Evaluation scores of the best-performing CLMR, CPC and other wave-form based models are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our best results are obtained after longer pre-training, of which the details are outlined in Section 3.7.</p><p>The performance on the larger Million Song Dataset is lower compared to the supervised benchmark, but is still remarkable given the use of a linear classifier. We attribute the difference to the more semantically complex tags in the Million Song Dataset, e.g., 'catchy', 'sexy', 'happy', or more similar genre tags, e.g., 'progressive rock', 'classic rock' and 'indie rock', which may not be easily linearly separable.</p><p>CPC also shows competitive performance with fully supervised models in the music classification task. Despite CPC's good performance, self-supervised training indeed does not require a memory bank or more complex loss functions, e.g., those incorporating mutual information or more explicit negative sampling strategies, to learn useful representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentations</head><p>The CLMR model relies on a pipeline of strong data augmentations to facilitate the learning of representations that are more robust and allow for better generalisation in the downstream task. In <ref type="table">Table 2</ref>, we show the linear evaluation scores obtained by taking a random slice of audio ('random cropping') and performing one additional, individual augmentation. While all datasets contain songs of variable length, we always sample a random slice of audio of the same size before applying other augmentations. This makes it harder to assess the individual contribution of each augmentation to the downstream task performance. We therefore consider an asymmetric data transformation setting: we only apply the augmentation(s) to one branch of the framework, while we settle with an identity function for the other branch (i.e., t(x j ) = x j ) <ref type="bibr">(Chen et al., 2020a)</ref>. The model is pre-trained from scratch for 1 000 epochs after which linear evaluation is performed.</p><p>When only taking a random slice of audio (i.e., a 'crop'), we achieve a PR-AUC score of 30.5. Most individual augmentations show an increase in performance, while adding gain or delay does not impact performance as much. Adding a filter to the augmentation pipeline increases the downstream performance more significantly.</p><p>Besides evaluating the individual contribution of each augmentation with p t = 1, we also vary this probability: p t ∈ {0, 0.4, 0.8}. This is done to assess the optimal amount of augmentation to each example, i.e., the contrastive learning task should neither too hard, nor too simple, for learning effective representations for the music classification task. The linear evaluation PR-AUC score is shown for each aug-   mentation under a different probability p t in <ref type="figure" target="#fig_3">Figure 4</ref>. For the Polarity and Filter transformations, performing them more often with a probability of p t = 0.8 is beneficial. For the Delay, Pitch and Reverb transformations, a transformation probability of p t = 0.4 works better than performing them more aggressively. Generally, we find that strong data augmentations result in more robust representations and better downstream task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Data Efficient Classification Experiments</head><p>Figures 5 and 6 show the PR-AUC scores obtained when increasing the amount of labels available during fine-tuning. For both datasets, self-supervised pre-training greatly improves preformance when less labeled data is available. Using 100× fewer labels, i.e., only 259 songs, CLMR scores 33.1% PR-AUC compared to 24.8% PR-AUC obtained with an equivalent, end-to-end trained supervised model trained on 25k songs. Pre-training using a self-supervised objective without labels therefore substantially improves efficient classification: only 1% of the labels are required while maintaining a similar performance. For the Million Song Dataset, a fully supervised end-to-end trained model exceeds CLMR at 10% of the labels, which are 24,190 unique songs in total. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Transfer Learning Experiments</head><p>Originally made for chord recognition, we use 461 contemporary pop songs recorded between the 1940's and 2000's from the McGill Billboard dataset <ref type="bibr" target="#b4">(Burgoyne et al., 2011)</ref>. The Free Music Archive dataset <ref type="bibr">(Defferrard et al., 2017)</ref> consists of 22 413 songs for the 'medium' version, and the fault-filtered GTZAN dataset <ref type="bibr" target="#b39">(Tzanetakis &amp; Cook, 2002;</ref><ref type="bibr" target="#b37">Sturm, 2013)</ref> contains 930 fragments of 30 seconds, both popular for music classification.</p><p>The results of the transfer learning experiments are shown in <ref type="table">Table 3</ref>. Both CPC and CLMR show the ability to learn effective representations from out-of-domain datasets without ground truth, and even exceed accuracy scores of previous, supervised end-to-end systems on raw audio <ref type="bibr" target="#b11">(Dieleman &amp; Schrauwen, 2014)</ref>. Moreover, both models even demonstrate the ability to learn useful representations on the much smaller GTZAN and Billboard datasets. The CLMR model performs better when it is pre-trained on larger datasets, which is expected as it heavily relies on the number of unique, independent examples that make the contrastive learning task harder, resulting in more robust representations. When pre-training on smaller datasets, CPC can find more useful representations, especially when adding an extra hidden layer to the fine-tune head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Batch Size</head><p>The complexity of contrastive learning increases with larger batch sizes, which may result in better representations. We pre-train from scratch until convergence with varying batch sizes and study its effect on the linear evaluation performance in <ref type="table">Table 4</ref>. While our smallest model already shows competitive performance compared to fully supervised models, the performance increased when using 96 examples per batch. Our largest model did not perform as expected, and scores consistently lower than our middle-sized model. We hypothesise that the task of inferring the positive pair of 2.6 second long raw musical audio fragments, in a pool of 910 negative samples, may require even longer training, or is simply too hard for the current encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Training Duration</head><p>Contrastive learning techniques benefit from longer training compared to their supervised equivalent <ref type="bibr">(Chen et al., 2020a)</ref>. While larger batch sizes increase the pretext task complexity, training longer increases the number of natural variations of the data due to the random augmentation scheme. We pre-train from scratch until convergence and set the batch size to 96. <ref type="table">Table 5</ref> shows that increasing the self-supervised training duration improves downstream performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Sample Rates</head><p>We show in <ref type="table">Table 6</ref> that there is a marginal penalty to the final scores for the self-supervised models when re-sampling the audio to 8 000 Hz and 16 000 Hz respectively, which is in line with previous work <ref type="bibr" target="#b29">(Lee et al., 2018)</ref>. Since resampling disturbs the frequency spectrum, we isolate its contribution by disregarding additional augmentations, i.e., only apply random cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Qualitative Analysis</head><p>For a qualitative view of what the self-supervised models learn from music, we demonstrate in <ref type="figure">Figure 7</ref> the magnitude spectrum of the learned filters of the sample-level convolutional layers (layers 1, 4 and 6) for CLMR and CPC, pre-trained on the MagnaTagATune dataset. In CLMR, the first layer is sensitive to a single, very small band of frequencies around 7500 Hz, while in higher layers the filters spread themselves first linearly and then non-linearly across the full range. CPC shows a similar pattern in the higher layers, but shows a strong activation of two frequencies that span an octave in the first layer. Conversely, the filters of the supervised-trained encoder have a non-linearity that is found in frame-level end-to-end learning <ref type="bibr" target="#b11">(Dieleman &amp; Schrauwen, 2014)</ref>, as well as in perceptual pitch scales such as mel or Bark scales. In the supplementary materials, we demonstrate that self-supervised models are also capable of showing this filter behavior.</p><p>Additionally, we show how cleanly separable the selfsupervised representations are using a t-SNE manifold <ref type="bibr" target="#b30">(Maaten &amp; Hinton, 2008)</ref> in <ref type="figure" target="#fig_6">Figure 8</ref>, and show in the supplementary materials that the difference in performance between self-supervised and supervised models is qualitatively  <ref type="figure">Figure 7</ref>. Normalised magnitude spectrum of the filters of the selfsupervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude. Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently. Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pretrained, converged CLMR model, the last three from a CPC model, on the MagnaTagATune dataset.</p><p>marginal in more detail: there is no single tag performance difference larger than 4% ROC-AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Representation Learning</head><p>The goal of representation learning is to identify features that make prediction tasks easier and more robust to the complex variations of natural data <ref type="bibr" target="#b0">(Bengio et al., 2013)</ref>. In unsupervised representation learning, generative modeling and likelihood-based models typically find useful representations of the data by attempting to reconstruct the observations on the basis of their learned representations <ref type="bibr" target="#b15">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b33">Radford et al., 2016)</ref>. Self-supervised representation learning aims to identify the explanatory factors of the data using an objective that is formulated with respect to the learned representations directly <ref type="bibr" target="#b12">(Doersch et al., 2015;</ref><ref type="bibr" target="#b40">Zhang et al., 2016;</ref><ref type="bibr" target="#b39">van den Oord et al., 2019;</ref><ref type="bibr" target="#b19">Hénaff et al., 2019;</ref><ref type="bibr" target="#b16">Grill et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Contrastive Learning for Music</head><p>Compared to vision, work on self-supervised learning in audio is still very limited. Contrastive predictive coding is a universal approach to contrastive learning, and has been successful for speaker and phoneme classification using raw audio, among other tasks (van den . PASE <ref type="bibr" target="#b31">(Pascual et al., 2019)</ref> introduces several self-supervised workers that solve regression or binary discrimation tasks, that jointly optimise an encoder for speech recognition. To improve the representations for mismatched acoustic con- ditions and their transferability, they apply augmentations to the input speech signal <ref type="bibr" target="#b34">(Ravanelli et al., 2020)</ref>. In music information retrieval, recent advances have been made in self-supervised pitch estimation <ref type="bibr" target="#b14">(Gfeller et al., 2020)</ref>, closely matching supervised, state-of-the-art baselines  despite being trained without ground truth labels. This work relies on preprocessing with a CQT transform. To the best of our knowledge, we are the first to perform self-supervised learning on raw audio waveforms of musical audio and evaluate them in a music task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented CLMR, a self-supervised contrastive learning framework that learns useful representations of raw waveforms of musical audio. The framework requires no preprocessing of the input audio and is trained without ground truth, which enables simple and straightforward pre-training on music datasets of unprecedented scale. We tested the learned, task-agnostic representations by finetuning a linear classifier on the music classification task on the MagnaTagATune and Million Song Dataset, achieving competitive performance compared to fully supervised models. We also showed that CLMR can achieve comparable performance using 100× fewer labels, and demonstrated the out-of-domain transferability of representations learned from pre-training on entirely different datasets of music. To foster reproducibility and future research on self-supervised learning in music information retrieval, we publicly release the pre-trained models and the source code of all experiments of this paper (1). The simplicity of training the model without a direct supervised signal and without preprocessing the audio, together with encouraging results obtained with a single linear layer optimised for a challenging music task, are exciting developments towards unsupervised learning on raw musical audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Audio Preprocessing</head><p>In this paper, we used raw audio waveform data for training in both the pre-training and linear evaluation phases. The default audio sample rate for all experiments is 22 050 Hz, except for the sample rate experiment in section 3.8. The MagnaTagATune dataset contains monophonic 30-second audio fragments in MP3 format, sampled at 16 000 Hz. Some of the audio fragments originate from the same song. We reconstructed original song by concatenating the fragments into a single file, to avoid occurances of fragments of the same song in the same batch of positive-and negative pairs, thereby ensuring i.i.d. data for training.</p><p>The audio files from the Million Song Dataset were obtained from the 7digital service, which provides stereo 30-second audio fragments in MP3 format sampled at 44 100 Hz.</p><p>All files were re-sampled to 22 000 Hz, 16 000 Hz and 8 000 Hz and decoded to the PCM format with ffmpeg, using the following command:</p><formula xml:id="formula_2">ffmpeg -i {input_file}.mp3 -ar {target_sample_rate} {output_file}.wav</formula><p>This is the only preprocessing step that we performed before training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Augmentation Details</head><p>The default pre-training setting, which we also used for our best models, uses 8 audio data augmentations. Not all augmentations are necessarily applied to all inputs: each independent data augmentation is applied with a probability tuned during hyperparameter gridsearch. The most effective augmentations and their probabilities are presented in Section 3.3. The implementation details for each augmentation are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Random Crop</head><p>The audio is cropped with a number of samples s ∈ {20 736, 43 740, 59 049} for sample rates 8 000, 16 000 and 22 050 Hz respectively. To ensure that every sample in the batch is of the same size, the fragment's window we can crop from with original length N is adjusted to N − s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Polarity inversion</head><p>The polarity of the audio signal is inverted by multiplying the amplitude of the signal by −1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Additive White Gaussian Noise</head><p>White Gaussian noise is added to the complete signal with a signal-to-noise ratio (SNR) of 80 decibels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Gain Reduction</head><p>The gain of the audio signal is reduced at random using a value drawn uniformly between -6 and 0 decibels. In our implementation, we use the torchaudio.transforms.Vol interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Frequency Filter</head><p>A frequency filter is applied to the signal using the essentia library <ref type="bibr">(Bogdanov et al., 2013)</ref>. We process the signal with either the LowPass or HighPass algorithm <ref type="bibr" target="#b41">(Zölzer et al., 2002)</ref>, which is determined by a coin flip.</p><p>For the low-pass filter, we draw the cut-off frequency from a uniform distributions between 2200 and 4000 Hz. All frequencies above the drawn cut-off frequency are filtered from the signal.</p><p>Similarly for the high-pass filter, we draw the cut-off frequency from a uniform distributions between 200 and 1200 Hz. All frequencies below the cut-off frequency are filtered from the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Delay</head><p>The signal is delayed by a value chosen randomly between 200 and 500 milliseconds, in 50ms increments. Subsequently, the delayed signal is added to the original signal with a volume factor of 0.5, i.e., we multiply the signal's amplitude by 0.5. An example implementation of this digital signal processing effect is given below in Python using PyTorch: The pitch of the signal is shifted up or down, depending on the pitch interval that is drawn from a uniform distribution between -5 and 5 semitones, i.e., up to a perfect fourth higher or lower than the original signal. We assume 12-tone equal temperament tuning that divides a single octave in 12 semitones.</p><p>Pitch shifting is done using the libsox library, which is interfaced from the wavaugment Python library <ref type="bibr" target="#b22">(Kharitonov et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8. Reverb</head><p>To alter the original signal's acoustics, we apply a Schroeder reverberation effect <ref type="bibr" target="#b35">(Schroeder, 1962)</ref>. This is again done using the libsox library that is interfaced from the wavaugment Python library <ref type="bibr" target="#b22">(Kharitonov et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Additional Hidden Layer</head><p>After pre-training with the self-supervised objective, we performed a linear evaluation to test the expressivity of the representations with a classifier of limited capacity. To further assess the representations' usability, we add a single hidden layer to our classifier and again measure the performance on the downstream task of music classification. The results of this experiment are shown in <ref type="table">Table C</ref>.1 for linear evaluation (left of the slashes; also shown in the main paper and repeated here for convenience) as well as when a hidden layer is added (right of the slashes), for different pre-training durations measured in epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Additional Qualitative Results</head><p>Figure C.1 shows t-SNE visualisations <ref type="bibr" target="#b30">(Maaten &amp; Hinton, 2008)</ref> of our best self-supervised models representations h CLMR and h CPC , for a randomly set of music tracks from the validation set. We show that both self-supervised models can cleanly seperate the classes. <ref type="figure">Figure C</ref>.2 shows the sorted tag-wise ROC-AUC scores for the top-50 tags in the MagnaTagATune dataset, reported for linear evaluation of the trained self-supervised CLMR and CPC models, and the fully end-to-end-trained supervised model. We show that no single tag loses more than 4% ROC-AUC when trained using self-supervised pre-training and fine-tuned with a linear classifier, as compared to the supervised benchmark. <ref type="figure">Figure C</ref>.3 shows the normalised magnitude spectrum of the filters of the self-supervised models CLMR and CPC in the sample-level convolution layers. We perform gradient ascent on a randomly initialised waveform of length 729, i.e., a value that is close to a typical frame size and also interacts conveniently with the convolutional structure of the encoder network, and subsequently calculate the magnitude spectrum. The x-axis plots the filter number, the y-axis the magnitude spectrum for a filter number. Lastly, we sort the plot by the frequency of the peak magnitude. Interestingly, CLMR shows a similar filter structure for the Billboard data set as fully supervised models that were trained on the MagnaTagATune dataset <ref type="bibr" target="#b11">(Dieleman &amp; Schrauwen, 2014;</ref><ref type="bibr" target="#b29">Lee et al., 2018)</ref>, i.e., filter structures that are also found in mel-and Bark-band filters <ref type="bibr" target="#b36">(Stevens et al., 1937;</ref><ref type="bibr" target="#b42">Zwicker, 1961)</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Performance and model complexity comparison of supervised models (grey) and self-supervised models (ours) in music classification of raw audio waveforms on the MagnaTagATune dataset to evaluate musical representations. Supervised models were trained end-to-end, while CLMR and CPC are pre-trained without ground truth: their scores are obtained by training a linear classifier on their learned representations but nonetheless perform competitively to the supervised models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The complete framework operating on raw audio, in which the contrastive learning objective is directly formulated in the latent space of correlated, augmented examples of pairs of raw audio waveforms of music.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>PR − AUC TAG scores for transformations under different, consecutive probabilities p ∈ {0.0, 0.4, 0.8}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Percentage of labels used for training vs. the achieved PR − AUC TAG score on the MagnaTagATune dataset Percentage of labels used for training vs. the achieved PR − AUC TAG score on the Million Song Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>t-SNE manifold visualisation from audio representations learned by a CLMR model of a subset of 10 music tracks, with each sixty 2.67 second long fragments. Each point represents a music fragment, each belonging to a differently coloured track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>C.1. t-SNE manifolds of the hidden vectors of music audio from a subset of 10 music tracks, i.e., in this case classes, from the validation set. Each point represents a 2.67 second long music fragment belonging to a music track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure C. 2 .</head><label>2</label><figDesc>Tag-wise ROC-AUC scores for the top-50 tags in the MagnaTagATune dataset, reported for linear, logistic regression classifiers trained on representations of self-supervised models CLMR and CPC, and compared to a fully supervised, end-to-end SampleCNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Tag prediction performance on the MagnaTagATune (MTAT) dataset and Million Song Dataset (MSD), compared with fully supervised models † trained on raw audio waveforms. We omit works that operate on audio in the time-frequency domain. For the supervised models, the tag-wise scores are obtained by end-to-end training. For the self-supervised models, the scores are obtained</figDesc><table><row><cell>Model</cell><cell cols="3">Dataset ROC-AUC TAG PR-AUC TAG</cell></row><row><cell cols="2">CLMR (ours) MTAT</cell><cell>88.5 (89.3)</cell><cell>35.4 (35.9)</cell></row><row><cell>Musicnn  †</cell><cell>MTAT</cell><cell>89.0</cell><cell>34.9</cell></row><row><cell cols="2">SampleCNN  † MTAT</cell><cell>88.6</cell><cell>34.4</cell></row><row><cell>CPC (ours)</cell><cell>MTAT</cell><cell>86.6 (88.0)</cell><cell>31.0 (33.0)</cell></row><row><cell>1D CNN  †</cell><cell>MTAT</cell><cell>85.6</cell><cell>29.6</cell></row><row><cell>Musicnn  †</cell><cell>MSD</cell><cell>87.4</cell><cell>28.5</cell></row><row><cell cols="2">SampleCNN  † MSD</cell><cell>88.4</cell><cell>-</cell></row><row><cell cols="2">CLMR (ours) MSD</cell><cell>85.7</cell><cell>25.0</cell></row></table><note>by training a linear, logistic regression classifier using the representations from self-supervised pre-training. Scores in parenthesis show performance when adding one hidden layer to the logistic regression classifier (an MLP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table C.1. Performance difference of a linear classifier and when a single hidden layer is added to the classifier on the downstream music classification performance, for different self-supervised pre-training durations.</figDesc><table><row><cell></cell><cell>Tag</cell><cell></cell><cell>Clip</cell></row><row><cell cols="2">Epochs ROC-AUC</cell><cell>PR-AUC</cell><cell>ROC-AUC</cell><cell>PR-AUC</cell></row><row><cell>10 000</cell><cell cols="4">88.5 / 89.3 35.4 / 35.9 93.2 / 93.5 69.3 / 70.0</cell></row><row><cell>3 000</cell><cell cols="4">88.5 / 88.9 35.1 / 35.5 93.0 / 93.3 69.2 / 69.7</cell></row><row><cell>1 000</cell><cell cols="4">88.3 / 88.6 34.4 / 34.9 92.3 / 93.1 68.6 / 69.2</cell></row><row><cell>300</cell><cell cols="4">87.1 / 87.4 32.7 / 32.5 92.0 / 92.0 66.6 / 66.7</cell></row><row><cell>100</cell><cell cols="4">86.4 / 86.6 30.9 / 31.3 91.3 / 91.3 64.1 / 64.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of Amsterdam. Correspondence to: Janne Spijkervet &lt;janne.spijkervet@gmail.com&gt;. 1 Code available at: https://github.com/spijkervet/clmr</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jordan B.L. Smith for his feedback on the draft. We would also like to extend our gratitude to the University of Amsterdam and SURFsara for giving us access to their Research Capacity Computing Services GPU cluster (Lisa).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) CLMR</head><p>(1) MTAT</p><p>Normalised magnitude spectrum of the filters of the self-supervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude. Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently. Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pre-trained, converged CLMR model, the last three from a CPC model, on the MagnaTagATune or Billboard datasets</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval</title>
		<meeting>the 12th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ISMIR 2011</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint Beat and Downbeat Tracking with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Böck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 17th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Essentia: an audio analysis library for music information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno>04/11/2013 2013</idno>
		<ptr target="http://hdl.handle.net/10230/32252" />
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference (ISMIR&apos;13)</title>
		<meeting><address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 12th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<idno>arXiv: 2002.05709</idno>
		<ptr target="http://arxiv.org/abs/2002.05709" />
		<title level="m">ple Framework for Contrastive Learning of Visual Representations</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semisupervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer: Incorporating Chord Segmentation Into Harmony Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143874</idno>
		<ptr target="https://doi.org/10.1145/1143844.1143874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fma: A dataset for music analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.01840" />
	</analytic>
	<monogr>
		<title level="m">18th International Society for Music Information Retrieval Conference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiscale approaches to music audio feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Society for Music Information Retrieval conference</title>
		<meeting>the 14th International Society for Music Information Retrieval conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>ISBN 9780615900650</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Visual Representation Learning by Context Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.167</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7410524/" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pitch Estimation Via Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velimirović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3527" to="3531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal pooling and multiscale learning for automatic annotation and ranking of music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR</title>
		<meeting>the 12th International Society for Music Information Retrieval Conference, ISMIR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<ptr target="http://arxiv.org/abs/1808.06670" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00991</idno>
		<title level="m">Data augmenting contrastive learning of speech representations in the time domain</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crepe: A convolutional representation for pitch estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461329</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Annotator subjectivity in harmony annotations of popular music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Koops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bransen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kent-Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Volk</surname></persName>
		</author>
		<idno type="DOI">10.1080/09298215.2019.1613436</idno>
		<idno>doi: 10.1080/ 09298215.2019.1613436</idno>
		<ptr target="https://doi.org/10.1080/09298215.2019.1613436" />
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="252" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Fully Convolutional Deep Auditory Model for Musical Chord Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MLSP.2016.7738895</idno>
		<ptr target="http://arxiv.org/abs/1612.05082" />
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-End Musical Key Estimation Using a Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02921" />
	</analytic>
	<monogr>
		<title level="m">25th European Signal Processing Conference</title>
		<meeting><address><addrLine>Kos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>EU-SIPCO</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation of algorithms using games: The case of music tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Society for Music Information Retrieval Conference</title>
		<meeting>the 10th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Endto-End Deep Convolutional Neural Networks Using Very Small Filters for Music Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samplecnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2605</idno>
		<idno>doi: 10.21437/ Interspeech.2019-2605</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2605" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-End Learning for Music Audio Tagging at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prockup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.02520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06434" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6989" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Natural sounding artificial reverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="223" />
			<date type="published" when="1962-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A scale for the measurement of the psychological magnitude pitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Sturm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.1461</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Musical Genre Classification of Audio Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">; C J C</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">A</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<ptr target="http://arxiv.org/abs/1807.03748" />
	</analytic>
	<monogr>
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. cs, stat</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Colorful Image Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DAFX-Digital audio effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Zölzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arfib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dutilleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loscos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rocchesso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Subdivision of the Audible Frequency Range into Critical Bands (Frequenzgruppen)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1908630</idno>
	</analytic>
	<monogr>
		<title level="j">America Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="1961-01" />
		</imprint>
	</monogr>
	<note>Acoustical Society of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
