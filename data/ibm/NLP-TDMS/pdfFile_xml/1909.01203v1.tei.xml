<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross View Fusion for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
							<email>haibo-qiu@outlook.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross View Fusion for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm). Our code is released at https:// github.com/ microsoft/ multiview-human-pose-estimation-pytorch. * This work is done when Haibo Qiu is an intern at Microsoft Research Asia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of 3D pose estimation has made significant progress due to the introduction of deep neural networks. Most efforts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref> have been devoted to estimating relative 3D poses from monocular images. The estimated poses are centered around the pelvis joint thus do not know their absolute locations in the environment (world coordinate system).</p><p>In this paper, we tackle the problem of estimating absolute 3D poses in the world coordinate system from multiple cameras <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>. Most works follow the pipeline of first estimating 2D poses and then recovering 3D pose from them. However, the latter step usually depends on the performance of the first step which unfortunately often has large errors in practice especially when occlusion or motion blur occurs in images. This poses a big challenge for the final 3D estimation.</p><p>On the other hand, using the Pictorial Structure Model (PSM) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref> for 3D pose estimation can alleviate the influence of inaccurate 2D joints by considering their spatial dependence. It discretizes the space around the root joint by an N × N × N grid and assigns each joint to one of the N 3 bins (hypotheses). It jointly minimizes the projection error between the estimated 3D pose and the 2D pose, along with the discrepancy of the spatial configuration of joints and its prior structures. However, the space discretization causes large quantization errors. For example, when the space surrounding the human is of size 2000mm and N is 32, the quantization error is as large as 30mm. We could reduce the error by increasing N , but the inference cost also increases at O(N 6 ), which is usually intractable.</p><p>Our work aims to address the above challenges. First, we obtain more accurate 2D poses by jointly estimating them from multiple views using a CNN based approach. It elegantly addresses the challenge of finding the corresponding locations between different views for 2D pose heatmap fusion. We implement this idea by a fusion neural network as shown in <ref type="figure">Figure 1</ref>. The fusion network can be integrated with any CNN based 2D pose estimators in an end-to-end manner without intermediate supervision.</p><p>Second, we present Recursive Pictorial Structure Model (RPSM), to recover the 3D pose from the estimated multiview 2D pose heatmaps. Different from PSM which directly discretizes the space into a large number of bins in order to control the quantization error, RPSM recursively discretizes the space around each joint location (estimated in the previous iteration) into a finer-grained grid using a small number of bins. As a result, the estimated 3D pose is refined step by step. Since N in each step is usually small, the inference speed is very fast for a single iteration. In our experiments,  <ref type="figure">Figure 1</ref>. Cross-view fusion for 2D pose estimation. The images are first fed into a CNN to get initial heatmaps. Then the heatmap of each view is fused with the heatmaps from other views through a fusion layer. The whole network is learned end-to-end. RPSM decreases the error by at least 50% compared to PSM with little increase of inference time.</p><p>For 2D pose estimation on the H36M dataset <ref type="bibr" target="#b10">[11]</ref>, the average detection rate over all joints improves from 89% to 96%. The improvement is significant for the most challenging "wrist" joint. For 3D pose estimation, changing PSM to RPSM dramatically reduces the average error from 77mm to 26mm. Even compared with the state-of-the-art method with an average error 52mm, our approach also cuts the error in half. We further evaluate our approach on the Total Capture dataset <ref type="bibr" target="#b26">[27]</ref> to validate its generalization ability. It still outperforms the state-of-the-art <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first review the related work on multi-view 3D pose estimation and discuss how they differ from our work. Then we discuss some techniques on feature fusion.</p><p>Multi-view 3D Pose Estimation Many approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> are proposed for multi-view pose estimation. They first define a body model represented as simple primitives, and then optimize the model parameters to align the projections of the body model with the image features. These approaches differ in terms of the used image features and optimization algorithms.</p><p>We focus on the Pictorial Structure Model (PSM) which is widely used in object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> to model the spatial dependence between the object parts. This technique is also used for 2D <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref> and 3D <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> pose estimation where the parts are the body joints or limbs. In <ref type="bibr" target="#b0">[1]</ref>, Amin et al. first estimate the 2D poses in a multi-view setup with PSM and then obtain the 3D poses by direct triangulation. Later Burenius et al. <ref type="bibr" target="#b3">[4]</ref> and Pavlakos et al. <ref type="bibr" target="#b17">[18]</ref> extend PSM to multi-view 3D human pose estimation. For example, in <ref type="bibr" target="#b17">[18]</ref>, they first estimate 2D poses independently for each view and then recover the 3D pose using PSM. Our work differs from <ref type="bibr" target="#b17">[18]</ref> in that we extend PSM to a recursive <ref type="bibr">Figure 2</ref>. Epipolar geometry: an image point Y u P back-projects to a ray in 3D defined by the camera Cu and Y u P . This line is imaged as I in the camera Cv. The 3D point P which projects to Y u P must lie on this ray, so the image of P in camera Cv must lie on I. version, i.e. RPSM, which efficiently refines the 3D pose estimations step by step. In addition, they <ref type="bibr" target="#b17">[18]</ref> do not perform cross-view feature fusion as we do.</p><p>Multi-image Feature Fusion Fusing features from different sources is a common practice in the computer vision literature. For example, in <ref type="bibr" target="#b33">[34]</ref>, Zhu et al. propose to warp the features of the neighboring frames (in a video sequence) to the current frame according to optical flow in order to robustly detect the objects. Ding et al. <ref type="bibr" target="#b6">[7]</ref> propose to aggregate the multi-scale features which achieves better segmentation accuracy for both large and small objects. Amin et al. <ref type="bibr" target="#b0">[1]</ref> propose to estimate 2D poses by exploring the geometric relation between multi-view images. It differs from our work in that it does not fuse features from other views to obtain better 2D heatmaps. Instead, they use the multi-view 3D geometric relation to select the joint locations from the "imperfect" heatmaps. In <ref type="bibr" target="#b11">[12]</ref>, multi-view consistency is used as a source of supervision to train the pose estimation network. To the best of our knowledge, there is no previous work which fuses multi-view features so as to obtain better 2D pose heatmaps because it is a challenging task to find the corresponding features across different views which is one of our key contributions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross View Fusion for 2D Pose Estimation</head><p>Our 2D pose estimator takes multi-view images as input, generates initial pose heatmaps respectively for each, and then fuses the heatmaps across different views such that the heatmap of each view benefits from others. The process is accomplished in a single CNN and can be trained endto-end. <ref type="figure">Figure 1</ref> shows the pipeline for two-view fusion. Extending it to multi-views is trivial where the heatmap of each view is fused with the heatmaps of all other views. The core of our fusion approach is to find the corresponding features between a pair of views.</p><p>Suppose there is a point P in 3D space. See <ref type="figure">Figure 2</ref>. Its projections in view u and v are Y u P ∈ Z u and Y v P ∈ Z v , respectively where Z u and Z v denote all pixel locations in the two views, respectively. The heatmaps of view u and  </p><formula xml:id="formula_0">v are F u = {x u 1 , · · · , x u |Z u | } and F v = {x v 1 , · · · , x v |Z v | }.</formula><p>The core idea of fusing a feature in view u, say x u i , with the features from F v is to establish the correspondence between the two views:</p><formula xml:id="formula_1">x u i ← x u i + |Z v | j=1 ω j,i · x v j , ∀i ∈ Z u ,<label>(1)</label></formula><p>where ω j,i is a to be determined scalar. Ideally, for a specific i, only one ω j,i should be positive, while the rest are zero. Specifically, ω j,i is positive when the pixel i in view u and pixel j in view v correspond to the same 3D point. Suppose we know only Y u P , how can we find the corresponding point Y v P in the image of a different view? We know Y v P is guaranteed to lie on the epipolar line I. But since we do not know the depth of P , which means it may move on the line defined by C u and Y u P , we cannot determine the exact location of Y v P on I. This ambiguity poses a challenge for the cross view fusion.</p><p>Our solution is to fuse x u i with all features on the line I. This may sound brutal at the first glance, but is in fact elegant. Since fusion happens in the heatmap layer, ideally, x v j should have large response at Y v P (the cyan point) and zeros at other locations on the epipolar line I. It means the non-corresponding locations on the line will contribute no or little to the fusion. So fusing all pixels on the epipolar line is a simple yet effective solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation</head><p>The feature fusion rule (Eq. (1)) can be interpreted as a fully connected layer imposed on each channel of the pose heatmaps where ω are the learnable parameters. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates this idea. Different channels of the feature maps, which correspond to different joints, share the same weights because the cross view relations do not depend on the joint types but only depend on the pixel locations in the camera views. Treating feature fusion as a neural network layer enables the end-to-end learning of the weights.</p><p>We investigate two methods to train the network. In the first approach, we clip the positive weights to zero during training if the corresponding locations are off the epipolar line. Negative weights are allowed to represent suppression relations. In the second approach, we allow the network to freely learn the weights from the training data. The final 2D pose estimation results are also similar for the two approaches. So we use the second approach for training because it is simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Limitation and Solution</head><p>The learned fusion weights which implicitly encode the information of epipolar geometry are dependent on the camera configurations. As a result, the model trained on a particular camera configuration cannot be directly applied to another different configuration.</p><p>We propose an approach to automatically adapt our model to a new environment without any annotations. We adopt a semi-supervised training approach following the previous work <ref type="bibr" target="#b20">[21]</ref>. First, we train a single view 2D pose estimator <ref type="bibr" target="#b30">[31]</ref> on the existing datasets such as MPII which have ground truth pose annotations. Then we apply the trained model to the images captured by multiple cameras in the new environment and harvest a set of poses as pseudo labels. Since the estimations may be inaccurate for some images, we propose to use multi-view consistency to filter the incorrect labels. We keep the labels which are consistent across different views following <ref type="bibr" target="#b20">[21]</ref>. In training the cross view fusion network, we do not enforce supervision on the filtered joints. We will evaluate this approach in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RPSM for Multi-view 3D Pose Estimation</head><p>We represent a human body as a graphical model with M random variables J = {J 1 , J 2 , · · · , J M } in which each variable corresponds to a body joint. Each variable J i defines a state vector J i = [x i , y i , z i ] as the 3D position of the body joint in the world coordinate system and takes its value from a discrete state space. See <ref type="figure" target="#fig_2">Figure 4</ref>. An edge between two variables denotes their conditional dependence and can be interpreted as a physical constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pictorial Structure Model</head><p>Given a configuration of 3D pose J and multi-view 2D pose heatmaps F, the posterior becomes <ref type="bibr" target="#b2">[3]</ref>: where Z(F) is the partition function and E are the graph edges as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. The unary potential functions φ conf i (J i , F) are computed based on the previously estimated multi-view 2D pose heatmaps F. The pairwise potential functions ψ limb (J m , J n ) encode the limb length constraints between the joints.</p><formula xml:id="formula_2">p(J |F) = 1 Z(F) M i=1 φ conf i (J i , F) (m,n)∈E ψ limb (J m , J n ),<label>(2)</label></formula><p>Discrete state space We first triangulate the 3D location of the root joint using its 2D locations detected in all views. Then the state space of the 3D pose is constrained to be within a 3D bounding volume centered at the root joint. The edge length s of the volume is set to be 2000mm. The volume is discretized by an N × N × N grid G. All body joints share the same state space G which consists of N 3 discrete locations (bins).</p><p>Unary potentials Every body joint hypothesis, i.e. a bin in the grid G, is defined by its 3D position in the world coordinate system. We project it to the pixel coordinate system of all camera views using the camera parameters, and get the corresponding joint confidence from F. We compute the average confidence over all camera views as the unary potential for the hypothesis.</p><p>Pairwise potentials Offline, for each pair of joints (J m ,J n ) in the edge set E, we compute the average distancel m,n on the training set as limb length priors. During inference, the pairwise potential is defined as:</p><formula xml:id="formula_3">ψ limb (J m , J n ) = 1, if l m,n ∈ [l m,n − ,l m,n + ] 0, otherwise ,<label>(3)</label></formula><p>where l m,n is the distance between J m and J n . The pairwise term favors 3D poses having reasonable limb lengths. In our experiments, is set to be 150mm.</p><p>Inference The final step is to maximize the posterior (Eq. (2)) over the discrete state space. Because the graph is acyclic, it can be optimized by dynamic programming with ( ) ( ) <ref type="figure">Figure 5</ref>. Illustration of the recursive pictorial structure model. Suppose we have estimated the coarse locations Lm and Ln for the two joints Jm and Jn, respectively, in the previous iteration. Then we divide the space around the two joints into finer-grained grids and estimate more precise locations. global optimum guarantee. The computational complexity is of the order of O(N 6 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recursive Pictorial Structure Model</head><p>The PSM model suffers from large quantization errors caused by space discretization. For example, when we set N = 32 as in the previous work, the quantization error is as large as 30mm (i.e. s 32×2 where s = 2000 is the edge length of the bounding volume). Increasing N can reduce the quantization error, but the computation time quickly becomes intractable. For example, if N = 64, the inference speed will be 64 = ( 64 32 ) 6 times slower. Instead of using a large N in one iteration, we propose to recursively refine the joint locations through a multiple stage process and use a small N in each stage. In the first stage (t = 0), we discretize the 3D bounding volume space around the triangulated root joint using a coarse grid (N = 16) and obtain an initial 3D pose estimation L = (L 1 , · · · , L M ) using the PSM approach.</p><p>Fo the following stages (t ≥ 1), for each joint J i , we discretize the space around its current location L i into an 2 × 2 × 2 grid G (i) . The space discretization here differs from PSM in two-fold. First, different joints have their own grids but in PSM all joints share the same grid. See <ref type="figure">Figure  5</ref> for illustration of the idea. Second, the edge length of the bounding volume decreases with iterations: s t = st−1 N . That is the main reason why the grid becomes finer-grained compared to the previous stage.</p><p>Instead of refining each joint independently, we simultaneously refine all joints considering their spatial relations. Recall that we know the center locations, sizes and the number of bins of the grids. So we can calculate the location of every bin in the grids with which we can compute the unary and pairwise potentials. It is worth noting that the pairwise potentials should be computed on the fly because it depends on the previously estimated locations. However, because we set N to be a small number (two in our experiments), this computation is fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relation to Bundle Adjustment [25]</head><p>Bundle adjustment <ref type="bibr" target="#b24">[25]</ref> is also a popular tool for refining 3D reconstructions. RPSM differs from it in two aspects. First, they reach different local optimums due to their unique ways of space exploration. Bundle adjustment explores in an incremental way while RPSM explores in a divide and conquer way. Second, computing gradients by finite-difference in bundle adjustment is not stable because most entries of heatmaps are zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets and Metrics</head><p>The H36M Dataset <ref type="bibr" target="#b10">[11]</ref> We use a cross-subject evaluation scheme where subjects 1, 5, 6, 7, 8 are used for training and 9, 11 for testing. We train a single fusion model for all subjects because their camera parameters are similar. In some experiments (which will be clearly stated), we also use the MPII dataset <ref type="bibr" target="#b1">[2]</ref> to augment the training data. Since this dataset only has monocular images, we do not train the fusion layer on these images.</p><p>The Total Capture Dataset <ref type="bibr" target="#b26">[27]</ref> we also evaluate our approach on the Total Capture dataset to validate its general applicability to other datasets. Following the previous work <ref type="bibr" target="#b26">[27]</ref>, the training set consists of "ROM1,2,3", "Walking1,3", "Freestyle1,2", "Acting1,2", "Running1" on subjects 1,2 and 3. The testing set consists of "Freestyle3 (FS3)", "Acting3 (A3)" and "Walking2 (W2)" on subjects 1,2,3,4 and 5. We use the data of four cameras (1,3,5,7) in experiments. We do not use the IMU sensors. We do not use the MPII dataset for training in this experiment. The hyper-parameters for training the network are kept the same as those on the H36M dataset.</p><p>Metrics The 2D pose estimation accuracy is measured by Joint Detection Rate (JDR). If the distance between the estimated and the groundtruth locations is smaller than a threshold, we regard this joint as successfully detected. The threshold is set to be half of the head size as in <ref type="bibr" target="#b1">[2]</ref>. JDR is the percentage of the successfully detected joints.</p><p>The 3D pose estimation accuracy is measured by Mean Per Joint Position Error (MPJPE) between the groundtruth 3D pose y = [p <ref type="bibr" target="#b2">3</ref> 1 , · · · , p 3 M ] and the estimated 3D poseȳ = [p <ref type="bibr" target="#b2">3</ref> 1 , · · · ,p 3 M ]:</p><formula xml:id="formula_4">MPJPE = 1 M M i=1 p 3 i −p 3 i 2</formula><p>We do not align the estimated 3D poses to the ground truth. This is referred to as protocol 1 in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref> 6. Experiments on 2D Pose Estimation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>We adopt the network proposed in <ref type="bibr" target="#b30">[31]</ref> as our base network and use ResNet-152 as its backbone, which was pretrained on the ImageNet classification dataset. The input <ref type="table">Table 1</ref>. This table shows the 2D pose estimation accuracy on the H36M dataset. "+MPII" means we train on "H36M+MPII". We show JDR (%) for six important joints due to space limitation.  <ref type="figure">Figure 6</ref>. Sample heatmaps of our approach. "Detected heatmap" denotes it is extracted from the image of the current view. "Warped heatmap" is obtained by summing the heatmaps warped from the other three views. We fuse the "warped heatmap" and the "detected heatmap" to obtain the "fused heatmap". For challenging images, the "detected heatmaps" may be incorrect. But the "warped heatmaps" from other (easier) views are mostly correct. Fusing the multi-view heatmaps improves the heatmap quality. image size is 320 × 320 and the resolution of the heatmap is 80 × 80. We use heatmaps as the regression targets and enforce l 2 loss on all views before and after feature fusion. We train the network for 30 epochs. Other hyper-parameters such as learning rate and decay strategy are kept the same as in <ref type="bibr" target="#b30">[31]</ref>. Using a more recent network structure <ref type="bibr" target="#b21">[22]</ref> generates better 2D poses. <ref type="table">Table 1</ref> shows the results on the most important joints when we train, either only on the H36M dataset, or on a combination of the H36M and MPII datasets. It compares our approach with the baseline method <ref type="bibr" target="#b30">[31]</ref>, termed Single, which does not perform cross view feature fusion. We also compare with two baselines which compute sum or max values over the epipolar line using the camera parameters. The hyper parameters for training the two methods are kept the same for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative Results</head><p>Our approach outperforms the baseline Single on all body joints. The improvement is most significant for the wrist joint, from 85.72% to 95.01%, and from 89.33% to 97.20%, when the model is trained only on "H36M" or on "H36M + MPII", respectively. We believe this is because "wrist" is the most frequently occluded joint and cross view fusion fuses the features of other (visible) views to help detect them. See the third column of <ref type="figure">Figure 6</ref> for an example. The right wrist joint is occluded in the current view. So the detected heatmap has poor quality. But fusing the features with those of other views generates a better heatmap. In addition, our approach outperforms the sum and max baselines. This is because the heatmaps are often noisy especially when occlusion occurs. Our method trains a fusion network to handle noisy heatmaps so it is more robust than getting sum/max values along epipolar lines.</p><p>It is also interesting to see that when we only use the H36M dataset for training, the Single baseline achieves very poor performance. We believe this is because the limited appearance variation in the training set affects the generalization power of the learned model. However, our fusion approach suffers less from the lack of training data. This is probably because the fusion approach requires the features extracted from different views to be consistent following a geometric transformation, which is a strong prior to reduce the risk of over-fitting to the training datasets with limited appearance variation.</p><p>The improved 2D pose estimations in turn help significantly reduce the error in 3D. We estimate 3D poses by direct triangulation in this experiment. <ref type="table">Table 2</ref> shows the 3D estimation errors on the six important joints. The error for the wrist joint (which gets the largest improvement in 2D estimation) decreases significantly from 64.18mm to 34.28mm. The improvement on the ankle joint is also as large as 15mm. The mean per joint position error over all joints (see (c) and (g) in <ref type="table">Table 3</ref>) decreases from 36.28mm to 27.90mm when we do not align the estimated 3D pose to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Qualitative Results</head><p>In addition to the above numerical results, we also qualitatively investigate in what circumstance our approach will improve the 2D pose estimations over the baseline. <ref type="figure">Figure   6</ref> shows four examples. First, in the fourth example (column), the detected heatmap shows strong responses at both left and right elbows because it is hard to differentiate them for this image. From the ground truth heatmap (the second row) we can see that the left elbow is the target. The heatmap warped from other views (fifth row) correctly localizes the left joint. Fusing the two heatmaps gives better localization accuracy. Second, the third column of <ref type="figure">Figure  6</ref> shows the heatmap of the right wrist joint. Because the joint is occluded by the human body, the detected heatmap is not correct. But the heatmaps warped from the other three views are correct because it is not occluded there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments on 3D Pose Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Implementation Details</head><p>In the first iteration of RPSM (t = 0), we divide the space of size 2, 000mm around the estimated location of the root joint into 16 3 bins, and estimate a coarse 3D pose by solving Eq. 2. We also tried to use a larger number of bins, but the computation time becomes intractable.</p><p>For the following iterations where t ≥ 1, we divide the space, which is of size s t = 2000 16×2 (t−1) , around each estimated joint location into 2 × 2 × 2 bins. Note that the space size s t of each joint equals to the size of a single bin in the previous iteration. We use a smaller number of bins here than that of the first iteration, because it can significantly reduce the time for on-the-fly computation of the pairwise potentials. In our experiments, repeating the above process for ten iterations only takes about 0.4 seconds. This is very light weight compared to the first iteration which takes about 8 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Quantitative Results</head><p>We design eight configurations to investigate different factors of our approach. <ref type="table">Table 3</ref> shows how different factors of our approach decreases the error from 94.54mm to 26.21mm.</p><p>RPSM vs. Triangulation: First, RPSM achieves significantly smaller 3D errors than Triangulation when 2D pose estimations are obtained by a relatively weak model. For instance, by comparing the methods (a) and (b) in <ref type="table">Table 3</ref>, we can see that, given the same 2D poses, RPSM significantly decreases the error, i.e. from 94.54mm to 47.82mm. This is attributed to the joint optimization of all nodes and the recursive pose refinement.</p><p>Second, RPSM provides marginal improvement when 2D pose estimations are already very accurate. For example, by comparing the methods (g) and (h) in <ref type="table">Table 3</ref> where the 2D poses are estimated by our model trained on the combined dataset ("H36M+MPII"), we can see the error decreases slightly from 27.90mm to 26.21mm. This is because the input 2D poses are already very accurate and <ref type="table">Table 3</ref>. 3D pose estimation errors MPJPE (mm) of different methods on the H36M dataset. The naming convention of the methods follows the rule of "A-B-C" where "A" indicates whether we use fusion in 2D pose estimation. "Single" means the cross view fusion is not used. "B" denotes the training datasets. "H36M" means we only use the H36M dataset and "+MPII" means we combine H36M with MPII for training. "C" represents the method for estimating 3D poses.  <ref type="table">Table 4</ref>. 3D pose estimation errors when different numbers of iterations t are used in RPSM. When t = 0, RPSM is equivalent to PSM. "+MPII" means we use the combined dataset "H36M+MPII" to train the 2D pose estimation model. The MPJPE (mm) are computed when no rigid alignment is performed between the estimated pose and the ground truth.</p><p>Methods direct triangulation gives reasonably good 3D estimations. But if we focus on some difficult actions such as "sitting", which gets the largest error among all actions, the improvement resulted from our RPSM approach is still very significant (from 40.47mm to 32.12mm).</p><formula xml:id="formula_5">t = 0 t = 1 t = 3 t = 5 t = 10</formula><p>In summary, compared to triangulation, RPSM obtains comparable results when the 2D poses are accurate, and significantly better results when the 2D poses are inaccurate which is often the case in practice.</p><p>RPSM vs. PSM: We investigate the effect of the recursive 3D pose refinement. <ref type="table">Table 4</ref> shows the results. First, the poses estimated by PSM, i.e. RPSM with t = 0, have large errors resulted from coarse space discretization. Second, RPSM consistently decreases the error as t grows and eventually converges. For instance, in the first row of Table 4, RPSM decreases the error of PSM from 95.23mm to 47.82mm which validates the effectiveness of the recursive 3D pose refinement of RPSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single vs. Fusion:</head><p>We now investigate the effect of the cross-view feature fusion on 3D pose estimation accuracy. <ref type="table">Table 3</ref> shows the results. First, when we use H36M+MPII datasets (termed as "+MPII") for training and use triangulation to estimate 3D poses, the average 3D pose error of our fusion model (g) is smaller than the baseline without fusion (c). The improvement is most significant for the most challenging "Sitting" action whose error decreases from 88.69mm to 40.47mm. The improvement should be attributed to the better 2D poses resulted from cross-view feature fusion. We observe consistent improvement for other different setups. For example, compare the methods (a) and (e), or the methods (b) and (f).</p><p>Comparison to the State-of-the-arts: We also compare our approach to the state-of-the-art methods for multi-view human pose estimation in <ref type="table">Table 5</ref>. Our approach outperforms the state-of-the-arts by a large margin. First, when we train our approach only on the H36M dataset, the MPJPE error is 31.17mm which is already much smaller than the previous state-of-the-art <ref type="bibr" target="#b23">[24]</ref> whose error is 52.80mm. As discussed in the above sections, the improvement should be attributed to the more accurate 2D poses and the recursive refinement of the 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Qualitative Results</head><p>Since it is difficult to demonstrate a 3D pose from all possible view points, we propose to visualize it by projecting it back to the four camera views using the camera parameters <ref type="table">Table 5</ref>. Comparison of the 3D pose estimation errors MPJPE (mm) of the state of the art multiple view pose estimators on the H36M datasets. We do NOT use the Procrustes algorithm to align the estimations to the ground truth. The result of "Multi-view Martinez" is reported in <ref type="bibr" target="#b23">[24]</ref>. The four state-of-the-arts do not use MPII dataset for training. So they are directly comparable to our result of 31.17mm.</p><p>Methods Average MPJPE PVH-TSP <ref type="bibr" target="#b26">[27]</ref> 87.3mm Multi-View Martinez <ref type="bibr" target="#b15">[16]</ref> 57.0mm Pavlakos et al. <ref type="bibr" target="#b17">[18]</ref> 56.9mm Tome et al. <ref type="bibr" target="#b23">[24]</ref> 52.8mm Our approach 31.17mm Our approach + MPII 26.21mm  and draw the skeletons on the images. <ref type="figure" target="#fig_4">Figure 7</ref> shows three estimation examples. According to the 3D geometry, if the 2D projections of a 3D joint are accurate for more than two views (including two), the 3D joint estimation is accurate. For instance, in the first example (first row of <ref type="figure" target="#fig_4">Figure 7)</ref>, the 2D locations of the right hand joint in the first and fourth camera view are accurate. Based on this, we can infer with high confidence that the estimated 3D location of the right hand joint is accurate. In the first example (row), although the right hand joint is occluded by the human body in the second view (column), our approach still accurately recovers its 3D location due to the cross view feature fusion. Actually, most leg joints are also occluded in the first and third views but the corresponding 3D joints are estimated correctly.</p><p>The second example gets a larger error of 40mm because the left hand joint is not accurately detected. This is because the joint is occluded in too many (three) views but only visible in a single view. Cross-view feature fusion contributes little in this case. For most of the testing images, the 3D MPJPE errors are between 20mm to 40mm. There are few cases (about 0.05%) where the error is as large as 120mm. This is usually when "double counting" happens. We visualize one such example in the last row of <ref type="figure" target="#fig_4">Figure 7</ref>. Because this particular pose of the right leg was rarely seen during training, the detections of the right leg joints fall on the left leg regions consistently for all views. In this case, the warped heatmaps corresponding to the right leg joints will also fall on the left leg regions thus cannot drag the right leg joints to the correct positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Generalization to the Total Capture Dataset</head><p>We conduct experiments on the Total Capture dataset to validate the general applicability of our approach. Our model is trained only on the Total Capture dataset. <ref type="table" target="#tab_4">Table  6</ref> shows the results. "Single-RPSM" means we do NOT perform cross-view feature fusion and use RPSM for recovering 3D poses. First, our approach decreases the error of the previous best model <ref type="bibr" target="#b25">[26]</ref> by about 17%. Second, the improvement is larger for the hard cases such as "FS3". The results are consistent with those on the H36M dataset. Third, by comparing the approaches of "Single-RPSM" and "Fusion-RPSM", we can see that fusing the features of different views improves the final 3D estimation accuracy significantly. In particular, the improvement is consistent for all different subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Generalization to New Camera Setups</head><p>We conduct experiments on the H36M dataset using NO pose annotations. The single view pose estimator <ref type="bibr" target="#b30">[31]</ref> is trained on the MPII dataset. If we directly apply this model to the test set of H36M and estimate the 3D pose by RPSM, the MPJPE error is about 109mm. If we retrain this model (without the fusion layer) using the harvested pseudo labels, the error decreases to 61mm. If we train our fusion model with the pseudo labels described above, the error decreases to 43mm which is already smaller than the previous supervised state-of-the-arts. The experimental results validate the feasibility of applying our model to new environments without any manual label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We propose an approach to estimate 3D human poses from multiple calibrated cameras. The first contribution is a CNN based multi-view feature fusion approach which significantly improves the 2D pose estimation accuracy. The second contribution is a recursive pictorial structure model to estimate 3D poses from the multi-view 2D poses. It improves over the PSM by a large margin. The two contributions are independent and each can be combined with the existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Two-view feature fusion for one channel. The top grid denotes the feature map of view A. Each location in view A is connected to all pixels in view B by a weight matrix. The weights are mostly positive for locations on the epipolar line (numbers in the yellow cells). Different locations in view A have different weights because they correspond to different epipolar lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Graphical model of human body used in our experiments. There are 17 variables and 16 edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>We project the estimated 3D poses back to the 2D image space and draw the skeletons on the images. Each row shows the skeletons of four camera views. We select three typical examples whose 3D MPJPE errors are 20, 40, 120mm, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>fusion layer camera 1 camera 2 gt heatmap detected heatmap detected heatmap fused heatmap fused heatmap gt heatmap L2 Loss L2 Loss L2 Loss L2 Loss</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>3D pose estimation errors MPJPE (mm) of different methods on the Total Capture dataset. The numbers reported for our method and the baselines are obtained without rigid alignment.</figDesc><table><row><cell>Methods</cell><cell cols="3">Subjects1,2,3</cell><cell></cell><cell>Subjects4,5</cell><cell></cell><cell>Mean</cell></row><row><cell></cell><cell>W2</cell><cell>FS3</cell><cell>A3</cell><cell>W2</cell><cell>FS3</cell><cell>A3</cell><cell></cell></row><row><cell>Tri-CPM [30]</cell><cell>79</cell><cell>112</cell><cell>106</cell><cell>79</cell><cell>149</cell><cell>73</cell><cell>99</cell></row><row><cell>PVH [27]</cell><cell>48</cell><cell>122</cell><cell>94</cell><cell>84</cell><cell>168</cell><cell>154</cell><cell>107</cell></row><row><cell>IMUPVH [27]</cell><cell>30</cell><cell>91</cell><cell>49</cell><cell>36</cell><cell>112</cell><cell>10</cell><cell>70</cell></row><row><cell>AutoEnc [26]</cell><cell>13</cell><cell>49</cell><cell>24</cell><cell>22</cell><cell>71</cell><cell>40</cell><cell>35</cell></row><row><cell>Single-RPSM</cell><cell>28</cell><cell>42</cell><cell>30</cell><cell>45</cell><cell>74</cell><cell>46</cell><cell>41</cell></row><row><cell>Fusion-RPSM</cell><cell>19</cell><cell>28</cell><cell>21</cell><cell>32</cell><cell>54</cell><cell>33</cell><cell>29</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing network structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pictorial structures for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="55" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3. 6m: Large scale datasets and predictive methods for 3D human sensing in natural environments. T-PAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Monet: Multiview semi-supervised keypoint via epipolar divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00104</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3D human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markerless motion capture of interacting characters using multi-view image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1145" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2500" to="2509" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3D: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bundle adjustmenta modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on vision algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Total capture: 3D human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1227" to="1241" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
