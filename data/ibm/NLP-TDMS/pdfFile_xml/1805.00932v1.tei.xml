<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Limits of Weakly Supervised Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-02">2 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Bharambe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maaten</forename><surname>Facebook</surname></persName>
						</author>
						<title level="a" type="main">Exploring the Limits of Weakly Supervised Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-02">2 May 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nearly all state-of-the-art visual perception algorithms rely on the same formula: (1) pretrain a convolutional network on a large, manually annotated image classification dataset and (2) finetune the network on a smaller, task-specific dataset. This formula <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> has been in wide use for several years and led to impressive improvements on numerous tasks. Examples include: object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, human pose estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, video recognition <ref type="bibr" target="#b8">[9]</ref>, monocular depth estimation <ref type="bibr" target="#b9">[10]</ref>, and so on. In fact, it is so effective that it would now be considered foolhardy not to use supervised pretraining.</p><p>The ImageNet dataset <ref type="bibr" target="#b10">[11]</ref> is the de facto pretraining dataset. While there are studies analyzing the effects of various ImageNet pretraining factors on transfer learning (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>) or the use of different datasets that are of the same size magnitude as ImageNet (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>), relatively little is known about pretraining on datasets that are multiple orders of magnitude larger ( <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> are the largest studies to date). The reasons for this are numerous: few such datasets exist, building new datasets is labor intensive, and large computational resources are needed to conduct experiments. Yet, given the central role of pretraining it is important to expand our scientific knowledge in this domain.</p><p>This paper tries to address this complex issue by studying an unexplored data regime: billions of images "labeled" in the wild with social media hashtags. This data source has the advantage of being large and continuously growing, as well as "free" from an annotation perspective since no manual labeling is required. However, the data source also has potential disadvantages: hashtags may be too noisy to serve as an effective supervisory signal and the image distribution might be biased in ways that harm transfer learning. It is not a priori obvious that training on this data will yield good transfer learning results.</p><p>The main result of this paper is that without manual dataset curation or sophisticated data cleaning, models trained on billions of Instagram images using thousands of distinct hashtags as labels exhibit excellent transfer learning performance. For example, we observe improvements over the state-of-the-art for image classification and object detection, where we obtain a single-crop, top-1 accuracy of 85.4% on the ImageNet-1k image-classification dataset and 45.2% AP on the COCO object-detection dataset <ref type="bibr" target="#b17">[18]</ref>, compared to 79.8% and 43.7%, respectively, when training (or pretraining) the same models on ImageNet-1k. Our primary goal, however, is to contribute novel experimental data about this previously unexplored regime. To that end, we conduct numerous experiments that reveal interesting trends. For example, we find that "hashtag engineering" (i.e., collecting images tagged with a specific subset of hashtags) is a promising new direction for improving transfer learning results, that training on large-scale hashtag data is unexpectedly robust to label noise, and that the features learned allow a simple linear classifier to achieve state-of-the-art ImageNet-1k top-1 accuracy of 83.6% without any finetuning (compared to 84.2% with finetuning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scaling up Supervised Pretraining</head><p>In our experiments, we train standard convolutional network architectures to predict hashtags on up to 3.5 billion public Instagram images. To make training at this scale practical, we adopt a distributed synchronous implementation of stochastic gradient descent with large (8k image) minibatches, following Goyal et al. <ref type="bibr" target="#b18">[19]</ref>. We experiment on a variety of datasets, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instagram Datasets</head><p>We use a simple data collection pipeline: <ref type="bibr" target="#b0">(1)</ref> We select a set of hashtags. <ref type="bibr" target="#b1">(2)</ref> We download images that are tagged with at least one of these hashtags. (3) Then, because multiple hashtags may refer to the same underlying concept, we apply a simple process that utilizes WordNet <ref type="bibr" target="#b19">[20]</ref> synsets to merge some hashtags into a single canonical form (e.g., #brownbear and #ursusarctos are merged). <ref type="bibr" target="#b3">(4)</ref> Finally, for each downloaded image, we replace each hashtag with its canonical form and discard any hashtags that were not in the selected set. The canonical hashtags are used as labels for training and evaluation.</p><p>By varying the selected hashtags and the number of images to sample, we can construct a variety of datasets of different sizes and visual distributions.  by completing a template, role-source-I-L, that indicates its role (training, validation, testing), source (IG for Instagram, IN for ImageNet, etc.), number of images I, and number of labels L. We use approximate image and label counts for convenience, for example "train-IG-940M-1.5k" is an Instagram dataset for training with ∼940e6 images and ∼1,500 labels. We omit the role and image count when it is clear from context or not useful to present. We design three hashtag sets for the Instagram data: (1) A ∼1.5k set with hashtags from the standard 1,000 IN-1k synsets (each synset contains at least one synonym, hence there are more hashtags than synsets). (2) A ∼17k set with hashtags that are synonyms in any of the noun synsets in WordNet. And (3) an ∼8.5k set with the most frequent hashtags from the 17k set. The hashtag set sizes are measured after merging the hashtags into their canonical forms. We hypothesize that the first set has a visual distribution similar to IN-1k, while the other two represent more general visual distributions covering fine-grained visual categories. Details of how these hashtags are selected and how the merging process works are given in supplemental material.</p><p>Image deduplication. When performing transfer learning, it is essential to understand and properly address overlap between training and test sets. Overlap can exists because images may come from the same underlying sources (e.g., Wikipedia, Flickr, Google). For instance, ∼5% of the images in the val-CUB-6k-200 set <ref type="bibr" target="#b20">[21]</ref> also appear in train-IN-1M-1k, and 1.78% of images in val-IN-50k-1k set are in the JFT-300M training set <ref type="bibr" target="#b16">[17]</ref>. To address this issue, we performed the following deduplication procedure: we compute R-MAC features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> for all candidate images using a ResNet-50 model, and use these features to find the k = 21 nearest neighbors for each of the images in our test sets (additional details are in the supplemental material). Subsequently, we manually inspected all images and their nearest neighbors to identify duplicates. This procedure uncovered 150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200 (0.17%), 151 val-Places-37k-365 (0.41%), and 6 val-COCO-5k-80 (0.12%) duplicates. In our results, we report the observed accuracy of our models; in the supplemental material, we report a conservative lower bound on accuracy by marking all duplicates as incorrect. Given the small percentage of duplicates, they do not impact our findings.</p><p>Discussion. Our datasets have two nice properties: public visibility and simplicity. By using publicly accessible images, the data used in our experiments is visible to everyone. To see what it looks like, the images are browsable by hashtag at https://www.instagram.com/explore/tags/ followed by a specific hashtag; for example https://www.instagram.com/explore/tags/brownbear shows images tagged with #brownbear. Our data is also taken from the "wild", essentially as-is, with minimal effort to sanitize it. This makes the dataset construction process particularly simple and transparent.</p><p>We contrast these properties with the JFT-300M dataset <ref type="bibr" target="#b16">[17]</ref>, which is not publicly visible and is the result of a proprietary collection process ("The [JFT-300M] images are labeled using an algorithm that uses a complex mixture of raw web signals, connections between web-pages and user feedback."). Additional details describing the collection of JFT-300M have not been publicly disclosed.</p><p>Despite our efforts to make the dataset content and collection process transparent, we acknowledge that, similar to JFT-300M, it is not possible for other research groups to know exactly which images we used nor to download them en masse. Hence it is not possible for others to replicate our results at this time. However, we believe that it is better if we undertake this study and share the results with the community than to not publish the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ImageNet Datasets</head><p>In addition to the standard IN-1k dataset, we experiment with larger subsets of the full ImageNet 2011 release that contains 14.2M images and 22k labels. We construct training and validation sets that include 5k and 9k labels. For the 5k set, we use the now standard IN-5k proposed in <ref type="bibr" target="#b14">[15]</ref> (6.6M training images). For the 9k label set, we follow the same protocol used to construct IN-5k, which involves taking the next most frequent 4k labels and all of the associated images (10.5M training images). In all cases, we use 50 images per class for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Models</head><p>We use residual networks with grouped convolutional layers, called ResNeXt <ref type="bibr" target="#b14">[15]</ref>. Our experiments use ResNeXt-101 32×Cd, which has 101 layers, 32 groups, and group widths C of: 4 (8B multiply-add FLOPs, 43M parameters), 8 (16B, 88M), 16 (36B, 193M), 32 (87B, 466M), and 48 (153B, 829M). Our implementation matches <ref type="bibr" target="#b18">[19]</ref>. We believe our results will generalize to other architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Loss function. In contrast to ImageNet, our Instagram datasets may contain multiple labels per image (because a user specified multiple hashtags). The average number of hashtags per image varies depending on the dataset; for instance, train-IG-1B-17k contains ∼2 hashtags per image. Our model computes probabilities over all hashtags in the vocabulary using a softmax activation and is trained to minimize the cross-entropy between the predicted softmax distribution and the target distribution of each image. The target is a vector with k non-zero entries each set to 1/k corresponding to the k ≥ 1 hashtags for the image.</p><p>We have also experimented with per-hashtag sigmoid outputs and binary logistic loss, but obtained significantly worse results. While counter-intuitive given the multi-label data, these findings match similar observations in <ref type="bibr" target="#b15">[16]</ref>. The successful application of sigmoid activations and logistic loss may require sophisticated label completion techniques <ref type="bibr" target="#b16">[17]</ref> and more hyper-parameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pretraining Details</head><p>Our models are trained by synchronous stochastic gradient descent (SGD) on 336 GPUs across 42 machines with minibatches of 8,064 images. Each GPU processes 24 images at a time and batch normalization (BN) <ref type="bibr" target="#b26">[27]</ref> statistics are computed on these 24 image sets. The length of the training schedule, measured in units of number-of-images-processed (i.e., minibatch size × total SGD updates), is determined by a heuristic: we choose two training extremes (for instance, 120 epochs on 1.2e6 images and 2 epochs on 3.5e9 images) and linearly interpolate the schedule between them to set the number-of-images-processed for each experiment. Schedules for each experiment are in the supplemental material. Our ResNeXt-101 32×16d networks took ∼22 days to train on 3.5B images.</p><p>To set the learning rate, we follow the linear scaling rule with gradual warmup described in <ref type="bibr" target="#b18">[19]</ref>. We use a warm-up from 0.1 up to 0.1/256 × 8064, where 0.1 and 256 are canonical learning rate and minibatch sizes <ref type="bibr" target="#b27">[28]</ref>. After the warm-up, the learning rate is multiplied by 0.5 at equally spaced steps, such that the total number of learning rate reductions is 20 over the course of training. The same settings are used when training on ImageNet and Instagram data, except that when training on ImageNet we use 128 GPUs in 16 machines (for a minibatch size of 3,072) due to the smaller dataset size and we use the standard learning rate schedule that involves three equally spaced reductions by a factor of 0.1. All other initialization and training details match <ref type="bibr" target="#b18">[19]</ref> and are summarized in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In our experiments, we pretrain convolutional networks for hashtag prediction and transfer those networks to a variety of tasks. There are two established protocols for judging the quality of a pretrained model (see <ref type="bibr" target="#b28">[29]</ref> §3 for a discussion). Both analyze how pretraining on a source task, e.g. IN-1k classification, leads to gains (or losses) on a target task, e.g. bird recognition or object detection.</p><p>Full network finetuning views pretraining as sophisticated weight initialization: the success of pretraining is judged by its impact on the target task after further training the network weights in a task-specific manner (i.e. finetuning). By contrast, feature transfer uses the pretrained network as a feature extractor: it judges the quality of the network by how effective its features are on other tasks, without updating any of the network parameters. These protocols are two extremes of a spectrum along which the proportion of pretrained weights that are finetuned varies from all to none. We employ both protocols in our experiments; at times one is more appropriate than the other.</p><p>Full network finetuning is performed by removing the hashtag-specific fully connected classification layer from the network and replacing it with a randomly initialized classification layer with one output per class in the target task. This modified network is then trained using SGD with momentum. We select the finetuning learning rate and schedule by grid search on a proper validation set for each target task. To do this, we randomly hold out a small portion of the training set (see supplemental material). This practice ensures that our results on the standard validation sets are clean.</p><p>Feature transfer is performed by training an L2-regularized linear logistic regressor on the training data for the target task using SGD. The features produced by the pretrained network are used as input into the classifier. We train the classifier until convergence to the global optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Classification Experiments</head><p>We evaluate Instagram pretraining by measuring classification accuracies on three classification target tasks: ImageNet <ref type="bibr" target="#b29">[30]</ref>, CUB2011 <ref type="bibr" target="#b20">[21]</ref>, and Places365 <ref type="bibr" target="#b13">[14]</ref>. We perform inference on 224×224 center-cropped images, and study the effects of (1) the hashtag vocabulary size, (2) the training set size, (3) the amount of noise in the hashtag targets, and (4) the hashtag sampling strategy.  <ref type="figure" target="#fig_1">Figure 1</ref> shows that pretraining for hashtag prediction substantially improves target task accuracy: on the standard IN-1k benchmark set, a network pretrained on nearly 1B Instagram images with 1.5k hashtags achieves a state-of-the-art accuracy of 84.2%-an improvement of 4.6% over the same model architecture  trained on IN-1k alone and a 1.5% boost over the prior state-of-the-art <ref type="bibr" target="#b30">[31]</ref>, which uses an optimized network architecture. The performance improvements due to Instagram pretraining vary between ImageNet tasks: on the 1k class task, the model pretrained with the IN-1k-aligned 1.5k hashtag set outperforms source networks trained on larger hashtag sets. This trend reverses as the number of target ImageNet classes increases: on 9k ImageNet target classes, the model pretrained with 17k hashtags strongly outperforms the 1.5k hashtag model. On the CUB2011 and Places365 target tasks, source models trained with the largest hashtag sets perform the best, likely, because the 17k hashtags span more objects, scenes, and fine-grained categories. These patterns are intuitive and suggest that alignment between the source and target label sets is an important factor.</p><p>We also show results in <ref type="figure" target="#fig_1">Figure 1</ref> using a larger 3.5B image set with 17k hashtags (dark purple bars), which performs best across all target tasks. Furthermore, following <ref type="bibr" target="#b31">[32]</ref>, we measure the rectified classification accuracy of this model on val-IN-1k. We present all incorrect classifications to five human annotators, asking whether or not the prediction is correct: if at least four annotators answer this question affirmatively the model's prediction is considered correct. Whereas the IN-1M-1k model obtained a rectified top-1 accuracy of 87.5% on val-IN-1k, our IG-3.5B-17k pretrained model achieved 90.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">How does the pretraining image set size impact accuracy?</head><p>This experiment studies the relationship between the number of images used in Instagram pretraining and classification accuracy on the target task. For these experiments, when transferring to the target task we keep the pretrained network weights fixed and only train a linear classifier for the target task. We make this choice because when the number of pretraining images is small relative to the number of target task images (e.g., 1M vs. 7M), the effect of pretraining is Target task: ImageNet-5k masked by the large amount of finetuning data (this was not the case in the previous experiment where the source task had orders of magnitude more images). <ref type="figure" target="#fig_2">Figure 2</ref> shows the classification accuracy on ImageNet validation sets (yaxis) as a function of the number of Instagram training images (x-axis; note the log scale) ranging from 3.5M to 3.5B images. The figure shows results for models pretrained to predict 1.5k hashtags (dashed lines) or 17k hashtags (solid lines) for ResNeXt-101 models with three different capacities (represented by different colors). <ref type="bibr" target="#b0">1</ref> The four panels correspond to ImageNet target tasks with three different number of classes (1k, 5k, 9k) and CUB2011.</p><p>In line with prior results <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, we observe near log-linear behavior: each time we multiply the amount of training data by a factor of x, we observe a fixed increase y in classification accuracy. While the scaling behavior is con-sistent across hashtag vocabulary sizes and models, the accuracy increase y is larger for higher-capacity networks: across all figures, the lines corresponding to ResNeXt-101 32×16d networks (purple) are steeper than those corresponding to 32×8d and 32×4d models. This result suggests that when training convolutional networks on billions of training images, current network architectures are prone to underfitting. We also observe log-linear scaling break down in two regimes: (1) because accuracy is bounded, endless log-linear scaling is not possible. On datasets like IN-1k and CUB2011 the ceiling effect necessarily creates sub-loglinear scaling. <ref type="formula">(2)</ref> We observe a deviation from log-linear scaling in the 1B to 3.5B image regime even without apparent ceiling effects on IN-{5k, 9k}.</p><p>These plots also illustrate an interesting effect of the hashtag vocabulary on the transfer task accuracy. On IN-1k, networks pretrained on the target-taskaligned 1.5k hashtags outperform those trained using a larger hashtag vocabulary, because the 1.5k hashtags were selected to match the ImageNet synsets. However, as the matching between hashtag vocabulary and target classes disappears and the visual variety in the transfer task increases, networks pretrained to recognize a larger number of hashtags increasingly outperform networks pretrained on fewer hashtags: on the IN-9k transfer task, the difference in accuracy between networks trained on 1.5k and those trained on 17k hashtags is ∼7%.</p><p>The highest accuracies on val-IN-1k are 83.3% (source: IG-940M-1k) and 83.6% (source: IG-3.5B-17k), both with ResNeXt-101 32×16d. These results are obtained by training a linear classifier on fixed features and yet are nearly as good as full network finetuning, demonstrating the effectiveness of the feature representation learned from hashtag prediction. These results also have low variance: we pretrained the ResNeXt-101 32×16d architecture of two different random samples of 1B images and then trained linear classifiers on IN-{1k, 5k, 9k} finding a difference in top-1 accuracy of less than 0.1% in all cases.</p><p>To test whether the above observations generalize to fine-grained classification, we repeated the experiments on the CUB2011 dataset, and show the results in <ref type="figure" target="#fig_2">Figure 2</ref>, bottom right. The curves reveal that when training data is limited, the 1.5k hashtag dataset is better, but once the number of training images surpasses ∼100M, the larger 17k hashtag dataset prevails, presumably because it represents a more diverse visual distribution with more fine-grained concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">What is the effect of hashtag label noise on model accuracy?</head><p>A major difference between hashtag supervision and the labels provided in datasets such as ImageNet is that hashtag supervision is inherently noisy: users may apply hashtags that are irrelevant to the visual content of the image, or they may have left out hashtags that would have been visually relevant <ref type="bibr" target="#b32">[33]</ref>. Because an exact characterization of this label noise is difficult, instead, we investigate the effect of injecting additional label noise on the accuracy of our networks. To do so, we pretrain ResNeXt-101 32×16d networks on a version of IG-1B-17k in which we randomly replaced p% of the hashtags by hashtags obtained by sampling from the marginal distribution over hashtags (excluding the tag to be replaced). <ref type="figure">Figure 3</ref> shows the ImageNet classification accuracy of the resulting networks for different numbers of classes at three levels, p, of artificial label noise as well as for a baseline in which no artificial label noise was added during pretraining. We only train the final linear classifier on the target task, because full finetuning may mask the damage caused by pretraining noise. The results suggest that the networks are remarkably resilient against label noise: a noise level of p = 10% leads to a loss of less than 1% in classification accuracy, and at p = 25% label noise, the reduction in accuracy is around 2%. These results suggest that label noise may be a limited issue if networks are trained on billions of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">How does the sampling of pretraining data impact accuracy?</head><p>Another difference between hashtag and ImageNet supervision is that, like in language modeling, hashtags are governed by a Zipfian distribution. Prior studies in language modeling found that resampling Zipfian distributions reduces the impact of the head of the word distribution on the overall training loss <ref type="bibr" target="#b33">[34]</ref>. Motivated by this work, we perform experiments in which we evaluate three different types of data sampling in the Instagram pretraining: (1) a natural sampling in which we sample images and hashtags according to the distribution by which they appear on Instagram; (2) square-root sampling <ref type="bibr" target="#b33">[34]</ref> in which we take the square-root of the head of the hashtag distribution, renormalize, and sample according to the resulting distribution (due to practical considerations, our implementation is slightly different; see supplemental material); and (3) uniform sampling in which we sample a hashtag uniformly at random, and then sample an image that has this hashtag associated to it uniformly at random <ref type="bibr" target="#b15">[16]</ref>. (Aside from this experiment, we always pretrain on Instagram data using square-root sampling.) As before, we only train the final linear classifier on the target task. <ref type="figure">Figure 4</ref> displays classification accuracy as a function of the number of Im-ageNet classes for networks that were pretrained on IG-1B-17k using the three sampling strategies. The results show that resampling of the hashtag distribution is important in order to obtain good transfer to ImageNet image-classification tasks: using uniform or square-root sampling leads to an accuracy improvement of 5 to 6% irrespective of the number of ImageNet classes in the transfer task. In line with prior results, the figure also shows that larger hashtag vocabularies lead to increasing accuracy improvements as the number of target classes grows.   <ref type="figure" target="#fig_3">Figure 5</ref>, indicate that with large-scale Instagram hashtag training, transfer-learning performance appears bottlenecked by model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">On what visual classes is Instagram pretraining most helpful?</head><p>Our results with different hashtag vocabularies suggest that choosing the right hashtag vocabulary may be at least as important as scaling model training to billions of images. Specifically, we expect that some hashtags are easier to predict because they are more "visually concrete" than others: whereas #eiffeltower corresponds to a very specific visual scene, #party may correspond to a large variety of visual scenes. We matched the 17k Instagram hashtags with a list of 40k "concreteness" values of nouns <ref type="bibr" target="#b34">[35]</ref> to obtain 5,730 hashtag concreteness values. <ref type="figure">Figure 6</ref> displays these hashtag concreteness values and the accuracy of predicting the hashtags correctly (measured in terms of AUC on balanced validation sets) in a scatter plot. The figure suggests a clear relation between the concreteness of a noun and the model's ability to predict the corresponding hashtag: the Pearson correlation, ρ, between both variables is 0.43. We also analyze the effect of Instagram pretraining on recognizing individual IN-1k classes. <ref type="figure">Figure 7</ref> shows the cumulative distribution function (CDF) of the absolute accuracy improvement per class of a ResNeXt-101 32×16d network pretrained on IG-3.5B-17k compared to the same network trained on IN-1k. Accuracy improves on more than 80% of the IN-1k classes, with 20% of classes gaining at least 10 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detection</head><p>We have looked at target tasks that require image classification, but we are also interested in observing if pretraining on Instagram hashtag data can improve object detection and instance segmentation tasks by finetuning networks on the COCO dataset <ref type="bibr" target="#b17">[18]</ref>. We use Mask R-CNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> and experiment with ResNeXt-101 FPN <ref type="bibr" target="#b36">[37]</ref> backbones of three different capacities (see <ref type="figure" target="#fig_5">Figure 8</ref>). We compare performance on the 2017 test-dev set using several different pretrained networks. As baselines, we use IN-{1k, 5k} pretraining (IN-9k performs no better than IN-5k) and compare them to IG-940M-1k and IG-1B-17k. For the largest model (32×16d) we also include results for IG-3.5B-17k. We use standard settings <ref type="bibr" target="#b35">[36]</ref> for end-to-end Mask R-CNN training with one exception: for the Instagram pretrained models we found it necessary to perform grid search for the finetuning learning rate on the validation set. We found that models pretrained on the Instagram data require finetuning learning rates that are ∼4-10× lower than ImageNet pretrained models (see supplemental material). This finding illustrates that finetuning recipes developed for ImageNet pretrained models do not transfer to new pretraining sets: a larger amount of pretraining data implies the need for lower finetuning learning rates. <ref type="figure" target="#fig_5">Figure 8</ref> shows two interesting trends. First, we observe that when using large amounts of pretraining data, detection is model capacity bound: with the lowest capacity model (32×4d), the gains from larger datasets are small or even negative, but as model capacity increases the larger pretraining datasets yield consistent improvements. We need even larger models to take advantage of the large-scale pretraining data. The second trend we observe comes from comparing COCO's default AP metric (average precision averaged over intersection-over-  union (IoU) overlap thresholds 0.5:0.95) to AP@50 (average precision computed at IoU threshold 0.5 only). The former emphasizes precise localization while the later allows for looser localization. We observe that the improvement over IN-{1k, 5k} pretraining from IG-1B-1k is much larger in terms of AP@50. Thus, the gains from Instagram pretraining may be primarily due to improved object classification performance, rather than spatial localization performance. Further evidence comes from experiments with keypoint detection using Mask R-CNN, where we found that compared to IN-1k pretraining, IG-1B-1k pretraining leads to worse results (65.3% vs. 67.0% keypoint AP). These two findings suggest that pretraining for Instagram hashtag classification may reduce spatial localization performance while improving classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our study is part of a larger body of work on training convolutional networks on large, weakly supervised image datasets. Sun et al. <ref type="bibr" target="#b16">[17]</ref> train convolutional networks on the JFT-300M dataset of 300 million weakly supervised images. Our Instagram datasets are an order of magnitude larger than JFT-300M, and collecting them required much less manual annotation work (see Section 2.1). Due to the larger training set size and the use of better network architectures, we obtain substantially higher accuracies on transfer tasks: e.g., we obtain 85.4% top-1 accuracy on ImageNet-1k, compared to 79.2% reported in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Other prior studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref> trained convolutional networks to predict words or n-grams in comments on a collection of 100 million Flickr photos and corresponding comments <ref type="bibr" target="#b38">[39]</ref>. Word or n-gram supervision is weaker than hashtag supervision because it is less structured, as reflected in the relatively poor transfer of features to ImageNet reported in <ref type="bibr" target="#b15">[16]</ref>. Other work <ref type="bibr" target="#b39">[40]</ref> also trained networks to predict hashtags on the Flickr dataset but, unlike our study, does not investigate transfer of the resulting networks to other tasks. In addition to Flickr hashtags, <ref type="bibr" target="#b40">[41]</ref> trained hard mixture of expert models on food-related Instagram hashtags; our focus is on standard recognition networks and general hashtags. Other studies on hashtag prediction <ref type="bibr" target="#b41">[42]</ref> do not train convolutional networks from scratch, but train linear classifiers to predict relevant hashtags from pre-defined image features. Several other works have trained models on web-scale image data for other purposes, such as face recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> and similarity search <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>, but to the best of our knowledge, we are the first to report the results of experiments that involve training convolutional networks from scratch on billions of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have attempted to explore the limits of supervised pretraining. In addition to producing state-of-the-art results on the ImageNet-1k benchmark task (85.4% single-crop, top-1 accuracy; 97.6% single-crop, top-5 accuracy) and several other vision tasks, our study has led to four important observations:</p><p>1. Our results suggests that, whilst increasing the size of the pretraining dataset may be worthwhile, it may be at least as important to select a label space for the source task to match that of the target task. We found that networks trained on a hashtag vocabulary that was designed to match the classes in the ImageNet-1k dataset outperformed those trained on twice as many images without such careful selection of hashtags <ref type="figure" target="#fig_2">(Figure 2, top left)</ref>. This observation paves the way for the design of "label-space engineering" approaches that aim to optimally select (weakly supervised) label sets for a particular target task. Such label-space engineering may be much more fruitful than further increasing the scale of the data on which models are trained. 2. In line with prior work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, we observe that current network architectures are underfitting when trained on billions of images. Whilst such underfitting does lead to very high robustness to noise in our hashtag targets <ref type="figure">(Figure 3)</ref>, our results do suggest that accuracy improvements on target tasks may be obtained by further increases of the capacity of our networks <ref type="figure" target="#fig_2">(Figure 2</ref>). Capacity may be increased, for instance, by increasing the number of layers and the number of filters per layer of existing architectures or by mixturesof-experts <ref type="bibr" target="#b40">[41]</ref> (using model parallelization across GPUs). However, it is not unthinkable that some of the design choices that were made in current network architectures are too tailored to ImageNet-1k classification, and need to be revisited when training on billions of images with hashtag supervision. 3. Our results also underline the importance of increasing the visual variety that we consider in our benchmark tasks. They show that the differences in the quality of visual features become much more pronounced if these features are evaluated on tasks with a larger visual variety. For instance, we find that the accuracy difference between models pretrained using two different vocabularies increases as the number of target classes increases <ref type="figure" target="#fig_2">(Figure 2</ref>): if we would have only evaluated our models on ImageNet-1k, we would have concluded they learned visual features of similar quality, whereas results on ImageNet-9k show that one model learns substantially better features than the other. We believe evaluation on more ImageNet classes is a good step towards a more comprehensive assessment of visual recognition models. 4. Results from transferring our models to object detection, instance segmentation, and keypoint detection tasks suggestion that training for large-scale hashtag prediction improves classification while at the same time possibly harming localization performance. This opens a future direction of modifying large-scale, weakly supervised pretraining tasks to better suit the localization needs of important target tasks like detection and human pose estimation.</p><p>In closing, we reflect on the remarkable fact that training for hashtag prediction, without the need for additional manual annotation or data cleaning, works at all. We believe our study illustrates the potential of natural or "wild" data compared to the traditional approach of manually designing and annotating datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hashtag Selection</head><p>All of our Instagram datasets are subsets of I, where each image in I is from a public Instagram post that has at least one hashtag in its caption (we do not consider hashtags from other sources, such as comments). Let H be the set of all unique hashtags associated with the images in I. To construct a dataset, we use a simple data collection pipeline: <ref type="bibr" target="#b0">(1)</ref> We select a set of hashtags that is a subset of H.</p><p>(2) We randomly samples images from I that are tagged with at least one of these selected hashtags. (3) Then, because multiple hashtags may refer to the same underlying concept, we apply a simple process (described below) that utilizes WordNet <ref type="bibr" target="#b19">[20]</ref> synsets to merge some hashtags into a single canonical form (e.g., #brownbear and #ursusarctos are merged). (4) Finally, for each sampled image, we replace each of its hashtags with its canonical form and discard any hashtags that were not in the selected set. The canonical hashtags are used as labels for training and evaluation.</p><p>Hashtag-to-synset matching. Rather than taking a random subset of hashtags from H in step (1) above, we start with a set of WordNet synsets S and filter the hashtags in H by accepting only the ones that match to any synset in S. To determine if a match exists, we define a function s(S, h) that returns the (possibly empty) subset of S that matches hashtag h ∈ H. The function s is implemented using the nltk interface to WordNet. Specifically, s returns the union of synsets found by querying WordNet using calls to nltk.corpus.wordnet.synsets(x) for different values of x, which are query strings derived from h. As values of x, we use the original hashtag h as well as all bigrams formed by inserting a space character at each position in h.</p><p>Canonical hashtag merging. Our hashtag merging process in step <ref type="formula">(3)</ref> is also based on WordNet synsets. We consider two hashtags h ∈ H and h ′ ∈ H duplicates if and only if s(S, h) = s(S, h ′ ). Herein, s(S, h) is the function defined above that returns the set of all synsets that h matches in S. For hashtag merging, we set S to all WordNet synsets. This conservative deduplication strategy merges two hashtags whenever they exactly coincide in all possible word senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Image Deduplication</head><p>We implemented a two-stage deduplication procedure. In the first stage, we search the set of 3.5 billion Instagram images 2 using an approximate nearest neighbor search algorithm to identify 128 potential duplicates for each query image (e.g., an image from val-IN-1k). In the second stage, we compute highquality image features for these duplicate candidates, compute pairwise distances between the candidates and the query image, and apply a conservative distance threshold to generate potential pairs of duplicates for annotation by human annotators. We describe both stages separately below.</p><p>In the first stage, we remove the last five 3 convolutional layers from a ResNet-50 model. We resize each of the Instagram images such that its height or width (whichever is larger) is 400 pixels, and perform a forward-pass through the truncated convolutional network. We compute regional maximum activations of convolutions (R-MAC) features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> from the resulting feature maps: i.e., we perform max-pooling on fixed sub-regions of the feature maps. R-MAC features substantially improve the performance of the deduplication method: they add invariance to cropping transformations that may have been applied on the images. We whiten the 2048-dimensional R-MAC features using PCA to obtain a 512-dimensional descriptor for each image. The resulting image descriptor is scalar-quantized to 8 bits per dimension to facilitate storage on disk.</p><p>To construct a searchable index of 3.5 billion images, we undo the scalar quantization, L2-normalize the resulting vector, and apply optimized product quantization (OPQ; <ref type="bibr" target="#b47">[48]</ref>) to further reduce the feature representation to 256 dimensions. Next, we apply a coarse product quantizer <ref type="bibr" target="#b48">[49]</ref> with 2 sub-quantizers that each operate on 128 of the 256 dimensions. Each sub-quantizer uses k-means clustering to quantize the 128 dimensions into 14 bits. The resulting 28-bit image representation is used to construct an inverted list of the 3.5 billion images. Finally, we apply a product quantizer with 32 sub-quantizers on the residual feature vector; each of the sub-quantizers uses k-means to quantize 256/32 = 8 dimensions of the image representation into 8 bits. The resulting 32-byte representation is stored with the image in the corresponding inverted list. All preprocessors and quantizers were trained on 2 million images; we implemented the image index using Faiss <ref type="bibr" target="#b44">[45]</ref>.</p><p>Given a query image from the validation set of a target task, we search the resulting index as described in <ref type="bibr" target="#b48">[49]</ref>. First, we compute R-MAC features for the query image, and preprocess the features using PCA, scalar quantization, and OPQ. Next, we apply the coarse quantizer on the resulting descriptor and find the 256 nearest sub-quantizers (in terms of Hamming distance). We compute the residual for each of these sub-quantizers, and compute the squared distances between this residual and the residuals stored in the corresponding entries in the inverted list. This produces distance estimates, which we use to select the 128 nearest neighbors (in terms of the distance map) efficiently using a max-heap implementation.</p><p>In the second stage, we compute R-MAC features for the query image and each of the 128 identified neighbors in the same way as in the first stage, however, in this stage we do not compress the 2048-dimensional R-MAC features in any way. We compute exact squared Euclidean distances between the features of the query and neighbor images, and apply a (very conservative) distance threshold of 0.6 on each of the distances. For each image in the query set (e.g., an image from val-IN-1k) that has at least one neighbor in IG-3.5B for which the R-MAC feature distance is smaller than this threshold, we manually annotated the 21 nearest neighbors (in terms of the R-MAC feature distance) to assess whether or not the query image has duplicates in the Instagram dataset.</p><p>This procedure led us to 150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200 (0.17%), 151 val-Places-37k-365 (0.41%), and 6 val-COCO-5k-80 (0.12%) images as duplicates. In the results in the main paper, we included these duplicates in the reported accuracies. In <ref type="table" target="#tab_7">Table 2</ref>, we also report a conservative lower bound on the accuracy of our best models that treats all images that have duplicates in the training set as being classified incorrectly 4 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training Details</head><p>Classification hyperparameters. Our training hyperparameters match those in <ref type="bibr" target="#b18">[19]</ref>. We briefly summarize them here for completeness. For SGD, we use Nesterov momentum [50] of 0.9 and weight decay of 0.0001. Weight decay is not applied to the batch normalization (BN) scale (γ) and bias (β) parameters. All convolutional filters are initialized according to <ref type="bibr" target="#b27">[28]</ref>. Weights of the final fully connected layer are sampled from a zero-mean Gaussian with standard deviation 0.01. Following <ref type="bibr" target="#b18">[19]</ref>, the final BN scale parameter γ of each residual block is initialized to 0 (instead of 1). We use standard image rescaling and data augmentation as described in <ref type="bibr" target="#b18">[19]</ref>. These settings are consistent across ImageNet and Instagram pretraining.</p><p>Our feature transfer experiments require training an L2-regularized logistic regressor. Since the objective function is convex, optimization is straightforward and we only need to perform a grid search for the L2 penalty. For train-CUB-200, the optimal penalty was 0.001; for all other datasets it was 0.0001. When training from scratch or performing full network finetuning, the training hyperparameters are more complex. We give details in <ref type="table" target="#tab_9">Tables 3 and 4</ref>.  the total training length is given in terms of dataset epochs. The learning rate (LR) is set to the initial LR at the start of training (the same for all cases, given as a minibatch size normalized reference value <ref type="bibr" target="#b18">[19]</ref>) and decayed by the specified LR decay factor according to the steps, also given in epochs for ImageNet datasets. For Instagram datasets, the total training length is given in terms of the number of images processed. The LR is decayed from the initial value by the specified factor at equally spaced steps; the total number LR decay steps is given in the LR steps column. We specify the schedules for the two dataset extremes for the cases of 1.5k and 17k hashtags. When training on a number of images between the two extremes, we linearly interpolate the training schedule.</p><p>Detection hyperparameters. For our object detection experiments, we found it necessary to perform grid search for the initial learning rate; using defaults that work well when finetuning from ImageNet-pretrained models worked poorly when finetuning with Instagram-pretrained models. The grid search was performed on the COCO val2017 set, which results in the paper are reported on the test-dev2017 set, which requires submitting results to the COCO evaluation server. The optimal initial learning rates are shown in <ref type="table" target="#tab_12">Table 5</ref>.  full network finetuning, we used a proper validation set held out from the training set of the target task. Using this validation set, we did a coarse grid search to find the initial LR (chosen from 0.0025/256, 0.00025/256, or 0.000025/256) and the weight decay (chosen from 0.01, 0.001, or 0.0001). In all cases, we fixed the length of the finetuning schedule based on some preliminary experiments.  All models are trained with a minibatch size 8 (images) for a total of 180k iterations. The learning rate is decreased by a factor of 0.1 at 120k and 160k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Data Resampling</head><p>Hashtag frequencies follow a Zipfian distribution. For example, in the 17k hashtag vocabulary, the most frequent hashtag (#fineart) appears more than 1 million times as often as the least frequent hashtag (#shirtfront). Training on the natural distribution may not be optimal and therefore we consider two alternative ways to sample: uniform and square root. In both cases, we compute a replication factor r(h) for each hashtag h: r(h) = max(1, φ(t/f (h))), where φ(x) = x in the case of uniform sampling and φ(x) = √ x in the case of square root sampling. Given an image I with (possibly multiple) hashtags {h i }, the image-level replication factor for I is computed as r(I) = max i r(h i ). For a set of n unique images, a list of training images is constructed by computing the replication factor for each image, duplicating the image the prescribed number of times, and then randomly permuting the list. <ref type="bibr" target="#b4">5</ref> The threshold t is selected such that the final list has a target length matching the desired training schedule length (e.g., processing 2 billion images during training).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. 1 . 1</head><label>11</label><figDesc>How does the Instagram hashtag set impact accuracy? Our first experiment varies the Instagram hashtag sets used in pretraining (1.5k, 8.5k, vs. 17k) whilst keeping other factors constant. We compute transfer learning results as top-1 classification accuracy on five target datasets: val-IN-1k, val-IN-5k, val-IN-9k, val-CUB-200, val-Places-365. For baseline models, we use ImageNet classification as a source task: we train networks on train-IN-1k, train-IN-5k, and train-IN-9k, and evaluate them on the corresponding validation sets (finetuning is not needed in these cases). For val-CUB-200 and val-Places-365, we use train-IN-1k as the baseline source task and finetune on train-CUB-200 and train-Places-365. Full network finetuning of ResNeXt-101 32×16d is used for all source-target pairs in which source and target are not the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Classification accuracy of ResNeXt-101 32×16d pretrained on IG-1B with different hashtag vocabularies (purple bars) on IN-{1k, 5k, 9k} (left) and CUB2011, Places365 (right). Baseline models (gray bars) are trained on IN-{1k, 5k, 9k} (left) and IN-1k (right), respectively. Full network finetuning is used. Higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Number of training images in source task (Instagram) Classification accuracies on IN-{1k, 5k, 9k} and CUB2011 target tasks as a function of the number of Instagram images used for pretraining for three network architectures (colors) and two hashtag vocabularies (dashed / solid lines). Only the linear classifier is trained on the target task. Higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Classification accuracy on val-IN-1kusing ResNeXt-101 32×{4,8 16, 32, 48}d   with and without pretraining on the IG-940M-1.5k dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Area under ROC curve (AUC) for hashtag prediction as a function of the hashtag concreteness [35], and corresponding least-squares fit (ρ = 0.43). CDF of absolute per-class accuracy improvements on IN-1k validation set of an Instagram-pretrained network compared to ImageNet baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Transfer to object detection and instance segmentation with Mask R-CNN. We compare ResNeXt-101 FPN backbones of three different capacities using a variety of source pretraining tasks. Higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>51. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (2016) 52. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: arXiv:1610.02357. (2016) 53. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inceptionv4, inception-resnet and the impact of residual connections on learning. In: Proceedings of the International Conference on Learning Representations (ICLR) Workshop. (2016) 54. Zhang, X., Li, Z., Loy, C., Lin, D.: Polynet: A pursuit of structural diversity in very deep networks. In: arXiv:1611.05725. (2016) 55. Chen, Y., Li, J., Xiao, H., Jin, X., Yan, S., Feng, J.: Dual path networks. In: arXiv:1707.01629. (2017) 56. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: arXiv:1709.01507. (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1 summarizes the datasets used in our experiments. Each dataset is named Name template Description train-IG-I-1.5k Instagram training set of I images and ∼1.5k hashtags from ImageNet-1k. train-IG-I-8.5kInstagram training set of I images and ∼8.5k hashtags from WordNet. train-IG-I-17kInstagram training set of I images and ∼17k hashtags from WordNet.</figDesc><table><row><cell>train-IN-1M-1k</cell><cell>The standard ImageNet-1k ILSVRC training set with 1.28M images.</cell></row><row><cell>val-IN-50k-1k</cell><cell>The standard ImageNet-1k ILSVRC validation set with 50k images.</cell></row><row><cell>train-IN-I-L val-IN-I-L train-CUB-6k-200</cell><cell>Extended ImageNet training set of I images and L ∈ {5k, 9k} labels. Extended ImageNet validation set of I images and L ∈ {5k, 9k} labels. The Caltech-UCSD Birds-200-2011 training set.</cell></row><row><cell>val-CUB-6k-200</cell><cell>The Caltech-UCSD Birds-200-2011 validation set.</cell></row><row><cell cols="2">train-Places-1.8M-365 The Places365-Standard training set (high-resolution version).</cell></row><row><cell>val-Places-37k-365</cell><cell>The Places365-Standard validation set (high-resolution version).</cell></row><row><cell cols="2">train-COCO-135k-80 The standard COCO detection training set (2017 version).</cell></row><row><cell>val-COCO-5k-80</cell><cell>The standard COCO detection validation set (2017 version).</cell></row><row><cell>test-COCO-20k-80</cell><cell>The standard COCO detection test-dev set (2017 version).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Summary of image classification datasets. Each dataset is named with a template, role-source-I-L, that indicates its role (training, validation, testing), source, number of images I, and number of labels L.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1B-17k, on val-IN-{1k, 5k, 9k} at three levels of injected label noise. The no-label-noise baseline is trained on the original hashtags. Only the linear classifier is trained on the target task.</figDesc><table><row><cell>ImageNet top-1 accuracy (in %)</cell><cell>30 40 50 60 70 80 90</cell><cell>82.1</cell><cell>81.5</cell><cell>80.2</cell><cell>76.1</cell><cell>52.6</cell><cell>51.7</cell><cell>50.3</cell><cell>46.1</cell><cell>42.7 Source task 41.9 40.6 No label noise 36.6 Label noise: 10% Label noise: 25% Label noise: 50%</cell><cell>ImageNet top-1 accuracy (in %)</cell><cell>30 40 50 60 70 80 90</cell><cell>72.9</cell><cell>78.2</cell><cell>76.7</cell><cell>43.6</cell><cell>48.9</cell><cell>48.5</cell><cell>33.3 Class sampling 39.0 38.8 Natural Square root Uniform</cell></row><row><cell cols="20">1,000 Number of classes in target task (ImageNet) 5,000 9,000 Classification accuracy ResNeXt-101 32×16d, pretrained on 20 Fig. 3: of IG-1,000 Number of classes in target task (ImageNet) 5,000 9,000 20 Fig. 4: Classification accuracy ResNeXt-101 32×4d, pretrained on of IG-1B-17k, on val-IN-{1k, 5k, 9k} for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">three different hashtag sampling strate-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">gies: natural sampling, uniform sampling,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">and square-root sampling. Only the linear</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">classifier is trained on the target task.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Target task: COCO detection (box AP)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">Target task: COCO detection (box AP@50)</cell></row><row><cell>COCO test-dev box AP (in %)</cell><cell>30 35 40 45 50 55</cell><cell>41.6</cell><cell>42.3</cell><cell>42.0</cell><cell cols="6">42.7 IN-1k (1.3M, 1k labels) 44.2 43.7 40.9 42.9 Source task 43.7 IN-5k (6.6M, 5k labels) IG (940M, 1.5k tags)</cell><cell cols="3">45.0 IG (1B, 17k tags) 45.2 44.8 45.2 IG (3.5B, 17k tags)</cell><cell>COCO test-dev box AP@50 (in %)</cell><cell>70 45 50 55 60 65</cell><cell>63.8</cell><cell>64.4</cell><cell>64.8</cell><cell>63.2</cell><cell>64.9</cell><cell>66.5</cell><cell>67.2</cell><cell>65.5</cell><cell>65.6</cell><cell>67.2</cell><cell>67.9</cell><cell>67.1</cell><cell>68.3</cell></row><row><cell></cell><cell>25</cell><cell></cell><cell cols="11">32x4d ResNeXt-101 capacity (in Mask R-CNN) 32x8d 32x16d</cell><cell></cell><cell>40</cell><cell></cell><cell cols="12">32x4d ResNeXt-101 capacity (in Mask R-CNN) 32x8d 32x16d</cell></row><row><cell></cell><cell></cell><cell cols="12">Target task: COCO detection (mask AP)</cell><cell></cell><cell></cell><cell cols="13">Target task: COCO detection (mask AP@50)</cell></row><row><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>COCO test-dev mask AP (in %)</cell><cell>30 35 40 45 50</cell><cell>37.0</cell><cell>37.5</cell><cell>37.2</cell><cell>36.3</cell><cell>38.0</cell><cell>39.2</cell><cell>38.6</cell><cell>37.8</cell><cell>38.6</cell><cell>39.6</cell><cell>39.7</cell><cell>39.3 39.4</cell><cell>COCO test-dev mask AP@50 (in %)</cell><cell>45 50 55 60 65</cell><cell>60.3</cell><cell>61.0</cell><cell>61.1</cell><cell>59.6</cell><cell>61.5</cell><cell>63.0</cell><cell>62.7</cell><cell>61.7</cell><cell>62.3</cell><cell>63.7</cell><cell>64.3</cell><cell>63.7</cell><cell>64.4</cell></row><row><cell></cell><cell>25</cell><cell></cell><cell cols="11">32x4d ResNeXt-101 capacity (in Mask R-CNN) 32x8d 32x16d</cell><cell></cell><cell>40</cell><cell></cell><cell cols="12">32x4d ResNeXt-101 capacity (in Mask R-CNN) 32x8d 32x16d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Lower bounds on reported accuracies. Measured top-1 accuracies (left column) and conservative lower bounds on those accuracies obtained by considering all test images with duplicates in the training set as being classified incorrectly (right column). See text in Section A.2 for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Training schedules for pretraining of models. For ImageNet datasets,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Training schedules for finetuning of models. For transfer learning with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Training schedules for finetuning of Mask R-CNN for detection on COCO.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The maximum number of images available for the 1.5k hashtag set is 940M.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is the largest data set we used and all other image sets are subsets of this one.<ref type="bibr" target="#b2">3</ref> The hyperparameters of our deduplication pipeline were manually tuned using preliminary experiments on the Holidays<ref type="bibr" target="#b46">[47]</ref> dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We note that similar lower bounds should also be computed for networks that are pretrained on ImageNet and then transferred to other datasets (for instance, it is estimated that approximately 5% of the test images in val-CUB-6k-200 are also in train-IN-1M-1k), but we did not perform the required duplicate analyses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">When an image with multiple hashtags is replicated r(I) times, each individual hashtag hi is removed as needed such that hi is only replicated r(hi) times.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Matthijs Douze, Aapo Kyrola, Andrew Dye, Jerry Pan, Kevin Wilfong, and Martin Englund for helpful discussions and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<title level="m">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes ImageNet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context. In: European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<ptr target="http://wordnet.princeton.edu" />
	</analytic>
	<monogr>
		<title level="j">WordNet: About WordNet</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-UCSD Birds</title>
		<meeting><address><addrLine>Caltech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01325</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral maxpooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Convnets and imagenet beyond accuracy: Explanations, bias detection, adversarial examples and model criticism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Storck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11443</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<title level="m">Concreteness ratings for 40 thousand generally known english word lemmas. Behavior Research Methods</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yfcc100m: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Separating self-expression and visual content in hashtag supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>arXiv 1711.09825</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">User conditional hashtag prediction for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Size matters: Exhaustive geometric verification for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometry consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimized product quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="744" to="755" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">A.5 Comparison with the State of the Art on ImageNet-1k Model Image size Parameters Mult-adds Top-1 Acc. (%) Top-5 Acc. (%) models: IG-3.5B-17k ResNeXt</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="32" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Result table adopted from Zoph et al. [31], to which we append the result of ResNeXt-101 32×Cd, with C ∈ {16, 32, 48} pretrained on Instagram hashtag data and finetuned on train-IN-1k. All results are based on a single image crop of the specified size (squared)</title>
	</analytic>
	<monogr>
		<title level="m">Comparison with the state of the art on the ImageNet-1k validation set</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>Our results demonstrate that pretraining on billions of images using their hashtags as labels significantly improve on the state-of-the-art ImageNet-1k results. particularly in the case of top-5 accuracy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
