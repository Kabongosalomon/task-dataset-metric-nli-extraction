<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Residual Networks for Context-based Image Outpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemek</forename><surname>Gardias</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arthur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Enhanced Residual Networks for Context-based Image Outpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although humans perform well at predicting what exists beyond the boundaries of an image, deep models struggle to understand context and extrapolation through retained information. This task is known as image outpainting and involves generating realistic expansions of an images boundaries. Current models use generative adversarial networks to generate results which lack localized image feature consistency and appear fake. We propose two methods to improve this issue: the use of a local and global discriminator, and the addition of residual blocks within the encoding section of the network. Comparisons of our model and the baselines L1 loss, mean squared error (MSE) loss, and qualitative differences reveal our model is able to naturally extend object boundaries and produce more internally consistent images compared to current methods but produces lower fidelity images. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of generative adversarial models has led to many new developments in the computer vision field. The task known as image inpainting, which aims to enhance or restore quality to parts of an image, requires a model to retain the context of the image while completing missing regions of the image, and iteratively improve the process of the generations. The problem explored in this paper is a significantly more ambiguous task than its inpainting alternative, and is commonly known as image outpainting. For this task, the input image boundaries are extrapolated outside of the original content of the input based upon the context of the image <ref type="bibr" target="#b0">[1]</ref>.</p><p>Image outpainting, sometimes referred to as image context interpretation and extrapolation, requires the context <ref type="bibr" target="#b0">1</ref> Worcester Polytechnic Institute.</p><p>Correspondence to: &lt;{pmgardias, etarthur, hsun2}@wpi.edu&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS/DS 541 Deep Learning Final Project, Worcester Polytechnic</head><p>Institute. Copyright 2020 by the author(s). <ref type="bibr" target="#b1">2</ref> A repository containing relevant source code, and accompanying instructions, is accessible via GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked input</head><p>Output GT of the image to be retained by the encoder to accurately regenerate the known input image, then create new portions to append to the edges of the image as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. There exists research on this task using generative adversarial networks, however one of the biggest issues with image outpainting is the new additions to the image, known as hallucinations, typically appear lower quality than the base image and therefore can easily be distinguished as fake by the human eye <ref type="bibr" target="#b0">[1]</ref>. Therefore, to effectively create higher quality hallucinations, better image evaluation methods are required to match the discrimination abilities of human interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Research Contributions</head><p>We provide insight into the use of a local discriminator alongside a global discriminator to improve the quality of hallucinations. Specifically, we analyze how dual discriminators assist in smoothing the sudden change in quality between hallucinations and original image. Additionally, we successfully implement residual blocks applied to the context encoder portion of our generative network to improve the quality of images. We evaluate the performance of these two networks by comparing to the baseline model provided by Van Hoorick using a time based remedy to the generator loss function <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review the previous works relating to this paper in three sub-fields: Generative Adversarial Networks, Image Inpainting, and Image Outpainting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative Adversarial Networks</head><p>Deep generative models have shown success in various tasks of image and video generation problems. By training the generator and discriminator in tandem, the generator can capture the real data distribution and create more realistic images given the latent input. However, GANs are often prone to collapse and are difficult to train. Therefore, some newer techniques have been applied attempting to reduce this issue such as the addition of residual blocks to the encoder component of the generator specifically researched within a super-resolution task context <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Image Inpainting</head><p>Image inpainting is the task of recovering missing regions or enhancing regions within images. Pathak et al. demonstrate that context encoder can achieve realistic results for generation of novel images sections for inpainting purposes <ref type="bibr" target="#b3">[4]</ref>. Recent improvements include a local discriminator to improve the generated feature consistency within the missing sections of the input image by identifying poor generations only within the area of the inpainted sections <ref type="bibr" target="#b4">[5]</ref>.</p><p>More recently, Liu et al. apply partial convolutions to improve overall consistency of generated images <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Outpaint</head><p>Image outpainting refers to predicting the region beyond the borders of an image. Compared to image inpainting, it has been significantly less studied. Although there are promising works on this problem, they include limitations such as application to very specific domain datasets or result in low quality of hallucinations [2] <ref type="bibr" target="#b6">[7]</ref>. More recent work utilizes a deep neural network with both global and local discriminator, recurrent content transfer, skip horizontal connection, and a global residual block to achieve impressive results <ref type="bibr" target="#b7">[8]</ref>. However, this approach focuses on recursive horizontal hallucinations on a natural scenes dataset, resulting in a network which prioritizes clearly defined horizontal lines often found in landscape images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we provide an overview of our architecture. Since the task is generative we use a GAN in a similar context-encoding method compared to current methods [2] <ref type="bibr" target="#b3">[4]</ref>. Our GAN architecture contains two major additions: the first being the use of residual blocks in the generator, and the second being the use of two discriminators. Below, we explore our architecture choices and some design decisions behind it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator</head><p>We propose a GAN architecture that aims to encode the context of the image and use deconvolutional layers to then generate the resulting image while retaining feature consistency. We apply a residual model to the generative network, first convolving the image down into its feature space, then using deconvolution to upsample the image into its expanded version. Although typical models that contain residual blocks often include batch normalization within the blocks, we do not include this activation layer per demonstrations of improvement in the deblurring work of Nah et al <ref type="bibr" target="#b8">[9]</ref>. The encoder section of the generative network contains multiple residual blocks in an attempt to improve image quality. <ref type="table">Table 1</ref> outlines the layers of our generative network, including both the encoder and decoder components. <ref type="table">Table 2</ref> showcases the individual layers which are used to construct a residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminator</head><p>Our discriminator network D consists of dual discriminators. One discriminator D G takes as input the entire generated image and applies a convolutional network to identify the image as real or generated, while the second discriminator D L is local, and is restricted solely to the boundaries between the input image and the hallucinations. These dis-</p><formula xml:id="formula_0">Layer Parameters Conv 3×3, stride=s in ReLU None Conv</formula><p>3×3, stride=s in ReLU None Add None <ref type="table">Table 2</ref>. Architecture of residual block used in generator G, including parameters. The residual block applies input strides sin to the convolution layer. Add is an element-wise addition of block input tensor x and output tensor of previous layer.</p><p>criminator outputs are averaged as such, where represents the binary mask which is originally applied to the training images:</p><formula xml:id="formula_1">D(x) = D L ( × x) + D G (x) 2 (1)</formula><p>The purpose of the local discriminator is to attempt to remove the clearly fake hallucinations that result from lowquality generations or manifestations, which immediately appear fake to the human eye. Since these happen in the boundary between the input image and the hallucinations, the local discriminator focuses on the hallucination alone.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>To evaluate the effectiveness of our model we compare MSE and L1 loss between each of our models. Furthermore, we qualitatively analyze the outputs from our models due to the inherently visual nature of this problem.</p><p>We use MIT CSAIL Places365-Standard, a large scale scene dataset used to train models for image context and recognition <ref type="bibr" target="#b9">[10]</ref>. This dataset contains around 2 million images, combined, of random scenes from outdoor and indoor scenery, including both simple landscapes and detailed, object-heavy images. We use this dataset as our primary method of evaluation because of its usage in Van Hoorick <ref type="bibr" target="#b1">[2]</ref>. Therefore, a direct comparison of MSE loss is more meaningful. Unfortunately, due to computing time constraints, training on the whole of the dataset was deemed implausible, therefore we drastically reduced the size of the training set to approximately 25,000 images and trained for 50 epochs. The size of the validation set was reduced to be relatively proportional to the training set decrease. During this process, we kept images which were chosen randomly from the full dataset, a process which adds some inherent weaknesses to the training of our model. To take into consideration these limitations, we trained three models:</p><p>• The model proposed by Van Hoorick, which is herein referred to as "baseline".</p><p>• A model which adds on a local discriminator to the existing context encoder architecture, averaging the results of the dual discriminators. This model is herein referred to as "local".</p><p>• A model which uses residual blocks as part of the context encoder network, as well as includes the dual discriminators. This model is herein referred to as "residual".</p><p>To address the weakness which training on a subset of the dataset introduce, we train the baseline model on the same training set which is used for the latter two models.</p><p>We trained each of our models with a fixed learning rate of α = 0.0003 and two Adam optimizers with β 1 = 0.5, β 2 = 0.999, following the same training considerations proposed by Van Hoorick <ref type="bibr" target="#b1">[2]</ref>. The loss functions are as follows:</p><formula xml:id="formula_2">L rec = ||x − G(x)|| 1 (2) L adv = ||D(G(x)) − 1|| 2 2 (3) L G = λ rec L rec + λ adv L adv<label>(4)</label></formula><formula xml:id="formula_3">L D = ||D(x) − 1|| 2 2 + ||D(G(x)) − 0|| 2 2<label>(5)</label></formula><p>We apply a time based remedy to the generator loss function that punishes the generator heavier for bad outputs as time progresses: </p><p>Enhanced Residual Networks for Context-based Image Outpainting</p><p>Masked input Baseline Local Residual GT <ref type="figure">Figure 2</ref>. Comparison of results for different models. Note the baseline result appears larger but contains the same ratio of hallucination to input pixels as the other generated images. Since humans still perform vastly greater than deep learning networks at this task we conclude our experiment with a qualitative discussion about the quality of hallucinations. As many previously mentioned models in this domain create noise or extrapolations in the hallucinations that clearly appear faulty, we aim to manually evaluate whether our model rectifies this issue or these manifestations are still present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked input Local GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We first provide a comparison of the losses for each model in <ref type="table" target="#tab_4">Table 4</ref> that demonstrates the residual model performs marginally better with respect to L1 and adversarial loss. However, as a comparison of the loss values does not cover all of the results in detail, we provide a further discussion of the qualitative results, which provides insights to the strengths and weakness of our approaches. The baseline model performs better at directly reconstructing the images compared to their ground truths, as demonstrated by the significantly lower MSE, however the local discriminator and residual models result in an L1 loss which is marginally lower than the baseline model's L1 loss. Since we set out not to improve the reconstruction but rather the quality of the reconstructed image, the change in MSE is expected. Finally, moderately lower adversarial loss of the local and residual models reflects our goal with the implementation of dual discriminators, demonstrating their strength in determining fake generated images.</p><p>We aim to create a model that generates cleaner or higher quality hallucinations such that it is not evident to the human eye if it is fake or real. Per <ref type="figure">Figure 2</ref>, we can demonstrate our approach is able to achieved a clearer continuous output. As shown below the second image is the output from the baseline model, and it is quite evident where the original image ends and the new hallucinations appear. Compared to the output from our local discriminator model, it is much harder to tell where the original image ends and the hallucinations appear. This is because our model is able to effectively continue object boundaries into the hallucinations which the baseline cannot. A side effect of this is that the whole image becomes blurrier which explains the increase in MSE, however qualitatively we believe the outputs show that our local discriminator model does a significantly better job at creating continuous realistic hallucinations compared to the baseline as we set out to achieve. Finally, our residual blocks model appears to revert the beneficial qualities of having two discriminators as shown by the obvious boundaries in the fourth image of <ref type="figure">figure 2</ref>. The residual blocks allowed for the input image to remain high quality, but the use of a local discriminator lowered the quality of the hallucinations resulting in a model very close in effectiveness to the baseline.</p><p>With further examination we can report that the local discriminator model may reduce the quality of the input image, but by doing so can create more realistic outputs as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The detail in the original image is not prioritized to be preserved but instead the overall continuity of the image is. For example, the individual peaks atop the structure are lost but the new generations match the quality of the blurred input image. Furthermore, it is evident that our model is able to effectively continue object bound-  aries into the hallucinations. Comparatively, the baseline model struggles to do this, and therefore our model provides a large improvement in this regard. Because of this the output image looks quite believable as there is internal consistency within the image that is lost in the baseline model.</p><p>Next, our residual blocks model which contains both a local and discriminator seemed to revert some of the benefits of having both discriminators. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the residual blocks model allowed for the input image to remain high quality but since it uses the local discriminator the hallucinations are of reduced quality. These two effects combined creates a clearly fake output image where the boundary of the original input image is clearly defined. The output appears marginally better than the baseline, however the hallucinations remain lower quality even though the entirety of the image is of higher quality than the local discriminator model's results.</p><p>Finally, due to the inclusion of the local discriminator our models place a significant emphasis on the hallucinations. This, combined with the fact that we do not use batch normalization, increases the likelihood of erroneous anomalies, as is exemplified in <ref type="figure" target="#fig_4">Figure 5</ref>. Since our model attempts to blur the boundary zones between the base image and the hallucinations the anomaly is stretched and warped across the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>A major limitation of our experiment was the size of the training set and number of epochs we were able to fit in our limited computing time. Using just 200,000 images from the Places365 dataset on the single NVIDIA Tesla P4 we used for training would have taken approximately a week for 200 epochs. Therefore, we reduced the dataset down to approximately 25,000 images randomly. The effects of this blind reduction of training data resulted in our models struggling to learn complex features such as humans and water details. This problem is even more compounded since humans were included in only a small subset of the images we used. This limitation could have been overcome with more time or better computational capabilities to train on a larger dataset. Alternatively, a smaller dataset could be used which requires significantly less information to be contained within the latent space. Furthermore, our reduced dataset could have been manually picked to only include images of landscapes to focus on one specific image type, albeit this approach would take an unreasonably excessive amount of tedious effort to properly Another limitation to our approach is that our models use a fixed average between the global and local discriminator. Therefore, it weighs the overall image and the boundary areas equally. This issues relates to the creation of anomalies, as shown in 5, and why the images appear blurrier or lower quality overall. The easiest way to rectify this would be to include a method of back-propagate the error to the discriminator-combining weight and allow the models to learn how to combine the discriminator outputs. The models would then hopefully be able to balance higher quality images and consistency whereas our discussed approach focuses exclusively on consistency.</p><p>Since the MSE loss in this task is the measure of how well the models can reconstruct the ground truth image it may not have been an appropriate measurement for our specific problem of increasing the hallucination boundary quality of the images. Since our local discriminator and global discriminator have separate goals the MSE loss of our models was ultimately higher than the baseline. Therefore, the quantitative measure of MSE loss fails to reveal how our models excel. Unfortunately, we were unable to find another loss function for image consistency and retrain in time to publish these results.</p><p>Finally, the residual blocks model demonstrate increased image fidelity of the original input image inside of the hallucinations. Adding the residual blocks to the local discriminator model seems to "revert" the focus of the gen-erator closer to the baseline, as the boundary regions become more defined. Therefore, we recommend further experimentation into the usage of residual blocks as part of the encoding network, including using residual blocks after the generation as a post-processing technique to construct higher quality output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>Image outpainting is an attractive task that currently suffers from low quality or obviously fake outputs. The models we created show that the use of a local and global discriminator allows for higher consistency within the output images and object boundaries may be continued into the newly generated areas. This is an improvement over other models and shows promise into ways to improve output quality. Further research is needed to improve the fidelity of these outputs, however the consistency of these images makes it significantly harder to detect fake outputs compared to other models.</p><p>Obvious improvements may be conducted related to the architecture and training of our models. Below we identify possible future work to extend our contribution to the problem:</p><p>• Comprehensive training on the entire two million image dataset for an increased number of epochs will yield significantly cleaner and robust results.</p><p>• The use of a local and global discriminator greatly improves object boundaries but dampens the overall quality of the image. This may be rectified by allowing the model to learn a combining weight for the combination of the dual discriminators using backpropagation.</p><p>• A more appropriate loss function to reinforce feature consistency across the boundary of the input image and the hallucination. For example, a loss function which rewards finer generated object boundaries, possibly even drawing from traditional computer vision techniques for feature detection across the ground truth and outpainting result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of image outpainting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, if n ≤ 10 0.005, if 10 &lt; n ≤ 30 0.015, otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Outpainting example of the local discriminator model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Outpainting result example for the residual encoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Example of an anomaly within the hallucination, generated during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Architecture of global and local discriminators DG and DL, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of all loss results across each of the models.</figDesc><table><row><cell>Model</cell><cell>L1</cell><cell cols="2">MSE Adversarial</cell></row><row><cell>Baseline</cell><cell>0.0956</cell><cell>0.047</cell><cell>0.1278</cell></row><row><cell>Local Discriminator</cell><cell>0.897</cell><cell>1.044</cell><cell>0.1014</cell></row><row><cell>Residual Encoder</cell><cell cols="2">0.08 0.7814</cell><cell>0.0941</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Painting Outside the Box: Image Outpainting with GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sabini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Rusak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08483</idno>
		<idno>arXiv: 1808.08483</idno>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image Outpainting and Harmonization using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basile Van Hoorick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10960</idno>
		<idno>arXiv: 1912.10960</idno>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02921</idno>
		<idno>arXiv: 1707.02921</idno>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07379</idno>
		<idno>arXiv: 1604.07379</idno>
		<title level="m">Context Encoders: Feature Learning by Inpainting</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>107:1-107:14</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2017)</title>
		<meeting>of SIGGRAPH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07723</idno>
		<idno>arXiv: 1804.07723</idno>
		<title level="m">Image Inpainting for Irregular Holes Using Partial Convolutions</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wide-Context Semantic Image Extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1399" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Very Long Natural Scenery Image Prediction by Outpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12688</idno>
		<idno>arXiv: 1912.12688</idno>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02177</idno>
		<idno>arXiv: 1612.02177</idno>
		<title level="m">Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Places: A 10 million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
