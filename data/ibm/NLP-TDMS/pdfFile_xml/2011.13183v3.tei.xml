<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TinaFace: Strong but Simple Baseline for Face Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjia</forename><surname>Zhu</surname></persName>
							<email>yanjia.zhu@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiang</forename><surname>Cai</surname></persName>
							<email>hongxiang.cai@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Zhang</surname></persName>
							<email>shuhan.zhang@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
							<email>chenhao.wang@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Xiong</surname></persName>
							<email>yichao.xiong@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TinaFace: Strong but Simple Baseline for Face Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face detection has received intensive attention in recent years. Many works present lots of special methods for face detection from different perspectives like model architecture, data augmentation, label assignment and etc., which make the overall algorithm and system become more and more complex. In this paper, we point out that there is no gap between face detection and generic object detection. Then we provide a strong but simple baseline method to deal with face detection named TinaFace. We use  as backbone, and all modules and techniques in TinaFace are constructed on existing modules, easily implemented and based on generic object detection. On the hard test set of the most popular and challenging face detection benchmark WIDER FACE <ref type="bibr" target="#b47">[48]</ref>, with single-model and single-scale, our TinaFace achieves 92.1% average precision (AP), which exceeds most of the recent face detectors with larger backbone. And after using test time augmentation (TTA), our TinaFace outperforms the current state-of-the-art method and achieves 92.4% AP. The code is available at https: // github. com/ Media-Smart/ vedadet/ tree/ main/ configs/ trainval/ tinaface .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection becomes a very important task in computer vision, since it is the first and fundamental step of most tasks and applications about faces, such as face recognition, verification, tracking, alignment, expression analysis etc.. Therefore, so many methods are presented in this field from different perspectives recently. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49]</ref> introduce annotated landmarks information as extra supervision signal, and some of others <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">58]</ref> pay more attention to the design of network. Besides, some new loss designs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16]</ref> and data augmentation methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref> are presented. What's more, a few works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b57">58]</ref> * Equal contribution. † Data analysis. ‡ Corresponding author. begin to redesign the matching strategy and label assignment process. Obviously, face detection seems to be gradually separated out from generic object detection and forms a new field.</p><p>Intuitively, face detection is actually an application of generic object detection. To some degree, face is an object. So naturally there are a series of questions to be asked, "what is the difference between face detection and generic object detection?", "Why not using generic object detection techniques to deal with face detection?", and "is it necessary to additionally design special methods for handling face detection?".</p><p>First, from the perspective of data, the properties that faces own also exist in objects, like pose, scale, occlusion, illumination, blur and etc.. And the unique properties in faces like expression and makeup can also correspond to distortion and color in objects. Then from the perspective of challenges encountered by face detection like multi-scale, small faces and dense scenes, they all exist in generic object detection. Thus, face detection seems to be just a subproblem of generic object detection. To better and further answer above questions, we provide a simple baseline method based on generic object detection to outperform the current stateof-the-art methods on the hard test set of WIDER FACE <ref type="bibr" target="#b47">[48]</ref>.</p><p>The main contributions of this work can be summarized as:</p><p>• Indicating that face detection is actually a one class generic object detection problem and can be handled by techniques in generic object detection.</p><p>• Providing a strong but simple baseline method for face detection named TinaFace. All ideas and modules used in TinaFace are based on generic object detection.</p><p>• With single-scale and single-model, we achieve 92.1% average precision(AP) in hard settings on the test subset of WIDER FACE, which already exceed most of recent methods with larger backbone and Test Time Augmentation (TTA). Our final model gets 92.4% AP in hard settings on the test subset and outperforms current stateof-the-art methods for face detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generic Object Detection. Generic object detection aims at locating and classifying the existing objects in the given picture. Before the booming of deep learning, generic object detection is mainly based on the hand-crafted feature descriptors like SIFT <ref type="bibr" target="#b23">[24]</ref> and HOG <ref type="bibr" target="#b4">[5]</ref>. And the most successful methods like DPM <ref type="bibr" target="#b7">[8]</ref> combine multi-scale hand-crafted features, sliding window, deformable part and SVM classifier to form a generic object detector. With AlexNet <ref type="bibr" target="#b14">[15]</ref> winning the championship of Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) by a large gap, the era of deep learning is coming, and generic object detection has been quickly dominated by deep learning methods. Two-stage methods start from R-CNN <ref type="bibr" target="#b9">[10]</ref> and Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>. And soon Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> proposes RPN network to replace the selective search to generate proposals by pre-define anchors, which becomes the most classical anchor-based generic object detection method. Based on Faster R-CNN <ref type="bibr" target="#b30">[31]</ref>, there are so many new methods presented like FPN <ref type="bibr" target="#b17">[18]</ref>, Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>, Cascade R-CNN <ref type="bibr" target="#b0">[1]</ref> and etc.. In order to overcome the high latency of two-stage methods, many one-stage methods are presented like series of YOLO <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, SSD <ref type="bibr" target="#b21">[22]</ref> and RetinaNet <ref type="bibr" target="#b18">[19]</ref>. To handling the multiple scale or small objects problem, YOLOs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> present novel anchor matching strategy including consideration of feedback of proposals and one ground-truth vs. one anchor, and also reweight the regression of width and height of objects. Then SSD <ref type="bibr" target="#b21">[22]</ref> uses a hierarchy of backbone features, while FPN <ref type="bibr" target="#b17">[18]</ref> presents feature pyramids. Besides, the series of SNIP <ref type="bibr" target="#b33">[34]</ref> and SNIPER <ref type="bibr" target="#b34">[35]</ref>, multi-scale training and multi-scale testing can also deal with the multiple scale problem.</p><p>In addition to the new method proposed in generic object detection, developments in other fields, like normalization methods and deep convolutional networks, also promote generic object detection. Batch normalization (BN) <ref type="bibr" target="#b13">[14]</ref> normalizes features within a batch along channel dimension, which can help models converge and enable models to train. In order to handle the dependency with batch size of BN, group normalization (GN) <ref type="bibr" target="#b43">[44]</ref> divides the channels into groups and computes within each group the mean and variance for normalization. Then for deep convolutional networks, after AlexNet <ref type="bibr" target="#b14">[15]</ref>, VGG [33] increases depth using an architecture with very small 3 × 3 convolution filters, GoogLeNet <ref type="bibr" target="#b35">[36]</ref> introduces Inception modules to use different numbers of small filters in parallel to form features of different receptive fields and help model to capture objects as well as context at multiple scales, and ResNet <ref type="bibr" target="#b10">[11]</ref> demonstrates the importance of the original information flow and presents skip connection to handle the degradation with deeper networks.</p><p>Face Detection. As an application of generic object detection, the history of face detection is almost the same. Before the era of deep learning, face detectors are also based on hand-crafted features like Haar <ref type="bibr" target="#b38">[39]</ref>. After the most popular and challenging face detection benchmark WIDER FACE dataset <ref type="bibr" target="#b47">[48]</ref> presented, face detection develops rapidly focusing on the extreme and real variation problem including scale, pose, occlusion, expression, makeup, illumination, blur and etc.. Almost all the recent face detection methods evolve from the existing generic object detection methods. Based on SSD <ref type="bibr" target="#b21">[22]</ref>, S 3 FD [58] extends anchor-associated layers to C3 stage and proposes a scale compensation anchor matching strategy in order to cover the small faces, Pyra-midBox <ref type="bibr" target="#b36">[37]</ref> proposes PyramidAnchors (PA), Low-level Feature Pyramid Networks (LFPN), Context-sensitive Predict Module (CPM) to emphasize the importance of context and data-anchor-sampling augmentation to increase smaller faces, and DSFD <ref type="bibr" target="#b15">[16]</ref> introduce a dual-shot detector using Improved Anchor Matching (IAM) and Progressive Anchor Loss (PAL). Then Based on RetinaNet <ref type="bibr" target="#b18">[19]</ref>, RetinaFace <ref type="bibr" target="#b5">[6]</ref> manually annotates five facial landmarks on faces to serve as extra supervision signal, RefineFace <ref type="bibr" target="#b56">[57]</ref> introduces five extra modules Selective Two-step Regression (STR), Selective Two-step Classification (STC), Scale-aware Margin Loss (SML), Feature Supervision Module (FSM) and Receptive Field Enhancement (RFE), and HAMBox <ref type="bibr" target="#b22">[23]</ref> emphasize the strong regression ability of some unmatched anchors and present an Online High-quality Anchor Mining Strategy (HAMBox). Besides, ASFD <ref type="bibr" target="#b50">[51]</ref> uses neural architecture search technique to automatically search the architecture for efficient multi-scale feature fusion and context enhancement.</p><p>To sum up, methods presented in face detection almost cover every part of deep learning training from data processing to loss designs. It is obvious that all of these methods focus on the challenge of small faces. However, actually there are so many methods in generic object detection, which we mention above, solving this problem. Therefore, based on some of these methods, we present TinaFace, a strong but simple baseline method for face detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TinaFace</head><p>Basically, we start from the one-stage detector Reti-naNet <ref type="bibr" target="#b18">[19]</ref> as some previous works do. The architecture of TinaFace is shown in <ref type="figure" target="#fig_0">Figure 1</ref> where the red dashed boxes demonstrate the different parts from RetinaNet <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deformable Convolution Networks</head><p>There is an inherent limitation in convolution operation, that is, we feed it with a strong prior about the sampling position which is fixed and rigid. Therefore, it is hard for networks to learn or encode complex geometric transformations, and the capability of models is limited. In order to further improve the capability of our model, we employ DCN <ref type="bibr" target="#b3">[4]</ref> into the stage four and five of the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inception Module</head><p>Multi-scale is always a challenge in generic object detection. The most common ways to deal with it are multi-scale training, FPN architecture and multi-scale testing. Besides, we employ inception module <ref type="bibr" target="#b35">[36]</ref> in our model to further enhance this ability. The inception module uses different numbers of 3 × 3 convolutional layers in parallel to form features of different receptive fields and then combine them, which help model to capture objects as well as context at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">IoU-aware Branch</head><p>IoU-aware <ref type="bibr" target="#b42">[43]</ref> is an extremely simple and elegant method to relieve the mismatch problem between classification score and localization accuracy of a single-stage object detector, which can help resort the classification score and suppress the false positive detected boxes (high score but low IoU). The architecture of IoU-aware is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, and the only difference is the purple part, a parallel head with a regression head to predict the IoU between the detected box and the corresponding ground-truth object. And this head only consists of a single 3 × 3 convolution layer, followed by a sigmoid activation layer. At the inference phase, the final detection confidence is computed by following equation,</p><formula xml:id="formula_0">score = p α i IoU (1−α) i<label>(1)</label></formula><p>where p i and IoU i are the original classification score and predicted IoU of ith detected box, and α ∈ [0, 1] is the hyperparameter to control the contribution of the classification score and predicted IoU to the final detection confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Distance-IoU Loss</head><p>The most common loss used in bbox regression is Smooth L1 Loss <ref type="bibr" target="#b8">[9]</ref> , which regresses the parameterizations of the four coordinates (box's center and its width and height). However, these optimization targets are not consistent with the regression evaluation metric IoU, that is, lower loss is not equivalent with higher IoU. Therefore, we turn to different IoU losses presented in past few years, directly regressing the IoU metric, such as GIoU <ref type="bibr" target="#b31">[32]</ref>, DIoU and CIoU <ref type="bibr" target="#b60">[61]</ref>. The reason we choose DIoU <ref type="bibr" target="#b60">[61]</ref> as our regression loss is that small faces is the main challenge of face detection since there are about two thirds data in WIDER FACE <ref type="bibr" target="#b47">[48]</ref> belong to small object and DIoU <ref type="bibr" target="#b60">[61]</ref> is more friendly to small objects. Practically, DIoU gets better performance on APsmall of the validation set of MS COCO 2017 <ref type="bibr" target="#b19">[20]</ref>. And theoretically, DIoU is defined as:</p><formula xml:id="formula_1">L DIoU = 1 − IoU + ρ 2 (b, b gt ) c 2<label>(2)</label></formula><p>where b and b gt denote the central points of predicted box and ground-truth box, ρ(·) is the Euclidean distance, and c is the diagonal length of the smallest enclosing box covering the two boxes. The extra penalty term ρ 2 (b,b gt ) c 2 proposes to minimize the normalized distance between central points of predicted box and ground-truth box. Compared to large objects, the same distance of central points in small objects will be penalized more, which help detectors learn more about small objects in regression.   <ref type="bibr" target="#b47">[48]</ref> is the largest face detection dataset, which contains 32,203 images and 393,703 faces. Since its variety of scale, pose, occlusion, expression, illumination and event, it is difficult and close to reality. The whole dataset is divided into train/val/test sets by ratio 50%/10%/40% within each event class. Furthermore, based on the detection rate of EdgeBox <ref type="bibr" target="#b63">[64]</ref>, each subset is defined into three levels of difficulty: 'Easy', 'Medium', 'Hard'. From the name of these three levels, we know that 'Hard' is more challenging. And from further analysis, we find that data in 'Hard' covers 'Medium' and 'Easy', which demonstrate that performance on 'Hard' can better reflect the effectiveness of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Feature Extractor. We use ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as backbone and Feature Pyramid Network (FPN) <ref type="bibr" target="#b17">[18]</ref> as neck to construct the feature extractor. This combination is widely used in almost all detectors, so we think it can serve as a fair playground for replication and comparison. In order to cover the tiny faces, FPN <ref type="bibr" target="#b17">[18]</ref> we employed extends to level P 2 like some previous works do. In total, there are 6 levels in FPN <ref type="bibr" target="#b17">[18]</ref> from level P 2 to P 7 .</p><p>Losses. The losses of classification, regression and IoU prediction are focal loss, DIoU loss and cross-entropy loss, respectively.</p><p>Normalization Method Batch Normalization (BN) <ref type="bibr" target="#b13">[14]</ref> is an extremely important technique for deep learning. It can help models converge and enable various networks to train. However, the performance of the model will degrade with the batch size decreasing especially when batch size is smaller than 4, caused by inaccurate batch statistics estimation. Considering that large volume GPUs are not widely used, which may cause problems for replication, with GeForce GTX 1080 Ti, we replace all the BN layer in network with Group Normalization <ref type="bibr" target="#b43">[44]</ref> which is a simple alternative to BN and independent of batch sizes, and the performance of which is stable.</p><p>Anchor and Assigner Settings Basically, we set 6 anchors from the set 2 4/3 × {4, 8, 16, 32, 64, 128} since there are 6 levels in our FPN <ref type="bibr" target="#b17">[18]</ref>. We adjust the base scale to 2 4/3 in order to better cover the tiny faces, use the mean value of aspect ratio of ground-truths as anchor ratio, and set three scales at step 2 1/3 in each level. For assigner, the IoU threshold for matching strategy is 0.35, and ignore-zone is not applied.</p><p>To better understand the advantage of our settings, we utilize the detection analysis tool 1 and conduct two experiments to get the distribution of positive samples assigned to each ground-truth shown in <ref type="figure" target="#fig_1">Figure 2</ref>. As illustrated in <ref type="figure" target="#fig_1">Figure 2a</ref>, although RetinaFace <ref type="bibr" target="#b5">[6]</ref> can recall most of the faces, it does not pay attention to the imbalance problem across scales, that is, small ground-truths get less positive anchors to train, while large one can get more, which leads the degraded performance on small ground-truths. Turning to <ref type="figure" target="#fig_1">Figure 2b</ref>, we notice that the imbalanced problem is largely relieved. The distribution of the number of positive assigned samples is highly similar across scale. Training Settings. We train the model by using SGD optimizer (momentum 0.9, weight decay 5e-4) with batch size 3 × 4 on three GeForce GTX 1080 Ti. The schedule of learning rate is annealing down from 3.75e-3 to 3.75e-5 every 30 epochs out of 630 epochs using the cosine decay rule. And in the first 500 iterations, learning rate linearly warms up from 3.75e-4 to 3.75e-3.</p><p>Testing Settings. Single Scale testing only contains a keep-ratio resize, which guarantees that the short and long </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on WIDER FACE</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we present the AP performance of models described in Section 3 on WIDER FACE validation subset. Our baseline model using single scale testing gets 95.9%, 95.2%, 92.4% in the three settings on the validation subset. Then we introduce DIoU <ref type="bibr" target="#b60">[61]</ref>, Inception <ref type="bibr" target="#b35">[36]</ref>, IoUaware <ref type="bibr" target="#b42">[43]</ref>, DCN <ref type="bibr" target="#b3">[4]</ref> modules and TTA to further improve the performance of detector by 1.1%, 1.1%, 1.0% on three settings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparsion with other methods on WIDER FACE</head><p>As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, we compare TinaFace with recent face detection methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b44">45]</ref> on both validation and testing subsets. For better comparsion, we pick up top-5 methods to form the <ref type="table" target="#tab_1">Table 2</ref> (HAMBox <ref type="bibr" target="#b22">[23]</ref> isn't listed in <ref type="figure" target="#fig_4">Figure 3</ref> since its results are not updated on the official website of WIDER FACE 2 ).</p><p>Surprisingly, with single-scale and single-model, our model already gets very promising and almost state-of-theart performance especially in the hard setting, which respectively outperforms ASFD-D6 <ref type="bibr" target="#b50">[51]</ref> in validation subset and test subset. Moreover, our model uses ResNet-50 as backbone, which is much smaller than what ASFD-D6 <ref type="bibr" target="#b50">[51]</ref> uses. In the case of using the same backbone, our final model with TTA outperforms the current state-of-the-art method HAMBox <ref type="bibr" target="#b22">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we point out that face detection is actually a one class generic object detection problem. It indicates that methods presented in generic object detection can be used for handling this problem. Then we present a strong but simple baseline method based on generic object detection for dealing with face detection named TinaFace to further illustrate this point. The whole network is simple and straightforward, and all the recent tricks equipped are easily implemented and built on existing modules. On the hard setting of the test subset of WIDER FACE, Our model without TTA already exceeds most recent face detection methods like ASFD-D6, which will be extremely efficient and effective. Besides, our final model achieves the state-of-the-art face detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The model architecture of TinaFace. (a) Feature Extractor: ResNet-50 [11] and 6 level Feature Pyramid Network [18] to extract the multi-scale features of input image. (b) Inception block to enhance receptive field. (c) Classification Head: 5 layers FCN for classification of anchors. (d) Regression Head: 5 layers FCN for regression of anchors to ground-truth objects boxes. (e) IoU Aware Head: a single convolutional layer for IoU prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The cumulative distribution and density function of the number of positive samples assigned to each ground-truth. Different colors represent different scales of ground-truth based on the evaluation across scales on COCO dataset. (a) distribution of Retinaface's [6] settings. (b) distribution of this work's settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Data Augmentation. First, crop the square patch from the original picture with a random size from the set [0.3, 0.45, 0.6, 0.8, 1.0] of the short edge of the original image and keep the overlapped part of the face box if its centre is within the crop patch. Then do photo distortion and random horizontal flip with the probability of 0.5. Finally, resize the patch into 640 × 640 and normalize.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label></label><figDesc>https://github.com/Media-Smart/volkscv edge of image do not surpass 1100 and 1650. Test Time Augmentation(TTA) is composed of multi-scale (the short edge of image is [500, 800, 1100, 1400, 1700]), shift (the direction is [(0, 0), (0, 1), (1, 0), (1, 1)]), horizontal flip and box voting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Precision-recall curves on the WIDER FACE validation and test subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AP performance on WIDER FACE validation subset</figDesc><table><row><cell cols="9">Baseline DIoU Inception IoU-aware DCN TTA Easy Medium Hard</cell></row><row><cell>√ √ √ √ √ √</cell><cell>-√ √ √ √ √</cell><cell>--√ √ √ √</cell><cell>---√ √ √</cell><cell>----√ √</cell><cell>-----√</cell><cell>0.959 0.959 0.958 0.963 0.963 0.970</cell><cell>0.952 0.952 0.952 0.955 0.957 0.963</cell><cell>0.924 0.927 0.928 0.929 0.930 0.934</cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>WIDER FACE dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AP performance of different methods on WIDER FACE validation subset and test subset</figDesc><table><row><cell>Val</cell><cell>Test</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://shuoyang1213.me/WIDERFACE/WiderFace_Results. html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selective refinement network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8231" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Retinaface: Single-stage dense face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face Detection with Feature Pyramids and Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Earp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00596</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Tiny Faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DSFD: Dual Shot Face Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pyramidbox++: High performance detector for finding tiny face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00386</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-Level Semantic Feature Detection: A New Perspective for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HAMBox: Delving Into Mining High-Quality Anchors on Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FA-RPN: Floating Region Proposals for Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SSH: Single Stage Headless Face Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">To boost or not to boost? on the limits of boosted trees for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd international conference on pattern recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3350" to="3355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="797" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning better features for face detection with feature fusion and segmentation supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxin</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08557</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<idno>null. IEEE</idno>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">747</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Face r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Detecting faces using regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">IoU-aware single-stage object detector for accurate localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="page">103911</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international joint conference on biometrics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Face detection through scalefriendly deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02863</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">MaskFace: multi-task face and landmark detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Yashunin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Baydasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vlasov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09412</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Extd: Extremely tiny face detector via iterative filter reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">ASFD: Automatic and Scalable Face Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11228</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Face detection using improved faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02142</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Accurate face detection for high performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faen</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01585</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feature agglomeration networks for single stage face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="180" to="189" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improved selective refinement network for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06651</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Refineface: Refinement neural network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Single-shot scale-aware network for real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="537" to="559" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Robust and high performance face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02350</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. 2020</title>
		<imprint>
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cms-rcnn: contextual multiscale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning for biometrics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Seeing Small Faces From Robust Anchor&apos;s Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
