<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 ROBUST SUBSPACE RECOVERY LAYER FOR UNSUPERVISED ANOMALY DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Hsin</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Minnesota Minneapolis</orgName>
								<address>
									<postCode>55455</postCode>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmian</forename><surname>Zou</surname></persName>
							<email>dzou@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Minnesota Minneapolis</orgName>
								<address>
									<postCode>55455</postCode>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
							<email>lerman@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Minnesota Minneapolis</orgName>
								<address>
									<postCode>55455</postCode>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 ROBUST SUBSPACE RECOVERY LAYER FOR UNSUPERVISED ANOMALY DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a "manifold" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Finding and utilizing patterns in data is a common task for modern machine learning systems. However, there is often some anomalous information that does not follow a common pattern and has to be recognized. For this purpose, anomaly detection aims to identify data points that "do not conform to expected behavior" <ref type="bibr" target="#b6">(Chandola et al., 2009)</ref>. We refer to such points as either anomalous or outliers. In many applications, there is no ground truth available to distinguish anomalous from normal points, and they need to be detected in an unsupervised fashion. For example, one may need to remove anomalous images from a set of images obtained by a search engine without any prior knowledge about how a normal image should look <ref type="bibr" target="#b46">(Xia et al., 2015)</ref>. Similarly, one may need to distinguish unusual news items from a large collection of news documents without any information whether a news item is usual or not <ref type="bibr" target="#b20">(Kannan et al., 2017)</ref>. In these examples, the only assumptions are that normal data points appear more often than anomalous ones and have a simple underlying structure which is unknown to the user. Some early methods for anomaly detection relied on Principal Component Analysis (PCA) <ref type="bibr" target="#b41">(Shyu et al., 2003)</ref>. Here one assumes that the underlying unknown structure of the normal samples is linear. However, PCA is sensitive to outliers and will often not succeed in recovering the linear structure or identifying the outliers <ref type="bibr" target="#b27">(Lerman &amp; Maunu, 2018;</ref><ref type="bibr" target="#b43">Vaswani &amp; Narayanamurthy, 2018)</ref>. More recent ideas of Robust PCA (RPCA) <ref type="bibr" target="#b45">(Wright et al., 2009;</ref><ref type="bibr" target="#b43">Vaswani &amp; Narayanamurthy, 2018)</ref> have been considered for some specific problems of anomaly detection or removal <ref type="bibr" target="#b53">(Zhou &amp; Paffenroth, 2017;</ref><ref type="bibr" target="#b36">Paffenroth et al., 2018)</ref>. RPCA assumes sparse corruption, that is, few elements of the data matrix are corrupted. This assumption is natural for some special problems in computer vision, in particular, background subtraction <ref type="bibr" target="#b9">(De La Torre &amp; Black, 2003;</ref><ref type="bibr" target="#b45">Wright et al., 2009;</ref><ref type="bibr" target="#b43">Vaswani &amp; Narayanamurthy, 2018)</ref>. However, a natural setting of anomaly detection with hidden linear structure may assume instead that a large portion of the data points are fully corrupted. The mathematical framework that addresses this setting is referred to as robust subspace recovery (RSR) <ref type="bibr" target="#b27">(Lerman &amp; Maunu, 2018)</ref>.</p><p>While Robust PCA and RSR try to extract linear structure or identify outliers lying away from such structure, the underlying geometric structure of many real datasets is nonlinear. Therefore, one needs to extract crucial features of the nonlinear structure of the data while being robust to outliers. In order to achieve this goal, we propose to use an autoencoder (composed of an encoder and a decoder) with an RSR layer. We refer to it as RSRAE <ref type="bibr">(RSR autoencoder)</ref>. It aims to robustly and nonlinearly reduce the dimension of the data in the following way. The encoder maps the data into a high-dimensional space. The RSR layer linearly maps the embedded points into a low-dimensional subspace that aims to learn the hidden linear structure of the embedded normal points. The decoder maps the points from this subspace to the original space. It aims to map the normal points near their original locations, and the anomalous points far from their original locations.</p><p>Ideally, the encoder maps the normal data to a linear space and any anomalies lie away from this subspace. In this ideal scenario, anomalies can be removed by an RSR method directly applied to the data embedded by the encoder. Since the linear model for the normal data embedded by the encoder is only approximate, we do not directly apply RSR to the embedded data. Instead, we minimize a sum of the reconstruction error of the autoencoder and the RSR error for the data embedded by the encoder. We advocate for an alternating procedure, so that the parameters of the autoencoder and the RSR layer are optimized in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">STRUCTURE OF THE REST OF THE PAPER</head><p>Section 2 reviews works that are directly related to the proposed RSRAE and highlights the original contributions of this paper. Section 3 explains the proposed RSRAE, and in particular, its RSR layer and total energy function. Section 4 includes extensive experimental evidence demonstrating effectiveness of RSRAE with both image and document data. Section 5 discusses theory for the relationship of the RSR penalty with the WGAN penalty. Section 6 summarizes this work and mentions future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS AND CONTRIBUTION</head><p>We review related works in Section 2.1 and highlight our contribution in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RELATED WORKS</head><p>Several recent works have used autoencoders for anomaly detection. <ref type="bibr" target="#b46">Xia et al. (2015)</ref> proposed the earliest work on anomaly detection via an autoencoder, while utilizing large reconstruction error of outliers. They apply an iterative and cyclic scheme, where in each iteration, they determine the inliers and use them for updating the parameters of the autoencoder. <ref type="bibr" target="#b3">Aytekin et al. (2018)</ref> apply 2 normalization for the latent code of the autoencoder and also consider the case of multiple modes for the normal samples. Instead of using the reconstruction error, they apply k-means clustering for the latent code, and identify outliers as points whose latent representations are far from all the cluster centers. <ref type="bibr" target="#b54">Zong et al. (2018)</ref> also use an autoencoder with clustered latent code, but they fit a Gaussian Mixture Model using an additional neural network. Restricted Boltzmann Machines (RBMs) are similar to autoencoders. <ref type="bibr" target="#b50">Zhai et al. (2016)</ref> define "energy functions" for RBMs that are similar to the reconstruction losses for autoencoders. They identify anomalous samples according to large energy values. <ref type="bibr" target="#b5">Chalapathy et al. (2017)</ref> propose using ideas of RPCA within an autoencoder, where they alternatively optimize the parameters of the autoencoder and a sparse residual matrix.</p><p>The above works are designed for datasets with a small fraction of outliers. However, when this fraction increases, outliers are often not distinguished by high reconstruction errors or low similarity scores. In order to identify them, additional assumptions on the structure of the normal data need to be incorporated. For example, <ref type="bibr" target="#b53">Zhou &amp; Paffenroth (2017)</ref> decompose the input data into two parts: low-rank and sparse (or column-sparse). The low-rank part is fed into an autoencoder and the sparse part is imposed as a penalty term with the 1 -norm (or 2,1 -norm for column-sparsity).</p><p>In this work, we use a term analogous to the 2,1 -norm, which can be interpreted as the sum of absolute deviations from a latent subspace. However, we do not decompose the data a priori, but minimize an energy combining this term and the reconstruction error. Minimization of the former term is known as least absolute deviations in RSR <ref type="bibr" target="#b27">(Lerman &amp; Maunu, 2018)</ref>. It was first suggested for RSR and related problems in <ref type="bibr" target="#b44">Watson (2001)</ref>; <ref type="bibr" target="#b10">Ding et al. (2006)</ref>; <ref type="bibr" target="#b52">Zhang et al. (2009)</ref>. The robustness to outliers of this energy, or of relaxed versions of it, was studied in <ref type="bibr" target="#b35">McCoy &amp; Tropp (2011);</ref><ref type="bibr" target="#b48">Xu et al. (2012)</ref>; ; ; <ref type="bibr" target="#b29">Lerman et al. (2015)</ref>; ; . In particular,  established its well-behaved landscape under special, though natural, deterministic conditions. Under similar conditions, they guaranteed fast subspace recovery by a simple algorithm that aims to minimize this energy.</p><p>Another directly related idea for extracting useful latent features is an addition of a linear selfexpressive layer to an autoencoder <ref type="bibr" target="#b18">(Ji et al., 2017)</ref>. It is used in the different setting of unsupervised subspace clustering. By imposing the self-expressiveness, the autoencoder is robust to an increasing number of clusters. Although self-expressiveness also improves robustness to noise and outliers, <ref type="bibr" target="#b18">Ji et al. (2017)</ref> aims at clustering and thus its goal is different than ours. Furthermore, their selfexpressive energy does not explicitly consider robustness, while ours does. <ref type="bibr" target="#b31">Lezama et al. (2018)</ref> consider a somewhat parallel idea of imposing a loss function to increase the robustness of representation. However, their goal is to increase the margin between classes and their method only applies to a supervised setting in anomaly detection, where the normal data is multi-modal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTRIBUTION OF THIS WORK</head><p>This work introduces an RSR layer within an autoencoder. It incorporates a special regularizer that enforces an outliers-robust linear structure in the embedding obtained by the encoder. We clarify that the method does not alternate between application of the autoencoder and the RSR layer, but fully integrates these two components. Our experiments demonstrate that a simple incorporation of a "robust loss" within a regular autoencoder does not work well for anomaly detection. We try to explain this and also the improvement obtained by incorporating an additional RSR layer.</p><p>Our proposed architecture is simple to implement. Furthermore, the RSR layer is not limited to a specific design of RSRAE but can be put into any well-designed autoencoder structure. The epoch time of the proposed algorithm is comparable to those of other common autoencoders. Furthermore, our experiments show that RSRAE competitively performs in unsupervised anomaly detection tasks. RSRAE addresses the unsupervised setting, but is not designed to be highly competitive in the semisupervised or supervised settings, where one has access to training data from the normal class or from both classes, respectively. In these settings, RSRAE functions like a regular autoencoder without taking an advantage of its RSR layer, unless the training data for the normal class is corrupted with outliers.</p><p>The use of RSR is not restricted to autoencoders. We establish some preliminary analysis for RSR within a generative adversarial network (GAN) <ref type="bibr" target="#b14">(Goodfellow et al., 2014;</ref> in Section 5. More precisely, we show that a linear WGAN intrinsically incorporates RSR in some special settings, although it is unclear how to impose an RSR layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RSR LAYER FOR OUTLIER REMOVAL</head><p>We assume input data {x (t) } N t=1 in R M , and denote by X its corresponding data matrix, whose t-th column is x (t) . The encoder of RSRAE, E , is a neural network that maps each data point,</p><formula xml:id="formula_0">x (t) , to its latent code z (t) = E (x (t) ) ∈ R D . The RSR layer is a linear transformation A ∈ R d×D that reduces the dimension to d. That is,z (t) = Az (t) ∈ R d . The decoder D is a neural network that mapsz (t) tox (t) in the original ambient space R M .</formula><p>We can write the forward maps in a compact form using the corresponding data matrices as follows:</p><formula xml:id="formula_1">Z = E (X),Z = AZ,X = D(Z).</formula><p>(1)</p><p>Ideally, we would like to optimize RSRAE so it only maintains the underlying structure of the normal data. We assume that the original normal data lies on a d-dimensional "manifold" in R D and thus the RSR layer embeds its latent code into R d . In this ideal optimization setting, the similarity between the input and the output of RSRAE is large whenever the input is normal and small whenever the input is anomalous. Therefore, by thresholding a similarity measure, one may distinguish between normal and anomalous data points.</p><p>In practice, the matrix A and the parameters of E and D are obtained by minimizing a loss function, which is a sum of two parts: the reconstruction loss from the autoencoder and the loss from the RSR layer. For p &gt; 0, an 2,p reconstruction loss for the autoencoder is</p><formula xml:id="formula_2">L p AE (E , A, D) = N t=1 x (t) −x (t) p 2 .</formula><p>(2)</p><p>In order to motivate our choice of RSR loss, we review a common formulation for the original RSR problem. In this problem one needs to recover a linear subspace, or equivalently an orthogonal projection P onto this subspace. Assume a dataset {y (t) } N t=1 and let I denote the identity matrix in the ambient space of the dataset. The goal is to find an orthogonal projector P of dimension d whose subspace robustly approximates this dataset. The least q-th power deviations formulation for q &gt; 0, or least absolute deviations when q = 1 <ref type="bibr" target="#b27">(Lerman &amp; Maunu, 2018)</ref>, seeks P that minimizeŝ</p><formula xml:id="formula_3">L(P) = N t=1 (I − P) y (t) q 2 .</formula><p>(3)</p><p>The solution of this problem is robust to some outliers when q ≤ 1 ; furthermore, q &lt; 1 can result in a wealth of local minima and thus q = 1 is preferable .</p><p>A similar loss function to (3) for RSRAE is</p><formula xml:id="formula_4">L q RSR (A) = λ 1 L RSR1 (A) + λ 2 L RSR2 (A) := λ 1 N t=1 z (t) − A T Az (t) z (t) q 2 + λ 2 AA T − I d 2 F ,<label>(4)</label></formula><p>where A T denotes the transpose of A, I d denotes the d × d identity matrix and · F denotes the Frobenius norm. Here λ 1 , λ 2 &gt; 0 are predetermined hyperparameters, though we later show that one may solve the underlying problem without using them. We note that the first term in the weighted sum of (4) is close to (3) as long as A T A is close to an orthogonal projector. To enforce this requirement we introduced the second term in the weighted sum of (4). In Appendix C we discuss further properties of the RSR energy and its minimization.</p><p>To emphasize the effect of outlier removal, we take p = 1 in (2) and q = 1 in (4). That is, we use the l 2,1 norm, or the formulation of least absolute deviations, for both reconstruction and RSR. The loss function of RSRAE is the sum of the two loss terms in (2) and (4), that is,</p><formula xml:id="formula_5">L RSRAE (E , A, D) = L 1 AE (E , A, D) + L 1 RSR (A).<label>(5)</label></formula><p>We remark that the sole minimization of L 1 AE , without L 1 RSR , is not effective for anomaly detection. We numerically demonstrate this in Section 4.3 and also try to explain it in Section 5.1.</p><p>Our proposed algorithm for optimizing (5), which we refer to as the RSRAE algorithm, uses alternating minimization. It iteratively backpropagates the three terms L 1 AE , L RSR1 , L RSR2 and accordingly updates the parameters of the RSR autoencoder. For clarity, we describe this basic procedure in Algorithm 1 of Appendix A. It is independent of the values of the parameters λ 1 and λ 2 . Note that the additional gradient step with respect to the RSR loss just updates the parameters in A. Therefore it does not significantly increase the epoch time of a standard autoencoder for anomaly detection. Another possible method, which we refer to as RSRAE+, is direct minimization of L RSRAE with predetermined λ 1 and λ 2 via auto-differentiation (see Algorithm 2 of Appendix A). Section 4.3 and Appendix I.2 demonstrate that in general, RSRAE performs better than RSRAE+, though it is possible that similar performance can be achieved by carefully tuning the parameters λ 1 and λ 2 when implementing RSRAE+.</p><p>We remark that a standard autoencoder is obtained by minimizing only L 2 AE , without the RSR loss. One might hope that minimizing L 1 AE may introduce the needed robustness. However, Section 4.3 and Appendix I.2 demonstrate that results obtained by minimizing L 1 AE or L 2 AE are comparable, and are worse than those of RSRAE and RSRAE+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We test our method 1 on five datasets: Caltech 101 <ref type="bibr" target="#b11">(Fei-Fei et al., 2007)</ref>, Fashion-MNIST <ref type="bibr" target="#b47">(Xiao et al., 2017)</ref>, Tiny Imagenet (a small subset of Imagenet <ref type="bibr" target="#b39">(Russakovsky et al., 2015)</ref>), Reuters-21578 <ref type="bibr" target="#b30">(Lewis, 1997)</ref> and 20 Newsgroups <ref type="bibr" target="#b24">(Lang, 1995)</ref>.</p><p>Caltech 101 contains 9,146 RGB images labeled according to 101 distinct object categories. We take the 11 categories that contain at least 100 images and randomly choose 100 images per category. We preprocess all 1100 images to have size 32 × 32 × 3 and pixel values normalized between −1 and 1. In each experiment, the inliers are the 100 images from a certain category and we sample c × 100 outliers from the rest of 1000 images of other categories, where c ∈ {0.1, 0.3, 0.5, 0.7, 0.9}.</p><p>Fashion-MNIST contains 28 × 28 grayscale images of clothing and accessories, which are categorized into 10 classes. We use the test set which contains 10,000 images and normalize pixel values to lie in [−1, 1]. In each experiment, we fix a class and the inliers are the test images in this class. We randomly sample c × 1,000 outliers from the rest of classes (here and below c is as above). Since there are around 1000 test images in each class, the outlier ratio is approximately c.</p><p>Tiny Imagenet contains 200 classes of RGB images from a distinct subset of Imagenet. We select 10 classes with 500 training images per class. We preprocess the images to have size 32 × 32 × 3 and pixel values in [−1, 1]. We further represent the images by deep features obtained by a ResNet <ref type="bibr" target="#b17">(He et al., 2016)</ref> with dimension 256 (Appendix I.1 provides results for the raw images). In each experiment, 500 inliers are from a fixed class and c × 500 outliers are from the rest of classes.</p><p>Reuters-21578 contains 90 text categories with multi-labels. We consider the five largest classes with single labels and randomly sample from them 360 documents per class. The documents are preprocessed into vectors of size 26,147 by sequentially applying the TFIDF transformer and Hashing vectorizer <ref type="bibr" target="#b38">(Rajaraman &amp; Ullman, 2011)</ref>. In each experiment, the inliers are the documents of a fixed class and c × 360 outliers are randomly sampled from the other classes.</p><p>20 Newsgroups contains newsgroup documents with 20 different labels. We sample 360 documents per class and preprocess them as above into vectors of size 10,000. In each experiment, the inliers are the documents from a fixed class and c × 360 outliers are sampled from the other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BENCHMARKS AND SETTING</head><p>We compare RSRAE with the following benchmarks: Local Outlier Factor (LOF) <ref type="bibr" target="#b4">(Breunig et al., 2000)</ref>, One-Class SVM (OCSVM) <ref type="bibr" target="#b40">(Schölkopf et al., 2000;</ref><ref type="bibr" target="#b0">Amer et al., 2013)</ref>, Isolation Forest (IF) <ref type="bibr" target="#b32">(Liu et al., 2012)</ref>, Deep Structured Energy Based Models (DSEBMs) <ref type="bibr" target="#b50">(Zhai et al., 2016)</ref>, Geometric Transformations (GT) <ref type="bibr" target="#b12">(Golan &amp; El-Yaniv, 2018)</ref>, and Deep Autoencoding Gaussian Mixture Model (DAGMM) <ref type="bibr" target="#b54">(Zong et al., 2018)</ref>. Of those benchmarks, LOF, OCSVM and IF are traditional, while powerful methods, for unsupervised anomaly detection and do not involve neural networks. DSEBMs, DAGMM and GT are more recent and all involve neural networks. DSEBMs is built for unsupervised anomaly detection. DAGMM and GT are designed for semi-supervised anomaly detection, but allow corruption. We use them to learn a model for the inliers and assign anomaly scores using the combined set of both inliers and outliers. GT only applies to image data. We briefly describe these methods in Appendix E.</p><p>We implemented DSEBMs, DAGMM and GT using the codes 2 from <ref type="bibr" target="#b12">Golan &amp; El-Yaniv (2018)</ref> with minimal modification so that they adapt to the data described above and the available GPUs in our machine. The LOF, OCSVM and IF methods are adapted from the scikit-learn packages.</p><p>We describe the structure of the RSRAE as follows. For the image datasets without deep features, the encoder consists of three convolutional layers: 5 × 5 kernels with 32 output channels, strides 2; 5 × 5 kernels with 64 output channels, strides 2; and 3 × 3 kernels with 128 output channels, strides 2. The output of the encoder is flattened and the RSR layer transforms it into a 10-dimensional vector. That is, we fix d = 10 in all experiments. The decoder consists of a dense layer that maps the output of the RSR layer into a vector of the same shape as the output of the encoder, and three deconvolutional layers: 3 × 3 kernels with 64 output channels, strides 2; 5 × 5 kernels with 32 output channels, strides 2; 5 × 5 kernels with 1 (grayscale) or 3 (RGB) output channels, strides 2. For the preprocessed document datasets or the deep features of Tiny Imagenet, the encoder is a fully connected network with size (32, 64, 128), the RSR layer linearly maps the output of the encoder to dimension 10, and the decoder is a fully connected network with size <ref type="bibr">(128, 64, 32, D)</ref> where D is the dimension of the input. Batch normalization is applied to each layer of the encoders and the decoders. The output of the RSR layer is 2 -normalized before applying the decoder. For DSEBMs and DAGMM we use the same number of layers and the same dimensions in each layer for the autoencoder as in RSRAE. For each experiment, the RSRAE model is optimized with Adam using a learning rate of 0.00025 and 200 epochs. The batch size is 128 for each gradient step. The setting of training is consistent for all the neural network based methods.</p><p>The two main hyperparameters of RSRAE are the intrinsic dimension d and learning rate. Their values were fixed above. Appendix G demonstrates stability to changes in these values.</p><p>All experiments were executed on a Linux machine with 64GB RAM and four GTX1080Ti GPUs. For all experiments with neural networks, we used TensorFlow and Keras. We report runtimes in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head><p>We summarize the precision and recall of our experiments by the AUC (area under curve) and AP (average precision) scores. For completeness, we include the definitions of these common scores in Appendix E. We compute them by considering the outliers as "positive". We remark that we did not record the precision-recall-F1 scores, as in <ref type="bibr" target="#b46">Xia et al. (2015)</ref>; <ref type="bibr" target="#b54">Zong et al. (2018)</ref>, since in practice it requires knowledge of the outlier ratio.</p><p>Figs. 1 and 2 present the AUC and AP scores of RSRAE and the methods described in Section 4.1 for the datasets described above, where GT is only applied to image data without deep features. For each constant c (the outlier ratio) and each method, we average the AUC and AP scores over 5 runs with different random initializations and also compute the standard deviations. For brevity of presentation, we report the averaged scores among all classes and designate the averaged standard deviations by bars.</p><p>The results indicates that RSRAE clearly outperforms other methods in most cases, especially when c is large. Indeed, the RSR layer was designed to handle large outlier ratios. For Fashion MNIST and Tiny Imagenet with deep features, IF performs similarly to RSRAE, but IF performs poorly on the document datasets. OCSVM is the closest to RSRAE for the document datasets but it is generally not so competitive for the image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPARISON WITH VARIATIONS OF RSRAE</head><p>We use one image dataset (Caltech 101) and one document dataset (Reuters-21578) and compare between RSRAE and three variations of it. The first one is RSRAE+ (see Section 3) with λ 1 = λ 2 = 0.1 in (4) (these parameters were optimized on 20 Newsgroup, though results with other choices of parameters are later demonstrated in Section G.3). The next two are simpler autoencoders without RSR layers: AE-1 minimizes L 1 AE , the 2,1 reconstruction loss; and AE minimizes L 2 AE , the 2,2 reconstruction loss (it is a regular autoencoder for anomaly detection). We maintain the same architecture as that of RSRAE, including the matrix A, but use different loss functions. <ref type="figure" target="#fig_2">Fig. 3</ref> reports the AUC and AP scores. We see that for the two datasets RSRAE+ with the prespecified λ 1 and λ 2 does not perform as well as RSRAE, but its performance is still better than AE and AE-1. This is expected since we chose λ 1 and λ 2 after few trials with a different dataset, whereas RSRAE is independent of these parameters. The performance of AE and AE-1 is clearly worse, and they are also not as good as some methods compared with in Section 4.2. At last, AE is generally comparable with AE-1. Similar results are noticed for the other datasets in Appendix I.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED THEORY FOR THE RSR PENALTY</head><p>We explain here why we find it natural to incorporate RSR within a neural network. In Section 5.1 we first review the mathematical idea of an autoencoder and discuss the robustness of a linear autoencoder with an 2,1 loss (i.e., RSR loss). We then explain why a general autoencoder with an 2,1 loss is not expected to be robust to outliers and why an RSR layer can improve its robustness. Section 5.2 is a first step of extending this view to a generative network. It establishes some robustness of WGAN with a linear generator, but the extension of an RSR layer to WGAN is left as an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ROBUSTNESS AND RELATED PROPERTIES OF AUTOENCODERS</head><p>Mathematically, an autoencoder for a dataset {x (t) } N t=1 ⊂ R D and a latent dimension d &lt; D is composed of an encoder E : R D → R d and a decoder D : R d → R D that minimize the following energy function with p = 2:</p><p>N t=1</p><formula xml:id="formula_6">x (t) − D • E (x (t) ) p 2 ,<label>(6)</label></formula><p>where • denotes function decomposition. It is a natural nonlinear generalization of PCA <ref type="bibr" target="#b15">(Goodfellow et al., 2016)</ref>. Indeed, in the case of a linear autoencoder, E and D are linear maps represented by matrices E ∈ R d×D and D ∈ R D×d , respectively, that need to minimize (among such matrices) the following loss function with p = 2 N t=1</p><formula xml:id="formula_7">x (t) − DEx (t) p 2 .<label>(7)</label></formula><p>We explain in Appendix D.1 that if (D , E ) is a minimizer of (7) with p = 2 (among E ∈ R d×D and D ∈ R D×d ), then D E is the orthoprojector on the d-dimensional PCA subspace. This means, that the latent code {E x (t) } N t=1 parametrizes the PCA subspace and an additional application of D to {E x (t) } N t=1 results in the projections of the data points {x (t) } N t=1 onto the PCA subspace. The recovery error for data points on this subspace is zero (as D E is the identity on this subspace), and in general, this error is the Euclidean distance to the PCA subspace,</p><formula xml:id="formula_8">x (t) − D E x (t)</formula><p>2 . Intuitively, the idea of a general autoencoder is the same. It aims to fit a nice structure, such as a manifold, to the data, where ideally D • E is a projection onto this nice structure. This idea can only be made rigorous for data approximated by simple geometric structure, e.g., by a graph of a sufficiently smooth function.  In order to extend these methods to anomaly detection, one needs to incorporate robust strategies, so that the methods can still recover the underlying structure of the inliers, and consequently assign lower recovery errors for the inliers and higher recovery errors for the outliers. For example, in the linear case, one may assume a set of inliers lying on and around a subspace and an arbitrary set of outliers (with some restriction on their fraction). PCA, and equivalently, the linear autoencoder that minimizes (7) with p = 2, is not robust to general outliers. Thus it is not expected to distinguish well between inliers and outliers in this setting. As explained in Appendix D.1, minimizing (7) with p = 1 gives rise to the least absolute deviations subspace. This subspace can be robust to outliers under some conditions, but these conditions are restrictive (see examples in ). In order to deal with more adversarial outliers, it is advised to first normalize the data to the sphere (after appropriate centering) and then estimate the least absolute deviations subspace. This procedure was theoretically justified for a general setting of adversarial outliers in <ref type="bibr" target="#b33">Maunu &amp; Lerman (2019)</ref>.</p><p>As in the linear case, an autoencoder that uses the loss function in (6) with p = 1 may not be robust to adversarial outliers. Unlike the linear case, there are no simple normalizations for this case. Indeed, the normalization to the sphere can completely distort the structure of an underlying manifold and it is also hard to center in this case. Furthermore, there are some obstacles of establishing robustness for the nonlinear case even under special assumptions.</p><p>Our basic idea for a robust autoencoder is to search for a latent low-dimensional code for the inliers within a larger embedding space. The additional RSR loss focuses on parametrizing the lowdimensional subspace of the encoded inliers, while being robust to outliers. Following the above discussion, we enhance such robustness by applying a normalization similar to the one discussed above, but adapted better to the structure of the network (see Section 4.1). The emphasis of the RSR Reuters-21578 layer is on appropriately encoding the inliers, where the encoding of the outliers does not matter. It is okay for the encoded outliers to lie within the subspace of the encoded inliers, as this will result in large recovery errors for the outliers. However, in general, most encoded outliers lie away from this subspace, and this is why such a mechanism is needed (otherwise, a regular autoencoder may obtain a good embedding).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RELATIONSHIP OF THE RSR LOSS WITH LINEARLY GENERATED WGAN</head><p>An open problem is whether RSR can be used within other neural network structures for unsupervised learning, such as variational autoencoders (VAEs) <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>. The latter two models are used in anomaly detection with a score function similar to the reconstruction error <ref type="bibr" target="#b1">(An &amp; Cho, 2015;</ref><ref type="bibr" target="#b42">Vasilev et al., 2018;</ref><ref type="bibr" target="#b49">Zenati et al., 2018;</ref><ref type="bibr" target="#b23">Kliger &amp; Fleishman, 2018)</ref>.</p><p>While we do not solve this problem, we establish a natural relationship between RSR and Wasserstein-GAN (WGAN) <ref type="bibr" target="#b16">Gulrajani et al., 2017)</ref> with a linear generator, which is analogous to the example of a linear autoencoder mentioned above.</p><formula xml:id="formula_9">Let W p denote the p-Wasserstein distance in R D (p ≥ 1). That is, for two probability distributions µ, ν on R D , W p (µ, ν) = inf π∈Π(µ,ν) E (x,y)∼π x − y p 2 1/p ,<label>(8)</label></formula><p>where Π(µ, ν) is the set of joint distributions with µ, ν as marginals. We formulate the following proposition (while prove it later in Appendix D.2) and then interpret it. Proposition 5.1. Let p ≥ 1 and µ be a Gaussian distribution on R D with mean m X ∈ R D and full-rank covariance matrix Σ</p><formula xml:id="formula_10">X ∈ R D×D (that is, µ is N (m X , Σ X )). Then min ν is N (mY,ΣY) W p (µ, ν) s.t. m Y ∈ R D rank(Σ Y ) = d<label>(9)</label></formula><p>is achieved when m Y = m X and Σ Y = P L Σ X P L , where for X ∼ µ</p><formula xml:id="formula_11">L = argmin dimL =d E X − P L X p 2 .<label>(10)</label></formula><p>The setting of this proposition implicitly assumes a linear generator of WGAN. Indeed, the linear mapping, which can be represented by a d × D matrix, maps a distribution in N (m X , Σ X ) into a distribution in N (m Y , Σ Y ) and reduces the rank of the covariance matrix from D to d. The proposition states that in this setting the underlying minimization is closely related to minimizing the loss function (3). Note that here p ≥ 1, however, if one further corrupts the sample, then p = 1 is the suitable choice <ref type="bibr" target="#b27">(Lerman &amp; Maunu, 2018)</ref>. This choice is also more appropriate for WGAN, since there is no p-WGAN for p = 1.</p><p>Nevertheless, training a WGAN is not exactly the same as minimizing the W 1 distance <ref type="bibr" target="#b16">(Gulrajani et al., 2017)</ref>, since it is difficult to impose the Lipschitz constraint for a neural network. Furthermore, in practice, the WGAN generator, which is a neural network, is nonlinear, and thus its output is typically non-Gaussian. The robustness of WGAN with a linear autoencoder, which we established here, does not extend to a general WGAN (this is similar to our earlier observation that the robustness of a linear autoencoder with an RSR loss does not generalize to a nonlinear autoencoder). We believe that a similar structure like the RSR layer has to be imposed for enhancing the robustness of WGAN, and possibly also other generative networks, but we leave its effective implementation as an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We constructed a simple but effective RSR layer within the autoencoder structure for anomaly detection. It is easy to use and adapt. We have demonstrated competitive results for image and document data and believe that it can be useful in many other applications.</p><p>There are several directions for further exploration of the RSR loss in unsupervised deep learning models for anomaly detection. First, we are interested in theoretical guarantees for RSRAE. A more direct subproblem is understanding the geometric structure of the "manifold" learned by RSRAE. Second, it is possible that there are better geometric methods to robustly embed the manifold of inliers. For example, one may consider a multiscale incorporation of RSR layers, which we expand on in Appendix D.3. Third, one may try to incorporate an RSR layer in other neural networks for anomaly detection that use nonlinear dimension reduction. We hope that some of these methods may be easier to directly analyze than our proposed method. For example, we are curious about successful incorporation of robust metrics for GANs or WGANs. In particular, we wonder about extensions of the theory proposed here for WGAN when considering a more general setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS OF RSRAE AND RSRAE+</head><p>The implementations of both RSRAE and RSRAE+ are simple. For completeness we provide here their details in algorithm boxes. The codes will be later posted in a supplementary webpage. Algorithm 1 describes RSRAE, which minimizes <ref type="formula" target="#formula_5">(5)</ref>   x (t) is anomalous 21: end if 22: end for 23: return Normality labels for t = 1, . . . , N We clarify some guidelines for choosing default parameters, which we follow in all reported experiments. We set AE , RSR1 and RSR2 to be zero. In general, we use networks with dense layers but for image data we use convolutional layers. We prefer using tanh as the activation function due to its smoothness. However, for a dataset that does not lie in the unit cube, we use either a ReLU function if all of its coordinates are positive, or a leaky ReLU function otherwise. The network parameters and the elements of A are initialized to be i.i.d. standard normal. In all numerical experiments, we set the number of columns of A to be 10, that is, d = 10. The learning rate is chosen so that there is a sufficient improvement of the loss values after each epoch. Instead of fixing T , we report the AUC and AP scores for different values of T . Algorithm 2 describes RSRAE+, which minimizes (5) with fixed λ 1 and λ 2 by auto-differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 RSRAE+</head><p>Input: Data {x (t) } N t=1 ; thresholds AE , T ; architecture and initial parameters of E , D, A (including number of columns of A); parameters of the the energy function λ 1 , λ 2 ; number of epochs &amp; batches; learning rate for backpropagation; similarity measure Output: Labels of data points as normal or anomalous 1: for each epoch do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Divide input data into batches 3:</p><p>for each batch do 4:</p><p>if L 1 AE (θ, A, ϕ) &gt; AE then 5:</p><formula xml:id="formula_12">Backpropagate L 1 AE (θ, A, ϕ) + λ 1 L 1 RSR1 (A) + λ 2 L 1 RSR2 (A) w.r.t. θ, A, ϕ &amp; update θ, A, ϕ 6: end if 7:</formula><p>end for 8: end for 9: for t = 1, . . . , N do 10:</p><p>Calculate similarity between x (t) andx <ref type="bibr">(t)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>if similarity ≥ T then 12:</p><p>x (t) is normal 13: else 14:</p><p>x (t) is anomalous 15: end if 16: end for 17: return Normality labels for t = 1, . . . , N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DEMONSTRATION OF RSRAE FOR ARTIFICIAL DATA</head><p>For illustrating the performance of RSRAE, in comparison with a regular autoencoder, we consider a simple artificial geometric example. We assume corrupted data whose normal part is embedded in a "Swiss roll manifold" 3 , which is a two-dimensional manifold in R 3 . More precisely, the normal part is obtained by mapping 1,000 points uniformly sampled from the rectangle [3π/2, 9π/2] × [0, 21] into R 3 by the function (s, t) → (t cos(t), s, t sin(t)).</p><p>The anomalous part is obtained by i.i.d. sampling of 500 points from an isotropic Gaussian distribution in R 3 with zero mean and standard deviation 2 in any direction. <ref type="figure">Fig. 4a</ref> illustrates such a sample, where the inliers are in black and the outliers are in blue. We remark that <ref type="figure">Fig 5a is</ref> identical.</p><p>We construct the RSRAE with the following structure. The encoder is composed of fully-connected layers of sizes <ref type="bibr">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128)</ref>. The decoder is composed of fully connected layers of sizes <ref type="bibr">(128,</ref><ref type="bibr">64,</ref><ref type="bibr">32,</ref><ref type="bibr">3)</ref>. Each fully connected layer is activated by the leaky ReLU function with α = 0.2. The intrinsic dimension for the RSR layer, that, is the number of columns of A, is d = 2.</p><p>For comparison, we construct the regular autoencoder AE (see Section 4.3). Recall that both of them have the same architecture (including the linear map A), but AE minimizes the 2 loss function in (6) (with p = 2) without an additional RSR loss. We optimize both models with 10,000 epochs and a batch gradient descent using Adam (Kingma &amp; Ba, 2014) with a learning rate of 0.01.</p><p>The reconstructed data (X) using RSRAE and AE are plotted in <ref type="figure">Figs. 4d and 5d</ref>, respectively. We further demonstrate the output obtained by the encoder and the RSR layer. The output of the encoder, Z = E (X), lies in R 128 . For visualization purposes we project it onto a R 3 as follows. We first find two vectors that span the image of A and we add to it the "principal direction" of Z orthogonal to the span of A. We project Z onto the span of these 3 vectors. <ref type="figure">Figs. 4b and 5b</ref> show these projections for RSRAE and AE, respectively. <ref type="figure">Figs. 4c and 5c</ref> demonstrate the respective mappings of Z by A during the RSR layer. <ref type="figure">Figs. 4d and 5d</ref> imply that the set of reconstructed normal points in RSRAE seem to lie on the original manifold, whereas the reconstructed normal points by AE seem to only lie near, but often not on the Swiss roll manifold. More importantly, the anomalous points reconstructed by RSRAE seem to be sufficiently far from the set of original anomalous points, unlike the reconstructed points by AE. Therefore, RSRAE can better distinguish anomalies using the distance between the original and reconstructed points, where small values are obtained for normal points and large ones for anomalous ones. <ref type="figure">Fig. 6</ref> demonstrates this claim. They plot the histograms of the distance between the original and reconstructed points when applying RSRAE and AE, where distances for normal and anomalous points are distinguished by color. Clearly, RSRAE distinguishes normal and anomalous data better than AE.</p><p>(a) Input data X  <ref type="figure">Figure 6</ref>: Demonstration of the reconstruction error distribution for RSRAE and AE.</p><formula xml:id="formula_14">Encoder −−−−−−→ E :R 3 →R 128 (b) Z = E (X) projected onto 3D RSR − −−−−−−− → linear mapping A:R 128 →R 2 (c)Z = AZ Decoder −−−−−→ D:R 2 →R 3 (d) Output of RSRAẼ X = D(Z)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FURTHER DISCUSSION OF THE RSR TERM</head><p>The RSR energy in (4) includes two different terms. The proposition below indicates that the second term of (4) is zero when plugging into it the solution of the minimization of the first term of (4) with the additional requirement that A has full rank. That is, in theory, one may only minimize the first term of (4) over the set of matrices A ∈ R d×D with full rank. We then discuss computational issues of this different minimization.</p><formula xml:id="formula_15">Proposition C.1. Assume that {z (t) } N t=1 ⊂ R D spans R D , d D and let A = argmin A∈R d×D rank(A)=d N t=1 z (t) − A T Az (t) 2 .<label>(12)</label></formula><p>Then A A T = I d .</p><p>Proof. Let A be an optimizer of (12) and P denote the orthogonal projection onto the range of A T A . Note that P can be written asÃ TÃ , whereÃ is a d × D matrix composed of an orthonormal basis of the range of P . Therefore, being an optimum of (12), A satisfies</p><formula xml:id="formula_16">z (t) − P z (t) 2 ≥ z (t) − A T A z (t) 2 , t = 1, · · · , N .<label>(13)</label></formula><p>On the other hand, the definition of orthogonal projection implies that</p><formula xml:id="formula_17">z (t) − P z (t) 2 ≤ z (t) − A T A z (t) 2 , t = 1, · · · , N .<label>(14)</label></formula><p>That is, equality is obtained in <ref type="formula" target="#formula_16">(13)</ref> and <ref type="formula" target="#formula_4">(14)</ref>. This equality and the fact that P is a projection on the range of A T A imply that</p><formula xml:id="formula_18">P z (t) = A T A z (t) , t = 1, · · · , N . (15) Since {z (t) } N t=1 spans R D , (15) results in P = A T A ,<label>(16)</label></formula><p>which further implies that</p><formula xml:id="formula_19">A A T A = A P = A .</formula><p>(17) Combining this observation (A A T A = A ) with the constraint that A has a full rank, we conclude that A A T = I d .</p><p>The minimization in (12) is nonconvex and intractable. Nevertheless,  propose a heuristic to solve it with some weak guarantees and  propose an algorithm with guarantees under some conditions. However, such a minimization is even more difficult when applied to the combined energy in (5), instead of (4). Therefore, we find it necessary to include the second term in (4) that imposes the nearness of A T A to an orthogonal projection (equivalently, of AA T to the identity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE ON RELATED THEORY FOR THE RSR PENALTY</head><p>In Section D.1 we characterize the solution of (7) via a subspace problem. Special case solutions to this problem include both the PCA subspace and the least absolute deviations subspace. In Section D.2 we prove Proposition 5.1. In Section D.3 we review some pure mathematical work that we find relevant to this discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 PROPERTY OF LINEAR AUTOENCODERS</head><p>The following proposition expresses the solution of (7) in terms of another minimization problem. After proving it, we clarify that the other minimization problem is related to both PCA and RSR. Proposition D.1. Let p ≥ 1, d &lt; D, and {x (t) } N t=1 ⊂ R D be a dataset with rank at least d. If (D , E ) ∈ R D×d × R d×D is a minimizer of (7), then</p><formula xml:id="formula_20">D E = P ,<label>(18)</label></formula><p>where P ∈ R D×D is a minimizer of N t=1</p><formula xml:id="formula_21">x (t) − Px (t) p 2 ,<label>(19)</label></formula><p>among all orthoprojectors P (that is, P = P T and P 2 = P) of rank d.</p><p>Proof. Let P be a minimizer of <ref type="formula" target="#formula_10">(19)</ref> and (D , E ) be a minimizer of (7). Since P is an orthoprojector of rank d it can be written as P = U U T , where U ∈ R D×d , and thus N t=1</p><formula xml:id="formula_22">x (t) − D E x (t) p 2 ≤ N t=1 x (t) − U U T x (t) p 2 = N t=1 x (t) − P x (t) p 2 .<label>(20)</label></formula><p>Let L denote the column space of D E . Then by the property of orthoprojection</p><formula xml:id="formula_23">x (t) − D E x (t) 2 ≥ x (t) − P L x (t) 2 for 1 ≤ t ≤ N<label>(21)</label></formula><p>and consequently N t=1</p><formula xml:id="formula_24">x (t) − D E x (t) p 2 ≥ N t=1 x (t) − P L x (t) p 2 ≥ N t=1 x (t) − P x (t) p 2 .<label>(22)</label></formula><p>The combination of <ref type="formula" target="#formula_22">(20)</ref> and <ref type="formula" target="#formula_24">(22)</ref> yields the following two equalities N t=1</p><formula xml:id="formula_25">x (t) − P L x (t) p 2 = N t=1 x (t) − P x (t) p 2 ,<label>(23)</label></formula><p>N t=1</p><formula xml:id="formula_26">x (t) − D E x (t) p 2 = N t=1 x (t) − P L x (t) p 2 .<label>(24)</label></formula><p>We note that (23) implies that P L is a minimizer of (19) (among all rank d orthoprojectors). We further note that <ref type="formula" target="#formula_23">(21)</ref> and <ref type="formula" target="#formula_4">(24)</ref> yield that for all 1 ≤ t ≤ N</p><formula xml:id="formula_27">x (t) − D E x (t) 2 = x (t) − P L x (t) 2 .<label>(25)</label></formula><p>Since D E x (t) ∈ L and P L is an orthoprojector we conclude from (25) that</p><formula xml:id="formula_28">D E x (t) = P L x (t) for 1 ≤ t ≤ N.<label>(26)</label></formula><p>We note that the definition of (D , E ) implies that L (which is the column space of D E ) is contained in the span of {x (t) } N t=1 . We also recall that the dimension of the span of {x (t) } N t=1 is at least the dimension of L , that is, d. Combining the latter facts with (26) we obtain that D E = P L . This and the fact that P L is a minimizer of (19) (which was derived from (23)) concludes (18).</p><p>Note that when p = 2, the energy function in (19) corresponds to PCA. More precisely, a minimizer P of (19) (among rank d orthoprojectors) is an orthoprojector on a d-dimensional PCA subspace, equivalently, a subspace spanned by top d eigenvectors of the sample covariance (we assume for simplicity linear, and not affine, autoencoder, so the PCA subspace is linear and thus when p = 2 the data is centered at the origin). This minimizer is unique if and only if the d-th eigenvalue of the sample covariance is larger than the (d + 1)-st eigenvalue. These elementary facts are reviewed in Section II-A of <ref type="bibr" target="#b27">Lerman &amp; Maunu (2018)</ref>.</p><p>When p = 1, the minimizer P of (19) <ref type="figure">(among rank d orthoprojectors)</ref> is an orthoprojector on the d-dimensional least absolute deviations subspace. This subspace is reviewed in Section II-D of <ref type="bibr" target="#b27">Lerman &amp; Maunu (2018)</ref> as a common approach for RSR. The minimizer is often not unique, where sufficient and necessary conditions for local minima of (19) are studied in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 PROOF OF PROPOSITION 5.1</head><p>Proof. We denote the subspace L in the left hand side of (10) by L in order to distinguish it from the generic notation L for subspaces. Consider the random variable X ∼ µ, Where µ is N (m X , Σ X ). Fix π ∈ Π(µ, ν). We note that</p><formula xml:id="formula_29">E (X,Y )∼π X − Y p 2 = R D R D x − y p 2 π(x, y)dx dy ≥ min dimL =d R D dist(x, L ) p R D π(x, y)dy dx (27) = min dimL =d R D dist(x, L ) p µ(x) dx = min dimL =d E X − P L X p 2 .</formula><p>The inequality in (27) holds since X is fixed and Y satisfies (X, Y ) ∼ π, so the distribution of Y is N (m Y , Σ Y ). Therefore, almost surely, Y takes values in the d-dimensional affine subspace {y ∈ R D : y − m Y ∈ range(Σ Y )}. Furthermore, we note that equality in <ref type="formula" target="#formula_7">(27)</ref> is achieved when Y = P L X.</p><p>We conclude the proof by showing that</p><formula xml:id="formula_30">m X ∈ L .<label>(28)</label></formula><p>Indeed, (28) implies that the orthogonal projection of X ∼ N (m X , Σ X ) onto L results in a random variable with distribution ν which is N (m X , P L Σ X P L ). By the above observation about the optimality of Y = P L X, the density of this distribution is the optimal solution of (9).</p><p>To prove (28), we assume without loss of generality that m X = 0. Denote the orthogonal projection of the origin onto the affine subspace L by m L and let L 0 = L − m L . We need to show that L = L 0 , or equivalently, m L = 0. We note L 0 is a linear subspace, m L is orthogonal to L 0 and thus there exists a rotation matrix O such that</p><formula xml:id="formula_31">OL 0 = {(0, · · · , 0, z D−d+1 , · · · , z D ) : z D−d+1 , · · · z D ∈ R} ,<label>(29)</label></formula><p>and</p><p>Om L = (m 1 , · · · , m D−d , 0, · · · , 0) .</p><p>For any x ∈ R D we note that µ(x) = µ(−x) since µ is Gaussian. Using this observation, other basic observations and the notation Ox = (x 1 , · · · , x D ) we obtain that</p><formula xml:id="formula_33">dist(x, L ) p µ(x) + dist(−x, L ) p µ(−x) = (dist(x, L ) p + dist(−x, L ) p ) µ(x) = (dist(Ox, OL ) p + dist(−Ox, OL ) p ) µ(x) =   D−d i=1 (x i − m i ) 2 p/2 + D−d i=1 (−x i − m i ) 2 p/2   µ(x) =   D−d i=1 (x i − m i ) 2 p/2 + D−d i=1 (x i + m i ) 2 p/2   µ(x) ≥ 2 D−d i=1 x i 2 p/2 µ(x) (31) = 2 dist(Ox, OL 0 ) p µ(x) = 2 dist(x, L 0 ) p µ(x) = (dist(x, L 0 ) p + dist(−x, L 0 ) p ) µ(x) = dist(x, L 0 ) p µ(x) + dist(−x, L 0 ) p µ(−x) .</formula><p>The inequality in (31) follows from the fact that for p ≥ 1, the function · p 2 is convex as it is a composition of the convex function · 2 : R d → R + and the increasing convex function (·) p :</p><formula xml:id="formula_34">R + → R + . Equality is achieved in (31) if m i = 0 for i = 1, · · · , D − d, that is, L = L 0 .</formula><p>Integrating the left and right hand sides of (31) over R D results in</p><formula xml:id="formula_35">R D dist(x, L ) p µ(x)dx ≥ R D dist(x, L 0 ) p µ(x)dx .<label>(32)</label></formula><p>Since L is a minimizer among all affine subspaces of rank d of</p><formula xml:id="formula_36">R D dist(x, L ) p µ(x) dx = E X − P L X p 2</formula><p>, equality is obtained in (32). Consequently, equality is obtained, almost everywhere, in (31). Therefore, L = L 0 and the claim is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 RELEVANT MATHEMATICAL THEORY</head><p>We note that a complex network can represent a large class of functions. Consequently, for a sufficiently complex network, minimizing the loss function in (6) results in minimum value zero. In this case the minimizing "manifold" contains the original data, including the outliers. On the other hand, the RSR loss term imposes fitting a subspace that robustly fits only part of the data and thus cannot result in minimum value zero. Nevertheless, imposing a subspace constraint might be too restrictive, even in the latent space. A seminal work by Jones (1990) studies optimal types of curves that contain general sets. This work relates the construction and optimal properties of these curves with multiscale approximation of the underlying set by lines. It was generalized to higher dimensions in <ref type="bibr" target="#b7">(David &amp; Semmes, 1993)</ref> and to a setting relevant to outliers in <ref type="bibr" target="#b25">(Lerman, 2003)</ref>. These works suggest loss functions that incorporate several linear RSR layers from different scales. Nevertheless, their pure setting does not directly apply to our setting. We have also noticed various technical difficulties when trying to directly implement these ideas to our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E BRIEF DESCRIPTION OF THE BASELINES AND METRICS</head><p>We first clarify the methods used as baselines in Section 4.</p><p>Local Outlier Factor (LOF) measures the local deviation of a given data point with respect to its neighbors. If the LOF of a data point is too large then the point is determined to be an outlier.</p><p>One-Class SVM (OCSVM) learns a margin for a class of data. Since outliers contribute less than the normal class, it also applies to the unsupervised setting <ref type="bibr" target="#b13">(Goldstein &amp; Uchida, 2016)</ref>. It is usually applied with a non-linear kernel.</p><p>Isolation Forest (IF) determines outliers by looking at the number of splittings needed for isolating a sample. It constructs random decision trees. A short path length for separating a data point implies a higher probability that the point is an outlier.</p><p>Geometric Transformations (GT) applies a variety of geometric transforms to input images and consequently creates a self-labeled dataset, where the labels are the types of transformations. Its anomaly detection is based on Dirichlet Normality score according to the softmax output from a classification network for the labels.</p><p>Deep Structured Energy-Based Models (DSEBMs) outputs an energy function which is the negative log probability that a sample follows the data distribution. The energy based model is connected to an autoencoder to avoid the need of complex sampling methods.</p><p>Deep Autoencoding Gaussian Mixture Model (DAGMM) is also a deep autoencoder model. It optimizes an end-to-end structure that contains both an autoencoder and an estimator for Gaussian Mixture Model. The anomaly detection is done after modeling the density function of the Gaussian Mixture Model.</p><p>Next, we review the definitions of the two metrics that we used: the AUC and AP scores <ref type="bibr" target="#b8">(Davis &amp; Goadrich, 2006)</ref>. In computing these metrics we identify the outliers as "positive". Both AUC and AP can be computed using the corresponding functions in the scikit-learn package <ref type="bibr" target="#b37">(Pedregosa et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F COMPARISON WITH RSR AND RCAE</head><p>We demonstrate basic properties of our framework by comparing it to two different frameworks. The first framework is direct RSR, which tries to model the inliers by a low-dimensional subspace, as opposed to the nonlinear model discussed in here. Based on careful comparison of RSR methods in <ref type="bibr" target="#b27">Lerman &amp; Maunu (2018)</ref>, we use the Fast Median Subspace (FMS) algorithm  and its normalized version, the Spherical FMS (SFMS). The other framework can be viewed a nonlinear version of RPCA, instead of RSR. It assumes sparse elementwise corruption of the data matrix, instead of corruption of whole data points, or equivalently, of some columns of the data matrix. For this purpose we use the Robust Convolutional Autoencoder (RCAE) algorithm of <ref type="bibr" target="#b5">Chalapathy et al. (2017)</ref>, who advocate it as "extension of robust PCA to allow for a nonlinear manifold that explains most of the data". We adopt the same network structures as in Section 4.1. <ref type="figure" target="#fig_6">Fig. 7</ref> reports comparisons of RSRAE, FMS, SFMS and RCAE on the datasets used in Section 4.2. We first note that both FMS and SFMS are not effective for the datasets we have been using. That is, the inliers in these datasets are not well-approximated by a linear model. It is also interesting to notice that without normalization to the sphere, FMS can be much worse than SFMS. That is, SFMS is often way more robust to outliers than FMS. This observation and the fact that there are no obvious normalization procedures a general autoencoder (see Section 5) clarifies why the mere use of the L 1 AE loss for an autoencoder is not expected to be robust enough to outliers. Comparing with RSRAE, we note that RCAE is not a competitive method for these datasets. This is not surprising since the model of RCAE, which assumes sparse elementwise corruption, does not fit well to the problem of anomaly detection, but to other problems, such as background detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G SENSITIVITY TO HYPERPARAMETERS</head><p>We examine the sensitivity of some of the reported results to changes in the hyperparameters. Section G.1 tests the sensitivity of RSRAE to changes in the intrinsic dimension d. Section G.2 tests the sensitivity of RSRAE to changes in the learning rate. Section G.3 tests the sensitivity of RSRAE+ to changes in λ 1 and λ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 SENSITIVITY TO THE INTRINSIC DIMENSION</head><p>In the experiments reported in Section 4 we fixed d = 10. Here we check the sensitivity of the reported results to changes in d. We use the same datasets of Section 4.2 with an outlier ratio of c = 0.5 and test the following values of d: <ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">5,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">12,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">30,</ref><ref type="bibr">40,</ref><ref type="bibr">50</ref>. <ref type="figure">Fig. 8</ref> reports the AUC and AP scores for these choice of d and for these datasets with c = 0.5. We note that, in general, our results are not sensitive to choices of d ≤ 30.</p><p>We believe that the structure of these datasets is complex, and is not represented by a smooth manifold of a fixed dimension. Therefore, low-dimensional encoding of the inliers is beneficial with various choices of low dimensions.</p><p>When d gets closer to D the performance deteriorates. Such a decrease in accuracy is noticeable for Reuters-21578 and 20 Newsgroups, where for both datasets D = 128. For the image data sets (without deep features) D = 1152 and thus only relatively small values of d were tested. As an example of large d for an image dataset, we consider the case of d = D = 1152 in Caltech101 with c = 0.5. In this case, AUC = 0.619 and AP = 0.512, which are very low scores.</p><p>We conclude that in our experiments (with c = 0.5), RSRAE was stable in d around our choice of d = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech 101</head><p>Fashion MNIST Tiny Imagenet Reuters-21578 20 Newsgroups <ref type="figure">Figure 8</ref>: AUC and AP scores for different choices of d. The datasets are the same as those in Section 4.2, where the outlier ratio is c = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 SENSITIVITY TO THE LEARNING RATE</head><p>In the experiments reported in Section 4 we fixed the learning rate for RSRAE to be 0.00025. Here we check the sensitivity of the reported results to changes in the learning rate. We use the same datasets of Section 4.2 with an outlier ratio of c = 0.5 and test the following values of the learning rate: 0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1. <ref type="figure">Fig. 9</ref> reports the AUC and AP scores for these values and for these datasets (with c = 0.5). We note that the performance is stable for learning rates not exceeding 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech 101 Fashion MNIST</head><p>Tiny Imagenet Reuters-21578 20 Newsgroups <ref type="figure">Figure 9</ref>: AUC and AP scores for various learning rates. The datasets are the same as those in Section 4.2, where the outlier ratio is c = 0.5.</p><p>G.3 SENSITIVITY OF RSRAE+ TO λ 1 AND λ 2</p><p>We study the sensitivity of RSRAE+ to different choices of λ 1 and λ 2 . We recall that RSRAE does not require these parameters. It is still interesting to check such sensitivity and find out whether careful tuning of these parameters in RSRAE+ can yield better scores than those of RSRAE. We use the same datasets of Section 4.2 with an outlier ratio of c = 0.5 and simultaneously test the following values of either λ 1 or λ 2 : 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0. Figs. 10 and 11 report the AUC and AP scores for these values and datasets (with c = 0.5). For each subfigure, the above values of λ 1 and λ 2 are recorded on the x and y axes, respectively. The darker colors of the heat map correspond to larger scores. For comparison, the corresponding AUC or AP score of RSRAE is indicated in the title of each subfigure.</p><p>We note that RSRAE+ is more sensitive to λ 1 than λ 2 . Furthermore, as λ 1 increases the scores are often more stable to changes in λ 1 . That is, the magnitudes of the derivatives of the scores with respect to λ 1 seem to generally decrease with λ 1 . In Section 4.3 we used λ 1 = λ 2 = 0.1 as this choice seemed optimal for the independent set of 20 Newsgroup. We note though that optimal hyperparameters depend on the dataset and it is thus not a good idea to optimize them using different datasets. They also depend on the choice of c, but for brevity we only test them with c = 0.5.</p><p>At last we note that the AUC and AP scores of RSRAE are comparable to the fine-tuned ones of RSRAE+ (where c = 0.5). We thus advocate using the alternating minimization of RSRAE, which is independent of λ 1 and λ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech 101</head><p>Fashion MNIST Tiny Imagenet  H RUNTIME COMPARISON <ref type="table" target="#tab_1">Table 1</ref> records runtimes for all the methods and datasets in Section 4.2 with the choice of c = 0.5. More precisely, a runtime is the the time needed to complete a single experiment, where 200 epoches were used for the neural networks. The table averages each runtime over the different classes.</p><p>Note that LOF, OCSVM and IF are faster than the rest of methods since they do not require training neural networks. We also note that the runtime of RSRAE is competitive in comparison to the other tested methods, that is, DSEBMs, DAGMM, and GT. The neural network structures of these four methods are the same, and thus the difference in runtime is mainly due to different pre and post processing.  <ref type="figure" target="#fig_1">Fig. 12</ref> presents the results for Tiny Imagenet without deep features. We see that RSRAE performs the best, but in general all the methods do not perform well. Indeed, the performance is significantly worse to that with deep features.</p><p>Tiny Imagenet <ref type="figure" target="#fig_1">Figure 12</ref>: AUC and AP scores for the Tiny Imagenet without using the deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 ADDITIONAL COMPARISON WITH VARIATIONS OF RSRAE</head><p>Figs. 13 and 14 extend the comparisons in Section 4.3 for additional datasets. The conclusion is the same. In general, RSRAE performs better by a large margin than AE and AE-1. On the other hand, RSRAE+ is often in between RSRAE and AE/AE-1. However, for 20 Newsgroups, RSRAE+ performs similarly to RSRAE, and possibly slightly better, than RSRAE. It seems that in this case our choice of λ 1 and λ 2 is good.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>AUC and AP scores for RSRAE using Caltech 101 and Fashion MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>AUC and AP scores for RSRAE using Tiny Imagenet with deep features, Reuters-21578 and 20 Newsgroups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>AUC and AP scores for RSRAE and alternative formulations using Caltech 101 and Reuters-21578.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Demonstration of the output of the encoder, RSR layer and decoder of RSRAE on a corrupted Swiss roll dataset. Demonstration of the output of the encoder, mapping by A, and decoder of AE on a corrupted Swiss roll dataset.(a) Error distribution for RSRAE. (b) Error distribution for AE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>AUC (area-under-curve) is the area under the Receiver Operating Characteristic (ROC) curve. Recall that the True Positive Rate (TPR), or Recall, is the number of samples correctly labeled as positive divided by the total number of actual positive samples. The False Positive Rate (FPR), on the other hand, is the number of negative samples incorrectly labeled as positive divided by the total number of actual negative samples. The ROC curve is a graph of TPR as a function of FPR. It is drawn by recording values of FPR and TPR for different choices of T in Algorithm 1. AP (average-precision) is the area under the Precision-Recall Curve. While Recall is the TPR, Precision is the number of samples correctly labeled as positive divided by the total number of predicted positives. The Precision-Recall curve is the graph of Precision as a function of Recall. It is drawn by recording values of Precision and Recall for different choices of T in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>AUC and AP scores for RSRAE, FMS, SFMS and RCAE. From top to bottom are the results using Caltech 101, Fashion MNIST, Tiny Imagenet with deep features, Reuters-21578 and 20 Newsgroups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>AUC and AP scores for RSRAE+ with various choices of λ 1 and λ 2 for Caltech 101, Fashion MNIST and Tiny Imagenet with deep features, where c = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>AUC and AP scores for RSRAE+ with various choices of λ 1 and λ 2 using Reuters-21578 and 20 Newsgroup, where c = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>AUC and AP scores for RSRAE and alternative formulations using Fashion MNIST and deep features of Tiny Imagenet, where c = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>AUC and AP scores for RSRAE and alternative formulations using Tiny Imagenet (images) and 20 Newsgroup, where c = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>by alternating minimization. It denotes the vectors of parameters of the encoder and decoder by θ and ϕ, respectively. Data {x (t) } N t=1 ; thresholds AE , RSR1 , RSR2 , T ; architecture and initial parameters of E , D, A (including number of columns of A); number of epochs &amp; batches; learning rate for backpropagation; similarity measure Output: Labels of data points as normal or anomalous 1: for each epoch do</figDesc><table><row><cell cols="2">Algorithm 1 RSRAE</cell></row><row><cell cols="2">Input: 2: Divide input data into batches</cell></row><row><cell>3:</cell><cell>for each batch do</cell></row><row><cell>4: 5:</cell><cell>if L 1 AE (θ, A, ϕ) &gt; AE then Backpropagate L 1</cell></row><row><cell>6:</cell><cell>end if</cell></row><row><cell>7:</cell><cell>if L 1 RSR1 (A) &gt; RSR1 then</cell></row><row><cell>8: 9:</cell><cell>Backpropagate L 1 RSR1 (A) w.r.t. A &amp; update A end if</cell></row><row><cell>10:</cell><cell>if L 1 RSR2 (A) &gt; RSR2 then</cell></row><row><cell>11: 12:</cell><cell>Backpropagate L 1 RSR2 (A) w.r.t. A &amp; update A end if</cell></row><row><cell>13:</cell><cell>end for</cell></row><row><cell cols="2">14: end for</cell></row><row><cell cols="2">15: for t = 1, . . . , N do</cell></row><row><cell>16:</cell><cell>Calculate similarity between x (t) andx (t)</cell></row><row><cell>17:</cell><cell>if similarity ≥ T then</cell></row><row><cell>18:</cell><cell>x (t) is normal</cell></row><row><cell>19:</cell><cell></cell></row></table><note>AE (θ, A, ϕ) w.r.t. θ, A, ϕ &amp; update θ, A, ϕ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Runtime comparison: runtimes (in seconds) are reported for all methods and datasets in Section 4.2, where the outlier ratio is c = 0.5. Since GT was only applied to the image datasets without deep features, its runtime is not available (N/A) for the last three datasets.We include some supplementary numerical results. In Section I.1 we show the results for Tiny Imagenet without deep features. In Section I.2 we extend the results reported in section 4.3 for the other datasets.I.1 TINY IMAGENET WITHOUT DEEP FEATURES</figDesc><table><row><cell>Benchmarks</cell><cell cols="5">Datasets Caltech 101 Fashion MNIST Tiny Imagenet Reuters-21578 20 Newsgroups</cell></row><row><cell>LOF</cell><cell>0.233</cell><cell>7.163</cell><cell>0.707</cell><cell>25.342</cell><cell>10.516</cell></row><row><cell>OCSVM</cell><cell>0.120</cell><cell>3.151</cell><cell>0.473</cell><cell>8.726</cell><cell>4.169</cell></row><row><cell>IF</cell><cell>0.339</cell><cell>1.485</cell><cell>0.511</cell><cell>20.481</cell><cell>6.751</cell></row><row><cell>GT</cell><cell>21.681</cell><cell>87.729</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>DSEBMs</cell><cell>14.293</cell><cell>46.933</cell><cell>25.194</cell><cell>41.083</cell><cell>33.852</cell></row><row><cell>DAGMM</cell><cell>21.066</cell><cell>71.632</cell><cell>41.211</cell><cell>83.551</cell><cell>60.720</cell></row><row><cell>RSRAE</cell><cell>6.305</cell><cell>33.853</cell><cell>10.940</cell><cell>32.061</cell><cell>18.869</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our implementation is available at https://github.com/dmzou/RSRAE.git 2 https://github.com/izikgo/AnomalyDetectionTransformations</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make swiss roll.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research has been supported by NSF award DMS18-30418. Part of this work was pursued when Dongmian Zou was a postdoctoral associate at the Institute for Mathematics and its Applications at the University of Minnesota. We thank Teng Zhang for his help with proving Proposition 5.1 (we discussed a related but different proposition with similar ideas of proofs). We thank Madeline Handschy for commenting on an earlier version of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhancing one-class support vector machines for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatallah</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slim</forename><surname>Abdennadher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description</title>
		<meeting>the ACM SIGKDD Workshop on Outlier Detection and Description</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speial Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/arjovsky17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering and unsupervised anomaly detection with l 2 normalized deep auto-encoder representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksu</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2018.8489068</idno>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LOF: identifying densitybased local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM sigmod record</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust, deep and inductive anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindafm</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Analysis of and on uniformly rectifiable sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Semmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">38</biblScope>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
	<note>Mathematical surveys and monographs</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Goadrich</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1143844.1143874</idno>
		<ptr target="http://doi.acm.org/10.1145/1143844.1143874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R1-PCA: rotational invariant l 1 -norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer vision and Image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9781" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">152173</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifiable sets and the traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invent Math</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Outlier detection for text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyenkyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haesun</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<idno type="DOI">https:/epubs.siam.org/doi/abs/10.1137/1.9781611974973.55</idno>
		<ptr target="https://epubs.siam.org/doi/abs/10.1137/1.9781611974973.55" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Novelty detection with GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Fleishman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy7EPh10W" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantifying curvelike structures of measures by using L 2 Jones quantities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<idno>0010-3640</idno>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1294" to="1365" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast, robust and non-convex subspace recovery. Information and Inference: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Maunu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="336" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An overview of robust subspace recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Maunu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1380" to="1410" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">l p -recovery of the most significant subspace among multiple subspaces with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="385" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust computation of linear models by convex relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="363" to="410" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reuters-21578 text categorization test collection. Distribution 1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs-Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">OLÉ: Orthogonal low-rank embedding-a plug and play geometric loss for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Musé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8109" to="8118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Isolation-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Robust subspace recovery with adversarial outliers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Maunu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.03275" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A well-tempered landscape for non-convex robust subspace recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Maunu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two proposals for robust PCA using semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1123" to="1160" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robust PCA for anomaly detection in cyber networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Paffenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Servi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01571</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mining of massive datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>David Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A novel anomaly detection scheme based on principal component classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanoksri</forename><surname>Sarinnapakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM Foundation and New Direction of Data Mining workshop</title>
		<meeting>ICDM Foundation and New Direction of Data Mining workshop</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilona</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02997</idno>
		<title level="m">q-space novelty detection with variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Static and dynamic robust PCA and matrix completion: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namrata</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1359" to="1379" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Some Problems in Orthogonal Distance and Non-Orthogonal Distance Regression. Defense Technical Information Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Watson</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=WKKWGwAACAAJ" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2080" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust PCA via outlier pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIT.2011.2173156</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3047" to="3064" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gaurav Manek, and Vijay Ramaseshan Chandrasekhar. Efficient GAN-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houssam</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Sheng Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Lecouat</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkXADmJDM" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A novel M-estimator for robust PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="749" to="808" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Median K-flats for hybrid linear modeling with many outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>IEEE 12th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Randy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJJLHbb0" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
