<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Adequacy of Untuned Warmup for Adaptive Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Booth School of Business</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Commerce</orgName>
								<orgName type="institution">U.S. Patent and Trademark Office</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
							<email>denisyarats@cs.nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Adequacy of Untuned Warmup for Adaptive Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adaptive optimization algorithms such as Adam are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, recent work proposes automatic variance rectification of Adam's adaptive learning rate, claiming that this rectified approach ("RAdam") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we refute this analysis and provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. We then provide some "rule-ofthumb" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over 2/(1 − β2) training iterations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stochastic gradient-based optimization serves as the workhorse training approach for many classes of parametric models, including neural networks. Stochastic gradient descent and its various first-order cousins <ref type="bibr" target="#b28">(Polyak 1964;</ref><ref type="bibr" target="#b23">Nesterov 1983</ref>) have enabled numerous advances in deep learning across domains <ref type="bibr" target="#b15">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b10">He et al. 2016;</ref><ref type="bibr" target="#b7">Gehring et al. 2017</ref>). More recently, adaptive optimization algorithms have become prevalent in training the largest deep learning models. These adaptive methods, which include Adagrad <ref type="bibr" target="#b6">(Duchi, Hazan, and Singer 2010)</ref>, RMSProp <ref type="bibr" target="#b12">(Hinton, Srivastava, and Swersky 2012)</ref>, and Adam <ref type="bibr" target="#b14">(Kingma and Ba 2014)</ref>, scale the step size for each individual parameter based on various gradient moments.</p><p>Many practitioners have adopted the Adam algorithm for general-purpose use; notably, the preponderance of recent state-of-the-art results in natural language processing <ref type="bibr" target="#b5">(Devlin et al. 2018;</ref><ref type="bibr" target="#b29">Radford et al. 2019;</ref><ref type="bibr" target="#b19">Liu et al. 2019b;</ref><ref type="bibr" target="#b2">Brown et al. 2020</ref>) have employed Adam, demonstrating the algorithm's Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Appendix available at https://arxiv.org/abs/1910.04209. ability to effectively train neural networks with parameter counts from 100 million to several billion. In these large-scale settings, Adam's global learning rate is usually annealed with a "warmup schedule" which promotes early-stage training stability by regulating the size of the parameter updates. The prevalent warmup schedule is a simple linear warmup, in which the global learning rate starts at zero and increases by a constant at each iteration until reaching its intended value. <ref type="bibr">1</ref> The parameters of these warmup schedules are typically tuned for each problem setting and model. <ref type="bibr" target="#b17">Liu et al. (2020)</ref> performed an analysis of Adam with warmup, concluding that Adam requires a warmup schedule to mitigate the large or divergent variance of the perparameter scale term. They then propose the rectified Adam ("RAdam") algorithm, which automatically corrects for this high variance. <ref type="bibr">Liu et al.</ref> highlight the robustness of RAdam, noting in particular that RAdam reduces or eliminates the need for tuning warmup schedules when using Adam. RAdam has been applied to domains including generative modeling <ref type="bibr" target="#b33">(Yamamoto, Song, and Kim 2020)</ref>, natural language processing <ref type="bibr" target="#b24">(Nguyen and Salazar 2019)</ref>, and video retrieval <ref type="bibr" target="#b18">(Liu et al. 2019a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>Our contributions in this work are as follows:</p><p>Reexamining RAdam and the variance-based motivation for warmup We dive into the inner operation of RAdam and find that it is precisely Adam with a fixed warmup schedule, with the only deviation being to perform four iterations of heavy-ball momentum <ref type="bibr" target="#b28">(Polyak 1964)</ref> at the outset. We then argue that the variance-based motivation for warmup is impaired as it overlooks the correlation between the first and second moment estimators, which is crucial for understanding the actual parameter updates applied by Adam.</p><p>Analyzing Adam's early-stage update magnitudes Shifting focus from gradients to parameter updates, we then perform a simulation-based analysis on the magnitudes of Adam's parameter updates. We find that even at a simulated local minimum of the objective, Adam exhibits considerable non-regularity in its early-stage parameter updates, shedding light on why Adam may require learning rate warmup to a greater extent than first-order optimization methods.</p><p>Demonstrating the sufficiency of untuned warmup We provide some simple and intuitive "rule-of-thumb" warmup schedules for Adam, all of which require no tuning. As our main empirical result, we demonstrate that these schedules result in substantively identical performance and training dynamics to those of RAdam across a wide range of models, problem settings, and hyperparameters, indicating that any claimed benefits can be achieved with lower complexity using off-the-shelf optimization tools. As a sensible untuned default, we recommend linear warmup over 2 · (1 − β 2 ) −1 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We begin with notation and a brief review of stochastic gradient descent and Adam. Primitives θ ∈ R p denotes a vector of model parameters. L(θ) : R p → R denotes a loss function to be minimized over the model parameters.L(θ) : R p → R denotes an unbiased approximator of the loss function (e.g. over a minibatch). ∇L(θ) and ∇L(θ) denote the gradient of L(θ) andL(θ), respectively. The terms θ,L(θ), and ∇L(θ) are subscriptable by t ≥ 0, the optimization time step ("training iteration"). θ 0 represents the initial model parameters.</p><p>We write optimization algorithms as per-iteration procedures ("update rules"), taking the basic form:</p><formula xml:id="formula_0">θ t ← θ t−1 − { . . . } "update step"</formula><p>Stochastic gradient descent The SGD algorithm, parameterized by learning rate α &gt; 0, performs the following procedure at each iteration t:</p><formula xml:id="formula_1">θ t ← θ t−1 − α · ∇L t−1 (θ t−1 )<label>(1)</label></formula><p>Adam The Adam algorithm <ref type="bibr" target="#b14">(Kingma and Ba 2014)</ref>, parameterized by global learning rate α &gt; 0, discount factors β 1 , β 2 ∈ (0, 1), and stability constant &gt; 0, performs the following procedure at each iteration t:</p><formula xml:id="formula_2">m t ← β 1 · m t−1 + (1 − β 1 ) · ∇L t−1 (θ t−1 ) (2) v t ← β 2 · v t−1 + (1 − β 2 ) · ∇L t−1 (θ t−1 ) 2 (3) θ t ← θ t−1 − α (1 − β t 1 ) −1 · m t (1 − β t 2 ) −1 · v t +<label>(4)</label></formula><p>where m, v ∈ R p denote auxiliary memory (interpretable as first moment and second moment estimators of ∇L t , respectively). By convention, m 0 = v 0 = 0. Warmup schedules For any optimization algorithm parameterized with a learning rate α, a warmup schedule ω can be applied. ω is a sequence of "warmup factors" ω t ∈ [0, 1], which serve to dampen the step size of each iteration t. Specifically, a warmup schedule is imposed by replacing α with α t = α · ω t in the algorithm's update rule.</p><p>Perhaps the most common functional form for the schedule is linear warmup, parameterized by a "warmup period" τ :</p><formula xml:id="formula_3">ω linear,τ t = min 1, 1 τ · t (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rectified Adam</head><p>The RAdam algorithm <ref type="bibr" target="#b17">(Liu et al. 2020)</ref>, parameterized identically to Adam, performs the following procedure at each iteration t:</p><formula xml:id="formula_4">ρ ∞ ← 2/(1 − β 2 ) − 1 (6) ρ t ← ρ ∞ − 2t · β t 2 /(1 − β t 2 ) (7) ω t ← (ρ t − 4)(ρ t − 2)ρ ∞ (ρ ∞ − 4)(ρ ∞ − 2)ρ t (8) m t ← β 1 · m t−1 + (1 − β 1 ) · ∇L t−1 (θ t−1 ) (9) v t ← β 2 · v t−1 + (1 − β 2 ) · ∇L t−1 (θ t−1 ) 2 (10) θ t ← θ t −    α · (1 − β t 1 ) −1 · m t ρ t ≤ 4 α · ω t · (1−β t 1 ) −1 ·mt √ (1−β t 2 ) −1 ·vt+ ρ t &gt; 4<label>(11)</label></formula><p>3 Rectified Adam, Adaptive Variance, and Update Steps</p><p>We begin by uncovering the precise behavior of RAdam, before delving into its underlying variance-based motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RAdam: Perform 4 Iterations of Momentum SGD, Then Use Adam with Fixed Warmup</head><p>Liu et al. describe RAdam as having two modes of operation: "divergent variance" and "convergent variance", corresponding respectively to the cases ρ t ≤ 4 and ρ t &gt; 4 in Equation 11. In the "divergent" phase, RAdam performs a variant of heavy-ball momentum SGD <ref type="bibr" target="#b28">(Polyak 1964</ref>). 2 Then, in the "convergent" phase, RAdam performs Adam, with the learning rate scaled down by ω t . However, this is not dynamic scaling based on the trainingtime behavior of the optimizer or the distribution of the gradients. Rather, ω t is a deterministic function of solely t and β 2 . Thus, the "convergent" phase is simply Adam with a fixed warmup schedule. We find that for all practically relevant values of β 2 , the condition ρ t ≤ 4 is simply t ≤ 4: Fact 3.1. Assume that 0.8 ≤ β 2 &lt; 1 and t is a positive integer. Then, for ρ t as defined in Equation 7:</p><formula xml:id="formula_5">ρ t ≤ 4 ⇐⇒ t ≤ 4</formula><p>Proof. See Appendix B.1.  On its face, using four iterations of momentum at the beginning of training seems arbitrary. In preliminary experimentation (including the experimental settings described in Section 5), we performed ablations over the following options for these four initial iterations: • Do absolutely nothing. • Use Adam with learning rate α · ω 5 (i.e. do exactly what RAdam does at the fifth iteration). • Use Adam with linear warmup to α · ω 5 (i.e. gradually warm up the learning rate to RAdam's fifth iteration). As expected for a decision affecting only four training iterations, the practical difference between these choices is uniformly negligible. Thus, the only possible benefit of RAdam stems from its custom warmup schedule ω t for the fifth iteration and beyond. We revisit this topic in Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variance-Based Motivation for RAdam and Warmup</head><p>Given the arbitrary nature of RAdam's operation, we proceed to investigate the motivation for RAdam, which Liu et al. also identify as the underlying motivation for warmup's crucial role in Adam. Liu et al. focus their principal analysis on the term 1−β t 2 vt . Fixing = 0, this term can be interpreted as Adam's "adaptive learning rate", which scales the global learning rate for each parameter before computing Adam's final update for that parameter. They identify that the quantity Var</p><formula xml:id="formula_6">1−β t 2 vt</formula><p>does not exist during the first few training iterations, 3 and even after converging to a finite value, continues to remain elevated for some time.</p><p>Perhaps the most immediate observation is that early-stage gradients are not zero-mean. In fact, at the beginning of optimization, the expected magnitude of the gradients ∇L t (θ t ) (i.e. absolute value of the deterministic gradients ∇L(θ t )) should dominate the gradient variance, since a randomlyinitialized model is exceedingly unlikely to be near a local minimum of L(θ t ). Indeed, on a demonstration training run of a feed-forward network on the EMNIST digit recognition task, we observe that the median coefficient of variation of the gradients <ref type="figure" target="#fig_1">(Figure 1a</ref>) starts at below 1, indicating that for most parameters, the expected value of the gradient exceeds the standard deviation during early-stage training. Only beyond training iteration 50 does the coefficient of variation consistently exceed 1. Relaxing the zero-mean assumption decreases Var</p><formula xml:id="formula_7">1−β t 2 vt</formula><p>considerably. 4 More important, however, is that m t and v t are not at all independent. <ref type="figure" target="#fig_1">Figure 1b</ref> reveals that in the EMNIST setting, the absolute value of the first moment estimator (|m t |) is extremely correlated with the square root of the second moment estimator ( √ v t ). Since Adam's parameter updates are proportional to m t / √ v t , high correlation between these two quantities implies that the magnitude of the updates are quite regular, despite the high variance of 1 vt . Indeed, during the first training iteration (t = 1), it is guaranteed that |m t | =</p><p>√ v t for all parameters, making all Adam parameter updates either −α or α (assuming = 0). Thus, even though Var</p><formula xml:id="formula_8">1−β t 2 vt</formula><p>is divergent, the magnitude of the parameter updates themselves are constant. Ironically, it is precisely when the adaptive learning rate's variance is "divergent" that the actual parameter update magnitudes have zero variance. This suggests that the adaptive learning rate may not be the best medium of analysis for understanding the role of warmup in Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">High Initial Update Step Magnitudes Necessitate Warmup in Adam</head><p>We provide an alternative view of the frequent necessity of learning rate warmup when using Adam. We do so by directly investigating the magnitudes of the update steps, perhaps the most proximate determinant of training stability. In stochastic gradient descent, parameter updates are simply the gradients multiplied by the learning rate. Warmup for SGD can thus be motivated as mitigating the large expected magnitudes of the gradients (directly proportional to update magnitudes) and rapid change in gradients at the beginning of training <ref type="bibr" target="#b9">(Goyal et al. 2017;</ref><ref type="bibr" target="#b8">Gotmare et al. 2019)</ref>. Similar logic can be employed for adaptive methods.</p><p>On the other hand, if a model's gradients have near-zero means and low gradient variances, the update steps are similarly well-regulated and optimization via SGD can be stable without any learning rate warmup. For example, a nearlyconverged model (thus having near-zero expected gradients and low gradient magnitudes) trained via SGD can have its optimization be stably restarted without learning rate warmup. This is not the case with Adam. We proceed to computationally analyze the magnitude of Adam's update step over the course of training. Specifically, we demonstrate via simulation that even when the model parameters θ t are initialized at an idealized local minimum of L(θ) (i.e. ∇L t (θ t ) has zero mean and is i.i.d. across time), the magnitude of Adam's update steps will still be quite high at the start of training, only gradually decaying toward a stationary distribution.</p><p>Simulation configuration All gradients are simulated as i.i.d. normal variables with zero mean and constant isotropic variance 10 −9 , thus approximating the optimization dynamics at an exact local minimum of L(θ). <ref type="bibr">5</ref> We sample independent gradient trajectories (each 1000 iterations long) for 25000 parameters. We then run the Adam optimizer with these sampled gradients and evaluate the distribution of the update step magnitudes (before multiplication by the global learning rate α) at each iteration. The Adam optimizer configuration is β 1 = 0.9, β 2 = 0.999, and = 0.</p><p>Simulation results <ref type="figure" target="#fig_2">Figure 2</ref> depicts the outcome of this computational simulation. As alluded to in Section 3.2, the update magnitudes for all parameters start at 1 · α. The update magnitudes gradually decay but continue to remain high for quite some time, only beginning to settle into a stationary distribution after 40 or so training iterations (with median update magnitude ≈ 0.16·α). We extend the trajectory length to 10000 and find that the median update step of the stationary distribution is approximately 0.153 · α.</p><p>These results imply that unlike SGD, Adam will always encounter early-stage training instability by way of large update magnitudes, even when the model is already initialized at a local minimum. This stands as a contributing factor to Adam's need for learning rate warmup above and beyond that of first-order methods.</p><p>Comparison to real-world, random initialization settings Finally, we examine the update step distribution of a model initialized away from a local minimum of L(θ). <ref type="figure" target="#fig_1">Figure 1c</ref> depicts the median parameter update magnitudes of Adam in the EMNIST setting from Section 3.2. We observe a qualitative similarity to the local minimum simulation results -the update magnitudes start at 1 · α, only gradually settling into a stationary distribution around 0.15 · α.</p><p>Note that the EMNIST optimization decreases more slowly in update magnitude and takes longer (≈ 100 training iterations) to settle into the stationary distribution. This suggests that the update step non-regularity observed in the idealized local minimum initialization setting is only exacerbated in the more realistic setting of random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rules of Thumb</head><p>Turning to the practical application of learning rate warmup, we first define a simple heuristic function, the effective warmup period, to characterize the dampening effect of warmup schedules. We then present and intuitively motivate two Adam warmup schedules that require no tuning and are thus usable as rules of thumb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effective Warmup Period</head><p>We define the effective warmup period T (ω) of a warmup schedule ω as follows:</p><formula xml:id="formula_9">T (ω) = ∞ t=1 (1 − ω t )</formula><p>Intuitively, this is the sum of the warmup's dampening effect across all of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exponential Warmup</head><p>We propose a simple "exponential warmup" schedule based on a decaying exponential and a constant τ :</p><formula xml:id="formula_10">ω expo,τ t = 1 − exp − 1 τ · t<label>(12)</label></formula><p>The constant τ is analogous to a linear warmup period, and we recommend τ = (1 − β 2 ) −1 as a rule of thumb:</p><formula xml:id="formula_11">ω expo,untuned t = 1 − exp (−(1 − β 2 ) · t)<label>(13)</label></formula><p>In choosing τ , our guiding (albeit extremely speculative) intuition is to have the warmup factor ω expo,τ t be roughly  equivalent to Adam's second moment bias correction term in Adam. This term, 1 − β t 2 , is the sum of the coefficients in the moving average estimation of the second moment, and can thus be interpreted as how "complete" the second moment estimator is at any given point in time. We briefly show the approximate correspondence between the bias correction term and the warmup factor: 6</p><formula xml:id="formula_12">1 − β t 2 = 1 − exp (log(β 2 ) · t) ≈ 1 − exp ((β 2 − 1) · t) = 1 − exp (−(1 − β 2 ) · t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linear Warmup</head><p>Recall the formulation of linear warmup:</p><formula xml:id="formula_13">ω linear,τ t = min 1, 1 τ · t</formula><p>As a similar rule of thumb to the exponential warmup schedule, we suggest performing linear warmup over τ = 2 · (1 − β 2 ) −1 iterations:</p><formula xml:id="formula_14">ω linear,untuned t = min 1, 1 − β 2 2 · t<label>(14)</label></formula><p>Our choice of τ is carried over from exponential warmup as a starting point. To preserve the same effective warmup period, the τ from the exponential rule-of-thumb is multiplied by 2 to account for the fact that exponential warmup decelerates over time, whereas linear warmup does not. We elaborate in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with RAdam</head><p>We first compare RAdam with the rule-of-thumb schedules (Equations 13 and 14) by computing their effective warmup periods across a range of β 2 values. 7 <ref type="figure" target="#fig_4">Figure 3a</ref> reveals that the effective warmup periods of RAdam and the rules of thumb are nearly identical across all practical values of β 2 , indicating that they have similar dampening effects over earlystage training.</p><p>We then proceed to examine the trajectory of the warmup schedule for the commonly used setting of β 2 = 0.999. <ref type="figure">Figure</ref> 3b reveals that the functional forms of the warmup factors are qualitatively similar in magnitudes. The warmup schedules for RAdam and the rule-of-thumb exponential warmup closely correspond in shape as well.</p><p>We thus posit that RAdam and the untuned rule-of-thumb warmup schedules are more or less interchangeable. An empirical verification follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate untuned exponential warmup (Equation 13), untuned linear warmup (Equation 14), and RAdam across a variety of supervised machine learning tasks. For brevity, all experimental settings are summarized in the main text and comprehensively detailed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Classification</head><p>Using each of the three warmup methods, we train a ResNet-50 model <ref type="bibr" target="#b10">(He et al. 2016</ref>) on the ILSVRC ("ImageNet") image classification dataset with various configurations of Adam. Specifically, we sweep over:</p><p>α (learning rate) ∈ 10 −4 , 10 −3 , 10 −2 β 2 ∈ {0.99, 0.997, 0.999}  We next examine the course of optimization for individual configurations of Adam's α and β 2 . <ref type="figure" target="#fig_5">Figure 4</ref> depicts the training loss using the popular "default" Adam configuration of learning rate α = 10 −3 and β 2 = 0.999, revealing that the behavior of these warmup methods is indeed nearly indistinguishable.</p><p>Appendix C.1 provides both training and validation metrics <ref type="figure">(Figures 7 and 8</ref> respectively) for all tested configurations, reinforcing this trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Modeling</head><p>Using each of the three warmup methods, we train a stateof-the-art Transformer-based language model from Baevski and Auli (2018) on WIKITEXT-103. We sweep over the following grid of Adam hyperparmeters: α (learning rate) ∈ 1 · 10 −4 , 3 · 10 −4 , 5 · 10 −4 β 2 ∈ {0.99, 0.998, 0.999} with β 1 = 0.9 and = 10 −7 fixed. As with image classification, we observe in <ref type="table" target="#tab_1">Table 2</ref> that the choice of warmup method has a minimal impact on training across different hyperparameters. <ref type="figure" target="#fig_6">Figure 5</ref> depicts the validation perplexity throughout training for the best Adam parametrization (α = 10 −4 and β 2 = 0.999), which similarly supports the indistinguishability of the warmup methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR</head><p>β2 Exponential Linear RAdam 1 · 10 −4 0.99 21.0 ± 0.1 21.0 ± 0.1 21.1 ± 0.1 1 · 10 −4 0.998 19.9 ± 0.0 19.9 ± 0.0 20.0 ± 0.0 1 · 10 −4 0.999 20.0 ± 0.0 20.0 ± 0.0 20.1 ± 0.1 3 · 10 −4 0.99</p><p>21.3 ± 0.3 20.8 ± 0.1 22.4 ± 0.0 3 · 10 −4 0.998 19.6 ± 0.0 19.6 ± 0.0 19.6 ± 0.1 3 · 10 −4 0.999 19.5 ± 0.0 19.5 ± 0.0 19.5 ± 0.0 5 · 10 −4 0.99</p><p>24.4 ± 2.4 24.1 ± 1.4 26.0 ± 1.8 5 · 10 −4 0.998 20.1 ± 0.0 20.0 ± 0.0 20.1 ± 0.0 5 · 10 −4 0.999 19.8 ± 0.0 19.7 ± 0.1 19.7 ± 0.0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Machine Translation</head><p>Finally, we evaluate the warmup methods on a large scale machine translation task. Using each of the three warmup methods, we train a Transformer model <ref type="bibr" target="#b32">(Vaswani et al. 2017)</ref> on the WMT16 English-German ("EN-DE") dataset. We fix Adam's β 1 = 0.9 and = 10 −7 and sweep over the following grid of Adam hyperparameters:</p><p>α (learning rate) ∈ 5 · 10 −5 , 8 · 10 −5 , 1 · 10 −4 β 2 ∈ {0.98, 0.99, 0.998, 0.999}</p><p>We observe no perceptible differences between the warmup methods in either final performance <ref type="table">(Table 3)</ref>, or in the training-time metrics of a single canonical configuration (α = 10 −4 and β 2 = 0.999, shown in <ref type="figure" target="#fig_7">Figure 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We discuss various consequences of our findings, along with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Extended Warmup Periods</head><p>The analysis of the update step magnitudes in Section 3.3 suggests shorter warmup periods than those typically used in practice. For example, using the setting of β 2 = 0.999,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR β2</head><p>Exponential Linear RAdam 5 · 10 −5 0.98 24.5 ± 0.1 24.4 ± 0.1 24.4 ± 0.1 5 · 10 −5 0.99 24.5 ± 0.0 24.5 ± 0.0 24.5 ± 0.1 5 · 10 −5 0.998 24.3 ± 0.2 24.4 ± 0.2 24.4 ± 0.1 5 · 10 −5 0.999 24.2 ± 0.1 24.2 ± 0.1 24.1 ± 0.1 8 · 10 −5 0.98</p><p>25.9 ± 0.1 25.9 ± 0.1 25.9 ± 0.1 8 · 10 −5 0.99</p><p>25.9 ± 0.2 25.9 ± 0.1 25.9 ± 0.0 8 · 10 −5 0.998 26.0 ± 0.1 25.2 ± 1.0 25.9 ± 0.1 8 · 10 −5 0.999 25.7 ± 0.1 25.8 ± 0.1 25.7 ± 0.0 1 · 10 −4 0.98</p><p>26.5 ± 0.1 26.6 ± 0.1 26.6 ± 0.1 1 · 10 −4 0.99 26.7 ± 0.1 26.6 ± 0.1 26.6 ± 0.0 1 · 10 −4 0.998 25.9 ± 0.9 26.5 ± 0.1 26.6 ± 0.0 1 · 10 −4 0.999 26.2 ± 0.2 26.4 ± 0.0 26.4 ± 0.0 <ref type="table">Table 3</ref>: BLEU score of Transformer on WMT16-EN-DE (means and standard deviations over 3 random seeds). Adam's update magnitudes in the theoretical model converge to a stationary distribution in roughly 40 iterations. If update magnitudes were the only relevant consideration, then a warmup schedule over a few hundred iterations would suffice to stabilize training. In contrast, the effective warmup periods of both RAdam and our rule-of-thumb schedules are roughly 1000 iterations for β 2 = 0.999. State-of-the-art methods with hand-tuned warmup schedules often go well beyond, using up to 10000 iterations of linear warmup in some cases <ref type="bibr" target="#b19">(Liu et al. 2019b;</ref><ref type="bibr" target="#b0">Baevski and Auli 2018;</ref><ref type="bibr" target="#b25">Ott et al. 2019)</ref>. Accordingly, we surmise that the precise channel by which Adam necessitates an extended period of warmup is still an unresolved question, likely related to the properties of the gradients at random initialization. Future work could rigorously investigate the effect of extended warmup periods on the training dynamics of Adam, beyond simple per-iteration statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Consequences of Update Step Invariance to Gradients</head><p>One ancillary finding of Section 3.3 is that the magnitudes of Adam's update steps during later stages of training are largely invariant to the properties or dynamics of the gradient dis-tribution -both the simulated local optimum and real-world random initialization settings result in convergence to similar stationary distributions of update magnitudes. This suggests that learning rate decay at later stages of training could be the only way to improve late-stage convergence, as Adam's late-stage update magnitudes do not appear to be very sensitive to the variance or stationarity of gradients. In particular, we suspect that variance-based methods of improving the late-stage convergence of SGD, such as increasing the batch size <ref type="bibr" target="#b31">(Smith et al. 2018)</ref>, will not yield comparable benefits when applied to Adam, as the stationary distribution of the update magnitudes will remain largely the same. Partially adaptable methods <ref type="bibr" target="#b3">(Chen and Gu 2018;</ref><ref type="bibr" target="#b13">Keskar and Socher 2017;</ref><ref type="bibr" target="#b21">Luo et al. 2019)</ref>, which interpolate between the full adaptivity of Adam and the non-adaptivity of SGD, may hold more promise for improving late-stage convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dynamic Warmup</head><p>All methods considered by this work use fixed warmup schedules, computed only as a function of the training iteration t and various hyperparameters. Such schedules will inevitably be brittle to some combination of problem setting, model, and optimizer configuration. Another direction for future work could be to devise truly dynamic mechanisms for scheduling warmup in Adam. Such a mechanism could (among other things) track and utilize auxiliary statistics, such as the running moments of the applied updates, in order to determine the stability of training at each iteration. This direction comes dangerously close to seeking the "holy grail" of an automatic learning rate tuner; existing attempts to devise such a method have achieved limited adoption as of yet <ref type="bibr" target="#b16">(Li, Tai, and E 2017;</ref><ref type="bibr" target="#b34">Zhang, Mitliagkas, and Ré 2017;</ref><ref type="bibr" target="#b1">Baydin et al. 2018)</ref>. What makes this potentially more tractable is that a maximum learning rate is still tuned and given a priori to the optimizer; the task is then restricted to dynamic scheduling of the learning rate from zero to this known constant, instead of an arbitrary range (0, ∞).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We show that the Rectified Adam (RAdam) algorithm can be characterized as four steps of momentum SGD, followed by Adam with a fixed warmup schedule. We also examine the shortcomings of a variance-based approach to analyzing the learning rate warmup heuristic, and we illustrate that Adam's frequent need for learning rate warmup can be partially explained by inspecting Adam's early-stage update step magnitudes when applied to an already-converged model.</p><p>RAdam's claimed benefits are its superior performance to Adam and its elimination of costly warmup schedule tuning. We obviate RAdam by providing two simple "rule-of-thumb" warmup schedules for Adam, both of which require no tuning. Linear warmup of Adam's learning rate over 2 · (1 − β 2 ) −1 iterations is functionally equivalent to RAdam across a wide range of settings. Hence, we suggest that practitioners considering the need for untuned warmup of Adam's learning rate first try linear warmup over 2 · (1 − β 2 ) −1 training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Full details of experimental setup A.1 System configuration</head><p>All experiments are performed using Python 3.7 and PyTorch version 1.2 <ref type="bibr" target="#b27">(Paszke et al. 2017)</ref> compiled with CUDA 10, on Ubuntu 18.04 systems containing 8 NVIDIA V100 GPUs each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Image classification</head><p>Experimentation is performed using the ILSVRC 2012 1000-class dataset ("ImageNet"; <ref type="bibr" target="#b30">Russakovsky et al. 2015</ref>) and a 50-layer convolutional residual network model <ref type="bibr">("ResNet-50";</ref><ref type="bibr" target="#b10">He et al. 2016)</ref>. The implementation follows that of <ref type="bibr">Paszke et al. (2016), 9</ref> with the only deviations being to enable alternative optimizer configurations, to enable intermediate metric logging, and to drop the last batch from each training epoch.</p><p>Training occurs over 90 epochs, with ten-fold learning rate decay after epochs 30 and 60. The minibatch size is 1024. The optimization objective is cross-entropy, with a decoupled weight decay <ref type="bibr" target="#b20">(Loshchilov and Hutter 2019)</ref> of 10 −4 .</p><p>Data augmentation includes horizontal flipping at random, as well as random 224-pixel crops. Validation is performed on 224-pixel center crops.</p><p>For Adam and RAdam, the following hyperparameters are fixed: β 1 = 0.9 and = 10 −8 . All other Adam parameters (warmup schedule, learning rate α, and β 2 ) are enumerated via parameter sweep as described in Section 5.1. Each Adam configuration is independently trained with 5 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Language modeling</head><p>We evaluate the state-of-the-art, Transformer-based language model described in <ref type="bibr" target="#b0">(Baevski and Auli 2018)</ref> on the WIKITEXT-103 dataset, consisting of 100M tokens with a size-260K vocabulary. We leverage the author's implementation provided in fairseq <ref type="bibr" target="#b7">(Gehring et al. 2017;</ref><ref type="bibr" target="#b25">Ott et al. 2019)</ref>, and train on 8 GPUs with half-precision floating point.</p><p>Our experimentation setup closely follows <ref type="bibr" target="#b0">(Baevski and Auli 2018)</ref>, except that we sweep over Adam parameters, such as warmup schedule, learning rate α, and β 2 , while keeping β 1 = 0.9 and = 10 −7 fixed (both for Adam and RAdam). The hyperparameter grid is presented in Section 5.2. Each Adam configuration is independently trained with 3 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Machine translation</head><p>Our setup employs a state-of-the-art Transformer model <ref type="bibr" target="#b32">(Vaswani et al. 2017)</ref> implemented in fairseq <ref type="bibr" target="#b25">(Ott et al. 2019)</ref>. We train on the WMT16 English-German large machine translation dataset, and evaluate on the newstest14 validation set.</p><p>As observed in <ref type="bibr" target="#b22">(Ma and Yarats 2019)</ref>, these state-of-the-art large-scale models are fragile to train with Adam and require either a carefully chosen optimization procedure, or robust optimizers that can sustain gradients with large variance, such as QHAdam <ref type="bibr" target="#b22">(Ma and Yarats 2019)</ref>. To eliminate this factor from our studies, we choose to lower the learning rate α to stabilize training, taking a marginal performance hit in training. Apart from that, our experimentation setup is identical to the one in <ref type="bibr" target="#b25">(Ott et al. 2019)</ref>.</p><p>We fix Adam parameters β 1 = 0.9 and = 10 −7 , and sweep over the warmup schedule, learning rate α, and β 2 , as described in Section 5.3. We again use half-precision floating point and train on 8 GPUs. As <ref type="bibr" target="#b25">(Ott et al. 2019</ref>) trains on 128 GPUs, we accumulate gradients over 16 minibatches before each optimization step to achieve an identical configuration. The BLEU score is averaged over 3 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Gradient analysis workhorse: EMNIST digit classification</head><p>The EMNIST digit classification task <ref type="bibr" target="#b4">(Cohen et al. 2017</ref>) serves as the workhorse for our gradient analysis studies. Our model is a simple feed-forward neural network with three hidden layers (sizes 200, 100, and 50) and uniform weight initialization with range inversely proportional to the square root of layer sizes.</p><p>Optimization is performed on the cross-entropy objective with the Adam optimizer. The Adam configuration is α = 10 −3 , β 1 = 0.9, β 2 = 0.999, = 10 −8 , and decoupled weight decay 10 −4 . The minibatch size is 256, Training occurs over 10000 training iterations. At each training iteration, 256 backwards passes are performed with independently sampled batches to collect a sample of the gradient distribution. Due to the cost of storing and analyzing the gradients of all parameters, we randomly sample 500 parameters from each weight matrix and only collect gradients for the sampled parameters. These samples are used to approximate the distribution of the gradient coefficients of variation. After the 256 backwards passes, one final pass is performed as a regular optimization step to update the model parameters and proceed to the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Miscellaneous derivations</head><p>This appendix provides miscellaneous informal derivations of statements in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Number of RAdam momentum iterations</head><p>Fact 3.1. Assume that 0.8 ≤ β 2 &lt; 1 and t is a positive integer. Then, for ρ t as defined in Equation 7:</p><formula xml:id="formula_15">ρ t ≤ 4 ⇐⇒ t ≤ 4</formula><p>Proof. We define ρ(t, β 2 ) to be the continuous version of ρ t , parameterized over both t and β 2 :</p><formula xml:id="formula_16">ρ(t, β 2 ) = 2 1 − β 2 − 1 − 2 · t · β t 2 1 − β t 2</formula><p>We then differentiate with respect to t:</p><formula xml:id="formula_17">∂ρ(t, β 2 ) ∂t = 2 · β t 2 · β t 2 − 1 − t · ln β 2 (1 − β t 2 ) 2 ∂ρ(t,β2) ∂t</formula><p>is thus positive for all t &gt; 0. We also differentiate with respect to β 2 , and take specific values thereof:</p><formula xml:id="formula_18">∂ρ(t, β 2 ) ∂β 2 = 2 · 1 (1 − β 2 ) 2 − t 2 · β t−1 2 (1 − β t 2 ) 2 ∂ρ(4, β 2 ) ∂β 2 = 2 · 1 (1 − β 2 ) 2 − 16 · β 3 (1 − β 4 2 ) 2 ∂ρ(5, β 2 ) ∂β 2 = 2 · 1 (1 − β 2 ) 2 − 25 · β 4 2 (1 − β 5 2 ) 2 ∂ρ(t,β2) ∂β2</formula><p>is thus positive for all β 2 ∈ (0, 1) at t = 4 and t = 5. Then, we take lim 2 · 4 · β 3 2 + 3 · β 2 2 + 2 · β 2 + 1 (1 + β 2 )(1 + β 2 2 ) − 1 = 5 − 1 = 4</p><p>Combining this result with the fact that ∂ρ(4,β2) ∂β2 is positive for β 2 ∈ (0, 1), it follows that ρ(4, β 2 ) &lt; 4 for all β 2 ∈ (0, 1).</p><p>Then, since ∂ρ(t,β2) ∂t &gt; 0 for all t &gt; 0, we have that ρ(t, β 2 ) &lt; 4 for all β 2 ∈ (0, 1) and t ∈ (0, 4]. We have thus shown that t ≤ 4 =⇒ ρ t ≤ 4 for positive integers t.</p><p>In the reverse direction, we evaluate ρ(5, 0.8):</p><p>ρ(5, 0.8) = 2 1 − 0.8 − 1 − 2 · 5 · 0.8 5 1 − 0.8 5 ≈ 9 − 4.87 ≈ 4.14 Similarly combining this result with the fact that ∂ρ(5,β2) ∂β2 is positive for β 2 ∈ (0, 1), then with the fact that ∂ρ(t,β2) ∂t &gt; 0 for all t &gt; 0, we have that ρ(t, β 2 ) 4.14 for all t ≥ 5, β 2 ∈ [0.8, 1). We have thus shown that t &gt; 4 =⇒ ρ t &gt; 4 for positive integers t, completing the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Thus follows a layman's description of RAdam: 1. Perform four iterations of heavy-ball momentum. 2. At iteration five and beyond, use Adam with a fixed warmup schedule. (a) Median coefficient of variation of gradients (calculated over 256 trials). (b) Pearson correlation between Adam's |m t | and √ v t . (c) Median parameter update magnitude ( = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Analysis of gradients and updates during the training of a simple feed-forward network on the EMNIST digit recognition task with the Adam optimizer -see Appendix A.5 for comprehensive details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Distribution of Adam's update step magnitudes at a simulated local minimum of L(θ) (quantiles:{2.5%, 25%, 50%, 75%, 97.5%}).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Effective warmup periods of RAdam and rule-of-thumb warmup schedules, as a function of β 2 .(b) RAdam and rule-of-thumb warmup schedules over time for β 2 = 0.999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of various characteristics of RAdam and rule-of-thumb warmup schedules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Mean training loss (5 seeds) of ResNet-50 on Imagenet, using Adam with α = 10 −3 and β 2 = 0.999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Mean validation perplexity (3 seeds) of Transformer LM on WIKITEXT-103, using Adam with α = 10 −4 and β 2 = 0.999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Mean validation perplexity (3 seeds) of Transformer on WMT16-EN-DE, using Adam with α = 10 −4 and β 2 = 0.999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>0.99 34.2% ± 0.1 34.2% ± 0.1 34.2% ± 0.1 10 −4 0.997 34.3% ± 0.2 34.2% ± 0.2 34.1% ± 0.1 10 −4 0.999 34.5% ± 0.1 34.4% ± 0.1 34.2% ± 0.3 10 −3 0.99 27.9% ± 0.1 28.0% ± 0.1 28.4% ± 0.1 10 −3 0.997 27.9% ± 0.1 27.9% ± 0.1 28.3% ± 0.1 10 −3 0.999 28.2% ± 0.1 28.3% ± 0.1 28.4% ± 0.1 10</figDesc><table><row><cell>presents the top-1 error rates at the end of training</cell></row><row><cell>for the three warmup methods. Across all configurations of</cell></row><row><cell>Adam, the top-1 error rates are indistinguishable between the</cell></row><row><cell>warmup methods. 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Validation perplexity of a Transformer LM on the WIKITEXT-103 dataset (means and standard deviations over 3 random seeds).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Linear warmup has also been deployed for first-order optimization -see, for example,<ref type="bibr" target="#b9">Goyal et al. (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The departure from standard heavy-ball momentum is in the bias correction by (1 − β t 1 ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The authors approximate1−β t 2 v tas having a scaled inverse χ 2 distribution, under the assumption that (1) all gradients are i.i.d. and zero-mean, and (2) a simple average approximates an exponential moving average.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Although Liu et al. do not comment on the relative magnitudes of Var 1−β t 2 v t, theirFig. 9reveals that coefficients of variation below 1 dampen that quantity by an order of magnitude or more.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that the behavior of Adam in this setting is invariant to the choice of variance constant.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The second step follows from a first-order Taylor expansion of log(β2) around β2 = 1. In practice, this approximation is extremely accurate for typical values of β2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For the purpose of this analysis, w {1,2,3,4} are all defined to be zero for RAdam.8  The best error rates fall roughly 3% behind those from SGD, as is typical with Adam on computer vision tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Commit hash ee964a2.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>B.2 Linear warmup period <ref type="bibr">(rule-of-thumb)</ref> We desire for the effective warmup period to be roughly equivalent between the exponential and linear rule-of-thumb schedulesthat is, T (w expo,untuned ) ≈ T (w linear,τ ). Solving approximately for τ : </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adaptive Input Representations for Neural Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1809.10853</idno>
		<ptr target="http://arxiv.org/abs/1809.10853" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Online Learning Rate Adaptation with Hypergradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cornish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkrsAzWAb" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="CoRRabs/2005.14165.URLhttps://arxiv.org/abs/2005.14165" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno>abs/1806.06763</idno>
		<ptr target="http://arxiv.org/abs/1806.06763" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EMNIST: Extending MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2017.7966217</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2017.7966217" />
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-14" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265" />
	</analytic>
	<monogr>
		<title level="m">COLT 2010 -The 23rd Conference on Learning Theory</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06-27" />
			<biblScope unit="page" from="257" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/gehring17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r14EOsCqKX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Accurate, Large Minibatch SGD: Training Im-ageNet in 1 Hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="CoRRabs/1706.02677.URLhttp://arxiv.org/abs/1706.02677" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>doi:10</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<idno>/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<title level="m">Neural networks for machine learning: Lecture 6a</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving Generalization Performance by Switching from Adam to SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1712.07628</idno>
		<ptr target="http://arxiv.org/abs/1712.07628" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Im-ageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12-03" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/li17f.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="2101" to="2110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgz2aEKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Use What You Have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://bmvc2019.org/wp-content/uploads/papers/0363-paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference 2019, BMVC 2019</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2019-09-09" />
			<biblScope unit="page">279</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive Gradient Methods with Dynamic Bound of Learning Rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg3g2R9FX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quasi-hyperbolic momentum and Adam for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1fUpoR5FQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transformers without Tears: Improving the Normalization of Self-Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<idno>abs/1910.05895</idno>
		<ptr target="http://arxiv.org/abs/1910.05895" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/examples" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t Decay the Learning Rate, Increase the Batch Size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1Yy1BxCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="CoRRabs/1706.03762.URLhttp://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parallel Wave-GAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">YellowFin and the Art of Momentum Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno>abs/1706.03471</idno>
		<ptr target="http://arxiv.org/abs/1706.03471" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
