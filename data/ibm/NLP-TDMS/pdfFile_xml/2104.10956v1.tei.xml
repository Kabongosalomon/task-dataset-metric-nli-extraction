<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
							<email>pangjiangmiao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of 2D detection and Monocular 3D Object Detection. Given an input RGB image, a 2D anchor-free detector needs to predict the distance between a foreground point to four sides of the 2D bounding box, while a monocular 3D anchor-free detector needs to regress the center location, 3D size and orientation of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging compared to conventional 2D case due to its inherent ill-posed property, which is mainly reflected on the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this technical report, we study this problem with a practice built on fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to image domain and decouple it as 2D and 3D attributes. Then the objects are distributed to different feature levels with the consideration of their 2D scales and assigned only according to the projected 3D-center for training procedure. Furthermore, the center-ness is redefined with a 2D Guassian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a fundamental problem in computer vision. It aims to identify objects of interest in the image and predict their categories and corresponding 2D bounding boxes. With the rapid progress of deep learning, 2D object detection has been well explored in recent years. Various models such as Faster R-CNN <ref type="bibr" target="#b24">[25]</ref>, RetinaNet <ref type="bibr" target="#b17">[18]</ref>, and FCOS <ref type="bibr" target="#b27">[28]</ref> greatly promote the progress of the field and benefit various practical applications like autonomous driving.</p><p>However, 2D information is not enough for an intelligent agent to perceive the 3D real world. For example, when an autonomous vehicle needs to run smoothly and safely on the road, it must have the accurate 3D information of objects around it to make secure decisions. Therefore, 3D object detection is becoming increasingly important in these robotic applications. Most state-of-the-art methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> rely on the accurate 3D information provided by LiDAR point clouds, but it is a heavy burden to install expensive LiDARs on each vehicle. So monocular 3D object detection, as a simple and cheap setting for deployment, becomes a much meaningful research problem nowadays.</p><p>Considering monocular 2D and 3D object detection have the exactly same input but different outputs, a straightforward solution for monocular 3D object detection is following the practices in the 2D domain but adding extra components to predict the additional 3D attributes of the objects. Some previous work <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref> keeps the prediction of 2D boxes and further regresses 3D attributes on top of 2D centers and region of interests. Others <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> simultaneously predict 2D and 3D boxes with 3D priors corresponding to each 2D anchor. Another stream of methods based on redundant 3D information <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> could predict extra keypoints for optimized results ultimately. In a word, the core underlying challenge is how to assign 3D targets to 2D domain with the 2D-3D correspondence and predict them afterwards.</p><p>In this technical report, we adopt a simple yet efficient method to enable a 2D detector predict 3D localization. We first project the commonly defined 7-DoF 3D locations onto the 2D image and get the projected center point, which we name it as 3D-center compared to the previous 2D-center. With this projection, the 3D-center actually contains 2.5D information, i.e., 2D location and its corresponding depth. The 2D location can be further reduced to the 2D offset from a certain point on the image, which serves as the only 2D attribute that can be normalized among different feature levels like in the 2D detection. In comparison, depth, 3D size and orientation are regarded as 3D attributes after decoupling. In this way, we transform the 3D targets with a center-based paradigm and avoid any necessary 2D detection or 2D-3D correspondence priors.</p><p>As a practical implementation, we build our method on FCOS <ref type="bibr" target="#b27">[28]</ref>, a simple anchor-free fully convolutional singlestage detector. We first distribute the objects to different feature levels with consideration of their 2D scales. Then the regression targets of each training sample are assigned only according to the projected 3D centers. In contrast to FCOS that denote the center-ness with the distances to the boundaries, we denote the 3D center-ness with a 2D Guassian distribution based on the 3D-center.</p><p>We evaluate our method on a popular large-scale dataset, nuScenes <ref type="bibr" target="#b2">[3]</ref> and achieved the 1st place on the camera track of this benchmark without any prior information. Moreover, we only need 2x less computing resources to train a baseline model with performance comparable to the previous best open-source method, CenterNet <ref type="bibr" target="#b33">[34]</ref>, in one day, also 3x faster than it. Both show that our framework is simple and efficient. Detailed ablation studies show the importance of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D Object Detection Research on 2D object detection has made great progress with the breakthrough of deep learning approaches. Modern methods can be divided into two branches, anchor-based and anchor-free, according to the base of initial guesses. Anchor-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref> benefit from the predefined anchors in terms of much easier regression while have many hyper-parameters to tune. In contrast, anchor-free methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34]</ref> do not need these prior settings and are thus neater with better universality. For simplicity, this paper takes FCOS, a representative anchor-free detector, as the baseline considering its capability of handling overlapped ground truths and scale variance problem.</p><p>From another perspective, monocular 3D detection is a more difficult task closely related to 2D detection. But there is few work investigating the connection and difference between them, which makes them isolated and not able to benefit from the advancement of each other. This report aims at taking the adaptation of FCOS as the example and further building a closer connection between these two tasks. Monocular 3D Object Detection Monocular 3D detection is more difficult compared to conventional 2D detection. The underlying key problem is the inconsistency of input 2D data modal and the output 3D predictions. Methods involving sub-networks Earlier work uses subnetworks to assist 3D detection. 3DOP <ref type="bibr" target="#b3">[4]</ref> and ML-Fusion <ref type="bibr" target="#b30">[31]</ref> use a depth estimation network while Deep3DBox <ref type="bibr" target="#b20">[21]</ref> uses a 2D object detector. They rely on the design and performance of these sub-networks, even external data and pretrained models, which makes the training inconvenient and introduces additional system complexity. Transform to 3D representation Another category is to convert the RGB input to other representations like OFT-Net <ref type="bibr" target="#b25">[26]</ref> and Pseudo-Lidar <ref type="bibr" target="#b29">[30]</ref>. Although these methods have shown promising performance, they actually still rely on dense depth labels and hence are not regarded as pure monocular approaches. There is also a domain gap between different depth sensors and LiDARs, which makes it hard to generalize to a new practical setting smoothly. Furthermore, the efficiency of processing a large amount of point clouds is also a significant issue to deal with when applying these methods to cases in the real world. End-to-end design like 2D detection Recent work notices these drawbacks and end-to-end frameworks are thus proposed. M3D-RPN <ref type="bibr" target="#b0">[1]</ref> implements a single-stage multi-class detector with an end-to-end region proposal network and depth-aware convolution. SS3D <ref type="bibr" target="#b12">[13]</ref> proposes to detect 2D key points and further predicts object characteristics with uncertainties. MonoDIS <ref type="bibr" target="#b26">[27]</ref> introduces a disentangling loss to reduce the instability of training procedure. Some of them still have multiple training stages or hand-crafted post-optimization phase, and all of these methods follow anchor-based manners, thus the consistency of 2D and 3D anchors needed to be determined. In contrast, anchor-free methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5]</ref> do not need to make statistics on the given data and can be better generalized to more complicated cases with more various classes or different intrinsic settings, so we choose to follow this paradigm.</p><p>Nevertheless, all of these works hardly study the key difficulty when applying a general 2D detector to monocular 3D detection. What should be kept or leveraged and what should be adjusted or focused on in this procedure are seldom discussed when proposing their new frameworks. In contrast, this technical report just concentrates on this point, which could provide a reference when applying a typical 2D detector framework to a closely related task. On this basis, <ref type="figure">Figure 2</ref>: An overview of our pipeline. To leverage the well-developed 2D feature extractors, we basically follow the typical design of backbone and neck for 2D detectors. For detection head, we first reformulate the 3D targets with center-based paradigm to decouple it as multi-task learning. The strategies for multi-level target assignment and center sampling are further adjusted accordingly to equip this framework with the better capability of handling overlapped ground truths and scale variance problem. a more in-depth understanding of the connection and difference of these two tasks will also be beneficial to further research of both communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Object detection is one of the most essential and challenging problems for scene understanding. Conventional 2D object detection expects the given model to predict 2D bounding boxes and category labels for each object of interest. Compared to it, monocular 3D detection needs us to predict 3D bounding boxes instead, which needs to be decoupled and transformed to 2D image plane as much as possible. In this section, we will first present an overview of our framework with our adopted reformulation of 3D targets, and then elaborate two corresponding technical designs, 2D guided multi-level 3D prediction and center sampling strategy, tailored to this task, which together make the 2D detector FCOS work in this 3D task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework Overview</head><p>A fully convolutional one-stage detector typically consists of three components: backbone for feature extraction, necks for multi-level branches construction and detection heads for dense predictions. Then we briefly introduce each of them. Firstly, we use ResNet101 <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> with deformable convolutions <ref type="bibr" target="#b6">[7]</ref> for feature extraction, which achieves a good trade-off between accuracy and efficiency in our experiments. We fixed parameters of the first convolutional block to avoid more memory overhead. The second module is the Feature Pyramid Network <ref type="bibr" target="#b16">[17]</ref>, which is a basic component for detecting objects at different scales. For clear clarification, we denote feature maps from level 3 to 7 as P3 to P7 as shown in the <ref type="figure">Figure 2</ref>. We follow the approach in the original FCOS to obtain P3 to P5 and simply downsample P5 with two convolutional blocks to obtain P6 and P7. All of these five feature maps are responsible for predictions of different scales afterwards. Finally for shared detection heads, there are two important issues to be dealt with. The first is how to assign targets to different levels of feature maps and different points, which is one of the core problems for different detectors and will be presented in the next subsection. The second is how to design the architecture. We follow the conventional design of RetinaNet <ref type="bibr" target="#b17">[18]</ref> and FCOS <ref type="bibr" target="#b27">[28]</ref>. Each shared head consists of 4 shared convolutional blocks and small heads for various regression targets. There are several alternative designs for the setting of small heads, which will be discussed in the experiments. From our experience, disentangled heads for targets with different measurements could benefit the training procedure. It appears more important for regression targets, including offset, depth, orientation, size and velocity. So we set one small head for each of them. So far, we have introduced the overall design of our framework. Next, let's formulate this problem more formally and present the detailed training and inference procedure. Regression Targets Let's first recall the formulation of anchor-free manners for object detection in FCOS. Given a feature map at layer i of the backbone, denoted as F i ∈ <ref type="figure">Figure 3</ref>: Our exploited rotation encoding scheme. Two objects with opposite orientation share the same rotation offset based on the 2-bin boundary, thus have the same sin value. To distinguish them, we predict an additional direction class from the regression branch.</p><p>R H×W ×C , we need to predict objects based on each point on this feature map, which correspond to uniformly distributed points on the original input image. Formally, for each location (x, y) on the feature map F i , suppose the total stride until layer i is s, then the corresponding location on the original image should be (sx + s 2 , sy + s 2 ). Different from anchor-based detectors, which regress targets by taking predefined anchors as reference, we directly predict objects based on these locations. Moreover, because we do not rely on anchors, the criterion for judging whether a point is foreground or not will no longer be the IOU between anchors and ground truths. Instead, as long as the point is near the box center enough, it could be a foreground point. In the 2D case, the model just needs to regress the distance of the point to top/bottom/left/right side, denoted as t, b, l, r in the <ref type="figure">Fig. 1</ref>. However, in the 3D case, it is non-trivial to regress the distance to six faces of the 3D bounding box. Instead, a more straightforward implementation is to convert the commonly defined 7-DoF regression targets to the 2.5D center and 3D size, in which 2.5D center can be easily transformed back to 3D space with camera intrinsic matrix. Regressing the 2.5D center could be further reduced to regressing the offset from the center to a specific foreground point, ∆x, ∆y, and its corresponding depth d respectively. In addition, to predict the allocentric orientation of the object, we follow the way in <ref type="bibr" target="#b31">[32]</ref> and divide it into two parts: angle θ with period π and 2-bin direction classification. The first component naturally models the IOU of our predictions with the ground truth boxes while the second component focuses on the adversarial case where two boxes have opposite orientations. Thanks to this angle encoding, our method surpasses another center-based framework, CenterNet, in terms of orientation accuracy, which will be compared in the experiments. The rotation encoding scheme is illustrated in <ref type="figure">Fig. 3</ref>. In addition to these regression targets related to the location and orientation of objects, we also regress a binary target c, namely center-ness, like FCOS. It serves as a soft binary classifier to determine which points are closer to centers, and helps suppress those low-quality predictions far away from object centers. More details are presented in Sec. 3.3.</p><p>To sum up, the regression branch needs to predict ∆x, ∆y, d, w, l, h, θ, v x , v y , direction class C θ and centerness c while the classification branch needs to output the class label of the object and its attribute label. Loss For classification and different regression targets, we define their loss respectively and take their weighted summation as the total loss. Firstly, for classification branch, we use the commonly used focal loss <ref type="bibr" target="#b17">[18]</ref> for object classification loss:</p><formula xml:id="formula_0">L cls = −α(1 − p) γ logp<label>(1)</label></formula><p>where p is the class probability of a predicted box, and we follow the settings, α = 0.25 and γ = 2, of the original paper. For attribute classification, we use a simple softmax classification loss, denoted as L attr .</p><p>For regression branch, we use smooth L1 loss for each regression targets except center-ness with corresponding weights considering their scales:</p><formula xml:id="formula_1">L loc = b∈(∆x,∆y,d,w,l,h,θ,vx,vy) SmoothL1(∆b)<label>(2)</label></formula><p>where the weight of ∆x, ∆y, w, l, h, θ error is 1, the weight of d is 0.2 and the weight of v x , v y is 0.05. It should be noted that although we employ exp(x) for depth prediction, we still compute the loss in the original depth space instead of the log space, which leads to more accurate detection in terms of object locations ultimately. We use the softmax classification loss and binary cross entropy (BCE) loss for direction classification and center-ness regression, denoted as L dir and L ct respectively. Finally, the total loss is:</p><formula xml:id="formula_2">L = 1 N pos (β cls L cls +β attr L attr +β loc L loc +β dir L dir +β ct L ct )<label>(3)</label></formula><p>where N pos is the number of positive predictions and β cls = β attr = β loc = β dir = β ct = 1.</p><p>Inference For inference procedure, given a input image, we just need to forward it through the framework and obtain bounding boxes with their class scores, attribute scores and center-ness predictions. We multiply the class score and center-ness as the confidence for each prediction and conduct rotated Non-Maximum Suppression (NMS) in the bird view as most 3D detectors to get the final results. Note that there are some transformations in the process, like rotation decoding and projecting the 2.5D center back to 3D space, which are basically inverse procedures of data preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D Guided Multi-Level 3D Prediction</head><p>In FCOS, the author has discussed two most important issues of target assignment: 1) Best Possible Recall (BPR) for anchor-free and anchor-based detectors, 2) Intractable ambiguity caused by overlaps of ground-truth boxes. The first problem has been well verified by the comparison in the original paper, which shows that the implementation of multi-level prediction through FPN can really improve BPR, and even achieve better results than anchor-based methods. Similarly, the conclusion of this problem is also applicable in our adapted framework. The second question will involve the specific setting of the regression target, which we will discuss next. In the original FCOS, we detect objects with different sizes in different levels of feature maps. Different from anchorbased methods, instead of assigning anchors with different sizes, FCOS directly assigns ground-truth boxes with different sizes to different levels of feature maps. Formally, it first computes the 2D regression targets, l*, r*, t*, b* for each location at each feature level, then for the location satisfying max(l * , r * , t * , b * ) &gt; m i or max(l * , r * , t * , b * ) &lt; m i−1 , it would be regarded as a negative sample, where m i denotes the maximum regression range for feature level i 1 . In comparison, we also follow this criterion in our implementation considering the scale of 2D detection is directly consistent with how large region we need to focus on. However, we only use 2D detection for filtering meaningless targets in this assignment step. After completing target assignment, our regression targets only include 3D related ones. Note that here we generate the 2D bounding boxes by computing the minimum and maximum coordinate of 8 projected vertices of 3D bounding boxes, so we do not need any 2D detection annotations or priors. Next, let us discuss how to deal with the ambiguity problem, i.e., when a point is inside multiple ground truth boxes in the same feature level, which box should be assigned to it. The usual way is to select according to the area of the 2D bounding box. The box with smaller area is selected as the target box for this point. We call this scheme as the area-based criterion. It is obvious that large objects will be paid less attention by such processing, which is also verified by our experiments <ref type="figure" target="#fig_0">(Figure 4)</ref>. Therefore, considering that the target of our regression is center-based, and the points closer to the center of the object can obtain more comprehensive and balanced local region features, so as to produce higher quality prediction, we propose a distance-based criterion, i.e., select the box with closer center as the regression target. Through simple verification <ref type="figure" target="#fig_0">(Figure 4</ref>), we find that this scheme significantly improves the best possible recall (BPR) and mAP of large objects, and also significantly improves the overall mAP (about 1%), which will be presented in the ablation study. In addition to center-based approach to deal with ambiguity, we also use the 3D-center to determine foreground points, i.e., only the points near the center enough will be regarded as positive samples. We define a hyper-parameter, radius, to measure this central portion. The points with distance smaller than radius × stride to the object center would be considered as positive, where radius is set to 1.5 in our experiments. Finally, we also follow the approach in original FCOS for distinguishing heads of different feature level, i.e., replacing each output x of different regression branches with s i x, where s i is a trainable scalar used for adjusting the base of exponential function for feature level i, which also brings a minor improvement in terms of detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Center-ness with 2D Guassian Distribution</head><p>In the original design of FCOS, center-ness c is defined by 2D regression targets, l*, r*, t*, b*:</p><formula xml:id="formula_3">c = min(l * , r * ) max(l * , r * ) × min(t * , b * ) max(t * , b * )<label>(4)</label></formula><p>Because our regression targets are changed to the 3D center-based paradigm, so we define center-ness by 2D Gaussian distribution with projected 3D center as the origin. The 2D Gaussian distribution is simplified as:</p><formula xml:id="formula_4">c = e −α((∆x) 2 +(∆y) 2 )<label>(5)</label></formula><p>where α is used to adjust the intensity attenuation from the center to the periphery and set to 2.5 in our experiments. We take it as the ground truth of center-ness and predict it from the regression branch for filtering low-quality predictions later. As mentioned earlier, this center-ness target ranges from 0 to 1, so we use the Binary Cross Entropy (BCE) loss for training that branch. We evaluate our framework on a large-scale commonly used dataset, nuScenes <ref type="bibr" target="#b2">[3]</ref>. NuScenes dataset consists of multi-modal data collected from 1000 scenes, which includes RGB images from 6 surrounding cameras, points from 5 Radars and 1 LiDAR. It is split into 700/150/150 scenes for training/validation/testing. There are overall 1.4M annotated 3D bounding boxes from 10 categories. Considering the variety of scenes and ground truths, it is becoming one of the most convincing benchmarks for 3D object detection. Therefore, we take it as the platform to validate the efficacy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For fair comparison with other methods, we use the official metrics, distance-based mAP and NDS, which are given by the benchmark. Next, we briefly introduce these two kinds of metrics as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Precision metric</head><p>Average Precision (AP) metric is generally used when evaluating performance of object detectors. Here instead of using Intersection over Union (IoU) for thresholding, nuScenes defines the match by 2D center distance d on the ground plane for decoupling detection from object size and orientation. On this basis, we calculate AP by computing the normalized area under the precision recall curve for recall and precision over 10%. Finally, mAP is computed over all matching thresholds, D = {0.5, 1, 2, 4} meters, and all categories C:</p><formula xml:id="formula_5">mAP = 1 |C||D| c∈C d∈D AP c,d<label>(6)</label></formula><p>True Positive metrics Apart from Average Precision, we also calculate 5 kinds of True Positive metrics, Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE). To obtain these measurements, we firstly define that predictions with center distance from the matching ground truth d ≤ 2m will be considered as true positives (TP). Then matching and scoring are conducted independently for each class of objects, and each metric is the average of cumulative mean at each recall levels above 10%. ATE is the Euclidean center distance in 2D (m). ASE is equal to 1 − IOU , IOU is calculated between predictions and labels after aligning their translation and orientation. AOE is the smallest yaw angle difference between predictions and labels (radians). Note that different from other classes measured on the full 360 • period, barrier is measured on 180 • period. AVE is the L2-Norm of the absolute velocity error in 2D (m/s). AAE is defined as 1 − acc, where acc refers to the attribute classification accuracy. Finally, given these metrics, we compute the mean TP metric (mTP) overall all categories:</p><formula xml:id="formula_6">mT P = 1 |C| c∈C T P c<label>(7)</label></formula><p>Note that some not well defined metrics will be omitted, like AVE for cones and barriers considering they are stationary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NuScenes Detection Score</head><p>The conventional mAP couples the evaluation of locations, sizes and orientations of detections and also could not capture some aspects in this setting like velocity and attributes, so this benchmark proposes a more comprehensive, decoupled but simple metric, nuScenes detection score (NDS):</p><formula xml:id="formula_7">N DS = 1 10 [5mAP + mT P ∈TP (1 − min(1, mT P ))] (8)</formula><p>where mAP is mean Average Precision (mAP) and TP is the set composed of five True Positive metrics. Considering mAVE, mAOE and mATE can be larger than 1, a bound is applied to them to limit them between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Network Architectures As shown in the <ref type="figure">Figure 2</ref>, our framework basically follows the design of FCOS. Given the  input image, we utilize ResNet101 as the feature extraction backbone followed by Feature Pyramid Networks (FPN) for generating multi-level predictions. Detection heads are shared among multi-level feature maps except that three scale factors are used to differentiate some of their final regressed results, including offsets, depths and sizes, respectively. Final small heads after the four shared convolutional layers for each regression targets are simply convolutional layers with kernel size and stride 1. All the convolutional modules are made up of basic convolution, batch normalization and activation layers, and normal distribution are leveraged for weights initialization. The overall framework is built on top of MMDetection3D <ref type="bibr" target="#b5">[6]</ref>.</p><p>Training Parameters For all experiments, we trained randomly initialized networks from scratch following endto-end manners. Models are trained with SGD optimizer, in which gradient clip and warm-up policy are exploited with learning rate 0.001, number of warm-up iterations 500, warm-up ratio 0.33 and batch size 32 on 16 GTX 1080Ti GPUs. Finally, to achieve a stable training procedure at the beginning, our baseline model is trained with weight 0.2 for depth regression. For a more competitive performance and a more accurate detector, we finetune our model with this weight switched to 1. Related results are presented in the ablation study. Data Augmentation Like previous work, we only implement image flip for data augmentation both when training and testing. Note that when flipping images, only offset is needed to be flipped as 2D attributes while 3D boxes need to be transformed correspondingly in 3D space. For test time augmentation, we average the score maps output by the detection heads except rotation and velocity related scores due to their inaccuracy. It is empirically a more efficient approach for augmentation than merging boxes at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we present experimental results quantitatively and qualitatively, and make a detailed ablation study in terms of important factors in the procedure of pushing our method towards the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Analysis</head><p>First, we give the results of quantitative analysis, which are shown in Tab. 1. We compare the results on the test set and validation set respectively. On the test set, we first compared with all the methods using RGB image as the input data. We achieved the best performance among them with mAP 0.358 and NDS 0.428, in which particularly we exceeded the previous best method more than 2% in terms of mAP. Benchmarks using LiDAR data as the input include PointPillars <ref type="bibr" target="#b13">[14]</ref>, which are faster and lighter, and CBGS <ref type="bibr" target="#b35">[36]</ref> (MEGVII in the Tab. 1) with relatively high performance. For the approaches which use the input of RGB image and Radar mixed data, we select CenterFusion <ref type="bibr" target="#b21">[22]</ref> as the benchmark. It can be seen that although our method has a certain gap with the high-performance CBGS, it even surpasses PointPillars and CenterFusion on mAP, which shows that we can solve this ill-posed prob- <ref type="figure">Figure 5</ref>: Qualitative analysis of detection results. 3D bounding boxes predictions are projected onto images from six different views and bird-view respectively. Boxes from different categories are marked with different colors. From left part, we can see the results are reasonable except some detection with false class predictions. Moreover, a few small objects are detected by our model while not annotated as ground truth, like barriers in the back/back right camera. However, apart from the intrinsic occlusion problem in this setting, there still exists noticeable inaccuracy in terms of depth and rotation predictions, which can be observed in the visualization from bird view. lem decently with enough data. At the same time, it can be seen that the methods using other modal of data have relatively better NDS, mainly because the mAVE is smaller. The reason is that other methods will introduce continuous multi-frame data, such as point cloud data from consecutive frames, so as to predict the speed of objects. The Radar itself also has the function of velocity measurement, so CenterFusion can achieve reasonable speed prediction even with a single frame image. However, these can not be achieved only by using a single frame image, so how to mine the speed information from consecutive frame images will be one of the directions that can be explored in the future. For detailed mAP for each category, please refer to Tab. 2 and the official benchmark. On the validation set, we compare our method with the best open-source center-based detector, CenterNet. Their method not only takes about 3 days to train (compared with our only 1 day to achieve comparable performance possibly thanks to our pretrained backbone to some extent), but also is inferior to our method except for mATE. In particular, thanks to our rotation encoding scheme, we have achieved a significant improvement in the accuracy of angle prediction. The significant improvement of mAP reflects the superior-ity of our multi-level prediction. On the basis of common improvement in these aspects, we finally achieved an improvement of about 9% on NDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Analysis</head><p>Then we show some qualitative results in the <ref type="figure">Fig. 5</ref> and 6 to give an intuitive understanding of the performance of our model. First of all, in the <ref type="figure">Fig. 5</ref>, we draw the 3D bounding boxes we predicted in the six-view images and the top view point clouds. We can see that from the perspective of image, our prediction results are very appealing. Especially for some small objects that are not labeled, such as the barriers in the camera at the rear right, they are not labeled, but are detected by our model. But at the same time, we should also see that our method still has obvious problems in depth estimation and identification of occluded objects. For example, it is difficult to detect the blocked car in the left rear image. Moreover, from the top view, especially in depth estimation, the results are not as good as those shown in the image. This is also in line with our expectation that depth estimation is still the core challenge in this ill-posed problem. In the <ref type="figure">Fig. 6</ref>, we show some failure cases, mainly focused <ref type="figure">Figure 6</ref>: Failure cases. As shown in this figure, our detectors perform poorly especially for occluded and large objects. We use yellow dotted circle to mark the failure case caused by occlusion, while use red dotted circle to mark the inaccurate large objects predictions. The former problem is intrinsic considering the ill-posed property of this task itself. So a direction to improve our method would be how to enhance the detection performance for large objects.</p><p>on the detection of large objects and occluded objects. In the camera view and top view, the yellow dotted circle is used to mark the blocked object which has not been successfully detected, while the red dotted circle is used to mark the detected large object with obvious deviation. The former is mainly manifest in the failure to find the object behind, while the latter is mainly manifest in the inaccurate estimation of the size and orientation of the object. The reasons behind the two failure cases are also different. The former is due to the inherent property of the current setting, which is difficult to be solved; the latter may be due to the fact that the receptive field of convolution kernel of the current model is not large enough, resulting in low performance of large object detection. Therefore, the future research direction may be more focused on the solution of the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>Finally, we show some important factors in the whole process of studying in Tab. 3. It can be seen that in the prophase process, transforming depth back to the original space to compute loss is an important factor to improve mAP, and distance-based target assignment is an important factor to improve the overall NDS. In the later promotion process, the stronger backbone, such as replacing original ResNet50 with ResNet101 and using DCN, is a very important factor. At the same time, due to the difference of scales and measurements, using disentangled heads for different regression targets is also an important way to improve the accuracy of angle prediction and NDS. Finally, we achieve the current state-of-the-art through simple augmentation, more training epochs and basic model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a simple yet efficient one-stage framework, FCOS3D, for monocular 3D object detection without any 2D detection or 2D-3D correspondence priors. In the framework, we first transform the commonly defined 7-DoF 3D targets to image domain and decouple it as 2D and 3D attributes to generally make it fit the 3D setting. On this basis, the objects are distributed to different feature levels with consideration of their 2D scales and further assigned only according to the 3D centers. In addition, the center-ness is redefined with a 2D Guassian distribution based on the 3D-center to be compatible with our target formulation. Experimental results with detailed ablation studies show the efficacy of our approach. For future work, a promising direction is how to better tackle the difficulty of depth and orientation estimation in this ill-posed setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Our proposed distance-based target assignment for dealing with ambiguity case could significantly improve the best possible recall (BPR) for each class, especially for large objects like trailers. Construction vehicle and traffic cone are abbreviated as CV and TC in thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the nuScenes dataset.</figDesc><table><row><cell>Methods</cell><cell>Dataset</cell><cell>Modality</cell><cell cols="7">mAP mATE mASE mAOE mAVE mAAE NDS</cell></row><row><cell>CenterFusion [22]</cell><cell>test</cell><cell cols="2">Camera &amp; Radar 0.326</cell><cell>0.631</cell><cell>0.261</cell><cell>0.516</cell><cell>0.614</cell><cell>0.115</cell><cell>0.449</cell></row><row><cell>PointPillars [14]</cell><cell>test</cell><cell>LiDAR</cell><cell>0.305</cell><cell>0.517</cell><cell>0.290</cell><cell>0.500</cell><cell>0.316</cell><cell>0.368</cell><cell>0.453</cell></row><row><cell>MEGVII [36]</cell><cell>test</cell><cell>LiDAR</cell><cell>0.528</cell><cell>0.300</cell><cell>0.247</cell><cell>0.379</cell><cell>0.245</cell><cell>0.140</cell><cell>0.633</cell></row><row><cell>LRM0</cell><cell>test</cell><cell>Camera</cell><cell>0.294</cell><cell>0.752</cell><cell>0.265</cell><cell>0.603</cell><cell>1.582</cell><cell>0.14</cell><cell>0.371</cell></row><row><cell>MonoDIS [27]</cell><cell>test</cell><cell>Camera</cell><cell>0.304</cell><cell>0.738</cell><cell>0.263</cell><cell>0.546</cell><cell>1.553</cell><cell>0.134</cell><cell>0.384</cell></row><row><cell>CenterNet [34] (HGLS)</cell><cell>test</cell><cell>Camera</cell><cell>0.338</cell><cell>0.658</cell><cell>0.255</cell><cell>0.629</cell><cell>1.629</cell><cell>0.142</cell><cell>0.4</cell></row><row><cell>Noah CV Lab</cell><cell>test</cell><cell>Camera</cell><cell>0.331</cell><cell>0.660</cell><cell>0.262</cell><cell>0.354</cell><cell>1.663</cell><cell>0.198</cell><cell>0.418</cell></row><row><cell>FCOS3D (Ours)</cell><cell>test</cell><cell>Camera</cell><cell>0.358</cell><cell>0.690</cell><cell>0.249</cell><cell>0.452</cell><cell>1.434</cell><cell>0.124</cell><cell>0.428</cell></row><row><cell>CenterNet [34] (DLA)</cell><cell>val</cell><cell>Camera</cell><cell>0.306</cell><cell>0.716</cell><cell>0.264</cell><cell>0.609</cell><cell>1.426</cell><cell>0.658</cell><cell>0.328</cell></row><row><cell>FCOS3D (Ours)</cell><cell>val</cell><cell>Camera</cell><cell>0.343</cell><cell>0.725</cell><cell>0.263</cell><cell>0.422</cell><cell>1.292</cell><cell>0.153</cell><cell>0.415</cell></row><row><cell>4. Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average precision for each class on the nuScenes test benchmark. CV and TC are abbreviation of construction vehicle and traffic cone in the table.</figDesc><table><row><cell>Methods</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>trailer</cell><cell>CV</cell><cell>ped</cell><cell cols="2">motor bicycle</cell><cell>TC</cell><cell>barrier mAP</cell></row><row><cell>LRM0</cell><cell>0.467</cell><cell>0.21</cell><cell>0.17</cell><cell cols="4">0.149 0.061 0.359 0.287</cell><cell>0.246</cell><cell>0.476</cell><cell>0.512</cell><cell>0.294</cell></row><row><cell>MonoDIS [27]</cell><cell>0.478</cell><cell>0.22</cell><cell cols="3">0.188 0.176 0.074</cell><cell>0.37</cell><cell>0.29</cell><cell>0.245</cell><cell>0.487</cell><cell>0.511</cell><cell>0.304</cell></row><row><cell cols="2">CenterNet [34] (HGLS) 0.536</cell><cell>0.27</cell><cell cols="5">0.248 0.251 0.086 0.375 0.291</cell><cell>0.207</cell><cell>0.583</cell><cell>0.533</cell><cell>0.338</cell></row><row><cell>Noah CV Lab</cell><cell cols="7">0.515 0.278 0.249 0.213 0.066 0.404 0.338</cell><cell>0.237</cell><cell>0.522</cell><cell>0.49</cell><cell>0.331</cell></row><row><cell>FCOS3D (Ours)</cell><cell>0.524</cell><cell>0.27</cell><cell cols="5">0.277 0.255 0.117 0.397 0.345</cell><cell>0.298</cell><cell>0.557</cell><cell>0.538</cell><cell>0.358</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on the nuScenes validation 3D detection benchmark.</figDesc><table><row><cell>Methods</cell><cell cols="7">mAP mATE mASE mAOE mAVE mAAE NDS</cell></row><row><cell>Baseline</cell><cell>0.227</cell><cell>0.868</cell><cell>0.272</cell><cell>0.778</cell><cell>1.326</cell><cell>0.393</cell><cell>0.282</cell></row><row><cell>+depth loss in original space</cell><cell>0.25</cell><cell>0.838</cell><cell>0.268</cell><cell>0.892</cell><cell>1.33</cell><cell>0.413</cell><cell>0.284</cell></row><row><cell>+flip augmentation</cell><cell>0.248</cell><cell>0.85</cell><cell>0.267</cell><cell>1.016</cell><cell>1.358</cell><cell>0.268</cell><cell>0.286</cell></row><row><cell cols="2">+dist-based target assign &amp; attr pred 0.257</cell><cell>0.832</cell><cell>0.268</cell><cell>0.852</cell><cell>1.2</cell><cell>0.18</cell><cell>0.316</cell></row><row><cell>+global NMS</cell><cell>0.26</cell><cell>0.828</cell><cell>0.267</cell><cell>0.85</cell><cell>1.371</cell><cell>0.18</cell><cell>0.317</cell></row><row><cell>+ResNet101</cell><cell>0.272</cell><cell>0.821</cell><cell>0.265</cell><cell>0.81</cell><cell>1.379</cell><cell>0.17</cell><cell>0.329</cell></row><row><cell>+disentangle heads</cell><cell>0.28</cell><cell>0.822</cell><cell>0.274</cell><cell>0.64</cell><cell>1.305</cell><cell>0.177</cell><cell>0.349</cell></row><row><cell>+DCN in backbone</cell><cell>0.295</cell><cell>0.806</cell><cell>0.268</cell><cell>0.511</cell><cell>1.315</cell><cell>0.17</cell><cell>0.372</cell></row><row><cell>+finetune w/ depth weight=1.0</cell><cell>0.316</cell><cell>0.755</cell><cell>0.263</cell><cell>0.458</cell><cell>1.307</cell><cell>0.169</cell><cell>0.393</cell></row><row><cell>+TTA</cell><cell>0.326</cell><cell>0.743</cell><cell>0.259</cell><cell>0.441</cell><cell>1.341</cell><cell>0.163</cell><cell>0.402</cell></row><row><cell>+more epochs &amp; ensemble</cell><cell>0.343</cell><cell>0.725</cell><cell>0.263</cell><cell>0.422</cell><cell>1.292</cell><cell>0.153</cell><cell>0.415</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We set the regression range as (0, 48, 96, 192, 384, ∞) for m 2 to m 7 in our experiments respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MMDetection3D: Open-MMLab next-generation platform for general 3D object detection</title>
		<idno>2020. 7</idno>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
	</analytic>
	<monogr>
		<title level="m">MMDetection3D Contributors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>Jörgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Centerfusion: Centerbased radar and camera fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reconfigurable voxels: A new representation for lidar-based point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno>2018. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Objects as points. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Class-balanced grouping and sampling for point cloud 3d object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssn: Shape signature networks for multiclass object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
