<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VERTEX FEATURE ENCODING AND HIERARCHICAL TEMPORAL MODELING IN A SPATIAL-TEMPORAL GRAPH CONVOLUTIONAL NETWORK FOR ACTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
							<email>konstantinos.papadopoulos@uni.lu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interdisciplinary Centre for Security, Reliability and Trust (SnT</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
							<email>enjie.ghorbel@uni.lu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interdisciplinary Centre for Security, Reliability and Trust (SnT</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Aouada</surname></persName>
							<email>djamila.aouada@uni.lu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interdisciplinary Centre for Security, Reliability and Trust (SnT</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ottersten</surname></persName>
							<email>bjorn.ottersten@uni.lu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interdisciplinary Centre for Security, Reliability and Trust (SnT</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VERTEX FEATURE ENCODING AND HIERARCHICAL TEMPORAL MODELING IN A SPATIAL-TEMPORAL GRAPH CONVOLUTIONAL NETWORK FOR ACTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper extends the Spatial-Temporal Graph Convolutional Network (ST-GCN) for skeleton-based action recognition by introducing two novel modules, namely, the Graph Vertex Feature Encoder (GVFE) and the Dilated Hierarchical Temporal Convolutional Network (DH-TCN). On the one hand, the GVFE module learns appropriate vertex features for action recognition by encoding raw skeleton data into a new feature space. On the other hand, the DH-TCN module is capable of capturing both short-term and long-term temporal dependencies using a hierarchical dilated convolutional network. Experiments have been conducted on the challenging NTU RGB-D-60 and NTU RGB-D 120 datasets. The obtained results show that our method competes with stateof-the-art approaches while using a smaller number of layers and parameters; thus reducing the required training time and memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Skeleton-based human action recognition has received a huge amount of attention in various applications, such as video surveillance, coaching, and rehabilitation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Recently, deep learning-based approaches have achieved impressive performance on large-scale datasets, by learning the appropriate features automatically from the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. These approaches rely either on Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN). However, they usually represent the skeleton sequences as vectors or 2D grids, ignoring inter-joint dependencies.</p><p>To express joint correlations both spatially and temporally, Yan et al. introduced the Spatial Temporal-Graph Convolutional Network (ST-GCN) <ref type="bibr" target="#b11">[12]</ref>. Their work takes advantage of Graph Convolutional Networks (GCN) <ref type="bibr" target="#b12">[13]</ref> extending the classical CNNs to graph convolutions. This architecture represents skeleton sequences as a graph composed of both temporal and spatial edges, by respectively considering the inter and intra-frame connections of joints. The effectiveness of this approach has motivated several extensions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> which, consider the most informative connections between joints instead of the predefined natural skeleton structure or construct the spatial-temporal graphs using additional features such as bone lengths.</p><p>However, all these methods only use raw skeleton features (joint coordinates and/or bone lengths) for the construction of spatial-temporal graphs. While offering a high-level description of the human body structure, these features may be lacking discriminative power for action recognition. Indeed, hand-crafted approaches have shown the limitation of using only raw skeleton joints as features in action recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Furthermore, the temporal dependencies of the graph are modeled by a single temporal convolutional layer. As a result, critical long-term dependencies might be not consistently described. Moreover, these approaches make use of a considerable number of ST-GCN blocks (10, in most cases), which significantly increases the number of parameters and consequently the computational complexity and the required memory.</p><p>In this paper, we assume that by encoding the vertex features in an end-to-end manner and modeling temporal longterm and short-term dependencies, less number of layers (and consequently parameters) will be needed. For that reason, two modules are introduced. The first module, referred to as Graph Vertex Feature Encoder (GVFE), is a trainable layer that transforms the feature space from the Euclidean coordinate system of joints to an end-to-end learned vertex feature space, optimized jointly with the ST-GCN. The second module incorporates a hierarchical structure of dilated temporal convolutional layers for modeling short-term and longterm temporal dependencies and replaces the standard temporal convolutional layers in the ST-GCN block. It is termed Dilated Hierarchical Temporal Graph Convolutional Network (DH-TCN). With the use of these two modules, we show that fewer layers are needed to reach the same or even higher performance in action recognition while needing less memory and training time than previous ST-GCN based approaches such as <ref type="bibr" target="#b15">[16]</ref>.</p><p>In summary, our contributions are the following: arXiv:1912.09745v1 [cs.CV] 20 Dec 2019</p><p>• the introduction of a Graph Vertex Feature Encoder (GVFE) module for encoding vertex features;</p><p>• the proposal of a Dilated Hierarchical Temporal Graph Convolutional Network (DH-TCN) module for modeling short and long-term dependencies;</p><p>• the design of a more compact and efficient graph-based framework for action recognition trained in an end-toend manner;</p><p>• the presentation of experimental validation and analysis of our approach on two challenging datasets.</p><p>The remainder of this paper is organized as follows: Section 2 recalls the background related to Spatial-Temporal Graph Convolutional Networks (ST-GCN) applied to action recognition. Section 3 details the proposed framework. Section 4 presents the experiments and analyzes the results. Finally, Section 5 concludes this paper and discusses possible extensions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skeleton Sequences as Graphs</head><p>Following a predefined structure indicating their interconnections, skeletons can be intuitively seen as graphs. Thus, Yan et al. <ref type="bibr" target="#b11">[12]</ref> described skeleton sequences of J joints and T frames as spatial-temporal graphs in which, at each time instance t, each joint i is assumed to be a vertex. Then, two kinds of edges are constructed to connect vertices: spatial edges that are the natural spatial joint connections and temporal edges connecting the same joint across time. This spatial-temporal graph is denoted as S = (V, E), with V the set of vertices and E the set of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial-Temporal Graph Convolutional Network</head><p>Considering the spatial-temporal graph S, a network called ST-GCN generalizing CNN to graphs has been proposed in <ref type="bibr" target="#b11">[12]</ref>. In this work, for an input feature map f in , a spatial graph convolution is applied, such that:</p><formula xml:id="formula_0">f out = Λ − 1 2 (A + I)Λ − 1 2 f in W,<label>(1)</label></formula><p>where f out is the output feature map, A the adjacency matrix, I the identity matrix,</p><formula xml:id="formula_1">Λ = [Λ ii ] i∈{1,...,J} such that Λ ii = j (A ij + I ij )</formula><p>and W is the weight matrix. For a graph of size (C in , J, T ), the dimension of the resulting tensor is (C out , J, T ), with C in and C out denoting respectively the number of input and output channels.</p><p>The temporal graph convolution consists of classical convolutions, performed on the output feature tensor f out . K denotes the kernel size of the temporal convolutional layer.</p><p>The input features f <ref type="bibr" target="#b0">(1)</ref> in incorporated in the first ST-GCN layer correspond to the joint coordinates such that ∀i, f <ref type="bibr" target="#b0">(1)</ref> in (v i , t) = P i (t) with P i (t) the 3D coordinate of the joint i at an instant t and consequently C in = 3. While these first layer features f <ref type="bibr" target="#b0">(1)</ref> in offer a representation easily understandable by the human, they might be not discriminative enough for the task of action recognition. Moreover, temporal graph convolutional layers capture only local dependencies, resulting in the presence of redundant information and neglecting long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APPROACH</head><p>In this section, the two novel modules, namely GVFE and DH-TCN, are presented. While GVFE aims at learning vertex features, DH-TCN temporally summarizes spatial-temporal graphs and consequently models long-term as well as shortterm dependencies. These two modules are integrated with the original ST-GCN <ref type="bibr" target="#b11">[12]</ref> framework. This full pipeline is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref> and is trained in an end-to-end manner. It is important to note that these modules are also complementary to other ST-GCN extensions such as AS-GCN <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Vertex Feature Encoding (GVFE)</head><p>As mentioned in Section 1, considering raw skeleton joint data as vertex features might not be informative enough for action recognition. To enhance the discriminative power of vertex features, we introduce the GVFE module that is directly placed before the first ST-GCN block. GVFE maps 3D skeleton coordinates, traditionally used as input features to the first ST-GCN block f <ref type="bibr" target="#b0">(1)</ref> in (v i ) = P i with i ∈ {1, ..., J}, from the Cartesian coordinate system R 3 to a learned feature space M ⊆ R Cout . Since this module is trained in an end-toend manner by optimizing the recognition error, we expect to obtain a more discriminative feature space M.</p><p>For each joint i, a separate Temporal Convolutional Network (TCN) is employed to encode raw data, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this context, TCNs show strong potential since (a) they do not allow information to flow from the future states to the past states, (b) the input and output sequences have the same length and (c) they model temporal dependencies. For each joint i, the new graph vertex featuresf <ref type="bibr" target="#b0">(1)</ref> in (v i ) obtained after applying the TCN are computed as follows,</p><formula xml:id="formula_2">f (1) in (v i ) = W T CN i * f (1) in (v i ) = W T CN i * P i ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">{W T CN i } 1≤i≤J is the collection of tensors contain- ing the kernel filters {W T CN i,j } of dimension R Cout×Tw×Cin , with j ∈ {1, .</formula><p>.., C out } the index of the filter and T w the temporal size of the filters. Note that we use the identity activation function. This module has the advantage of being applicable to any graph-based network, regardless of the application.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dilated Hierarchical Temporal Graph Convolutional Network</head><p>The modeling of temporal dependencies is crucial in action recognition. However, in several ST-GCN-based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, temporal dependencies are modeled using only one convolutional layer. As a result, long-term dependencies that can be important for modeling actions are not well encoded.</p><p>To that end, we propose to replace the temporal convolutions of each ST-GCN block with a module that encodes both short-term and long-term dependencies. Given the output feature map f (k) out resulting from the k th Spatial GCN (S-GCN) block (with k ∈ [1, k total ] and k total the total number of ST-GCN blocks), this module, termed Dilated Hierarchical Temporal Convolutional Network (DH-TCN), is composed of N successive dilated temporal convolutions. The association of these two blocks is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Each layer output f (k,n) temp of order n of DH-TCN is obtained as follows,</p><formula xml:id="formula_4">f (k,n) temp = F W DH i * l f (k,n−1) temp , with f (k,0) temp = f (k) out ,<label>(3)</label></formula><p>where {W DH } 1≤i≤J is the tensor containing the trainable temporal filters of dimension R Cout×Tw 1 ×Cout with T w1 their temporal dimension and * l refers to the convolution operator with a dilation of l = 2 n , n ∈ [0, N − 1]. The hierarchical architecture with different dilation ensures the modeling of long-term dependencies. At the same time, the residual connection depicted in <ref type="figure" target="#fig_2">Fig. 3</ref> enables the preservation of the information of short-term dependencies.</p><p>The entire DH-TCN module is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Each hierarchical layer is composed of a dilated temporal convolution, a ReLU activation function, and a batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Our framework has been tested on two well-known benchmarks, namely NTU RGB+D 60 (NTU-60) <ref type="bibr" target="#b18">[19]</ref> and NTU RGB+D 120 (NTU-120) <ref type="bibr" target="#b19">[20]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Settings</head><p>NTU RGB+D Dataset (NTU-60): NTU RGB+D is a Kinectacquired dataset which consists of 56, 880 videos. This dataset includes 60 actions performed by 40 subjects. The data are collected from 3 different angles, at −45 • , 0 • and 45 • with respect to the human body. In our experiments, we follow the same protocols (cross-view and cross-subject settings) proposed in <ref type="bibr" target="#b18">[19]</ref>. NTU RGB+D 120 Dataset (NTU-120): NTU RGB+D 120 Dataset extends the original NTU dataset by adding 60 additional action classes to the existing ones and 66 more subjects. The recording angles remain the same at −45 • , 0 • and 45 • with respect to the human body, but more setups (height and distance) are considered (32 instead of 18). We consider the same evaluation protocol ( cross-setup and cross-subject settings) suggested in <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The implementation of our approach is based on the PyTorch ST-GCN <ref type="bibr" target="#b11">[12]</ref> and AS-GCN <ref type="bibr" target="#b15">[16]</ref> codes. In both approaches, we include the GVFE module before the first ST-GCN block and we replace the temporal convolutions of each block with the DH-TCN module. For the spatial GCN, we use the same parameters suggested in <ref type="bibr" target="#b11">[12]</ref>. The number of output channels in GVFE is set to C out = 8 and we use N = 2 hierarchical modules in DH-TCN. The temporal window of the DH-TCN module is set to T w = 9. The Stochastic Gradient Descent optimizer is used with a decaying learning rate of 0.01. In contrast to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> that makes use of 10 ST-GCN or AS-GCN blocks, we use only 4 blocks with k ∈ {1, ..., 4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Comparison with state-of-the-art</head><p>In this section, we compare our approach with recent skeleton-based methods, such as SkeleMotion <ref type="bibr" target="#b6">[7]</ref>, Body Pose Evolution Map <ref type="bibr" target="#b20">[21]</ref>, Multi-Task CNN with RotClips <ref type="bibr" target="#b7">[8]</ref>, Two-Stream Attention LSTM <ref type="bibr" target="#b21">[22]</ref>, Skeleton Visualization (Single Stream) <ref type="bibr" target="#b8">[9]</ref>, Multi-Task Learning Network <ref type="bibr" target="#b22">[23]</ref> and more particularly with two the graph-based baselines namely ST-GCN <ref type="bibr" target="#b11">[12]</ref> and AS-GCN <ref type="bibr" target="#b15">[16]</ref>. GVFE and DH-TCN modules are incorporated in both ST-GCN <ref type="bibr" target="#b11">[12]</ref> and AS-GCN <ref type="bibr" target="#b15">[16]</ref> methods. The obtained accuracy of recognition on NTU-60 and NTU-120 datasets are reported in <ref type="table">Table 1</ref>.</p><p>On NTU-120, we obtain the best accuracy of recognition of the state-of-the-art for both settings. Indeed, our approach used with AS-GCN (GVFE+AS-GCN w/ DH-TCN) reaches 78.3% and 79.8% for cross-subject and cross-setup settings, respectively. These positive results are also confirmed when testing our approach with ST-GCN (GVFE + ST-GCN w/ DH-TCN). Indeed, we improve the accuracy of the original ST-GCN by 0.6% up to 2.9%.</p><p>On NTU-60, the achieved scores are among the best of the state-of-the-art but remain slightly inferior to the original ST-GCN and AS-GCN (with respectively 79.1%−88.2% against 81.5% − 88.3% and 85.3% − 92.8% against 86.8% − 94.2%). Although being slightly inferior, it is important to highlight that only 4 blocks are used in our case (against 10 for ST-GCN and AS-GCN). The method based on Body Pose Evolution Map <ref type="bibr" target="#b20">[21]</ref> remains the best performing approach on NTU-60. However, this method registers an accuracy inferior to our approach by 13.7% − 12.9%, while the difference is less important on NTU-60 with only a gap of 6.4% − 2.5% making our method more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Impact of the number of blocks</head><p>As mentioned earlier, our approach utilizes only 4 ST-GCN or AS-GCN blocks instead of 10. For a fair comparison with the baselines, we also test ST-GCN <ref type="bibr" target="#b11">[12]</ref> and AS-GCN <ref type="bibr" target="#b15">[16]</ref> when using only 4 blocks. The recognition accuracy of these experiments is reported in <ref type="table">Table 2</ref>. Our method (GVFE + ST-GCN w/ DH-TCN) shows a significant performance boost in both settings of over 22% compared to ST-GCN with 4 blocks. Similarly, the recognition accuracy remains higher than the original AS-GCN compared to our method (GVFE + AS-GCN w/ DH-TCN). However, in this case, the accuracy boost is less impressive with an increase of 1.4% for cross-subject settings and 0.4% for cross-setup settings. This could be explained by the 7 extra spatial-temporal convolutional blocks after the maxPooling layer in the AS-GCN network, which add more discriminative power to the full pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Ablation Study</head><p>To analyze the contribution of each component of our framework, an ablation study was conducted. For this purpose, we removed each time a component and report the obtained performance on both NTU-120 dataset for the cross-setup setting. The results are reported in <ref type="table">Table 3</ref>.</p><p>Our approach, which combines both the GVFE and the DH-TCN modules, achieves 74.2% mean accuracy, which is higher by 22.4% than the original ST-GCN approach with 4 ST-GCN blocks. When using only the GVFE, the mean accuracy reaches 70.9%. We tested different configurations in this case, such as attaching a Rectified Linear Unit (ReLU) or a Batch Normalization Unit (BN). In both cases, the performance was degraded (68.9% and 66.7%, respectively), since these units distort the joint motion trajectories.</p><p>Moreover, we conducted experiments by incorporating only the DH-TCN module. The mean accuracy, in this case, reached 68.3%, showing that GVFE and DH-TCN modules trained in an end-to-end manner can offer a significant performance boost. <ref type="table">Table 1</ref>. Accuracy of recognition (%) on NTU-60 and NTU-120 datasets. The evaluation is performed using cross-view and cross-subject settings on NTU-60 and cross-subject and cross-setup settings on NTU-120. *These values have not been reported in the state-of-the-art and the available codes have been used to obtain the recognition accuracy of these algorithms on NTU-120.</p><p>Method NTU-60 (%) NTU-120 (%) X-subject X-view X-subject X-setup SkeleMotion <ref type="bibr" target="#b6">[7]</ref> 76  <ref type="table">Table 2</ref>. Accuracy of recognition (%) using only 4 ST-GCN or AS-GCN blocks on NTU-120 dataset for cross-subject and cross-setup settings. *These values are not reported in the state-of-the-art. Thus, the available codes have been used to obtain these results.</p><p>Method X-subject X-setup ST-GCN (4 blocks) <ref type="bibr" target="#b11">[12]</ref> 45.3 * 51.8 * GVFE + ST-GCN w/ DH-TCN (4 blocks -ours) 73.0 74.2 AS-GCN (4 blocks) <ref type="bibr" target="#b15">[16]</ref> 76.9 * 79.4 * GVFE + AS-GCN w/ DH-TCN (4 blocks -ours) 78.3 79.8 <ref type="table">Table 3</ref>. Ablation study: accuracy of recognition (%) on NTU-120 dataset for cross-setup settings using ST-GCN as a baseline. *These values are not reported in the state-of-theart. Thus, the available codes have been used to obtain these results Method Accuracy (%) ST-GCN (4 blocks) <ref type="bibr" target="#b11">[12]</ref> 51.8 * GVFE + ST-GCN (4 blocks) 70.9 ST-GCN w/ DH-TCN (4 blocks) 68.3 GVFE + ST-GCN w/ DH-TCN (4 blocks -ours) 74.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Number of parameters and training time</head><p>Although our method makes use of two additional modules compared to the baselines, the use of only 4 blocks reduces the number of parameters. For instance, When using our method (GVFE + AS-GCN w/ DH-TCN) with 4 blocks, the number of parameters drops from 7420696 to 7370568 compared to the original AS-GCN with 10 blocks, while keeping almost the same accuracy on NTU-60 or even increasing it on NTU-120. Consequently, the training time is also reduced.</p><p>As an example, on NTU-120 for cross-setup settings, our approach requires 24029 seconds less than the original AS-GCN for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, two novel modules for ST-GCN based methods have been proposed called GVFE and DH-TCN. These modules enable the reduction of the number of needed blocks and parameters while conserving almost the same or improving the recognition accuracy. Instead of relying on raw skeleton features such as skeleton joints, GVFE learns and generates graph vertex features in an end-to-end manner. To model simultaneously long-term and short-term dependencies, DH-TCN makes use of hierarchical dilated temporal convolutional layers. The relevance of these modules has been confirmed thanks to the performance achieved on two well-known datasets. Some future extensions are under consideration, such as applying a similar hierarchical model to replace the spatial graph convolutional layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed approach. In the first step, the GVFE module generates graph features. The new graph is given as an input to the Modified ST-GCN blocks composed of a Spatial-Graph Convolutional Network (S-GCN) and a Dilated Hierarchical Temporal Convolutional Network (DH-TCN). Finally, a SoftMax layer classifies the spatial-temporal graph features resulting from the last Modified ST-GCN block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the GVFE module structure: it is composed of J TCN blocks. For each joint, one TCN block is separately used in order to conserve the natural skeleton structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of S-GCN + DH-TCN block. Spatial features are extracted from the S-GCN module and are, then, fed into DH-TCN module. Green color is used for Batch Normalization units, blue for ReLU and orange for 2D Convolutional Layers.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>This work was funded by the European Union's Horizon 2020 research and innovation project STARR under grant agreement No.689947, and by the National Research Fund (FNR), Luxembourg, under the project C15/IS/10415355/3D-ACT/Björn Ottersten. We would, also, like to thank Christian Hundt from NVIDIA AI Technology Center Luxembourg for his valuable input and fruitful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An extension of kernel learning methods using a modified logeuclidean distance for fast and accurate skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Boonaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Boutteau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lecoeuche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Savatier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A view-invariant framework for fast skeleton-based action recognition using a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Baptista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himadri</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girum</forename><surname>Demisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twostage rgb-based action detection using augmented 3d poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Baptista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph based skeleton motion representation and similarity measurement for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13025</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ferdous Sohel, and Farid Boussaid</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pose encoding for robust skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Girum G Demisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">View-invariant action recognition from rgb data via 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Baptista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Girum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Demisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2542" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kinematic spline curves: A temporal invariant descriptor for fast action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Boutteau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Boonaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Savatier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lecoeuche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamila</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
	<note>Senjian An, Ferdous Sohel, and Farid Boussaid</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
