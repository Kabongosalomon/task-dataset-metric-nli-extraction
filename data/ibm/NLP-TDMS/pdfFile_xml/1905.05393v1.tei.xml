<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-theart approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data augmentation techniques such as cropping, translation, and horizontal flipping are commonly used to train large neural networks <ref type="bibr" target="#b26">(Lin et al., 2013)</ref>. Augmentation transforms images to increase the diversity of image data. While deep neural networks can be trained on enormous numbers of data examples to exhibit excellent performance on tasks such as image classification, they contain a likewise enormous number of parameters, which causes overfitting. Data augmentation acts as a regularizer to combat this. However, most approaches used in training state-of-the-art networks only use basic types of augmentation.   <ref type="figure">Figure 1</ref>. PBA matches AutoAugment's classification accuracy across a range of different network models on the CIFAR-10 dataset, while requiring 1,000x less GPU hours to run. For the full set of results, refer to <ref type="table" target="#tab_1">Table 2</ref>. Assuming an hourly GPU cost of $1.5, producing a new augmentation policy costs around $7.5 for PBA vs $7,500 with AutoAugment. The same scaling holds for the SVHN dataset as well.</p><p>architectures have been investigated in depth <ref type="bibr" target="#b23">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b16">He et al., 2015;</ref><ref type="bibr" target="#b43">Szegedy et al., 2015;</ref><ref type="bibr" target="#b39">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b48">Zagoruyko &amp; Komodakis, 2016;</ref><ref type="bibr" target="#b18">Huang et al., 2016;</ref><ref type="bibr" target="#b15">Han et al., 2016)</ref>, less focus has been put into discovering strong types of data augmentation and data augmentation policies that capture data invariances.</p><p>A key consideration when applying data augmentation is picking a good set of augmentation functions, since redundant or overly aggressive augmentation can slow down training and introduce biases into the dataset <ref type="bibr" target="#b14">(Graham, 2014)</ref>. Many recent methods learn augmentation policies to apply different functions to image data. Among these, Au-toAugment <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref> stands out with state-of-theart results in CIFAR-10 <ref type="bibr" target="#b22">(Krizhevsky, 2009</ref>), CIFAR-100 <ref type="bibr" target="#b22">(Krizhevsky, 2009)</ref>, and ImageNet <ref type="bibr" target="#b7">(Deng et al., 2009)</ref>. Using a method inspired by Neural Architecture Search <ref type="bibr" target="#b49">(Zoph &amp; Le, 2016)</ref>, Cubuk et al. learn a distilled list of augmentation functions and associated probability-magnitude values, resulting in a distribution of possible augmentations which can be applied to each batch of data. However, the search technique used in the work is very computationally expensive, and code has not been released to reproduce it. In this work, we address these issues with a simple and efficient algorithm for augmentation policy learning.</p><p>arXiv:1905.05393v1 [cs.CV] 14 May 2019 <ref type="table">Table 1</ref>. Comparison of pre-computation costs and test set error (%) between this paper, AutoAugment (AA), and the previous best published results. Previous results did not pre-compute augmentation policies. AutoAugment reported estimated cost in Tesla P100 GPU hours, while PBA measured cost in Titan XP GPU hours. Besides PBA, all metrics are cited from <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>. For more detail, see Our formulation of data augmentation policy search, Population Based Augmentation (PBA), reaches similar levels of final performance on a variety of neural network models while utilizing orders of magnitude less compute. We learn a robust augmentation policy on CIFAR-10 data in five hours using one NVIDIA Titan XP GPU, and we visualize its performance in <ref type="figure">Figure 1</ref>. Relative to the several days it takes to train large CIFAR-10 networks to convergence, the cost of running PBA beforehand is marginal and significantly enhances results. These results are summarized in <ref type="table">Table 1</ref>. PBA leverages the Population Based Training algorithm <ref type="bibr" target="#b21">(Jaderberg et al., 2017)</ref> to generate an augmentation schedule that defines the best augmentation policy for each epoch of training. This is in contrast to a fixed augmentation policy that applies the same transformations independent of the current epoch number.</p><p>We release code to run and evaluate our augmentation search algorithm at https://github.com/ arcelien/pba. This allows an ordinary workstation user to easily experiment with the search algorithm and augmentation operations. A particularly interesting use case would be to introduce new augmentation operations, perhaps targeted towards a particular dataset or image modality, and be able to quickly produce a tailored, high performing augmentation schedule. Our code uses the Ray <ref type="bibr" target="#b28">(Moritz et al., 2017)</ref> implementation of PBT, which allows for easy parallelization across and within GPUs and CPUs.</p><p>This paper is organized as follows: First, we cover relevant background and AutoAugment (Section 2). We then introduce the PBA algorithm (Section 3). We describe the augmentation schedules PBA discovers and its performance on several datasets. Finally, we seek to understand the efficiency gains of PBA through ablation studies and comparison with baseline methods (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related Work</head><p>We first review types of data augmentation for image recognition, which improve generalization with limited data by applying transformations to generate additional samples. Common techniques such as random cropping, flipping, rotating, scaling, and translating are used by top performing models for training on MINST, CIFAR-10, and ImageNet datasets <ref type="bibr" target="#b38">(Simard et al., 2003;</ref><ref type="bibr" target="#b4">Ciresan et al., 2012;</ref><ref type="bibr" target="#b45">Wan et al., 2013;</ref><ref type="bibr" target="#b35">Sato et al., 2015;</ref><ref type="bibr" target="#b23">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b26">Lin et al., 2013;</ref><ref type="bibr" target="#b15">Han et al., 2016)</ref>. Some additional approaches to generate augmented data include image combining <ref type="bibr" target="#b20">(Inoue, 2018;</ref><ref type="bibr" target="#b31">Okafor et al., 2018)</ref>, elastic distortions <ref type="bibr" target="#b46">(Wong et al., 2016)</ref>, and generative adversarial networks <ref type="bibr" target="#b0">(Antoniou et al., 2017)</ref>.</p><p>Augmentation has been shown to have a large impact on image modalities where data is scare or expensive to generate, like medical imaging <ref type="bibr" target="#b3">(Bowles et al., 2018;</ref><ref type="bibr" target="#b10">Frid-Adar et al., 2018)</ref> or non-supervised learning approaches <ref type="bibr" target="#b29">(Mundhenk et al., 2017)</ref>.</p><p>Several papers have attempted to automate the generation of data augmentations with data-driven learning. These use methods such as manifold learning <ref type="bibr">(Paschali et al., 2019)</ref>, Bayesian Optimization <ref type="bibr" target="#b44">(Tran et al., 2017)</ref>, and generative adversarial networks which generate transformation sequences <ref type="bibr" target="#b34">(Ratner et al., 2017)</ref>. Additionally, <ref type="bibr" target="#b24">(Lemley et al., 2017)</ref> uses a network to combine pairs of images to train a target network, and (DeVries &amp; Taylor, 2017) injects noise and interpolates images in an autoencoder learned feature space. AutoAugment <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref> uses reinforcement learning to optimize for accuracy in a discrete search space of augmentation policies.</p><p>Our approach was inspired by work in hyperparameter optimization. There has been much previous work to welltune hyperparameters, especially in Bayesian Optimization <ref type="bibr" target="#b42">(Srinivas et al., 2009;</ref><ref type="bibr" target="#b2">Bergstra et al., 2011;</ref><ref type="bibr" target="#b40">Snoek et al., 2012;</ref><ref type="bibr" target="#b19">Hutter et al., 2011)</ref>, which are sequential in nature and expensive computationally. Other methods incorporate parallelization or use non-bayesian techniques <ref type="bibr" target="#b25">(Li et al., 2016;</ref><ref type="bibr" target="#b12">Golovin et al., 2017;</ref><ref type="bibr" target="#b37">Shah &amp; Ghahramani, 2015;</ref><ref type="bibr" target="#b41">Springenberg et al., 2016;</ref><ref type="bibr" target="#b13">González et al., 2016)</ref> but still either require multiple rounds of optimization or large amounts of compute. These issues are resolved in Population Based Training <ref type="bibr" target="#b21">(Jaderberg et al., 2017)</ref>, which builds upon both evolutionary strategies <ref type="bibr" target="#b5">(Clune et al., 2008)</ref> and random search <ref type="bibr" target="#b1">(Bergstra &amp; Bengio, 2012)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">AutoAugment</head><p>Cubuk et al. shows that using a diverse, stochastic mix of augmentation operations can significantly reduce generalization error. They automate the search over the space of data augmentation policies in a method called AutoAugment, which significantly improves neural network model accuracy on a variety of image datasets. AutoAugment follows an approach similar to work in the neural architecture search area <ref type="bibr" target="#b50">(Zoph et al., 2017;</ref><ref type="bibr" target="#b33">Pham et al., 2018)</ref> where a controller RNN network is trained via reinforcement learning to output augmentation policies maximizing for accuracy ( <ref type="figure" target="#fig_1">Figure  2</ref>). However, this approach is expensive in both time and compute, as the signal for the controller has to be generated by training thousands of models to convergence on different augmentation policies and evaluating final validation accuracy.  <ref type="bibr" target="#b20">(Inoue, 2018)</ref>. Each operation has two associated parameters: probability and magnitude. The authors used discrete probability values from 0% to 100%, in increments of 10%. Magnitude can range from 0 to 9 inclusive, but a few operations ignore this value and apply a constant effect. <ref type="figure">Figure 3</ref>. Augmentations applied to a CIFAR-10 "car" class image, at various points in our augmentation schedule learned on Reduced CIFAR-10 data. The maximum number of operations applied is sampled from 0 to 2. Each operation is formatted with name, probability, and magnitude value respectively.</p><p>A policy would then consist of five sub-policies, each consisting of two operations and associated parameters. For every batch of data, one randomly selected sub-policy would be applied. In total, the final policy for AutoAugment concatenated the five best performing polices for a total of 25 sub-policies.</p><p>To learn an augmentation policy, 15,000 sampled policies were evaluated on a Wide-ResNet-40-2 (40 layers, widening factor of 2) child model <ref type="bibr" target="#b48">(Zagoruyko &amp; Komodakis, 2016)</ref> by taking the validation accuracy after training for 120 epochs on a "reduced" dataset. For CIFAR-10, this consists of 4,000 images from the training set, and for SVHN, 1,000 images. CIFAR-100 is trained with a transferred augmentation policy from CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Population Based Augmentation</head><p>In this section we introduce the design and implementation of the PBA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Why Augmentation Schedules?</head><p>The end goal of PBA is to learn a schedule of augmentation policies as opposed to a fixed policy. As we will see, this choice is responsible for much of the efficiency gains of PBA (Section 4). Though the search space for schedules over training epochs is larger than that of fixed policies f ∈ F by a factor of |F | |epochs| , counter-intuitively, PBA shows that it is far more efficient to search for a good schedule than a fixed policy. Several factors contribute to this.</p><p>First, estimating the final test error of a fixed augmentation policy is difficult without running the training of a child model to completion. This is true in particular because the choice of regularizing hyperparameters (e.g., data augmentation functions) primarily impacts the tail end of training. Therefore, estimating the final performance of a given fixed augmentation policy requires training a model almost to completion. In contrast, it is straightforward to reuse prior computations to estimate the performance of two variants of a schedule that share a prefix.</p><p>Second, there is reason to believe that it is easier to find a good augmentation policy when searching in the space of schedules. An augmentation function that can reduce generalization error at the end of training is not necessarily a good function at initial phases. Such functions would be selected out when holding the augmentation function fixed for the entirely of training. And though the space of schedules is large, most good schedules are necessarily smooth and hence easily discoverable through evolutionary search algorithms such as PBT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning a Schedule</head><p>In PBA we consider the augmentation policy search problem as a special case of hyperparameter schedule learning. Thus, we leverage Population Based Training (PBT) <ref type="bibr" target="#b21">(Jaderberg et al., 2017)</ref>: a hyperparameter search algorithm which optimizes the parameters of a network jointly with their hyperparameters to maximize performance. The output of PBT is not an optimal hyperparameter configuration but rather a trained model and schedule of hyperparameters. In PBA, we are only interested in the learned schedule and discard the child model result (similar to AutoAugment). This learned augmentation schedule can then be used to improve the training of different (i.e., larger and costlier to train) models on the same dataset.</p><p>PBT executes as follows. To start, a fixed population of models are randomly initialized and trained in parallel. At certain intervals, an "exploit-and-explore" procedure is applied to the worse performing population members, where the model clones the weights of a better performing model (i.e., exploitation) and then perturbs the hyperparameters of the cloned model to search in the hyperparameter space (i.e., exploration). Because the weights of the models are cloned and never reinitialized, the total computation required is the computation to train a single model times the population size.</p><p>The Ray framework <ref type="bibr" target="#b28">(Moritz et al., 2017)</ref> includes a parallelized implementation of PBT (https://ray. readthedocs.io/en/latest/tune.html) which handles the exploit-and-explore process in the backend. This</p><p>Algorithm 1 The PBA augmentation policy template, the parameters of which are optimized by PBT. The parameter vector is a vector of (op, prob, mag) tuples. There are two instances of each op in the vector, and this parameter cannot be changed. PBT learns a schedule for the prob and mag parameters during the course of training a population of child models.</p><formula xml:id="formula_0">Input: data x, parameters p, [list of (op, prob, mag)] Shuffle parameters Set count = [0, 1, 2] with probability [0.2, 0.3, 0.5] for (op, prob, mag) in p do if count = 0 then break end if if random(0, 1) &lt; prob then count = count − 1 x = op(x, mag) end if end for Return x</formula><p>implementation allows a user to deploy multiple trials on the same GPU, provided there is enough GPU memory. When the models only require a fraction of the computation resources and memory of an entire GPU, as in this work, training is sped up by fully utilizing the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Policy Search Space</head><p>In Algorithm 1, we describe the augmentation policy function used in PBA and the optimization strategy we adapt from PBT. The challenge here is defining a smooth parameterization of the augmentation policy so that PBT can incrementally adopt good augmentations, while still allowing good coverage of the search space within a limited number of perturbations.</p><p>To make PBA more directly comparable with AutoAugment, we attempt to preserve the qualities of the AutoAugment formulation when possible, using the same augmentation functions, a similar number of total augmentation functions in the policy, and the same set of magnitude variants per function as applicable. Our augmentation policy search space consists of the augmentation operations from Au-toAugment, less SamplePairing <ref type="bibr" target="#b20">(Inoue, 2018)</ref>, for a total of 15 operations. We use the same code and magnitude options derived from PIL operations to ensure a fair comparison based on search algorithm performance.</p><p>We define a set of hyperparameters consisting of two magnitude and probability values for each operation, with discrete possibilities for each. This gives us 30 operation-magnitudeprobability tuples for a total of 60 hyperparameters. Like AutoAugment, we have 10 possibilities for magnitude and Algorithm 2 The PBA explore function. Probability parameters have possible values from 0% to 100% in increments of 10%, and magnitdue parameters have values from 0 to 9 inclusive.</p><p>Input: Params p, list of augmentation hyperparameters for param in p do if random(0, 1) &lt; 0.2 then Resample param uniformly from domain else amt = [0,1,2,3] uniformly at random if random(0, 1) &lt; 0.5 then param = param + amt else param = param − amt end if Clip param to stay in domain end if end for 11 possibilities for probability. When we apply augmentations to data, we first shuffle all operations and then apply operations in turn until a limit is reached. This limit can range from 0 to 2 operations.</p><p>Similar to the AutoAugment policy, PBA allows for two of the same augmentation operations to be applied to a single batch of data. Due to the use of a schedule, a single operation the PBA search space includes (10 × 11) 30 ≈ 1.75 × 10 61 possibilities, compared to 2.8 × 10 32 for Au-toAugment. For discussion about the hyperparameter priors encoded within this policy template, see Section 4.3. Our policy template formulation is primarily motivated by the need to directly compare results with AutoAugment rather than optimizing for the best possible policy template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">PBA Implementation</head><p>We describe the formulation of our search in the format of PBT experiments <ref type="bibr" target="#b21">(Jaderberg et al., 2017)</ref>.</p><p>Step: In each iteration we run an epoch of gradient descent.</p><p>Eval: We evaluate a trial on a validation set not used for PBT training and disjoint from the final test set.</p><p>Ready: A trial is ready to go through the exploit-andexplore process once 3 steps/epochs have elapsed.</p><p>Exploit: We use Truncation Selection <ref type="bibr" target="#b21">(Jaderberg et al., 2017)</ref>, where a trial in the bottom 25% of the population clones the weights and hyperparameters of a model in the top 25%.</p><p>Explore: See Algorithm 2 for the exploration function. For each hyperparameter, we either uniformly resample from all possible values or perturb the original value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Analysis</head><p>In this section, we describe experiments we ran to better understand the performance and characteristics of the PBA algorithm. We seek to answer the following questions:</p><p>1. How does classification accuracy and computational cost of PBA compare to state-of-the-art and random search baselines?</p><p>2. Where does the performance gain of PBA come fromdoes having a schedule of augmentations really matter, or is a stationary distribution sufficient?</p><p>3. How does PBA performance scale with the amount of computation used?</p><p>4. How sensitive is PBA to the hyperparameters of the optimization procedure -did we just move part of the optimization process into hyperparameter selection?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Baselines</head><p>Accuracy (CIFAR-10, CIFAR-100, SVHN) We first compare PBA to other state-of-the-art methods on the CIFAR-10 (Krizhevsky, 2009) and SVHN <ref type="bibr" target="#b30">(Netzer et al., 2011)</ref> datasets. Following <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>, we search over a "reduced" dataset of 4,000 and 1,000 training images for CIFAR-10 and SVHN respectively. Comparatively, CIFAR-10 has a total of 50,000 training images and SVHN has 73,257 training images with an additional 531,131 "extra" training images. PBA is run with 16 total trials on the Wide-ResNet-40-2 model to generate augmentation schedules.</p><p>For the augmentation policy, we initialize all magnitude and probability values to 0, as we hypothesize that less augmentation is required early in training when the validation accuracy is close to training accuracy. However, since training error decreases faster than validation error as training progresses, more regularization should be required, so we expect the probability and magnitude values to increase as training progresses. This would counteract overfitting as we introduce the model to more diverse data.</p><p>We then train models on the full training datasets, using the highest performing augmentation schedules discovered on the reduced datasets. The schedule learned on reduced CIFAR-10 is used to train final models on reduced CIFAR-10, CIFAR-10, and CIFAR-100. The schedule learned on reduced SVHN is used to train final models on reduced SVHN and SVHN. We report results in <ref type="table" target="#tab_1">Table 2</ref>. Each model is evaluated five times with different random initializations, and we report both the mean and standard deviation test set error in %.</p><p>The models we evaluate on include: Wide-ResNet-28-10 (Zagoruyko &amp; Komodakis, 2016), Shake-Shake (26 2x32d) <ref type="table" target="#tab_1">Table 2</ref>. Test set error (%) on CIFAR-10, CIFAR-100, and SVHN. Lower is better. The baseline applies regular random crop and horizontal flip operations. Cutout is applied on top of the baseline, and PBA/AutoAugment are applied on top of Cutout. We report the mean final test error of 5 random model initializations. We used the models: Wide-ResNet-28-10 (Zagoruyko &amp; Komodakis, 2016), Shake-Shake (26 2x32d) <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x96d) <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x112d) <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, and PyramidNet with ShakeDrop <ref type="bibr" target="#b15">(Han et al., 2016;</ref><ref type="bibr" target="#b47">Yamada et al., 2018)</ref>. Code for AA eval on SVHN was not released, so differences between our implementations could impact results. Thus, we report AA* from our re-evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Model Baseline Cutout AA AA* PBA CIFAR-10 Wide-ResNet-28-10 3.87 3.08 2.68 2.58 ± 0.062 Shake-Shake <ref type="formula">(26 2x32d</ref>  <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x96d) <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, Shake-Shake (26 2x112d) <ref type="bibr" target="#b11">(Gastaldi, 2017)</ref>, and Pyramid-Net with ShakeDrop <ref type="bibr" target="#b15">(Han et al., 2016;</ref><ref type="bibr" target="#b47">Yamada et al., 2018)</ref>. PyramidNet with Shake-Drop uses a batch size of 64, and all other models use a batch size of 128. For Wide-ResNet-28-10 and Wide-ResNet-40-2 trained on SVHN, we use the step learning rate schedule proposed in , and for all others we use a cosine learning rate with one annealing cycle <ref type="bibr" target="#b27">(Loshchilov &amp; Hutter, 2016)</ref>. For all models, we use gradient clipping with magnitude 5. For specific learning rate and weight decay values, see the supplementary materials.</p><p>Additionally, we report Baseline, Cutout, and AutoAugment (AA) results found in <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>. For baseline, standard horizontal flipping and cropping augmentations were used. The training data is also normalized by the respective dataset statistics. For Cutout, a patch of size 16x16 is used for all CIFAR datasets, and size 20x20 for SVHN datasets. This applied with 100% chance to each image. Au-toAugment and PBA apply additional augmentations on top of the Cutout set (note that this possibly includes a second application of Cutout). The exception is Reduced SVHN, where the first 16x16 Cutout operation is removed as it was found to reduce performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>On Reduced CIFAR-10, we run PBA for 200 epochs, creating a policy schedule defined over 200 epochs.</p><p>To extend the policy to Shake-Shake and PyramidNet models trained for 1800 epochs, we scale the length of the original schedule linearly.</p><p>While model accuracy on Reduced CIFAR-10 would have likely been improved with hyperparamater tuning for the reduced dataset size and smaller Wide-ResNet-40-2 model, our result shows that no hyperparameter tuning is required for high performance.</p><p>Overall, the PBA learned schedule leads AutoAugment slightly on PyramidNet and Wide-ResNet-28-10, and performs comparably on Shake-Shake models, showing that the learned schedule is competitive with state-of-the-art.</p><p>We visualize the discovered schedule used in training our final CIFAR models in <ref type="figure" target="#fig_3">Figure 4</ref>. For the AutoContrast, Equalize, and Invert augmentations, magnitude values were ignored. From the probability values, our schedule seems to contain all augmentations to at least a moderate degree at some point, which is reasonable given our random perturb exploration method. However, there is emphasis on Cutout, Posterize, Invert, Equalize, and AutoContrast throughout the schedule. <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref> suggests that color-based transformations are more useful on CIFAR compared to geometric ones, and our results also indicate this. However, they also found that the Invert transformation is almost never used, while it was very common in our schedule. A possible explanation may be that a model is able to better adapt to Invert when using a nonstationary policy. PBA may be exploring systematically different parts of the design space than AutoAugment. Alternatively, it may be that by the randomness in PBA, Cutout was introduced and impacted performance. It may be fruitful to explore combinations (a) Operation magnitudes increase rapidly in the initial phase of training, eventually reaching a steady state around epoch 130.  Plots showing the evolution of PBA operation parameters in the discovered schedule for CIFAR-10. Note that each operation actually appears in the parameter list twice; we take the mean parameter value for each operation in this visualization.</p><p>of PBA and AutoAugment to design nonstationary policies with more precision from a RNN Controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>We additionally evaluate on CIFAR-100 using the same augmentation schedule discovered using Reduced CIFAR-10. We find that these results are also competitive with AutoAugment and significantly better than Baseline or only applying Cutout.</p><p>SVHN We ran PBA for 160 epochs on a 1,000 image Reduced SVHN dataset to discover an augmentation policy schedule without tuning any parameters of the algorithm. See the appendix for a visualization of an example PBA policy on the SVHN dataset.</p><p>We then trained models on both the Reduced SVHN and SVHN Full (core training data with extra data), using the discovered schedule. Except for the Wide-ResNet-28-10 model on Reduced SVHN, training was done without tuning, <ref type="figure">Figure 5</ref>. Plot of the expected best child test accuracy after a given number of random trials on Wide-ResNet-40-2. Random policy schedules were generated by randomly selecting intervals of length between 1 and 40, and then selecting a random policy for the interval. All values were selected uniformly from the domain.</p><p>using the hyperparamters from AutoAugment. We were able to obtain a policy comparable with AutoAugment. This demonstrates the robustness of the PBA algorithm across datasets.</p><p>Examining the learned policy schedule, we observe that Cutout, Translate Y, Shear X, and Invert stand out as being present with high probability across all epochs. This fits with the findings of <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref>, indicating that Invert and geometric transformations are successful in SVHN because it is important to learn invariances to these augmentations. From another perspective, all of the augmentations appear with reasonable probability at some point in the schedule, which suggests that using a preliminary strategy like AutoAugment to filter out poor performing augmentations would be an interesting direction to explore.</p><p>Computational Cost AutoAugment samples and evaluates ∼15,000 distinct augmentation policies on child models, which requires about 15000 * 120 = 1.8m epochs of training. In comparison, PBA leverages PBT to learn a schedule with a population of 16 child models. PBA uses 200 epochs of training per child model, for a total of 3200 epochs, or over 500x less compute than AutoAugment.</p><p>As a second baseline, we also train 250 child models with randomly selected augmentation policies, and 250 child models with randomly selected augmentation schedules. In <ref type="figure">Figure 5</ref>, we use this data to plot the expected maximum child model test accuracy after a given number of random trials. As shown, it takes over 250 trials for the expected child accuracy, which is strongly correlated with final accuracy, to approach that reached by a single 16-trial PBA run. Hence, PBA still provides over an order of magnitude speedup here. Real-time Overhead Since PBT trains all members of its population simultaneously, the minimal real-time overhead is just the time it takes to train one child model. In practice, there is a slight overhead from the mutation procedures triggered by PBT, but the overall search time is still small compared to the time to train the primary model. In contrast, AutoAugment leverages reinforcement-learning based techniques, in which a Recurrent Neural Network (RNN) controller is trained with the reinforcement learning algorithm Proximal Policy Optimization (PPO) <ref type="bibr" target="#b36">(Schulman et al., 2017)</ref>. Using this strategy, new augmentation policies can only be sampled and trained after the previous batch of samples has completed, so parallelization is limited to the batch size of the PPO update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Does having a schedule matter?</head><p>PBA distinguishes itself from AutoAugment by learning a augmentation policy schedule, where the distribution of augmentation functions can vary as a function of the training epoch. To check whether a schedule contributes to performance, we try training the model using (1) the last augmentation policy of the PBA schedule as a fixed policy, (2) the augmentation schedule with the order of policies shuffled but the duration of each policy fixed, and (3) the augmentation schedule collapsed into a time-independent stationary distribution of augmentations (i.e., a policy is sampled independently for each batch of data, where each policy is weighted by its duration).</p><p>In <ref type="table" target="#tab_5">Table 3</ref>, we see that training with the PBA Fixed Policy degrades accuracy by ∼10% percent on average, which is significantly worse than training with the full schedule. Compared to using Cutout, the fixed policy gives up ∼50% of gains on Wide-ResNet-28-10, Shake-Shake 32, and Shake-Shake 96, and ∼10% of gains on Shake-Shake 112 and PyramidNet. This shows that the augmentation schedule improves accuracy over a fixed policy, especially on smaller models.</p><p>Similarly, when we evaluated the shuffled schedules (only on Wide-ResNet-28-10), accuracy is also significantly lower, showing that a stationary distribution derived from the sched-ule does not emulate the schedule. We hypothesize that schedule improves training by allowing "easy" augmentations in the initial phase of training while still allowing "harder" augmentations to be added later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyperparameter Tuning and Sensitivity</head><p>We did not tune the discrete space for magnitude or probability options to keep our policy easy to compare to Au-toAugment. We have two copies of each operation, as the AutoAugment sub-policy is able to contain two copies of the same operation as well.</p><p>For the search algorithm, we lightly tuned the explore function and the distribution for count in Algorithm 1, the maximum number of augmentation functions to apply for each batch of data. While we keep the maximum value of count at 2 in line with AutoAugment's length 2 subpolicy, there may be room for performance improvement by carefully tuning the distribution.</p><p>We tried perturbation intervals of 2 and 4 once, but did not find this value to be sensitive. We also tried to run PBT for 100 epochs, but found this to slightly decrease performance when evaluated on models for 200 epochs.</p><p>It may be interesting to consider training a larger child model (e.g, Shake-Shake) for 1,800 epochs to generate a schedule over the full training duration and eliminate the need to stretch the schedule. In a similar vein, an experiment to use PBT directly on the full CIFAR-10 dataset or Wide-ResNet-28-10 model may lead to better performance, and is computationally feasible with PBA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces PBA, a novel formulation of data augmentation search which quickly and efficiently learns state-of-the-art augmentation policy schedules. PBA is simple to implement within any PBT framework, and we release the code for PBA as open source. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PBA Scalability with Compute</head><p>In <ref type="figure" target="#fig_4">Figure 6</ref> we look at how large the model and PBT population size is necessary to learn an effective schedule. The population size determines how much of the search space is explored during training, and also the computational overhead of PBA. Our results indicate that a population size of 16 WRN-40-2 models performs the best. Having more than 16 trials seems not to help, and having less than 16 seems to lead to decreased performance. However, we found that results could fluctuate significantly between runs of PBT, most likely due to exploring a very limited search space with a noisy exploration strategy.</p><p>Besides WRN-40-2, we also tried to use a ResNet-20 <ref type="bibr" target="#b17">(He et al., 2016)</ref> model for PBT population, which required about half the compute. Empirical results (in <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>) suggest that the ResNet-20 population does not achieve as high of a test accuracy as with WRN-40-2, but results were relatively close. Because a ResNet-20 model has much less parameters, training accuracy plateaus faster than WRN-40-2, which may change the effects of augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Hyperparameters</head><p>The hyperparameters used to train WideResNet-40-2 to discover augmentation schedules, and also the ones used to train final models, are displayed in <ref type="table">Table 5</ref>. For full details on the hyperparameters and implementation, see the open source code. <ref type="figure">Figure 7</ref>. Augmentations applied to a SVHN "4" class image, at various points in our augmentation schedule learned on Reduced SVHN data. Each operation is formatted with name, probability, and magnitude value respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SVHN discovered schedule</head><p>See <ref type="figure">Figure 7</ref> for a visualization of the policy on an example image and <ref type="figure">Figure 8</ref> for a visualization of an example PBA policy on the SVHN dataset.</p><p>Examining the learned policy schedule, we observe that Cutout, Translate Y, Shear X, and Invert stand out as being present with high probability across all epochs. This fits with the findings of <ref type="bibr" target="#b6">(Cubuk et al., 2018)</ref> indicating that Invert and geometric transformations are successful in SVHN because it is important to learn invariances to these augmentations. From another perspective, all of the augmentations appear with reasonable probability at some point in the schedule, which suggests that using a preliminary strategy like AutoAugment to filter out poor performing augmentations would be an interesting direction to explore. (b) Normalized plot of operation probability parameters over time. <ref type="figure">Figure 8</ref>. Plots showing the evolution of PBA operation parameters in a schedule learned on Reduced SVHN. Note that each operation actually appears in the parameter list twice; we take the mean parameter value for each operation in this visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of AutoAugment and PBA augmentation strategies. In contrast to AutoAugment, PBA learns a schedule instead of a fixed policy. It does so in a short amount of time by using the PBT algorithm to jointly optimize augmentation policy parameters with the child model. PBA generates a single augmentation function f (x, t) where x is an input image and t the current epoch, compared to AutoAugment's ensemble of augmentation policies f i (x), each of which has several further sub-policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) Normalized plot of operation probability parameters over time. The distribution flattens out towards the end of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Plots showing the evolution of PBA operation parameters in the discovered schedule for CIFAR-10. Note that each operation actually appears in the parameter list twice; we take the mean parameter value for each operation in this visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of relative cost of search to test accuracy of augmentation policies evaluated on WideResNet-28-10. Child model WRN-40-2 was evaluated for population sizes from 2 to 64, and ResNet-20 was evaluated for sizes 2 to 32. All policies were trained on Reduced CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>While neural network 1 EECS, UC Berkeley, Berkeley, California, USA 2 Current affiliation: X, Mountain View, California, USA 3 covariant.ai, Berkeley, California, USA. Correspondence to: Daniel Ho &lt;daniel.ho@berkeley.edu&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">. *CIFAR-100 models are trained with the</cell></row><row><cell cols="3">policies learned on CIFAR-10 data.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Value</cell><cell>Previous Best</cell><cell>AA</cell><cell>PBA</cell></row><row><cell>CIFAR-10</cell><cell>GPU Hours</cell><cell>-</cell><cell>5000</cell><cell>5</cell></row><row><cell></cell><cell>Test Error</cell><cell>2.1</cell><cell>1.48</cell><cell>1.46</cell></row><row><cell cols="2">CIFAR-100 GPU Hours</cell><cell>-</cell><cell>0*</cell><cell>0*</cell></row><row><cell></cell><cell>Test Error</cell><cell>12.2</cell><cell>10.7</cell><cell>10.9</cell></row><row><cell>SVHN</cell><cell>GPU Hours</cell><cell>-</cell><cell>1000</cell><cell>1</cell></row><row><cell></cell><cell>Test Error</cell><cell>1.3</cell><cell>1.0</cell><cell>1.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Ablation study: We evaluate models on CIFAR-10 using a fixed policy (the last policy of the PBA schedule learned on Reduced CIFAR-10), shuffled schedule order, and a fully collapsed schedule, comparing to results with the original PBA schedule. See Section 4.2 for further explanation. We evaluate each model once, and some combinations were not evaluated due to cost considerations.</figDesc><table><row><cell>Model</cell><cell cols="4">Cutout Fixed Policy Order-shuffled Fully-shuffled</cell><cell>PBA</cell></row><row><cell>Wide-ResNet-28-10</cell><cell>3.08</cell><cell>2.76</cell><cell>2.66</cell><cell>2.89</cell><cell>2.576 ± 0.062</cell></row><row><cell>Shake-Shake (26 2x32d)</cell><cell>3.02</cell><cell>2.73</cell><cell>-</cell><cell>-</cell><cell>2.54 ± 0.10</cell></row><row><cell>Shake-Shake (26 2x96d)</cell><cell>2.56</cell><cell>2.33</cell><cell>-</cell><cell>-</cell><cell>2.03 ± 0.11</cell></row><row><cell>Shake-Shake (26 2x112d)</cell><cell>2.57</cell><cell>2.09</cell><cell>-</cell><cell>-</cell><cell>2.03 ± 0.080</cell></row><row><cell>PyramidNet+ShakeDrop</cell><cell>2.31</cell><cell>1.55</cell><cell>-</cell><cell>-</cell><cell>1.46 ± 0.077</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Test error during PBT search and policy schedule evaluated afterwards, for varying population sizes and models. PBA Search with variation of model and compute, on Reduced CIFAR-10 dataset. ResNet-20 (Res) took approximately half the compute of WideResNet-40-2 (WRN). Number in title is the population size, and speedup is relative to AutoAugment. Note that models with larger population sizes, while scoring high during the search, don't actually perform better when re-evaluated. Hyperparameters used for evaluation on CIFAR-10, CIFAR-100, and (R)educed-CIFAR-10. Besides Wide-ResNet-28-10 and Wide-ResNet-40-2 on Reduced SVHN, no hyperparameter tuning was done. Instead, all hyperparameters are the same as those used in AutoAugment.</figDesc><table><row><cell>Model</cell><cell></cell><cell>8-Res</cell><cell cols="5">16-Res 32-Res 16-WRN 32-WRN 64-WRN</cell></row><row><cell cols="2">WRN-40-2 during search</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.8484</cell><cell>0.8446</cell><cell>0.8523</cell></row><row><cell>WRN-40-2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.8452</cell><cell>0.8445</cell><cell>0.8446</cell></row><row><cell cols="2">ResNet-20 during search</cell><cell cols="3">0.7484 0.7657 0.7619</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-20</cell><cell></cell><cell cols="3">0.7457 0.7545 0.7534</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WRN-28-10</cell><cell></cell><cell cols="3">0.9711 0.9721 0.9740</cell><cell>0.9740</cell><cell>0.9736</cell><cell>0.9703</cell></row><row><cell>Relative Speedup</cell><cell></cell><cell>2250x</cell><cell>1125x</cell><cell>562.5x</cell><cell>562.5x</cell><cell>281.25x</cell><cell>140.625x</cell></row><row><cell>Dataset</cell><cell></cell><cell>Model</cell><cell></cell><cell cols="4">Learning Rate Weight Decay Batch Size</cell></row><row><cell>CIFAR-10</cell><cell cols="3">Wide-ResNet-40-2</cell><cell>0.1</cell><cell></cell><cell>0.0005</cell><cell>128</cell></row><row><cell>CIFAR-10</cell><cell cols="3">Wide-ResNet-28-10</cell><cell>0.1</cell><cell></cell><cell>0.0005</cell><cell>128</cell></row><row><cell>CIFAR-10</cell><cell cols="3">Shake-Shake (26 2x32d)</cell><cell>0.01</cell><cell></cell><cell>0.001</cell><cell>128</cell></row><row><cell>CIFAR-10</cell><cell cols="3">Shake-Shake (26 2x96d)</cell><cell>0.01</cell><cell></cell><cell>0.001</cell><cell>128</cell></row><row><cell>CIFAR-10</cell><cell cols="3">Shake-Shake (26 2x112d)</cell><cell>0.01</cell><cell></cell><cell>0.001</cell><cell>128</cell></row><row><cell>CIFAR-10</cell><cell cols="3">PyramidNet+ShakeDrop</cell><cell>0.05</cell><cell cols="2">0.00005</cell><cell>64</cell></row><row><cell>CIFAR-100</cell><cell cols="3">Wide-ResNet-28-10</cell><cell>0.1</cell><cell></cell><cell>0.0005</cell><cell>128</cell></row><row><cell>CIFAR-100</cell><cell cols="3">Shake-Shake (26 2x96d)</cell><cell>0.01</cell><cell></cell><cell>0.0025</cell><cell>128</cell></row><row><cell>CIFAR-100</cell><cell cols="3">PyramidNet+ShakeDrop</cell><cell>0.025</cell><cell></cell><cell>0.0005</cell><cell>64</cell></row><row><cell>R-CIFAR-10</cell><cell cols="3">Wide-ResNet-28-10</cell><cell>0.05</cell><cell></cell><cell>0.005</cell><cell>128</cell></row><row><cell>R-CIFAR-10</cell><cell cols="3">Shake-Shake (26 2x96d)</cell><cell>0.025</cell><cell></cell><cell>0.0025</cell><cell>128</cell></row><row><cell>SVHN</cell><cell cols="3">Wide-ResNet-40-2</cell><cell>0.05</cell><cell></cell><cell>0.005</cell><cell>128</cell></row><row><cell>SVHN</cell><cell cols="3">Wide-ResNet-28-10</cell><cell>0.005</cell><cell></cell><cell>0.001</cell><cell>128</cell></row><row><cell>SVHN</cell><cell cols="3">Shake-Shake (26 2x96d)</cell><cell>0.01</cell><cell cols="2">0.00015</cell><cell>128</cell></row><row><cell>R-SVHN</cell><cell cols="3">Wide-ResNet-28-10</cell><cell>0.05</cell><cell></cell><cell>0.01</cell><cell>128</cell></row><row><cell>R-SVHN</cell><cell cols="3">Shake-Shake (26 2x96d)</cell><cell>0.025</cell><cell></cell><cell>0.005</cell><cell>128</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Richard Liaw, Dogus Cubuk, Quoc Le, and the ICML reviewers for helpful discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Edwards</surname></persName>
		</author>
		<idno>abs/1711.04340</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gan augmentation: Augmenting training data using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hammers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Dickie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valdés Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wardlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno>abs/1810.10863</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multicolumn deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6248110</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2012.6248110" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural selection fails to optimize mutation rates for long-term adaptation on rugged fitness landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ofria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Lenski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanjuán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1145" to="1146" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<ptr target="http://arxiv.org/abs/1805.09501" />
		<title level="m">Learning augmentation policies from</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi: 10.1109/ CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dataset augmentation in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1702.05538</idno>
		<ptr target="https://arxiv.org/abs/1702.05538" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data augmentation using gan for improved liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Shake-shake regularization. CoRR, abs/1705.07485</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.07485" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Google Vizier: A Service for Black-Box Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Karro</surname></persName>
		</author>
		<ptr target="https://research.google.com/pubs/archive/46180.pdf" />
		<editor>Sculley, D.</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch bayesian optimization via local penalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fractional max-pooling. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6071</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1610.02915</idno>
		<ptr target="http://arxiv.org/abs/1610.02915" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densely</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<ptr target="http://arxiv.org/abs/1608" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LION</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<idno>abs/1801.02929</idno>
		<ptr target="http://arxiv.org/abs/1801.02929" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.09846</idno>
		<ptr target="http://arxiv.org/abs/1711.09846" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Smart augmentation -learning an optimal data augmentation strategy. CoRR, abs/1703.08383</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.08383" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient hyperparameter optimization and infinitely many armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno>abs/1603.06560</idno>
		<ptr target="http://arxiv.org/abs/1603.06560" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<ptr target="http://arxiv.org/abs/1608.03983" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ray: A distributed framework for emerging AI applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno>abs/1712.05889</idno>
		<ptr target="http://arxiv.org/abs/1712.05889" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improvements to context based self-supervised learning. CoRR, abs/1711.06379</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.06379" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analysis of rotation matrix and colour constancy data augmentation in classifying images of animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Okafor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
		<idno>doi: 10.1080/ 24751839.2018.1479932</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information and Telecommunication</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data augmentation with manifold exploring geometric transformations for increased performance and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guha Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno>abs/1901.04420</idno>
		<imprint>
			<publisher>CoRR</publisher>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1802.03268</idno>
		<ptr target="http://arxiv.org/abs/1802.03268" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compose domainspecific transformations for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hussian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<idno>abs/1709.01643</idno>
		<ptr target="https://arxiv.org/abs/1709.01643" />
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Apac</surname></persName>
		</author>
		<title level="m">Augmented pattern classification with neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<ptr target="http://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Parallel predictive entropy search for batch global optimization of expensive objective functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>abs/1511.07130</idno>
		<ptr target="http://arxiv.org/abs/1511.07130" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2003.1227801</idno>
		<ptr target="http://dx.doi.org/10.1109/ICDAR.2003.1227801" />
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Gaussian process bandits without regret: An experimental design approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<idno>abs/0912.3995</idno>
		<ptr target="http://arxiv.org/abs/0912.3995" />
		<imprint>
			<date type="published" when="2009" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename></persName>
		</author>
		<idno>abs/1710.10564</idno>
		<ptr target="http://arxiv.org/abs/1710.10564" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding data augmentation for classification: when to warp? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stamatescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno>abs/1609.08764</idno>
		<ptr target="http://arxiv.org/abs/1609.08764" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakedrop Regularization</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1802.02375</idno>
		<ptr target="http://arxiv.org/abs/1802.02375" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<ptr target="http://arxiv.org/abs/1605.07146" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
		<ptr target="http://arxiv.org/abs/1611.01578" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1707.07012</idno>
		<ptr target="http://arxiv.org/abs/1707.07012" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
