<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Independently Recurrent Neural Network (IndRNN)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wanqing</forename><forename type="middle">Li</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Deep Independently Recurrent Neural Network (IndRNN)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are known to be difficult to train due to the gradient vanishing and exploding problems and thus difficult to learn long-term patterns and construct deep networks. To address these problems, this paper proposes a new type of RNNs with the recurrent connection formulated as Hadamard product, referred to as independently recurrent neural network (IndRNN), where neurons in the same layer are independent of each other and connected across layers. Due to the better behaved gradient backpropagation, IndRNN with regulated recurrent weights effectively addresses the gradient vanishing and exploding problems and thus long-term dependencies can be learned. Moreover, an IndRNN can work with non-saturated activation functions such as ReLU (rectified linear unit) and be still trained robustly. Different deeper IndRNN architectures, including the basic stacked IndRNN, residual IndRNN and densely connected IndRNN, have been investigated, all of which can be much deeper than the existing RNNs. Furthermore, IndRNN reduces the computation at each time step and can be over 10 times faster than the commonly used Long short-term memory (LSTM). Experimental results have shown that the proposed IndRNN is able to process very long sequences and construct very deep networks. Better performance has been achieved on various tasks with IndRNNs compared with the traditional RNN, LSTM and the popular Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L ONG-TERM dependency is important for many applications. Especially for applications processing temporal sequences such as action recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and language processing <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the past information is important for the recognition of the future events. There are also applications exploring spatial context information such as scene segmentation <ref type="bibr" target="#b4">[5]</ref> and spatial pooling <ref type="bibr" target="#b5">[6]</ref>. To explore the long-term dependency, recurrent neural networks (RNNs) <ref type="bibr" target="#b6">[7]</ref> have been widely used and have achieved impressive results. Compared with the feed-forward networks such as the convolutional neural networks (CNNs), a recurrent connection is added where the hidden state at the previous time step is used as an input to obtain the current state, in order to keep memory of the past information. The update of the hidden states at each time step follows:</p><formula xml:id="formula_0">h t = σ(Wx t + Uh t−1 + b)<label>(1)</label></formula><p>where x t ∈ R M and h t ∈ R N are the input and hidden state at time step t, respectively. W ∈ R N ×M , U ∈ R N ×N and b ∈ R N are the weights for the current input and the recurrent input, and the bias of the neurons, respectively. σ is an element-wise activation function of the neurons, and M and N are the dimension of the input and the number of neurons in this RNN layer, respectively. Due to the recurrent connections with repeated multiplication of the recurrent weight matrix, training of the RNNs suffers from the gradient vanishing and exploding problem. Despite the efforts in initialization and training techniques <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, it is still very difficult to learn long-term dependency. Several RNN variants such as the long short- <ref type="bibr">Li</ref>  term memory (LSTM) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and the gated recurrent unit (GRU) <ref type="bibr" target="#b14">[15]</ref> have been proposed to address the gradient problems. However, the use of the hyperbolic tangent and the sigmoid functions as the activation function in these variants results in gradient decay over layers. While some researches investigate using deep RNNs with LSTM <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, much research has only focused on using some relative shallow RNNs such as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. On the other hand, the existing RNN models share the same component σ(Wx t + Uh t−1 + b) in <ref type="bibr" target="#b0">(1)</ref>, where the recurrent connection connects all the neurons through time. This makes it hard to interpret and understand the roles of each individual neuron (e.g., what patterns each neuron responds to) without considering the others. Moreover, with the recurrent connections, matrix product is performed at each time step and the computation cannot be easily paralleled, leading to a very time-consuming process when dealing with long sequences.</p><formula xml:id="formula_1">• S.</formula><p>In this paper, we propose a new type of RNN, referred to as independently recurrent neural network (IndRNN). In the proposed IndRNN, the recurrent inputs are processed with the Hadamard product as h t = σ(Wx t +u h t−1 +b). This provides a number of advantages over the traditional RNNs including:</p><p>• Able to process longer sequences: the gradient vanishing and exploding problem is effectively addressed by regulating the recurrent weights, and long-term memory can be kept in order to process long sequences. Experiments have demonstrated that an IndRNN can well process sequences over 5000 steps.</p><p>• Able to construct deeper networks: multiple layers of IndRNNs can be efficiently stacked, especially with skip-connection and dense connection, to increase the depth of the network. An example of 21-layer arXiv:1910.06251v3 [cs.CV] 9 Dec 2020 residual IndRNN and deep densely connected In-dRNN are demonstrated in the experiments.</p><p>• Able to be robustly trained with non-saturated functions such as ReLU: with the gradient backpropagation through time better behaved, non-saturated function such as ReLU <ref type="bibr" target="#b22">[23]</ref> can be used as the activation function and be trained robustly. IndRNN with ReLU is used throughout the experiments.</p><p>• Able to interpret the behaviour of IndRNN neurons independently without the effect from the others: since the neurons in one layer are independent of each other, each neuron's behaviour can be interpreted individually. Moreover, the relationship between the range of the memories and the recurrent weights are established through gradient backpropagation, and the memories learned by the task can be understood by visualizing the recurrent weights, as illustrated in experiments.</p><p>• Reduced complexity. With the new recurrent connections based on element-wise vector product, which is much more efficient than the matrix product, the complexity of IndRNN is greatly reduced compared with the traditional RNNs (over 10 times faster than the cuDNN LSTM).</p><p>Experiments have demonstrated that IndRNN performs much better than the traditional RNN, LSTM and Transformer models on the tasks of the adding problem, sequential MNIST classification, language modelling and action recognition. With the advantages brought by IndRNN, we are able to further show:</p><p>• Better performance can be achieved with deeper IndRNN architectures as verified for the sequential MNIST classification, language modelling and skeleton-based action recognition tasks.</p><p>• Better performance can be achieved by learning with longer dependency as verified for the language modelling tasks.</p><p>Part of this paper has appeared in the conference paper <ref type="bibr" target="#b23">[24]</ref> where IndRNN is introduced and verified on some tasks without further analysing its advantage. Significant extension has been made in this paper. 1) New deep In-dRNN architecture, densely connected IndRNN is proposed to enhance the feature reuse in addition to the residual IndRNN architecture. 2) The relationship between memory and recurrent weight is established through gradient backpropagation, and the learned memories are visualized for skeleton-based action recognition as an example. 3) More experiments are added including a new task, i.e., word-level language modelling. 4) Experiments are conducted to verify that IndRNN with longer temporal dependency and deeper architecture can achieve better performance. 5) Comparison has been made to the popular Transformer <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> on sequential MNIST classification, Char-and Word-level language modelling and skeleton based action recognition. 6) A faster implementation with CUDA optimization is provided and made publicly available, which can be over 10 times faster than the cuDNN LSTM.</p><p>The rest of this paper is organized as follows. Section 2 describes the related work in the literature. Section 3 presents the proposed IndRNN with its gradient backpropagation through time process. It also describes the relationship between the recurrent weight and memory, and its complexity compared with the existing methods. Section 4 explains different deep IndRNN architectures and Section 5 presents the experimental results. Finally, conclusions are drawn at Section 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>It is known that a simple RNN suffers from the gradient vanishing and exploding problem due to the repeated multiplication of the recurrent weight matrix, which makes it very difficult to train and capture long dependencies. In order to solve the gradient vanishing and exploding problem, long short-term memory (LSTM) <ref type="bibr" target="#b26">[27]</ref> was introduced, with a constant error carousel (CEC) to enforce a constant error flow through time. Multiplicative gates including input gate, output gate and forget gate are employed to control the information flow, resulting in many more parameters than the simple RNN. A well known LSTM variant is the gated recurrent unit (GRU) <ref type="bibr" target="#b14">[15]</ref> composed of a reset gate and an update gate, which reduces the number of parameters to some extent. It has been reported in various papers <ref type="bibr" target="#b13">[14]</ref> that GRU achieves similar performance as LSTM. There are also some other LSTM variants <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> reported in the literature. However, these architectures <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> generally take a similar form as LSTM and show a similar performance as well, so they are not discussed further. LSTM and its variants use gates on the input and the recurrent input to regulate the information flow through the network. However, the use of gates based on the recurrent input prevents parallel computation and thus increases the computational complexity of the whole network. To reduce the complexity and process the states of the network over time in parallel, QRNN (Quasi-Recurrent Neural Network) <ref type="bibr" target="#b29">[30]</ref> and SRU (Simple Recurrent Unit) <ref type="bibr" target="#b30">[31]</ref> were proposed where the recurrent connections are fixed to be identity connection and controlled by gates with only input information, thus making most of the computation parallel. While this strategy greatly simplifies the computational complexity, it reduces the capability of their RNNs since the recurrent connections are no longer trainable. By contrast, the proposed IndRNN with regulated recurrent weights addresses the gradient exploding and vanishing problems without losing the power of trainable recurrent connections and without involving gate parameters. Moreover, IndRNN reduces computation and runs much faster than LSTM and even SRU <ref type="bibr" target="#b30">[31]</ref>.</p><p>There are also some simple RNN variants trying to solve the gradient vanishing and exploding problem by altering the recurrent connections. In <ref type="bibr" target="#b31">[32]</ref>, a skip RNN was proposed where a binary state update gate is added to control the network to skip the processing of one time step. In this way, fewer time steps may be processed and the gradient vanishing and exploding problem can be alleviated to some extent. In <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, a unitary evolution RNN was proposed where the recurrent weights are empirically defined in the form of a unitary matrix. In this case, the norm of the backpropagated gradient can be bounded without exploding. In <ref type="bibr" target="#b34">[35]</ref>, a Fourier Recurrent Unit (FRU) was proposed where the hidden states over time are summarized with Fourier basis functions and the gradients are bounded. However, such methods usually introduce other transforms on the recurrent weight which complicates the recurrent units, making them hard to use and interpret. On the contrary, the proposed IndRNN further simplifies the recurrent connections and makes all the neurons in one layer independent from others, thus easier to interpret (e.g. understanding the long and short memories kept by the task).</p><p>There are also attempts at using the non-saturated activation functions such as ReLU (rectified linear unit) to reduce the gradient decay problem introduced by the activation function. While this reduces the gradient decay problem, it greatly aggravates the effect introduced by the repeated use of the recurrent weight matrix, making the network highly unstable. To alleviate this problem, works on initialization and training techniques, such as initializing the recurrent weights to a proper range or regulating the norm of the gradients over time, were proposed in the literature. In <ref type="bibr" target="#b9">[10]</ref>, an initialization technique was proposed for an RNN with ReLU activation, termed as IRNN, which initializes the recurrent weight matrix to be the identity matrix and bias to be zero. In <ref type="bibr" target="#b10">[11]</ref>, the recurrent weight matrix was further suggested to be a positive definite matrix with the highest eigenvalue being unity and all the remaining eigenvalues less than 1. In <ref type="bibr" target="#b7">[8]</ref>, the geometry of RNNs was investigated and a path-normalized optimization method for training was proposed for RNNs with ReLU activation. In <ref type="bibr" target="#b8">[9]</ref>, a penalty term on the squared distance between successive hidden states' norms was proposed to prevent the exponential growth of IRNN's activation. Although these methods help ease gradient exploding, they are not able to completely avoid the problem (e.g. the eigenvalues of the recurrent weight matrix may still be larger than 1 in the process of training and lead to gradient exploding). Moreover, the training of an RNN with ReLU is very sensitive to the learning rate. When the learning rate is large, the gradient is likely to explode. The proposed IndRNN addresses the gradient problems by regulating the recurrent weights, which are then mapped to the constraint on the maximum gradient. It can work with ReLU and be trained robustly. As a result, an IndRNN is able to process very long sequences (e.g. over 5000 steps as demonstrated in the experiments).</p><p>In addition to solving the gradient vanishing and exploding problem in order to process long sequences and learn long-term dependency, efforts are also put into investigating deeper RNN networks. Compared with the deep CNN architectures which could be over 100 layers such as the residual CNN <ref type="bibr" target="#b35">[36]</ref> and the densely connected CNN <ref type="bibr" target="#b36">[37]</ref>, most of the existing RNN architectures only consist of several layers (2 or 3 for example <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>). This is partly due to the gradient vanishing and exploding problem which already results in the difficulty in training a single-layer RNN. A deeper RNN <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> may further aggravate this problem. For LSTM models, since all the gate functions, input and output modulations employ sigmoid or hyperbolic tangent functions as the activation function, it suffers from the gradient decay over layers when multiple LSTM layers are stacked into a deep model. Currently, a few models were reported that employ residual connections <ref type="bibr" target="#b35">[36]</ref> between LSTM layers to make the network deeper <ref type="bibr" target="#b39">[40]</ref>. As shown in <ref type="bibr" target="#b40">[41]</ref>, residual networks behave like ensembles of relatively shallow networks. For the residual connections to work efficiently, each residual block desires a good gradient behaviour. Due to gradient decay problem in each LSTM, the deep LSTM model with the residual connections cannot significantly improve the performance, which is also observed in <ref type="bibr" target="#b41">[42]</ref>. In <ref type="bibr" target="#b42">[43]</ref>, a recurrent highway network (RHN) was proposed where at each time step, multiple layers with highway connections are used to process the recurrent state. Compared with the LSTM, the transition process is greatly deepened. Recently, Transformers <ref type="bibr" target="#b24">[25]</ref> have been proposed using self-attention. It does not contain any recurrent connections, hence resembles a CNN more than a RNN. All in all, RNN architectures that can be stacked with multiple layers and efficiently trained are still highly desired. The proposed IndRNN well addresses this issue and can construct very deep models with the use of ReLU, residual or dense connections (e.g. over 20 layers as demonstrated in the experiments).</p><p>Other than the above works making RNN process long sequences and construct deep networks, there are also other variants such as the multiplicative integration <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, multidimensional <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> or bidirectional extensions, which is beyond the scope of this paper and thus not further explained. One interesting result reported in <ref type="bibr" target="#b28">[29]</ref> shows that the capacities of existing RNNs are very similar and the difference in their performances are driven primarily by differences in training effectiveness. Compared with the existing RNNs, the proposed IndRNN uses a simple formulation, and is easy and robust to train with different kinds of techniques such as the ReLU, batch normalization and dropout from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INDEPENDENTLY RECURRENT NEURAL NET-WORK (INDRNN)</head><p>The proposed independently recurrent neural network (In-dRNN) follows:</p><formula xml:id="formula_2">h t = σ(Wx t + u h t−1 + b)<label>(2)</label></formula><p>where x t ∈ R M and h t ∈ R N are the input and hidden state at time step t, respectively. W ∈ R N ×M , u ∈ R N and b ∈ R N are the weights for the current input and the recurrent input, and the bias of the neurons, respectively. is the Hadamard product, σ is an element-wise activation function of the neurons, and N is the number of neurons in this RNN layer. Compared with the traditional RNN where the recurrent weight U is a matrix and processes the recurrent input using matrix product, the recurrent weight u in IndRNN is a vector and processes the recurrent input with element-wise vector product. Each neuron in one layer is independent from others, thus termed as "independently recurrent". For the n-th neuron, the hidden state h n,t can be obtained as</p><formula xml:id="formula_3">h n,t = σ(w n x t + u n h n,t−1 + b n )<label>(3)</label></formula><p>where w n , u n and b n are the n-th row of the input weight, recurrent weight and bias, respectively. The proposed In-dRNN can be extended to a convolutional IndRNN where, instead of processing input of each time step using a fully  connected weight (Wx t ), it is processed with convolutional operation (W * x t , where * denotes the convolution operator). <ref type="figure" target="#fig_1">Fig. 1b</ref> illustrates the proposed IndRNN unfolded in time. Each solid dot represents a neuron in a layer and each line represents a connection. It can be seen that the processing of each neuron in one layer is independent from each other as noted by different colors. Each neuron only receives information from the input and its own hidden state at the previous time step. That is, each neuron in an IndRNN deals with one type of spatial-temporal pattern independently. On the contrary, neurons in the simple RNN are connected together over time by the recurrent weight connection as shown in <ref type="figure" target="#fig_1">Fig. 1a</ref>. <ref type="figure" target="#fig_1">Fig. 1b</ref> also shows that one more IndRNN layer can explore the correlation (crosschannel information) among the neurons in one layer as the neuron in the following layer takes output of all the neurons in the previous layer as input. The relationship between IndRNN and the conventional RNN is illustrated in the Appendix A, where we have shown that under certain circumstances (e.g. linear activation), a traditional RNN is a special case of a two-layer IndRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backpropagation Through Time for An IndRNN</head><p>Since the neurons in each layer are independent of each other, the gradient backpropagation through time for neurons in one layer can also be performed independently. For the n-th neuron h n,t = σ(w n x t + u n h n,t−1 ) where the bias is ignored, suppose the objective trying to minimize at time step T is J n . Then the gradient back propagated to the time step t is</p><formula xml:id="formula_4">∂Jn ∂hn,t = ∂Jn ∂hn,T ∂hn,T ∂hn,t = ∂Jn ∂hn,T T −1 k=t ∂h n,k+1 ∂h n,k = ∂Jn ∂hn,T T −1 k=t σ n,k+1 un = ∂Jn ∂hn,T u T −t n T −1 k=t σ n,k+1 (4)</formula><p>where σ n,k+1 is the derivative of the element-wise activation function. It can be seen that the gradient only involves the exponential term of a scalar value u n which can be easily regulated, and the gradient of the activation function which is often bounded in a certain range. Compared with the gradients of an RNN ( ∂J</p><formula xml:id="formula_5">∂h T T −1 k=t diag(σ (h k+1 ))U T where diag(σ (h k+1 ))</formula><p>is the Jacobian matrix of the element-wise activation function), the gradient of an IndRNN directly depends on the value of the recurrent weight (which is changed by a small magnitude according to the learning rate) instead of matrix product (which is mainly determined by its eigenvalues and can be changed significantly even though the change to each matrix entries is small <ref type="bibr" target="#b47">[48]</ref>). Thus the training of an IndRNN is more robust than a traditional RNN.</p><p>To address the gradient exploding and vanishing problem over time, we need to regulate the exponential term "u T −t n T −1 k=t σ n,k+1 " to an appropriate range (ignoring the gradient backpropagated from the objective to the hidden state at time step T , ∂Jn ∂h n,T ). Note that for some activation functions such as ReLU, when not activated, its derivative is 0 and the neuron cannot learn anything. Here we only discuss the gradient when neurons are activated ( T −1 k=t σ n,k+1 = 0). For gradient exploding, denoting the magnitude of the largest gradient value without exploding by γ, the constraint "|u T −t n T −1 k=t σ n,k+1 | ≤ γ" avoids the gradient exploding problem. Accordingly, the recurrent weight |u n | satisfies |u n | ≤ (T −t) γ | T −1 k=t σ n,k+1 | . For the commonly used activation functions such as ReLU and tanh, their derivatives are no larger than 1, i.e., |σ n,k+1 | ≤ 1. Especially for ReLU, its gradient σ is either 0 or 1. Therefore, this constraint can be simplified to |u n | ≤ (T −t) √ γ. For especially long sequences, it can be further simplified to |u n | ≤ 1.</p><p>On the other hand, for the gradient vanishing problem, the magnitude of the gradient needs to be larger than 0, which can be met by |u n | &gt; 0. In the literature and practice, when gradient vanishing is referred, usually we expect the gradient after many time steps (i.e. T − t) to be larger than a small value ( ) in order for the gradient to effectively make TABLE 1: Relationship between recurrent weight and memory through gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Gradient Recurrent weight</head><p>Long-term effective after T − t steps:</p><formula xml:id="formula_6">∂J n,T ∂h n,t &gt; |un| ∈ ( (T −t) | T −1 k=t σ n,k+1 | , +∞)</formula><p>Short-term reduced after T − t steps:</p><formula xml:id="formula_7">∂J n,T ∂h n,t ≤ |un| ∈ [0, (T −t) | T −1 k=t σ n,k+1 | ]</formula><p>not exploding:</p><formula xml:id="formula_8">∂J n,T ∂h n,t ≤ γ |un| ∈ [0, (T −t) γ | T −1 k=t σ n,k+1 | ]</formula><p>a change. Therefore, we further strength the above gradient constraint to |u T −t n T −1 k=t σ n,k+1 | &gt; . Accordingly, we can obtain the range for the recurrent weights to effectively tackle the gradient vanishing as |u n | &gt; (T −t)</p><formula xml:id="formula_9">| T −1 k=t σ n,k+1 | .</formula><p>One advantage of IndRNN is that it can work robustly with ReLU as activation function. For ReLU, when the neuron is activated, its gradient is fixed to be 1 without decay, greatly alleviating the gradient vanishing problem in the recurrent processing. Therefore, the constraint can be simplified to</p><formula xml:id="formula_10">|u n | &gt; (T −t) √ .</formula><p>In other words, with the recurrent weight regulated in this range |u n | &gt; (T −t) √ , IndRNN with ReLU can avoid gradient vanishing. This is the constraint for one neuron, and for IndRNN with many neurons, this constraint can be further relaxed. For a multiple layers RNN with each layer containing multiple neurons, different neurons may learn different features including features from different ranges of memories. For IndRNN, neurons in one layer are independent from each other, the vanishing gradient of one neuron does not affect the learning of other neurons. For example, some IndRNN neurons in the first few layers may only learn the spatial information at its own time step projecting the input into a feature space, then the following layers can process such features in the temporal domain. Such neurons in the first few layers do not concern any memory and the gradient backpropagated from time is zero (gradient vanishing from the perspective of temporal gradient backpropagation for this specific neuron), but the gradients backpropagated from the deep layers at the current step still exist. Moreover, with the temporal gradient backpropagation of neurons independent in one layer, the above neurons do not affect the learning of other neurons in the same layer. Therefore, the condition of avoiding gradient vanishing shown above (|u n | &gt; (T −t) √ ) can be further relaxed to |u n | &gt; (T −t) √ , ∃u n ∈ u, which can be easily met. Although this condition can be forcibly applied in the implementation, in practice, we found that there is no need for even such a relaxed constraint. The recurrent weights generally distribute in the whole range of memories even under no constraints as shown in the following Subsection 5.3.3 <ref type="figure" target="#fig_7">Fig. 6</ref>, and highly unlikely that all the recurrent weights are trained to be close to zero with the gradient of the whole network close to zero.</p><p>Therefore, for robust training of IndRNN without gradient exploding and vanishing, only the constraint |u n | ≤ (T −t) √ γ is needed and thus used in the experiments. Note that the regulation on the recurrent weight u is different from the gradient or gradient norm clipping technique. For the gradient clipping or gradient norm clipping <ref type="bibr" target="#b48">[49]</ref>, the calculated gradient has already exploded and is forced back to a predefined range. The gradients for the following steps may keep exploding. In this case, the gradient of the other layers relying on this neuron may not be accurate. On the contrary, the regulation proposed here essentially maintains the gradient in an appropriate range without affecting the gradient backpropagated through this neuron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Weight and Memory</head><p>In this Subsection, the relationship between the recurrent weight and the memory keeping mechanism in IndRNN is analyzed to explicitly show the range of the recurrent weights for keeping different ranges of memories.</p><p>One of the key roles of RNNs is to keep memories of the past. That is to say, the current state at time step T can be affected by the past state at time step t. In other words, a change of the past state (h t ) changes the current state (h T ). From the perspective of gradient backpropagation, the gradient from h T propagated to h t could still be effectively updating the weights. Assume the minimum effective gradient is , and the gradient follows | ∂J n,T ∂hn,t | &gt; . Accordingly, a range for the recurrent weight to keep memory of T − t time steps can be obtained, which is</p><formula xml:id="formula_11">|u n | ∈ ( (T −t) | T −1</formula><p>k=t σ n,k+1 | , +∞) according to (4) (ignoring the gradient backpropagated from the objective to the hidden state at time step T , ∂Jn ∂h n,T ). On the contrary, when only short-term memory is required, the gradient from h T propagated to h t should not be able to effectively update the weights, and the gradient follows | ∂J n,T ∂hn,t | ≤ . The range of the recurrent weight can be obtained accordingly as |u n | ∈ [0, (T −t)</p><formula xml:id="formula_12">| T −1 k=t σ n,k+1 | ].</formula><p>In an extreme case, the short-term memory is no memory of the past at all, where the recurrent weight is 0 and the gradient backpropagated from the future time steps is zero (the gradient backpropagated from the deeper layers still exists). The short-term memories can be important for the performance of the network as well. Especially for a multiple layers RNN, the short-term memories from the earlier layers may be accumulated in the following layer with long-term memory, which is also the reason that the gradient constraint on the vanishing gradient in the above subsection can be alleviated. Together with the basic requirement of avoiding the gradient exploding described above, the relationships among recurrent weight and memory through the gradient can be depicted as <ref type="table">Table 1</ref>. With this relationship between the recurrent weight and memory, the range of memories learned by the task can be visualized through the learned recurrent weights, which is illustrated in the experiments.</p><p>Without any prior knowledge, the short-term memories and long-term memories are assumed to be both important for sequence problems. Therefore, the recurrent weight can be initialized in the range [0, (T −t)</p><formula xml:id="formula_13">γ | T −1 k=t σ n,k+1 | ], where different neurons can keep memories of different lengths.</formula><p>For ReLU, it can be simplified to |u n | ∈ [0, (T −t) √ γ]. The training of the network can further refine the network to learn required memories for the task by updating the recurrent weights. On the other hand, for tasks such as classification after reading a sequence, for the last layer, only the hidden state at the last time step is used for classification. In order to make the hidden state at the last time step rich of information as much as possible, it is better to keep the long-term memory to aggregate useful information from the beginning to the last time step. Therefore, the recurrent weights can be initialized in the range</p><formula xml:id="formula_14">[ (T −t) | T −1 k=t σ n,k+1 | , (T −t) γ | T −1 k=t σ n,k+1 | ] (which is [ (T −t) √ , (T −t) √ γ]</formula><p>for ReLU) to only keep long-term memory, which is also observed to improve performance in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity in terms of the Number of Parameters and Computation</head><p>Regarding the number of parameters, for a N -neuron RNN network with input of dimension M , the number of parameters in a traditional RNN is M × N + N × N + N , while the number of parameters for LSTM is 4 * (M ×N +N ×N +N ), which is much larger than the simple RNN. By contrast, IndRNN uses a simple formulation with less parameters, M × N + 2 × N . Even for a two-layer IndRNN where both layers consist of N neurons, the number of parameters is M × N + N × N + 4 × N , which is of a similar order to the traditional RNN. Usually the number of parameters used in a network is large (e.g. much larger than 3N ), therefore, the number of parameters introduced by the recurrent weight vectors and extra bias vector (3 × N ) is negligible compared with the recurrent weight matrix (N × N ) used in the traditional RNN. And obviously, IndRNN uses much less parameters than LSTM.</p><p>From the perspective of computation, the processing of the input (Wx t + b) is independent at different timesteps and can be implemented in parallel, which is the same for the conventional RNNs. For the processing of the recurrent input (which is usually the most time-consuming part in RNNs), IndRNN only applies one element-wise vector product operation, one adding operation and one activation calculation, involving less computation than the traditional RNNs with matrix product. Moreover, IndRNN works efficiently with ReLU, which is more efficient than other saturated functions such as the tanh and sigmoid used in the traditional RNNs. To further accelerate the computation on GPU, similarly as SRU <ref type="bibr" target="#b30">[31]</ref> we implemented a fast CUDA optimized version based on PyTorch which is made publicly available. It combines the operations of elementwise vector product, adding and activation calculation to avoid the latency in calling each function separately. In our experiments, we found that this is more accurate than using the forward and backward time separately for each batch. The one-layer IndRNN and two-layer IndRNN are evaluated and the cuDNN LSTM and SRU <ref type="bibr" target="#b30">[31]</ref> (both one layer) are used for comparison. The detailed setup is shown in Subsection 5.2.1. It can be seen that IndRNN (both 1layer and 2-layer) takes much less time than LSTM. To be specific, for sequences of length 256, 512 and 1024, a 1layer IndRNN is 4.3, 7.6 and 12.9 times faster than LSTM, respectively. Even a 2-layer IndRNN is 2.9, 4.8 and 8.0 times faster than a 1-layer LSTM for sequences of length 256, 512 and 1024, respectively. It is worth noting that while the time for processing longer sequences using LSTM is dramatically increasing, the time using IndRNN only increases a very small amount. This indicates that compared with the matrix product (in the weight processing of the input), the element-wise product and addition is much faster, making IndRNN highly efficient with a comparable computation as a feedforward network. Moreover, it can also be seen that the one-layer IndRNN is faster than SRU <ref type="bibr" target="#b30">[31]</ref> and even two-layer IndRNN is faster than SRU <ref type="bibr" target="#b30">[31]</ref> for processing sequence of length 1000, which should be the case since SRU uses more computations at each time step. The memory consumptions of different RNNs also follow closely to the number of parameters and the computation, which will not be further detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP INDEPENDENTLY RECURRENT NEURAL NETWORK</head><p>As described in the above Section, IndRNN can be stacked to construct multiple-layer IndRNN networks to better explore the cross-channel information. One important advantage of IndRNN is that it can work robustly with nonsaturated activation functions such as ReLU, since the gradient behaviour of IndRNN can be well regulated. The nonsaturated functions provide stable gradient propagation when activated (feature values larger than 0), which can greatly facilitate the development of deep networks. While in the current CNNs, non-saturated activation functions such as ReLU have been widely used and become the default setting, it is still not prevailing in the construction of RNNs due to the gradient exploding over time. Although there are some attempts on using ReLU as activation function such as IRNN <ref type="bibr" target="#b9">[10]</ref>, it is very unstable and even carefully trained with a very small learning rate (10 −5 ), the model may still explode. Most of the RNN models adopt gates with saturated functions to address the gradient exploding problem. To the best of our knowledge, IndRNN is the only one that stably and completely works with non-saturated activation functions such as ReLU without any gate function including saturated activation. Thus IndRNN can efficiently construct deep networks.</p><p>Several deep IndRNN architectures are proposed in this paper, including the basic stacked IndRNN, residual IndRNN (res-IndRNN) and densely connected IndRNN (dense-IndRNN). First, the building block for the basic IndRNN architecture is shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>, where "Weight" and "IndRec+ReLU" denote the processing of input and the recurrent process at each step with ReLU as the activation function. In addition, batch normalization, denoted as "BN", can also be employed in the IndRNN network before or after the activation function as shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>. By stacking this block, a deep basic IndRNN network can be constructed.  Since the weight layer (Wx t + b) is used to process the input, it is natural to extend it to multiple layers to deepen the processing, making the architecture flexible with different depths to process input features.</p><p>The second type of deep IndRNN architecture is the residual IndRNN shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>, which adds an identity shortcut (skip-connection) <ref type="bibr" target="#b35">[36]</ref> to bypass the non-linear transformation of the input features. Denoting the nonlinear transformation at layer l and time step t by F l,t , it follows x l,t = x l−1,t + F l,t (x l−1,t ), where x l,t is the output feature at layer l. Since the non-linear transformations containing IndRNN are shared over time, F l,t can be simplified to F l and accordingly, x l,t = x l−1,t + F l (x l−1,t ). It can be seen that the skip-connection does not affect the processing in the time dimension, but makes the deeper feature a summation of the shallower feature and a residual feature processed with F l . In this case, the gradient can be directly propagated to the previous layers through the skip-connection, greatly alleviating the gradient decay problem across layers. Therefore, substantially deeper residual IndRNN can be constructed. <ref type="figure" target="#fig_4">Fig. 3b</ref> shows the non-linear transformation F l,t of type "pre-activation" similarly as in <ref type="bibr" target="#b49">[50]</ref>, which uses the processing of a composite function of three consecutive operations: batch normalization (BN), the recurrent processing and activation calculation (IndRec+ReLU) over time, and the weight processing (Weight), denoted by "BN-IndRec+ReLU-Weight" for simplicity. Also in each residual block, different numbers of "BN-IndRec+ReLU-Weight" operation can be used, and two operations are shown in the figure.</p><p>The third type of deep IndRNN architecture is the densely connected IndRNN shown in <ref type="figure" target="#fig_4">Fig. 3c</ref>, which, instead of using a skip-connection, uses a concatenation operation to combine all the features of the previous layers <ref type="bibr" target="#b36">[37]</ref>. It follows x l,t = C(x l−1,t , F l (x l−1,t )), where C is concatenation operation. By substituting x l−1,t recursively, it can be easily seen that x l,t is a combination of all the features in the previous layers and the non-linear transformation of the features in the previous layer. This also forms an identity function among deeper layers and shallower layers, thus allowing gradients to be properly propagated. Compared with the residual IndRNN, it explicitly processes all the features from all the previous layers, encouraging the feature reuse and thus reducing the number of parameters. Different composite functions can be used as in the residual IndRNN, and <ref type="figure" target="#fig_4">Fig. 3c</ref> shows an example with the composite function "Weight-BN-IndRec+ReLU". As the number of layers concatenated together increases, the dimension of features also increases, leading to an increased number of parameters. Therefore, transition layers as in <ref type="bibr" target="#b36">[37]</ref> are used to separate dense layers into dense blocks to reduce the dimension of features. More details on the densely connected IndRNN architecture settings are shown in the following Subsection 5.1 and Appendix B.</p><p>We would like to note that while the residual connection and dense connection can also be applied to the other RNN models, they may behave better on IndRNN. As reported in <ref type="bibr" target="#b40">[41]</ref>, residual networks behave like ensembles of relatively shallow networks. The performance of residual networks is highly dependent on the depth of the model (without residual connections) that can be constructed and effectively trained. In other words, it still depends on the gradient of the model without the residual connection. The dense connection can also be analysed similarly. Therefore, combined with the residual or dense connections, IndRNN is expected to work better than other RNN models with better gradient behaviour over layers. In all, IndRNN can construct deep RNN models and even deeper models with residual and dense connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this Section, first, the capability of IndRNN in processing long sequences and constructing deep networks is verified. Then the performance of the proposed IndRNN on various tasks are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Setup</head><p>ReLU was used for activation throughout the experiments. For tasks with output at every timestep such as the language modeling problem in Subsection 5.2.3, the recurrent weights of all layers were initialized uniformly in the range [0, (T −t) √ γ], while for tasks such as classification where only the output at the last time step in the last layer is used, the recurrent weights of the last layer were initialized uniformly in the range [ (T −t) √ , (T −t) √ γ] as described in Subsection 3.2. The recurrent weights are constrained to be in the range of |u n | ≤ (T −t) √ γ for IndRNN with ReLU as analysed in Subsection 3.1. This is especially important in processing long sequences where a small change of recurrent weights may significantly change the gradients. In our experiments, γ is simply set to 1.0 for long sequences since (T −t) √ γ is already close to 1 when T is very large. For short sequences such as 20 steps, the constraint can also be removed as the gradient exploding problem is not very severe in such cases. We have also conducted an ablation study on γ, which also verifies that the performance is not very sensitive to the setting of γ as long as it is in a reasonable range (values in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> for example).</p><p>To accelerate training, batch normalization was used except in the simple adding problem. Moreover, for classification tasks where the whole sequence is processed for output at the final time step, the statistics used in the batch normalization layer were obtained based on the samples at all time steps, while for other tasks such as language modeling which cannot access information from future time steps, the statistics are obtained based on all the samples in each time step. When dropout is applied, the dropout mask is shared over time to avoid the clipping of long memory. Weight decay of 10 −4 is used for the weight parameters (without applying to the recurrent weight and bias). All the networks were trained using the Adam optimization method <ref type="bibr" target="#b50">[51]</ref> with initial learning rate 2 × 10 −4 . The learning rate is reduced by a factor of 5 when the accuracy (or loss) on the validation data no longer improves (drops) (with patience set to 100).</p><p>For the densely connected IndRNN, the network shape was simply following the conventional denseCNN <ref type="bibr" target="#b36">[37]</ref>. Each dense layer (the non-linear transformation function) consists of two composite functions as shown in <ref type="figure" target="#fig_4">Fig. 3c</ref> and produces k feature maps, termed as the growth rate. The first composite function is called the bottleneck layer and the number of neurons is set to four times (4*k) the growth rate. For each transition layer, the number of the neurons is set to be the half (50%) of the input feature maps. The dense block configuration is set to <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4)</ref>, where in the first, second and third dense block, 8, 6 and 4 dense layers are used, respectively. This keeps a relatively similar number of neurons in each dense block. Note that it is different from the denseCNN <ref type="bibr" target="#b36">[37]</ref> because the tasks in the following experiments do not concern pooling (which reduces the size of the features). For the whole network, one IndRNN layer with six times (6*k) the growth rate are used to process the input first before going through the following dense layers. In the following, the residual IndRNN and the densely connected IndRNN are noted as res-IndRNN and dense-IndRNN, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Verification of Processing Long Sequences and Constructing Deep Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Adding Problem</head><p>The adding problem <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref> is commonly used to evaluate the performance of RNN models. Two sequences of length T are taken as input. The first sequence is uniformly sampled in the range (0, 1) while the second sequence consists of two entries being 1 and the rest being 0. The output is the sum of the two entries in the first sequence indicated by the two entries of 1 in the second sequence. Three different lengths of sequences, T = 100, 500 and 1000, were used for the experiments to show whether the tested models have the ability to model long-term memory.</p><p>The RNN models included in the experiments for comparison are the traditional RNN with tanh, LSTM, IRNN (RNN with ReLU). The proposed IndRNN was evaluated with ReLU activation function. Since GRU achieved similar performance as LSTM <ref type="bibr" target="#b13">[14]</ref>, it is not discussed here. RNN, LSTM, and IRNN are all one layer while the IndRNN model is two layers. 128 hidden units were used for all the models, and the number of parameters for RNN, LSTM, and twolayer IndRNN are 16K, 67K and 17K, respectively. It can be seen that the two-layer IndRNN has a comparable number of parameters to that of the one-layer RNN, while many more parameters are needed for LSTM. As discussed in Subsection 3.1, the recurrent weight is constrained in the range of |u n | ∈ (0,</p><formula xml:id="formula_15">T √</formula><p>2) for the IndRNN. Mean squared error (MSE) was used as the objective function and the Adam optimization method <ref type="bibr" target="#b50">[51]</ref> was used for training. The baseline performance (predicting 1 as the output regardless of the input sequence) is mean squared error of 0.167 (the variance of the sum of two independent uniform distributions). The initial learning rate was set to 2 × 10 −3 for models with tanh activation and set as 2 × 10 −4 for models with ReLU activations. However, as the length of the sequence increases, the IRNN model does not converge and thus a smaller initial learning rate (10 −5 ) was used. The learning rate was reduced by a factor of 10 every 20K training steps. The training data and testing data were all generated randomly throughout the experiments, different from <ref type="bibr" target="#b32">[33]</ref> which only used a set of randomly pre-generated data.</p><p>The results are shown in <ref type="figure" target="#fig_5">Fig. 4a, 4b and 4c</ref>. First, for short sequences (T = 100), most of the models (except RNN with tanh) performed well as they converged to a very small error (much smaller than the baseline). When the length of the sequences increases, the IRNN and LSTM models have difficulties in converging, and when the sequence length reaches 1000, IRNN and LSTM cannot minimize the error any more. However, the proposed IndRNN can still converge to a small error very quickly. This indicates that the proposed IndRNN can model a longer-term memory than the traditional RNN and LSTM.</p><p>From the figures, it can also be seen that the traditional RNN and LSTM can only keep a mid-range memory (about 500 -1000 time steps). To evaluate the proposed IndRNN model for very long-term memory, experiments on sequences with length 5000 were conducted where the result is shown in <ref type="figure" target="#fig_5">Fig. 4d</ref>. It can be seen that IndRNN can still model it very well. Note that the noise in the result of IndRNN is because the initial learning rate (2 × 10 −4 ) was relatively large and once the learning rate dropped, the performance became robust. This demonstrates that IndRNN can effectively address the gradient exploding and vanishing problem over time and keep a long-term memory.</p><p>The complexity of IndRNN was evaluated using the adding problem (with the similar configuration as above) compared with the cuDNN LSTM and SRU <ref type="bibr" target="#b30">[31]</ref> which has been reported to be much faster than LSTM. Sequences of different lengths including 256, 512 and 1024 are all tested to be consistent with <ref type="bibr" target="#b30">[31]</ref>. A GPU P100 is used as the test platform and the training time for each batch (averaged over 100 batches) including both the forward and backward time (milliseconds) are measured. The comparison is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. It can be seen that IndRNN (both 1-layer and 2-layer) takes much less time than LSTM, and reaches 12.9 times faster than LSTM when sequence length is 1024. It is also faster than SRU <ref type="bibr" target="#b30">[31]</ref>, and even a two-layer IndRNN is faster than SRU for processing sequence of length 1000. Moreover, while the time for processing longer sequences using LSTM is dramatically increasing, the time using IndRNN only increases a little bit, indicating that the recurrent connection is very efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Sequential MNIST Classification</head><p>Sequential MNIST classification has been widely used to evaluate RNN models in capturing long dependencies. The pixels of MNIST digits <ref type="bibr" target="#b53">[54]</ref> are presented sequentially to the networks and classification is performed after reading all pixels. Each digit image consists of 784 pixels (28 * 28), and to achieve a good classification accuracy, the models need to remember the patterns of such a long sequence, thus verifying the capability of the RNN models in capturing long-term dependency. To make the task even harder, the pixels are first processed with a fixed random permutation and then used for classification, known as the permuted MNIST classification. First, a six-layer IndRNN is used for test. Each layer contains 128 neurons, and batch normalization is inserted after each layer. Dropout with a dropping probability of 0.1 is used after each layer. 5% of the training data is reserved for validation. The results are shown in <ref type="table" target="#tab_1">Table 2</ref> in comparison with the existing methods. It can be seen that IndRNN achieved better performance than the existing RNN models.</p><p>To further verify that IndRNN can be stacked into very deep networks and still be effectively trained with a good performance, a deep IndRNN with 12 layers, a deep residual IndRNN (12 layers) and a densely connected IndRNN are also tested. For the deep IndRNN and the residual IndRNN, the settings are the same as the above 6-layer IndRNN, e.g., 128 neurons and dropout (0.1) applied after each layer. For the densely connected IndRNN, the growth rate is set to 16, and dropout is applied after the input (0.2), each dense layer (0.2), each bottleneck layer (0.1) and each transition layer (0.1). The results are also shown in <ref type="table" target="#tab_1">Table 2</ref>. Compared with the 6-layer IndRNN, deeper IndRNN (12 layers) achieves better performance, higher than 99% and 97% in terms of accuracy for the normal MNIST classification and permuted MNIST classification, respectively. Moreover, the residual IndRNN and the densely connected IndRNN outperforms the simple stacked IndRNN by further improving the accuracy to 99.48% and 97.20%, respectively, validating its effectiveness.</p><p>To experimentally compare the gradient behaviour of different models, the gradient flow over time and layer depth are illustrate in <ref type="figure" target="#fig_6">Fig. 5</ref>. Models of 8 layers are used for different RNNs and the other settings are the same as above. The gradients are obtained under a similar training accuracy (92%) for all RNNs, i.e., a similar loss and gradient backpropagated from the objective. For the gradient flow over time, the gradient backpropagated to the input is used with 784 entries. For the gradient flow over layer depth, the average gradient norm of the input processing weights, which is in the layer depth gradient backpropagation path, is used. The gradients of the first layer is omitted since it processes the pixel input with different dimensions from the other layers. The gradients are obtained as the average  <ref type="figure" target="#fig_1">-IndRNN (21 layers, 100 steps)</ref> 1.17 dense-IndRNN (50 steps) <ref type="bibr" target="#b0">1</ref>.19 dense-IndRNN (100 steps) <ref type="bibr">1.16</ref> gradient norms over an epoch to be stable for comparison. <ref type="figure" target="#fig_6">Fig. 5a</ref> shows the gradient change over time and <ref type="figure" target="#fig_6">Fig. 5b</ref> further zoomed the result of RNN and LSTM for better visualization. It can be seen that the gradient of RNN and LSTM are usually smaller than IndRNN, and the gradient decays over time. Especially for RNN, the gradient decays very quickly and close to zero in the time direction. By contrast, IndRNN maintains a relatively large gradients over time without decaying. On the other hand, as shown in <ref type="figure" target="#fig_6">Fig. 5c and 5d</ref>, in the direction of layer depth, IndRNN also maintains relatively large gradients while the gradients of RNN and LSTM drops quickly. Especially for LSTM, the gradient decays severely in the layer depth direction which agrees with the analysis. It is worth noting that a LSTM of 12 layers cannot be trained to converge in our experiments mostly due to the above gradient problem. In all, the gradient of IndRNN behaves very well in both directions of time and layer depth as theoretically analysed above, and thus it is able to construct deep models and process long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Char-level Penn Treebank</head><p>The character-level and word-level language modeling tasks are also used for evaluation. In this subsection, we first evaluate the performance using the character-level Penn Treebank (PTB-c) dataset. The test setting is similar to <ref type="bibr" target="#b52">[53]</ref>.</p><p>Different network architectures are also tested, including a six-layer IndRNN, a 21-layer residual IndRNN (to demonstrate that the IndRNN network can be very deep with the residual connections) and a densely connected IndRNN. For the six-layer IndRNN and the 21-layer residual IndRNN, 2000 hidden neurons are used and dropout with dropping probabilities of 0.25 and 0.3 were used, respectively. While for the densely connected IndRNN, the growth rate is set to 256 and 600 hidden neurons are used for the embedding layer. Dropout is applied after each dense layer (0.5), each bottleneck layer (0.1) and each transition layer (0.4). Dropout of 0.3 is used at the last transition layer, which is connected to the output layer. The batch size is set to 128 and the sequence length T = 50 and T = 100 were both tested in training and testing. <ref type="table" target="#tab_2">Table 3</ref> shows the performance of the proposed IndRNN models in comparison with the existing methods, in terms of bits per character metric (BPC). Compared with the traditional RNN and LSTM models, it can be seen that the proposed IndRNN model achieved better performance. In addition, most IndRNN models in <ref type="table" target="#tab_2">Table 3</ref> except the one with 6 layers and 50 steps outperformed the popular language model Transformer. By comparing the performance of the proposed IndRNN model with different depth, it can be seen that a deeper residual IndRNN or a densely connected IndRNN further improves the performance over the relatively shallow (6 layers) model. Moreover, an improvement can be further achieved with longer temporal dependencies (from time step 50 to 100), indicating that the proposed IndRNN can effectively learn long-term patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Word-level Penn Treebank</head><p>In this subsection, the performance of the proposed IndRNN on the word-level Penn Treebank dataset is evaluated. The test setting is similar to <ref type="bibr" target="#b17">[18]</ref>. A 12-layer residual IndRNN with 2000 neurons in each layer and a densely connected IndRNN with the growth rate of 256 were used for test. An embedding layer with 600 neurons is used. The weight tying <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> of the input embedding and the final output weight is adopted and accordingly the last transition layer contains 600 neurons, which processes the IndRNN features from 2000 to 600. For the residual IndRNN, dropout with a dropping probability of 0.45 was used among IndRNN layers while 0.5 was used for the last IndRNN layer. For the densely connected IndRNN, dropout is applied after each dense layer (0.5), each bottleneck layer (0.1) and each transition layer (0.4). Dropout of 0.65 is used after the embedding layer and the last transition layer, and dropout (0.2) on the embedding weight as in <ref type="bibr" target="#b18">[19]</ref> is also applied for both IndRNN networks. The batch size is set to 128. Sequence length T = 50 was used in training and testing.</p><p>The results are shown in <ref type="table" target="#tab_3">Table 4</ref> in comparison with the existing methods. It can be seen that the proposed IndRNN model achieved better performance than the traditional RNN and LSTM models including the AWD-LSTM <ref type="bibr" target="#b18">[19]</ref>, which is heavily optimized with weight drop, activation regularization, etc. The results can be further enhanced by post-processing with techniques such as neural cache models <ref type="bibr" target="#b61">[62]</ref>, mixture of ssoftmaxes (MOS) <ref type="bibr" target="#b62">[63]</ref> and dynamic evaluation <ref type="bibr" target="#b63">[64]</ref>. Here, dynamic evaluation <ref type="bibr" target="#b63">[64]</ref> is used as an example and the others are not further discussed. The results are shown in the end of <ref type="table" target="#tab_3">Table 4</ref> with a perplexity of 50.97, which is also better than the AWD-LSTM <ref type="bibr" target="#b18">[19]</ref> with the dynamic evaluation. Notice that both res-IndRNN and dense-IndRNN models outperformed the Transformer as shown in <ref type="table" target="#tab_3">Table 4</ref>.  <ref type="bibr" target="#b64">[65]</ref> 92.0 Deep RNN <ref type="bibr" target="#b37">[38]</ref> 107.5 CharCNN <ref type="bibr" target="#b65">[66]</ref> 78.9 LSTM <ref type="bibr" target="#b17">[18]</ref> 114.5 LSTM+Recurrent dropout <ref type="bibr" target="#b51">[52]</ref> 87.0 LSTM+Zoneout <ref type="bibr" target="#b17">[18]</ref> 77.4 LSTM+Variational Dropout <ref type="bibr" target="#b66">[67]</ref> 73.4 Pointer Sentinel LSTM <ref type="bibr" target="#b67">[68]</ref> 70.9 RHN <ref type="bibr" target="#b42">[43]</ref> 65.4 Neural Architecture Search <ref type="bibr" target="#b58">[59]</ref> 62.4 AWD-LSTM <ref type="bibr" target="#b18">[19]</ref> 58.8 AWD-LSTM+Finetue <ref type="bibr" target="#b18">[19]</ref> 57.3 AWD-LSTM+Finetue <ref type="bibr" target="#b18">[19]</ref>+dynamic eval <ref type="bibr" target="#b63">[64]</ref> 51.1 SRU ( [31]) <ref type="bibr">60.3</ref> Transformer (reported in <ref type="bibr" target="#b25">[26]</ref>) 122.37 R-Transformer <ref type="bibr" target="#b25">[26]</ref> 84.38 res-IndRNN (12 layers) <ref type="bibr" target="#b57">58</ref>.99 dense-IndRNN <ref type="bibr" target="#b54">55</ref>.24 dense-IndRNN+dynamic eval <ref type="bibr" target="#b63">[64]</ref> 49.95   <ref type="bibr" target="#b72">[73]</ref> 86.5% 91.1% TCN + TTN <ref type="bibr" target="#b73">[74]</ref> 77.55% 84.25% STGCN <ref type="bibr" target="#b74">[75]</ref> 81.5% 88.3% PB-GCN <ref type="bibr" target="#b75">[76]</ref> 87.5% 93.2% 1 Layer PLSTM <ref type="bibr" target="#b19">[20]</ref> 62.05% 69.40% 2 Layer PLSTM <ref type="bibr" target="#b19">[20]</ref> 62.93% 70.27% ST-LSTM + Trust Gate <ref type="bibr" target="#b0">[1]</ref> 69.2% 77.7% JL d+RNN <ref type="bibr" target="#b21">[22]</ref> 70.26% 82.39% STA-LSTM <ref type="bibr" target="#b20">[21]</ref> 73.40% 81.20% Pose conditioned STA-LSTM <ref type="bibr" target="#b76">[77]</ref> 77.10% 84.50% R-Transformer <ref type="bibr" target="#b25">[26]</ref> 75.69% 81.56% dense-IndRNN-aug 86.70% 93.97%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Skeleton based Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">NTU RGB+D dataset</head><p>In this subsection, the skeleton based action recognition <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b77">[78]</ref> is used to evaluate the performance of the proposed IndRNN. The widely used NTU RGB+D dataset <ref type="bibr" target="#b19">[20]</ref>. The NTU RGB+D dataset contains 56880 sequences (over 4 million frames) of 60 action classes, including Cross-Subject (CS) (40320 and 16560 samples for training and testing, respectively) and Cross-View (CV) (37920 and 18960 samples for training and testing, respectively) evaluation protocols <ref type="bibr" target="#b19">[20]</ref>. In each evaluation protocol, 5% of the training data (randomly selected) was used for evaluation as suggested in <ref type="bibr" target="#b19">[20]</ref>. The joint coordinates of two subject skeletons were used as input. If only one is present, the second was set as zero. For this dataset, when multiple skeletons are present in the scene, the skeleton identity captured by the Kinect sensor may be changed over time. Therefore, an alignment process was first applied to keep the same skeleton saved in the same data array over time. 20 frames were sampled from each instance as one input in the same way as in <ref type="bibr" target="#b78">[79]</ref> and batch size was set to 128. First, IndRNN with different settings are evaluated first, including different IndRNN architectures (plain IndRNN and dense IndRNN), different depths (4 layers and 6 layers) and different activation functions (ReLU and Tanh). To be specific, a four-layer IndRNN and a six-layer IndRNN with 512 hidden neurons were both tested to demonstrate the effectiveness of the proposed IndRNN with a plain IndRNN architecture. ReLU and tanh are both evaluated to show the performance of IndRNN under different activation functions. Then a densely connected IndRNN is used to further validate the effectiveness of a deep architecture. The growth rate is set to 48. For the four-layer IndRNN and six-layer IndRNN, dropout <ref type="bibr" target="#b66">[67]</ref> was applied after each IndRNN layer with a dropping probability of 0.4 and 0.3 for models with ReLU and tanh, respectively, for both CS and CV settings. For the densely connected IndRNN, dropout is applied after the input processing layer (0.5), each dense layer (0.5), each bottleneck layer (0.1) and each transition layer (0.3). For comparison, we also included the deep RNN and LSTM of different layers to show the difference in constructing deep RNNs. The settings of the RNN and LSTM models are similar to <ref type="bibr" target="#b19">[20]</ref> with 128 neurons for LSTM and 512 neurons for RNN (both with dropout 0.5). 1 layer, 2 layer, 4 layers and 6 layers are used for evaluation.</p><p>The results of different settings are shown in <ref type="table" target="#tab_4">Table 5</ref>. By comparing the performance of RNN, LSTM and plain IndRNN, it can be seen that the proposed IndRNN greatly improves the performance over the conventional RNN models on the same task. For CS, LSTM of 2 layers can only achieve accuracies of 67.93% while a 4-layer IndRNN already achieved 78.58%, improving performance by over 10%. For CV, LSTM of 2 layers only achieved accuracies of 77.55% while 4-layer IndRNN already achieved 83.75%, improving performance by over 6%. The results of RNN and LSTM in our experiments are higher than those reported in <ref type="bibr" target="#b19">[20]</ref> and here we use the better results for comparison. On the other hand, by comparing the performance of RNN, LSTM and IndRNN with different layers, it can be seen that the performance of RNN and LSTM cannot be further improved by simply increasing the number of layers, which is the same as demonstrated in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b78">[79]</ref>. On the contrary, by increasing the 4-layer IndRNN to a 6-layer IndRNN, the performance is further improved to 81.80% and 87.97% for CS and CV, respectively. Moreover, with a deep densely connected IndRNN, the performance is further improved to 84.88% and 90.34% for CS and CV, respectively, which is significantly better than the conventional RNN and LSTM methods. From the perspective of different activation functions, it can be seen that IndRNN with tanh also achieves much better performance than the conventional LSTM, although slightly worse than IndRNN with ReLU. Considering the wide use and spread of the non-saturated activation function such as ReLU, we focus on IndRNN with ReLU. More importantly, ReLU does not introduce any weight decay when activated, making the gradient vanishing problem greatly alleviated. Therefore, we only presented the results of IndRNN with tanh in this experiment and not further span on it.</p><p>To further demonstrate that IndRNN can construct very deep networks, a 100 layers IndRNN with residual connections is further tested. The setting is the same as above, only the learning rate is set to 6 * 10 −4 to reduce overfitting. The results are also shown in <ref type="table" target="#tab_4">Table 5</ref>. It can be seen that a deeper IndRNN still can be trained robustly with improved performance. We further conducted an ablation study on the γ, the largest gradient magnitude to avoid gradient exploding, which is the only hyperparameter introduced by IndRNN. It also controls the range of the recurrent weights by |u n | ≤ (T −t) √ γ. Experiments with different γs are conducted on the CS setting with a 6-layer IndRNN and results are shown in <ref type="table" target="#tab_6">Table 7</ref>. It can be seen that the results are relatively close, indicating that IndRNN is not very sensitive to the choice of γ when processing short sequences. Moreover, γ can be simply set to 1, which shows a good performance and avoids the gradient vanishing and exploding as described in Subsection 3.1. Accordingly, the recurrent weights are constrained to be in the range of</p><formula xml:id="formula_16">[−1, 1].</formula><p>The final results on the NTU RGB+D dataset are shown in <ref type="table" target="#tab_5">Table 6</ref> including comparisons with the existing methods. Considering that feature augmentation including the geometric features <ref type="bibr" target="#b21">[22]</ref> and temporal feature augmentation <ref type="bibr" target="#b75">[76]</ref> have shown to be useful, feature augmentation is further adopted on top of the densely connected IndRNN. In addition, we also applied the data augmentation in the time dimension where sequences of length 10-30 are randomly sampled as input. The result is also shown in <ref type="table" target="#tab_5">Table 6</ref>. It can be seen that the performance is further improved and better than the RNN based methods even equipped with enhancing techniques such as attention.</p><p>It is noted that the performance is slightly worse than the GCN based models <ref type="bibr" target="#b75">[76]</ref>, which applied graph convolution to process the input spatially. However, the performance of IndRNN is only achieved with the coordinates as input without any spatial processing techniques. It should be pointed out that GCN can be used together with IndRNN to take the advantages of both networks, for instance, skeleton features instead of simple coordinates can be extracted using a GCN in a local spatial/temporal window and used as input to an IndRNN. This will be studied in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">NTU RGB+D 120 dataset</head><p>The new NTU RGB+D 120 dataset <ref type="bibr" target="#b79">[80]</ref>, enhanced on the NTU RGB+D dataset is also adopted for evaluation. It contains 114480 video samples (over 8 million frames) from 120 different action classes, by adding another 57600 samples and another 60 classes from the NTU RGB+D dataset. Both the CS and CV evaluation protocols from the NTU RGB+D dataset are used. The other settings include the training and testing setups are the same as the one used in the above NTU RGB+D dataset. The results on the NTU RGB+D 120 dataset are shown in <ref type="table" target="#tab_7">Table 8</ref> with comparisons against the existing methods. Similar to the results on the NTU RGB+D dataset, the proposed IndRNN achieves much better performance against the conventional RNN/LSTM based methods. It reaches 74.60% and 77.37% in terms of accuracy for the CS and CV settings, respectively, which is much better than the 61.2% and 63.3% for attention enhanced LSTM methods and the state-of-the-art methods using CNN with accuracy of 66.9% and 67.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Visualization of the Recurrent Weight and Memory</head><p>With the mapping between the recurrent weight and memory shown in subsection 3.2, the memory learned by the network for the task can be understood by visualizing the recurrent weights. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the histograms of the learned recurrent weights (the above six-layer IndRNN obtained under the CS setting for the NTU RGB+D dataset). It can be seen that for the first 5 layers, the learned recurrent weights severely deviate from the uniformly initialized recurrent weights. Especially for the higher layers such as layer 4 and layer 5, the recurrent weights are mostly around 1, learning the long-term memory. On the contrary, for shallower layers such as layer 0 and layer 1, in addition to the large recurrent weights around 1, there is also a group of weights being close to 0, learning the very shortterm memory. This makes sense since the shallow layers process the very basic features in a short period while as the layers increase, high-level features in a longer period are processed. Surprisingly, the number of recurrent weights around 0.5 is small indicating that only a small number of neurons keep mid-range memory. Another interesting fact is that although the recurrent weights are initialized to be positive, part of the recurrent weights are learned to be negative. Note that for keeping different ranges of memories, only the absolute value of the recurrent weights are relevant and thus the negative recurrent weights can still keep different ranges of memories. However, negative recurrent weights may cause oscillation as shown in the gradient backpropagation process. It is hard to intuitively understand how the negative recurrent weights work in the network yet. For the last layer, it can be seen that most of the recurrent weights stay closely to 1 to keep long-term memory, which agrees with the previous assumption shown at the end of Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed an independently recurrent neural network (IndRNN) with a new recurrent connection using the Hadamard product, making neurons in one layer independent of each other. It effectively solves the gradient vanishing and exploding problems by regulating the recurrent weights, which makes it able to be efficiently process very long sequences. Different deep IndRNN architectures, including the basic IndRNN, residual IndRNN and densely connected IndRNN, have been proposed, which can be much deeper than the traditional RNNs. With the gradient better regulated in IndRNN, non-saturated activation functions such as ReLU can be used and trained very robustly. Moreover, with the neurons in one layer independent from each other, each neuron can be better interpreted without effects from others. Also, the relationship between the memory and the recurrent weight has been established through gradient backpropagation, and the learned memories can be understood by investigating the recurrent weights. Experiments on multiple fundamental tasks have verified the advantages of the proposed IndRNN over existing RNN models.  traditional RNN with a nonlinear activation function in the experiments. Therefore, the cross-channel information can be well explored with a multiple-layer IndRNN although IndRNN neurons are independent of each other in each layer. In all, multiple-layer IndRNNs can be used in place of the traditional RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DESCRIPTION OF THE DENSELY CONNECTED IN-DRNN SETTING</head><p>The framework of the densely connected IndRNN process is shown in <ref type="figure">Fig. 7</ref>, which generally follows the conventional denseCNN <ref type="bibr" target="#b36">[37]</ref>. First, the input is processed into features usually with the embedding layer (in the language modelling task) or one IndRNN layer (in the other recognition tasks). Then the initial features are processed with several dense IndRNN blocks and transition layers. As stated in Subsection 5.1, three dense IndRNN blocks are used with 8, 6 and 4 dense IndRNN layers, respectively. While in the conventional denseCNN <ref type="bibr" target="#b36">[37]</ref> the last transition layer is not needed after the final dense block, we find that it usually improves the performance in our experiments. Therefore, the last transition layer is also added in our dense-IndRNN. Finally, a classifier is added in the end for recognition. <ref type="figure">Fig. 8</ref> further shows the details of the dense IndRNN block. Each dense IndRNN layer produces k channels of output features, termed as growth rate. After one dense IndRNN layer, the features are increased to n + k channels, where n represents the channels of the features from the previous layers. For the next IndRNN layer, the first weight processes the n + k dimensional to a 4k-channel feature, then further processed by BN and IndRec+ReLU. The second weight processes the 4k-channel feature to kchannel and further processed for output. Concatenated with the input n+k-channel feature, the final output feature of the dense IndRNN layer becomes n + 2k channels. For the transition layers, it processes the feature (N channels) from the dense IndRNN block and reduces it to half (N/2 channels). The key component of dense-IndRNN different from the residual-IndRNN is the concatenation operation of the dense layers and the shortcut, which allows the features to be reused in the following layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The conventional simple RNN with recurrent connections connecting all neurons over time. The proposed IndRNN with independent recurrent connections making each neuron in one layer independent while connected over layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of a conventional simple RNN and the proposed IndRNN unfolded in time. Each solid dot represents a neuron in a layer and each line represents a connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>illustrates the computation complexity comparison in terms of computation time on different lengths of sequences including 256, 512 and 1024. The program is tested on a GPU P100 and the training time for each batch (averaged over 100 batches) including both the forward and backward time (milliseconds) are shown in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Complexity comparison in terms of time training one batch (milliseconds) for processing sequences of different lengths. Note that in our experiments the time for one forward or backward process is not very accurate due to the precision of the system clock since the time taken by the forward process is very short. Therefore, the total time of training one batch (including fetching data, forward pass, loss calculation and backward pass) is averaged over 100 batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of different deep IndRNN architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Results of the adding problem for different sequence lengths. The legends for all figures are the same and thus only shown in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the gradient flow over time and layer depth for different RNNs. (b) and (d) are zoomed from (a) and (c), respectively, with the gradients of RNN and LSTM for better visualization and comparison. The gradient of RNN decays quickly over time and the gradient of LSTM decays quickly over layer depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Histograms of the learned recurrent weights for different layers, where (a)-(f) correspond to the 1st-6th layers. x axis and y axis represent the value of the recurrent weight and the frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Framework of the densely connected IndRNN used in the experiments. Detailed illustration of the dense IndRNN block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Y. Gao are with Shandong University, Jinan, China. Email: {shuaili, ybgao}@sdu.edu.cn.</figDesc><table /><note>• W. Li and C. Cook are with University of Wollongong, NSW 2522, Australia. E-mail: {wanqing, ccook}@uow.edu.au Manuscript received October 10, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Results in terms of accuracy (%) for the sequential MNIST and permuted MNIST.</figDesc><table><row><cell></cell><cell cols="2">MNIST pMNIST</cell></row><row><cell>IRNN [10]</cell><cell>95.0</cell><cell>82</cell></row><row><cell>uRNN [33]</cell><cell>95.1</cell><cell>91.4</cell></row><row><cell>RNN-path [8]</cell><cell>96.9</cell><cell>-</cell></row><row><cell>LSTM [33]</cell><cell>98.2</cell><cell>88</cell></row><row><cell>LSTM+Recurrent dropout [52]</cell><cell>-</cell><cell>92.5</cell></row><row><cell>LSTM+Recurrent batchnorm [53]</cell><cell>-</cell><cell>95.4</cell></row><row><cell>LSTM+Zoneout [18]</cell><cell>-</cell><cell>93.1</cell></row><row><cell cols="2">LSTM+Recurrent batchnorm+Zoneout [18] -</cell><cell>95.9</cell></row><row><cell>Transformer (reported in [26])</cell><cell>98.2</cell><cell>-</cell></row><row><cell>R-Transformer [26]</cell><cell>99.1</cell><cell>-</cell></row><row><cell>IndRNN (6 layers)</cell><cell>99.0</cell><cell>96.0</cell></row><row><cell>IndRNN (12 layers)</cell><cell>99.37</cell><cell>96.84</cell></row><row><cell>res-IndRNN (12 layers)</cell><cell>99.39</cell><cell>97.02</cell></row><row><cell>dense-IndRNN</cell><cell>99.48</cell><cell>97.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Results of char-level PTB for our proposed IndRNN model in comparison with results reported in the literature, in terms of BPC.</figDesc><table><row><cell>Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>Results of word-level PTB for our proposed IndRNN model in comparison with results reported in the literature, in terms of perplexity.</figDesc><table><row><cell>Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc>Result comparison of the RNN based skeleton action recognition methods on NTU RGB+D dataset.</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>1 Layer RNN (reported in [20])</cell><cell>56.02%</cell><cell>60.24%</cell></row><row><cell>2 Layer RNN (reported in [20])</cell><cell>56.29%</cell><cell>64.09%</cell></row><row><cell>1 Layer LSTM (reported in [20])</cell><cell>59.14%</cell><cell>66.81%</cell></row><row><cell>2 Layer LSTM (reported in [20])</cell><cell>60.09%</cell><cell>67.29%</cell></row><row><cell>1 Layer RNN</cell><cell>57.84%</cell><cell>64.28%</cell></row><row><cell>2 Layer RNN</cell><cell>62.17%</cell><cell>71.00%</cell></row><row><cell>4 Layer RNN</cell><cell>61.78%</cell><cell>68.28%</cell></row><row><cell>6 Layer RNN</cell><cell>58.23%</cell><cell>63.49%</cell></row><row><cell>1 Layer LSTM</cell><cell>66.01%</cell><cell>73.87%</cell></row><row><cell>2 Layer LSTM</cell><cell>67.93%</cell><cell>77.55%</cell></row><row><cell>4 Layer LSTM</cell><cell>66.02%</cell><cell>69.12%</cell></row><row><cell>6 Layer LSTM</cell><cell>58.50%</cell><cell>67.42%</cell></row><row><cell>IndRNN-tanh (4 layers)</cell><cell>76.56%</cell><cell>85.32%</cell></row><row><cell>IndRNN-tanh (6 layers)</cell><cell>80.36%</cell><cell>88.61%</cell></row><row><cell>IndRNN (4 layers)</cell><cell>77.23%</cell><cell>88.30%</cell></row><row><cell>IndRNN (6 layers)</cell><cell>80.44%</cell><cell>89.45%</cell></row><row><cell>res-IndRNN (100 layers)</cell><cell>83.26%</cell><cell>92.00%</cell></row><row><cell>dense-IndRNN</cell><cell cols="2">83.38% 91.81%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 :</head><label>6</label><figDesc>Results of all skeleton based methods on NTU RGB+D dataset.</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>SkeletonNet(CNN) [69]</cell><cell>75.94%</cell><cell>81.16%</cell></row><row><cell>JDM+CNN [70]</cell><cell>76.20%</cell><cell>82.30%</cell></row><row><cell>Clips+CNN+MTLN [71]</cell><cell>79.57%</cell><cell>84.83%</cell></row><row><cell cols="2">Enhanced Visualization+CNN [72] 80.03%</cell><cell>87.21%</cell></row><row><cell>HCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 :</head><label>7</label><figDesc>Ablation study on the γ (recurrent weight constraint).</figDesc><table><row><cell>γ</cell><cell>1</cell><cell>2</cell><cell>10</cell><cell>No Constraint</cell></row><row><cell></cell><cell cols="3">80.82% 80.68% 80.43%</cell><cell>80.24%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8 :</head><label>8</label><figDesc>Results of all skeleton based methods on NTU RGB+D 120 dataset.</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Part-aware LSTM [20] (reported in [80])</cell><cell>25.5%</cell><cell>26.3%</cell></row><row><cell>Soft RNN [81] (reported in [80])</cell><cell>36.3%</cell><cell>44.9%</cell></row><row><cell>ST-LSTM [79]</cell><cell>55.7%</cell><cell>57.9%</cell></row><row><cell>GCA-LSTM [82]</cell><cell>58.3%</cell><cell>59.2%</cell></row><row><cell>Two-Stream Attention LSTM [83]</cell><cell>61.2%</cell><cell>63.3%</cell></row><row><cell>Body Pose Evolution Map [84]</cell><cell>64.6%</cell><cell>66.9%</cell></row><row><cell>SkeleMotion [85]</cell><cell>66.9%</cell><cell>67.7%</cell></row><row><cell>IndRNN (4 layers)</cell><cell>70.01%</cell><cell>72.05%</cell></row><row><cell>IndRNN (6 layers)</cell><cell>72.39%</cell><cell>75.04%</cell></row><row><cell>dense-IndRNN</cell><cell>74.62%</cell><cell>77.37%</cell></row><row><cell>dense-IndRNN-aug</cell><cell cols="2">76.55% 79.18%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Yanbo Gao is currently with the School of Software, Shandong University (SDU), Jinan, China, as an Associate Professor. She was with the School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China, as a Post-doctor from 2018-2020. She received her Ph.D. degree from UESTC in 2018. Her research interests include video coding, 3D video processing and light field image coding. She was a co-recipient of the best student paper awards at the IEEE BMSB 2018.</figDesc><table><row><cell>Input</cell><cell>Input Processing</cell><cell>Dense Block</cell><cell>Transition</cell><cell>...</cell><cell>Dense Block</cell><cell>Transition</cell><cell>Classifier</cell><cell>Output</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A RELATIONSHIP BETWEEN INDRNN AND RNN</head><p>The relationship between IndRNN and the traditional RNN is illustrated in the following, where we show that under certain circumstances the traditional RNN is only a special case of a two-layers IndRNN. First, from the perspective of number of neurons, as shown in 3.3, for a N -neuron RNN network with input of dimension M , the number of parameters using traditional RNN is M × N + N × N , while the number of parameters using one-layer IndRNN is M × N + N . For a two-layers IndRNN where both layers consist of N neurons, the number of parameters is M × N + N × N + 2 × N , which is of a similar order to the traditional RNN. Therefore, in the following we compare a two-layer IndRNN with a single layer RNN. For simplicity, the bias term is ignored for both IndRNN and traditional RNN.</p><p>Assume a simple N -neuron two-layer network where the recurrent weights for the second layer are zero which means the second layer is just a fully connected layer shared over time. The Hadamard product (u h t−1 ) can be represented in the form of matrix product by diag(u 1 , u 2 , . . . , u N )h t−1 . In the following, diag(u 1 , u 2 , . . . , u N ) is shortened as diag(u i ). Assume that the activation function is a linear function σ(x) = x. The first and second layers of a two-layer IndRNN can be represented by <ref type="formula">(5)</ref> and <ref type="formula">(6)</ref>, respectively.</p><p>Assuming W s is invertible, then</p><p>Thus</p><p>By assigning U = W s diag(u f i )W −1 s and W = W s W f , it becomes h t = Wx t + Uh t−1 <ref type="bibr" target="#b8">(9)</ref> which is a traditional RNN. Note that this only imposes the constraint that the recurrent weight (U) is diagonalizable. Therefore, the simple two-layer IndRNN network can represent a traditional RNN network with a diagonalizable recurrent weight (U). In other words, under linear activation, a traditional RNN with a diagonalizable recurrent weight (U) is a special case of a two-layer IndRNN. It is known that a non-diagonalizable matrix can be made diagonalizable with a perturbation matrix composed of small entries. A stable RNN network needs to be robust to small perturbations (in order to deal with precision errors for example). It is possible to find an RNN network with a diagonalizable recurrent weight matrix to approximate a stable RNN network with a non-diagonalizable recurrent weight matrix. Therefore, a traditional RNN with a linear activation is a special case of a two-layer IndRNN. For a traditional RNN with a nonlinear activation function, its relationship with the proposed IndRNN is yet to be established theoretically. However, we have shown empirically that the proposed IndRNN can achieve better performance than a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drawing and recognizing chinese characters with recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="849" to="862" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image captioning and visual question answering based on attributes and external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1367" to="1381" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fully trainable network with rnn-based pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in psychology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pathnormalized optimization of recurrent neural networks with relu activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3477" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regularizing rnns by stabilizing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving performance of recurrent neural network with relu nonlinearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vartak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03771</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Residual lstm: Design of a deep recurrent architecture for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layer trajectory lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno>abs/1808.09522</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On geometric features for skeletonbased action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">R-transformer: Recurrent neural network enhanced transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1907.05572</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimal gated unit for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Capacity and trainability in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Training rnns as fast as cnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fullcapacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via fourier recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploring the depths of recurrent neural networks with stochastic residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2856" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grid long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Neural Networks</title>
		<meeting>the 17th International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laguerre&apos;s method applied to the matrix eigenvalue problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Parlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">87</biblScope>
			<biblScope unit="page" from="464" to="485" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recurrent dropout without memory loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
		<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast-slow recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mujika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5917" to="5926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2766" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Learning Representations</title>
		<meeting>eeding of the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Skeletonnet: Mining deep part features for 3-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="731" to="735" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="624" to="628" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Temporal transformer networks: Joint learning of invariant and discriminative time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">270</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Pose-conditioned spatiotemporal attention for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2568" to="2583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3671" to="3680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global contextaware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
