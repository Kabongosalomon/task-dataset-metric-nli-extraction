<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign, ‡ Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">eBay Research Lab</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">eBay Research Lab</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">eBay Research Lab</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">eBay Research Lab</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CI-FAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep CNNs are well suited for large-scale learning based visual recognition tasks because of its highly scalable training algorithm, which only needs to cache a small chunk (mini-batch) of the potentially huge volume of training data during sequential scans (epochs). They have achieved increasingly better performance in recent years.</p><p>As datasets become bigger and the number of object categories becomes larger, one of the complications that come along is that visual separability between different object categories is highly uneven. Some categories are much harder to distinguish than others. Take the categories in CIFAR100 as an example. It is easy to tell an Apple from a Bus, but harder to tell an Apple from an Orange. In fact, both Apples and Oranges belong to the same coarse category fruit and vegetables while Buses belong to another coarse category vehicles 1, as defined within CIFAR100. Nonetheless, most deep CNN models nowadays are flat N-way classifiers, which share a set of fully connected layers. This makes us wonder whether such a flat structure is adequate for distinguishing all the difficult categories. A very natural and intuitive alternative organizes classifiers in a hierarchical manner according to the divide-and-conquer strategy. Although hierarchical classification has been proven effective for conventional linear classifiers <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22]</ref>, few attempts have been made to exploit category hierarchies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> in deep CNN models.</p><p>Since deep CNN models are large models themselves, organizing them hierarchically imposes the following challenges. First, instead of a handcrafted category hierarchy, how can we learn such a category hierarchy from the training data itself so that cascaded inferences in a hierarchical classifier will not degrade the overall accuracy while dedicated fine category classifiers exist for hard-to-distinguish categories? Second, a hierarchical CNN classifier consists of multiple CNN models at different levels. How can we leverage the commonalities among these models and effectively train them all? Third, it would also be slower and more memory-consuming to run a hierarchical CNN classifier on a novel testing image. How can we alleviate such limitations?</p><p>In this paper, we propose a generic and principled hierarchical architecture, Hierarchical Deep Convolutional Neural Network (HD-CNN), that decomposes an image classification task into two steps. An HD-CNN first uses a coarse category CNN classifier to separate easy classes from one another. More challenging classes are routed downstream to fine category classifiers that focus on confusing classes. We adopt a module design principle and every HD-CNN is built upon a building block CNN. The building block can be chosen to be any of the currently top ranked single CNNs. Thus HD-CNNs can always benefit from the progress of single CNN design. An HD-CNN follows the coarse-to-fine  classification paradigm and probabilistically integrates predictions from the fine category classifiers. Compared with the building block CNN, the corresponding HD-CNN can achieve lower error at the cost of a manageable increase in memory footprint and classification time.</p><p>In summary, this paper has the following contributions. First, we introduce a novel hierarchical architecture, called HD-CNN, for image classification. Second, we develop a scheme for learning the two-level organization of coarse and fine categories, and demonstrate various components of an HD-CNN can be independently pretrained. The complete HD-CNN is further fine-tuned using a multinomial logistic loss regularized by a coarse category consistency term. Third, we make the HD-CNN scalable by compressing the layer parameters and conditionally executing the fine category classifiers. We have performed evaluations on the medium-scale CIFAR100 dataset and the large-scale ImageNet 1000-class dataset, and our method has achieved state-of-the-art performance on both of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is inspired by progresses in CNN design and efforts on integrating a category hierarchy with linear classifiers. The main novelty of our method is a new scalable HD-CNN architecture that integrates a category hierarchy with deep CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Neural Networks</head><p>CNN-based models hold state-of-the-art performance in various computer vision tasks, including image classifcation <ref type="bibr" target="#b17">[18]</ref>, object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, and image parsing <ref type="bibr" target="#b6">[7]</ref>. Recently, there has been considerable interest in enhancing CNN components, including pooling layers <ref type="bibr" target="#b35">[36]</ref>, activation units <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>, and nonlinear layers <ref type="bibr" target="#b20">[21]</ref>. These enhancements either improve CNN training <ref type="bibr" target="#b35">[36]</ref>, or expand the network learning capacity. This work boosts CNN performance from an orthogonal angle and does not redesign a specific part within any existing CNN model. Instead, we design a novel generic hierarchical architecture that uses an existing CNN model as a building block. We embed multiple building blocks into a larger hierarchical deep CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Category Hierarchy for Visual Recognition</head><p>In visual recognition, there is a vast literature exploiting category hierarchical structures <ref type="bibr" target="#b31">[32]</ref>. For classification with a large number of classes using linear classifiers, a common strategy is to build a hierarchy or taxonomy of classifiers so that the number of classifiers evaluated given a testing image scales sub-linearly in the number of classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. The hierarchy can be either predefined <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16]</ref> or learnt by top-down and bottom-up approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, the predefined category hierarchy of ImageNet dataset is utilized to achieve the trade-offs between classification accuracy and specificity. In <ref type="bibr" target="#b21">[22]</ref>, a hierarchical label tree is constructed to probabilistically combine predictions from leaf nodes. Such hierarchical classifier achieves significant speedup at the cost of certain accuracy loss.</p><p>One of the earliest attempts to introduce a category hierarchy in CNN-based methods is reported in <ref type="bibr" target="#b28">[29]</ref> but their main goal is transferring knowledge between classes to improve the results for classes with insufficient training examples. In <ref type="bibr" target="#b2">[3]</ref>, various label relations are encoded in a hierarchy. Improved accuracy is achieved only when a subset of training images are relabeled with internal nodes in the hierarchical class tree. They are not able to improve the accuracy in the original setting where all training images are labeled with leaf nodes. In <ref type="bibr" target="#b33">[34]</ref>, a hierarchy of CNNs is introduced but they experimented with only two coarse categories mainly due to scalability constraints. HD-CNN exploits the category hierarchy in a novel way that we embed deep CNNs into the hierarchy in a scalable manner and achieves superior classification results over the standard CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of HD-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>The following notations are used below. A dataset consists of images {x i , y i } i . x i and y i denote the image data and label, respectively. There are C fine categories of images in the dataset {S f j } C j=1 . We will learn a category hierarchy with K coarse categories {S c k } K k=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">HD-CNN Architecture</head><p>HD-CNN is designed to mimic the structure of category hierarchy where fine categories are divided into coarse categories as in <ref type="figure" target="#fig_0">Fig 1(a)</ref>. It performs end-to-end classification as illustrated in <ref type="figure" target="#fig_0">Fig 1(b)</ref>. It mainly comprises four parts, namely shared layers, a single coarse category component B, multiple fine category components {F k } K k=1 and a single probabilistic averaging layer. On the left side of <ref type="figure" target="#fig_0">Fig 1  (b)</ref> are the shared layers. They receive raw image pixel as input and extract low-level features. The configuration of shared layers are set to be the same as the preceding layers in the building block net.</p><p>On the top of <ref type="figure" target="#fig_0">Fig 1(b)</ref> are independent layers of coarse category component B which reuses the configuration of rear layers from the building block CNN and produces a fine prediction {B f ij } C j=1 for an image x i . To produce a prediction {B ik } K k=1 over coarse categories, we append a fine-tocoarse aggregation layer which aggregates fine predictions into coarse ones when a mapping from fine categories to coarse ones P : [1, C] → [1, K] is given. The coarse category probabilities serve two purposes. First, they are used as weights for combining the predictions made by fine category components. Second, when thresholded, they enable conditional executions of fine category components whose corresponding coarse probabilities are sufficiently large.</p><p>In the bottom-right of <ref type="figure" target="#fig_0">Fig 1 (b)</ref> are independent layers of a set of fine category classifiers {F k } K k=1 , each of which makes fine category predictions. As each fine component only excels in classifying a small set of categories, they produce a fine prediction over a partial set of categories. The probabilities of other fine categories absent in the partial set are implicitly set to zero. The layer configurations are mostly copied from the building block CNN except that in the final classification layer the number of filters is set to be the size of partial set instead of the full categories.</p><p>Both coarse category component and fine category components share common layers. The reason is three-fold. First, it is shown in <ref type="bibr" target="#b34">[35]</ref> that preceding layers in deep networks response to class-agnostic low-level features such as corners and edges, while rear layers extract more classspecific features such as dog face and bird's legs. Since low-level features are useful for both coarse and fine classification tasks, we allow the preceding layers to be shared by both coarse and fine components. Second, it reduces both the total floating point operations and the memory footprint of network execution. Both are of practical significance to deploy HD-CNN in real applications. Last but not the least, it can decrease the number of HD-CNN parameters which is critical to the success of HD-CNN training.</p><p>On the right side of <ref type="figure" target="#fig_0">Fig 1 (b)</ref> is the probabilistic averaging layer which receives fine category predictions as well as coarse category prediction and produces a weighted average as the final prediction.</p><formula xml:id="formula_0">p(x i ) = K k=1 B ik p k (x i ) K k=1 B ik (1)</formula><p>where B ik is the probability of coarse category k for the image x i predicted by the coarse category component B. p k (x i ) is the fine category prediction made by the fine category component F k .</p><p>We stress that both coarse and fine category components reuse the layer configurations from the building block CNN. This flexible modular design allows us to choose the best module CNN as the building block, depending on the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning a Category Hierarchy</head><p>Our goal of building a category hierarchy is grouping confusing fine categories into the same coarse category for which a dedicated fine category classifier will be trained. We employ a top-down approach to learn the hierarchy from the training data.</p><p>We randomly sample a held-out set of images with balanced class distribution from the training set. The rest of the training set is used to train a building block net. We obtain a confusion matrix F by evaluating the net on the held-out set. A distance matrix D is derived as D = 1 − F and its diagonal entries are set to be zero. D is further transformed by D = 0.5 * (D + D T ) to be symmetric. The entry D ij measures how easy it is to discriminate categories i and j. Spectral clustering is performed on D to cluster fine categories into K coarse categories. The result is a twolevel category hierarchy representing a many-to-one mapping P d : [1, C] → [1, K] from fine to coarse categories. Here, the coarse categories are disjoint. Overlapping Coarse Categories With disjoint coarse categories, the overall classification depends heavily on the coarse category classifier. If an image is routed to an incorrect fine category classifier, then the mistake can not be corrected as the probability of ground truth label is implicitly set to zero there. Removing the separability constraint between coarse categories can make the HD-CNN less dependent on the coarse category classifier.</p><p>Therefore, we add more fine categories to the coarse categories. For a certain fine classifier F k , we prefer to add those fine categories {j} that are likely to be misclassfied into the coarse category k. Therefore, we estimate the likelihood u k (j) that an image in fine category j is misclassified into a coarse category k on the held-out set.</p><formula xml:id="formula_1">u k (j) = 1 S f j i∈S f j B d ik<label>(2)</label></formula><p>B d ik is the coarse category probability which is obtained by aggregating fine category probabilities {B f ij } j according to the mapping P d :</p><formula xml:id="formula_2">B d ik = j|P d (j)=k B f ij .</formula><p>We threshold the likelihood u k (j) using a parametric variable u t = (γK) −1 and add to the partial set S c k all fine categories {j} such that u k (j) ≥ u t . Note that each branching component gives a full set prediction when u t = 0 and a disjoint set prediction when u t = 1.0. With overlapping coarse categories, the category hierarchy mapping P d is extended to be a many-tomany mapping P o and the coarse category predictions are updated accordingly B o ik = j|k∈P o (j) B ij . Note the sum of {B o ik } K k=1 exceeds 1 and hence we perform L 1 normalization. The use of overlapping coarse categories was also shown to be useful for hierarchical linear classifiers <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HD-CNN Training</head><p>As we embed fine category components into HD-CNN, the number of parameters in rear layers grows linearly in the number of coarse categories. Given the same amount of training data, this increases the training complexity and the risk of over-fitting. On the other hand, the training images within the stochastic gradient descent mini-batch are probabilistically routed to different fine category components. It requires larger mini-batch to ensure parameter gradients in the fine category components are estimated by a sufficiently large number of training samples. Large training mini-batch both increases the training memory footprint and slows down the training process. Therefore, we decompose the HD-CNN training into multiple steps instead of training the complete HD-CNN from scratch as outlined in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pretraining HD-CNN</head><p>We sequentially pretrain the coarse category component and fine category components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Initializing the Coarse Category Component</head><p>We first pretrain a building block CNN F p using the training set. As both the preceding and rear layers in coarse category component resemble the layers in the building block CNN, we copy the weights of F p into coarse category component for initialization purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Pretraining the Rear Layers of Fine Category Components</head><p>Fine category components {F k } k can be independently pretrained in parallel. Each F k should specialize in classifying fine categories within the coarse category S c k . Therefore, the pretraining of each F k only uses images {x i |i ∈ S c k } from the coarse category S c k . The shared preceding layers are already initialized and kept fixed in this stage. For Step 1: Pretrain HD-CNN 3:</p><p>Step 1.1: Initialize coarse category component 4:</p><p>Step 1.2: Pretrain fine category components <ref type="bibr">5:</ref> Step 2: Fine-tune the complete HD-CNN each F k , we initialize all the rear layers except the last convolutional layer by copying the learned parameters from the pretrained model F p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Fine-tuning HD-CNN</head><p>After both coarse category component and fine category components are properly pretrained, we fine-tune the complete HD-CNN. Once the category hierarchy as well as the associated mapping P o is learnt, each fine category component focuses on classifying a fixed subset of fine categories. During fine-tuning, the semantics of coarse categories predicted by the coarse category component should be kept consistent with those associated with fine category components. Thus we add a coarse category consistency term to regularize the conventional multinomial logistic loss. Coarse category consistency The learnt fine-to-coarse category mapping P : [1, C] → [1, K] provides a way to specify the target coarse category distribution {t k }. Specifically, t k is set to be the fraction of all the training images within the coarse category S c k under the assumption the distribution over coarse categories across the training dataset is close to that within a training mini-batch.</p><formula xml:id="formula_3">t k = j|k∈P (j) |S j | K k =1 j|k ∈P (j) |S j | ∀k ∈ [1, K]<label>(3)</label></formula><p>The final loss function we use for fine-tuning the HD-CNN is shown below.</p><formula xml:id="formula_4">E = − 1 n n i=1 log(p yi ) + λ 2 K k=1 (t k − 1 n n i=1 B ik ) 2 (4)</formula><p>where n is the size of training mini-batch. λ is a regularization constant and is set to λ = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">HD-CNN Testing</head><p>As we add fine category components into the HD-CNN, the number of parameters, memory footprint and execution time in rear layers, all scale linearly in the number of coarse categories. To ensure HD-CNN is scalable for large-scale visual recognition, we develop conditional execution and layer parameter compression techniques. Conditional Execution. At test time, for a given image, it is not necessary to evaluate all fine category classifiers as most of them have insignificant weights B ik as in Eqn 1.</p><p>Their contributions to the final prediction are negligible. Conditional executions of the top weighted fine components can accelerate the HD-CNN classification. Therefore, we threshold B ik using a parametric variable B t = (βK) −1 and reset B ik to zero when B ik &lt; B t . Those fine category classifiers with B ik = 0 are not evaluated. Parameter Compression. In HD-CNN, the number of parameters in rear layers of fine category classifiers grows linearly in the number of coarse categories. Thus we compress the layer parameters at test time to reduce memory footprint. Specifically, we choose the Product Quantization approach <ref type="bibr" target="#b13">[14]</ref> to compress the parameter matrix W ∈ R m×n by first partitioning it horizontally into segments of width s. W = [W 1 , ..., W (n/s) ]. Then k-means clustering is used to cluster the rows in W i , ∀i ∈ [1, (n/s)]. By only storing the nearest cluster indices in a 8-bit integer matrix I ∈ R m×(n/s) and cluster centers in a single-precision floating number matrix C ∈ R k×n , we can achieve a compression factor (32mn)/(32kn + 8mn/s). The hyperparameters for parameter compression are (s, k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Overview</head><p>We evaluate HD-CNN on the benchmark datasets CI-FAR100 <ref type="bibr" target="#b16">[17]</ref> and ImageNet <ref type="bibr" target="#b3">[4]</ref>. HD-CNN is implemented on the widely deployed Caffe <ref type="bibr" target="#b14">[15]</ref> software. The network is trained by back propagation <ref type="bibr" target="#b17">[18]</ref>. We run all the testing experiments on a single NVIDIA Tesla K40c card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">CIFAR100 Dataset</head><p>The CIFAR100 dataset consists of 100 classes of natural images. There are 50K training images and 10K testing images. We follow <ref type="bibr" target="#b10">[11]</ref> to preprocess the datasets (e.g. global contrast normalization and ZCA whitening). Randomly cropped and flipped image patches of size 26 × 26 are used for training. We adopt a NIN network 1 with three stacked layers <ref type="bibr" target="#b20">[21]</ref>. We denote it as CIFAR100-NIN which will be the HD-CNN building block. Fine category components share preceding layers from conv1 to pool1 which accounts for 6% of the total parameters and 29% of the total floating point operations. The remaining layers are used as independent layers.</p><p>For building the category hierarchy, we randomly choose 10K images from the training set as held-out set. Fine categories within the same coarse categories are visually more similar. We pretrain the rear layers of fine category components. The initial learning rate is 0.01 and it is decreased by a factor of 10 every 6K iterations. Fine-tuning is performed for 20K iterations with large mini-batches of size 256. The initial learning rate is 0.001 and is reduced by a factor of 10 once after 10K iterations. For evaluation, we use 10-view testing <ref type="bibr" target="#b17">[18]</ref>. We extract five 26 × 26 patches (the 4 corner patches and the center patch) as well as their horizontal reflections and average their predictions. The CIFAR100-NIN net obtains 35.27% testing error. Our HD-CNN achieves testing error of 32.62% which improves the building block net by 2.65%. Category hierarchy. During the construction of the category hierarchy, the number of coarse categories can be adjusted by the clustering algorithm. We can also make the coarse categories either disjoint or overlapping by varying the hyperparameter γ. Thus we investigate their impacts on the classification error. We experiment with 5, 9, 14 and 19 coarse categories and vary the value of γ. The best results are obtained with 9 overlapping coarse categories and γ = 5 as shown in <ref type="figure" target="#fig_2">Fig 2 left</ref>. A histogram of fine category occurrences in 9 overlapping coarse categories is shown in <ref type="figure" target="#fig_2">Fig  2 right</ref>. The optimal value of coarse category number and hyperparameter γ are dataset dependent, mainly affected by the inherent hierarchy within the categories. Shared layers. The use of shared layers makes both com-putational complexity and memory footprint of HD-CNN sublinear in the number of fine category classifiers when compared to the building block net. Our HD-CNN with 9 fine category classifiers based on CIFAR100-NIN consumes less than three times as much memory as the building block net without parameter compression. We also want to investigate the impact of the use of shared layers on the classification error, memory footprint and the net execution time ( <ref type="table" target="#tab_3">Table 2</ref>). We build another HD-CNN where coarse category component and all fine category components use independent preceding layers initialized from a pretrained building block net. Under the single-view testing where only a central cropping is used, we observe a minor error increase from 34.36% to 34.50%. But using shared layers dramatically reduces the memory footprint from 1356 MB to 459 MB and testing time from 2.43 seconds to 0.28 seconds.</p><p>Conditional executions. By varying the hyperparameter β, we can effectively affect the number of fine category components that will be executed. There is a trade-off between execution time and classification error. A larger value of β leads to higher accuracy at the cost of executing more components for fine categorization. By enabling conditional executions with hyperparameter β = 6, we obtain a substantial 2.8X speed up with merely a minor increase in error from 34.36% to 34.57% ( <ref type="table" target="#tab_3">Table 2</ref>). The testing time of HD-CNN is about 2.5 times as much as that of the building block net. Parameter compression. As fine category CNNs have independent layers from conv2 to cccp6, we compress them and reduce the memory footprint from 447MB to 286MB with a minor increase in error from 34.57% to 34.73%. Comparison with a strong baseline. As our HD-CNN memory footprint is about two times as much as the building block model <ref type="table" target="#tab_3">(Table 2)</ref>, it is necessary to compare a stronger baseline of similar complexity with HD-CNN. We adapt CIFAR100-NIN and double the number of filters in all convolutional layers which accordingly increases the memory footprint by three times. We denote it as CIFAR100-NINdouble and obtain error 34.26% which is 1.01% lower than that of the building block net but is 1.64% higher than that of HD-CNN.</p><p>Comparison with model averaging. HD-CNN is fundamentally different from model averaging <ref type="bibr" target="#b17">[18]</ref>. In model averaging, all models are capable of classifying the full set of the categories and each one is trained independently.</p><p>The main sources of their prediction differences are different initializations. In HD-CNN, each fine category classifier only excels at classifying a partial set of categories.</p><p>To compare HD-CNN with model averaging, we independently train two CIFAR100-NIN networks and take their averaged prediction as the final prediction. We obtain an error of 35.13%, which is about 2.51% higher than that of HD-CNN ( <ref type="table" target="#tab_1">Table 1</ref>). Note that HD-CNN is orthogonal to the model averaging and an ensemble of HD-CNN networks can further improve the performance. Coarse category consistency. To verify the effectiveness of coarse category consistency term in our loss function (4), we fine-tune a HD-CNN using the traditional multinomial logistic loss function. The testing error is 33.21%, which is 0.59% higher than that of a HD-CNN fine-tuned with coarse category consistency <ref type="table" target="#tab_1">(Table 1)</ref>.</p><p>Comparison with state-of-the-art. Our HD-CNN improves on the current two best methods <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b29">[30]</ref> by 2.06% and 1.16% respectively and sets new state-of-the-art results on CIFAR100 <ref type="table" target="#tab_1">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">ImageNet 1000-class Dataset</head><p>The ILSVRC-2012 ImageNet dataset consists of about 1.2 million training images, 50, 000 validation images. To demonstrate the generality of HD-CNN, we experiment with two different building block nets. In both cases, HD-CNNs achieve significantly lower testing errors than the building block nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Network-In-Network Building Block Net</head><p>We choose a public 4-layer NIN net 2 as our first building block as it has greatly reduced number of parameters compared to AlexNet <ref type="bibr" target="#b17">[18]</ref> but similar error rates. It is denoted as ImageNet-NIN. In HD-CNN, various components share preceding layers from conv1 to pool3 which account for 26% of the total parameters and 82% of the total floating point operations.</p><p>We follow the training and testing protocols as in <ref type="bibr" target="#b17">[18]</ref>. Original images are resized to 256 × 256. Randomly cropped and horizontally reflected 224 × 224 patches are used for training. At test time, the net makes a 10-view averaged prediction. We train ImageNet-NIN for 45 epochs. The top-1 and top-5 errors are 39.76% and 17.71%.</p><p>To build the category hierarchy, we take 100K training images as the held-out set and find 89 overlapping coarse categories. Each fine category CNN is fine-tuned for 40K iterations while the initial learning rate 0.01 is decreased by a factor of 10 every 15K iterations. Fine-tuning the complete HD-CNN is not performed as the required minibatch size is significantly higher than that for the building block net. Nevertheless, we still achieve top-1 and top-5 errors of 36.66% and 15.80% and improve the building block net by 3.1% and 1.91%, respectively ( <ref type="table" target="#tab_4">Table 3</ref>). The classwise top-5 error improvement over the building block net is shown in <ref type="figure" target="#fig_5">Fig 4 left</ref>. Case studies We want to investigate how HD-CNN corrects the mistakes made by the building block net. <ref type="figure" target="#fig_3">In Fig 3,</ref>    collect four testing cases. In the first case, the building block net fails to predict the label of the tiny hermit crab in the top 5 guesses. In HD-CNN, two coarse categories #6 and #11 receive most of the coarse probability mass. The fine category component #6 specializes in classifying crab breeds and strongly suggests the ground truth label. By combining the predictions from the top fine category classifiers, the HD-CNN predicts hermit crab as the most probable label.</p><p>In the second case, the ImageNet-NIN confuses the ground truth hand blower with other objects of close shapes and appearances, such as plunger and barbell. For HD-CNN, the coarse category component is also not confident about which coarse category the object belongs to and thus assigns even probability mass to the top coarse categories. For the top 3 fine category classifiers, #74 strongly predicts ground truth label while the other two #49 and #40 rank the ground truth label at the 2nd and 4th place respectively. Overall, the HD-CNN ranks the ground truth label at the 1st place. This demonstrates HD-CNN needs to rely on multiple fine category classifiers to make correct predictions for difficult cases.</p><p>Overlapping coarse categories.To investigate the impact of overlapping coarse categories on the classification, we train another HD-CNN with 89 fine category classifiers using disjoint coarse categories. It achieves top-1 and top-5 errors of 38.44% and 17.03% respectively, which is higher than those of the HD-CNN using overlapping coarse category hierarchy by 1.78% and 1.23% <ref type="table" target="#tab_4">(Table 3</ref>). Conditional executions. By varying the hyperparameter β, we can control the number of fine category components that will be executed. There is a trade-off between execution time and classification error as shown in <ref type="figure" target="#fig_5">Fig 4 right</ref>. A larger value of β leads to lower error at the cost of more executed fine category components. By enabling conditional executions with hyperparameter β = 8, we obtain a substantial 6.3X speed up with merely a minor increase of single-view testing top-5 error from 16.62% to 16.75% <ref type="table" target="#tab_3">(Table 2)</ref>. With such speedup, the HD-CNN testing time is less than 3 times as much as that of the building block net. Parameter compression. We compress independent layers conv4 and cccp7 as they account for 60% of the parame-   Comparison with model averaging. As the HD-CNN memory footprint is about three times as much as the building block net, we independently train three ImageNet-NIN nets and average their predictions. We obtain top-5 error 17.11% which is 0.6% lower than the building block but is 1.31% higher than that of HD-CNN <ref type="table" target="#tab_4">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">VGG-16-layer Building Block Net</head><p>The second building block net we use is a 16-layer CNN from <ref type="bibr" target="#b25">[26]</ref>. We denote it as ImageNet-VGG-16-layer 3 . The layers from conv1 1 to pool4 are shared and they account for 5.6% of the total parameters and 90% of the total floating number operations. The remaining layers are used as independent layers in coarse and fine category classifiers. We follow the training and testing protocols as in <ref type="bibr" target="#b25">[26]</ref>. For training, we first sample a size S from the range [256, 512] and resize the image so that the length of short edge is S. Then a randomly cropped and flipped patch of size 224 × 224 is used for training. For testing, dense evaluation is performed on three scales {256, 384, 512} and the aver-3 https://github.com/BVLC/caffe/wiki/Model-Zoo aged prediction is used as the final prediction. Please refer to <ref type="bibr" target="#b25">[26]</ref> for more training and testing details. On ImageNet validation set, ImageNet-VGG-16-layer achieves top-1 and top-5 errors 24.79% and 7.50% respectively.</p><p>We build a category hierarchy with 84 overlapping coarse categories. We implement multi-GPU training on Caffe by exploiting data parallelism <ref type="bibr" target="#b25">[26]</ref> and train the fine category classifiers on two NVIDIA Tesla K40c cards. The initial learning rate is 0.001 and it is decreased by a factor of 10 every 4K iterations. HD-CNN fine-tuning is not performed. Due to large memory footprint of the building block net <ref type="table" target="#tab_3">(Table 2)</ref>, the HD-CNN with 84 fine category classifiers cannot fit into the memory directly. Therefore, we compress the parameters in layers fc6 and fc7 as they account for over 85% of the parameters. Parameter matrices in fc6 and fc7 are of size 4096 × 25088 and 4096 × 4096. Their compression hyperparameters are (s, k) = (14, 64) and (s, k) = (4, 256). The compression factors are 29.9 and 8 respectively. The HD-CNN obtains top-1 and top-5 errors 23.69% and 6.76% on ImageNet validation set and improves over ImageNet-VGG-16-layer by 1.1% and 0.74% respectively. Comparison with state-of-the-art. Currently, the two best nets on ImageNet dataset are GoogLeNet <ref type="bibr" target="#b30">[31]</ref>  <ref type="table" target="#tab_5">(Table 4</ref>) and VGG 19-layer network <ref type="bibr" target="#b25">[26]</ref>. Using multi-scale multicrop testing, a single GoogLeNet net achieves top-5 error 7.9%. With multi-scale dense evaluation, a single VGG 19layer net obtains top-1 and top-5 errors 24.8% and 7.5% and improves top-5 error of GoogLeNet by 0.4%. Our HD-CNN decreases top-1 and top-5 errors of VGG 19-layer net by 1.11% and 0.74% respectively. Furthermore, HD-CNN slightly outperforms the results of averaging the predictions from VGG-16-layer and VGG-19-layer nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and Future Work</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) A two-level category hierarchy where the classes are taken from ImageNet 1000-class dataset. (b) Hierarchical Deep Convolutional Neural Network (HD-CNN) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>HD-CNN training algorithm 1: procedure HD-CNN TRAINING 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: HD-CNN 10-view testing error against the number of coarse categories on CIFAR100 dataset. Right: Histogram of fine category occurrences in 9 overlapping coarse categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Case studies on ImageNet dataset. Each row represents a testing case. Column (a): test image with ground truth label. Column (b): top 5 guesses from the building block net ImageNet-NIN. Column (c): top 5 Coarse Category (CC) probabilities. Column (d)-(f): top 5 guesses made by the top 3 fine category CNN components. Column (g): final top 5 guesses made by the HD-CNN. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Left: Class-wise HD-CNN top-5 error improvement over the building block net. Right: Mean number of executed fine category classifiers and top-5 error against hyperparameter β on the ImageNet validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>10-view testing errors on CIFAR100 dataset. Notation CCC=coarse category consistency.</figDesc><table><row><cell>Method</cell><cell>Error</cell></row><row><cell>Model averaging (2 CIFAR100-NIN nets)</cell><cell>35.13</cell></row><row><cell>DSN [19]</cell><cell>34.68</cell></row><row><cell>CIFAR100-NIN-double</cell><cell>34.26</cell></row><row><cell>dasNet [30]</cell><cell>33.78</cell></row><row><cell>Base: CIFAR100-NIN</cell><cell>35.27</cell></row><row><cell>HD-CNN, no finetuning</cell><cell>33.33</cell></row><row><cell>HD-CNN, finetuning w/o CCC</cell><cell>33.21</cell></row><row><cell>HD-CNN, finetuning w/ CCC</cell><cell>32.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Comparison of testing errors, memory footprint</cell></row><row><cell cols="4">and testing time between building block nets and HD-CNNs</cell></row><row><cell cols="4">on CIFAR100 and ImageNet datasets. Statistics are col-</cell></row><row><cell cols="4">lected under single-view testing. Three building block nets</cell></row><row><cell cols="4">CIFAR100-NIN, ImageNet-NIN and ImageNet-VGG-16-</cell></row><row><cell cols="4">layer are used. The testing mini-batch size is 50. No-</cell></row><row><cell cols="4">tations: SL=Shared layers, CE=Conditional executions,</cell></row><row><cell cols="2">PC=Parameter compression.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>top-1, top-5</cell><cell>Memory</cell><cell>Time</cell></row><row><cell></cell><cell></cell><cell>(MB)</cell><cell>(sec.)</cell></row><row><cell>Base:CIFAR100-NIN</cell><cell>37.29</cell><cell>188</cell><cell>0.04</cell></row><row><cell>HD-CNN w/o SL</cell><cell>34.50</cell><cell>1356</cell><cell>2.43</cell></row><row><cell>HD-CNN</cell><cell>34.36</cell><cell>459</cell><cell>0.28</cell></row><row><cell>HD-CNN+CE</cell><cell>34.57</cell><cell>447</cell><cell>0.10</cell></row><row><cell>HD-CNN+CE+PC</cell><cell>34.73</cell><cell>286</cell><cell>0.10</cell></row><row><cell>Base:ImageNet-NIN</cell><cell>41.52, 18.98</cell><cell>535</cell><cell>0.19</cell></row><row><cell>HD-CNN</cell><cell cols="2">37.92, 16.62 3544</cell><cell>3.25</cell></row><row><cell>HD-CNN+CE</cell><cell>38.16, 16.75</cell><cell>3508</cell><cell>0.52</cell></row><row><cell>HD-CNN+CE+PC</cell><cell>38.39, 16.89</cell><cell>1712</cell><cell>0.53</cell></row><row><cell>Base:ImageNet-VGG-</cell><cell>32.30, 12.74</cell><cell>4134</cell><cell>1.04</cell></row><row><cell>16-layer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HD-CNN+CE+PC</cell><cell cols="2">31.34, 12.26 6863</cell><cell>5.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of 10-view testing errors between ImageNet-NIN and HD-CNN. Notation CC=Coarse category. The compression factors are 4.8 and 2.7. The compression decreases the memory footprint from 3508MB to 1712MB and merely increases the top-5 error from 16.75% to 16.89% under single-view testing (Table 2).</figDesc><table><row><cell>Method</cell><cell>top-1, top-5</cell></row><row><cell>Base:ImageNet-NIN</cell><cell>39.76, 17.71</cell></row><row><cell cols="2">Model averaging (3 base nets) 38.54, 17.11</cell></row><row><cell>HD-CNN, disjoint CC</cell><cell>38.44, 17.03</cell></row><row><cell>HD-CNN</cell><cell>36.66, 15.80</cell></row><row><cell cols="2">ters in ImageNet-NIN. Their parameter matrices are of size</cell></row><row><cell cols="2">1024 × 3456 and 1024 × 1024 and we use compression</cell></row><row><cell cols="2">hyperparameters (s, k) = (3, 128) and (s, k) = (2, 256).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Errors on ImageNet validation set.</figDesc><table><row><cell>Method</cell><cell>top-1, top-5</cell></row><row><cell>GoogLeNet,multi-crop [31]</cell><cell>N/A,7.9</cell></row><row><cell>VGG-19-layer, dense [26]</cell><cell>24.8,7.5</cell></row><row><cell>VGG-16-layer+VGG-19-layer,dense</cell><cell>24.0,7.1</cell></row><row><cell>Base:ImageNet-VGG-16-layer,dense</cell><cell>24.79,7.50</cell></row><row><cell>HD-CNN,dense</cell><cell>23.69,6.76</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mavenlin/cuda-convnet/blob/ master/NIN/cifar-100_def</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://gist.github.com/mavenlin/ d802a5849de39225bcc6</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrated that HD-CNN is a flexible deep CNN architecture to improve over existing deep CNN models. We showed this empirically on both CIFAR-100 and Image-Net datasets using three different building block nets. As part of future work, we plan to extend HD-CNN architectures to those with more than 2 hierarchical levels and also verify our empirical results in a theoretical framework.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical image annotation using semantic hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bannour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and balanced: Efficient label tree learning for large scale object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic label sharing for learning with many categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative learning of relaxed hierarchy for large-scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and using taxonomies for fast visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual concept learning: Combining machine vision and bayesian generalization on concept hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Deeplysupervised nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building and using a semantivisual image hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic label trees for efficient large scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic hierarchies for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constructing category hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to share visual appearance for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of visual object class hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6116</idno>
		<title level="m">Improving deep neural networks with probabilistic maxout units</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic hierarchies for image annotation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Tousch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning hierarchical similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Errordriven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale category structure aware image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting object hierarchy: Combining models from different category levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
