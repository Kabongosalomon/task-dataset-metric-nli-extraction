<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Format Contrastive Learning of Audio Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
							<email>luyuwang@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Aäron van den Oord Google Deepmind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Aäron van den Oord Google Deepmind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Format Contrastive Learning of Audio Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances suggest the advantage of multi-modal training in comparison with single-modal methods. In contrast to this view, in our work we find that similar gain can be obtained from training with different formats of a single modality. In particular, we investigate the use of the contrastive learning framework to learn audio representations by maximizing the agreement between the raw audio and its spectral representation. We find a significant gain using this multi-format strategy against the single-format counterparts. Moreover, on the downstream AudioSet and ESC-50 classification task, our audio-only approach achieves new state-ofthe-art results with a mean average precision of 0.376 and an accuracy of 90.5%, respectively.</p><p>Apart from the raw audio format, traditional signal processing allows us to convert the wavefroms into the spectral representations via shoft-time Fourier transforms (STFTs) <ref type="bibr" target="#b30">[31]</ref>. Such spectrograms can further be retrieved into log-mel filter banks and mel-frequency cepstral coefficients (MFCCs), which are the dominant formats for supervised and unsupervised audio recognition</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised learning leverages proxy tasks to learn useful representations of the data without requiring manually annotated labels. In computer vision, methods using contrastive losses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> stand out on the ImageNet benchmark <ref type="bibr" target="#b7">[8]</ref>, which learns by maximizing the similarity between augmented views from the same image. Contrastive learning also facilitates the recent rapid progress in unsupervised speech recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Good speech representations should be able to extract transient linguistic information. Therefore, these works rely on context prediction models that output representations with a fine-grain temporal resolution, while excluding non-speech sound that can distract the model from the task. For both image and speech recognition, the gaps between supervised and unsupervised representations have largely been eliminated.</p><p>Unlike speech recognition, the multi-instance audio events classification problem requires discriminative representations to tell the differences among a broad class of audio events. AudioSet <ref type="bibr" target="#b14">[15]</ref> is the ImageNet-scale dataset for general audio understanding, which contains 527 highly imbalanced event classes. Recent development in this direction has mainly been focusing on supervised learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, a triplet-based unsupervised approach is introduced to learn audio features from augmented spectrograms. Later, the effectiveness of contrastive predictive coding (CPC) is invstigated in <ref type="bibr" target="#b20">[21]</ref>, which operates on raw waveforms and is widely used for speech models. Meanwhile, recent works show that better representations can be learned by the proxy task of predicting whether the visual and audio signals come from the same video <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. On the AudioSet benchmark, <ref type="bibr" target="#b25">[26]</ref> shows that it is beneficial to maximize the coincidence between video and audio with the contrastive loss. There is a clear advantage by further taking the additional text modality into account <ref type="bibr" target="#b27">[28]</ref>. However, the state-of-the-art unsupervised audio model is still lagging behind the supervised one <ref type="bibr" target="#b18">[19]</ref> (mean average precision (mAP) 0.309 vs 0.439). However, to the best of our knowledge, all previous works consider only one format of the audio modality. In this paper, we investigate the use of contrastive learning to learn audio representations from multiple formats. Different from the models that drive the current progress on learning unsupervised speech representations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, our method does not rely on the context prediction network, and directly contrast two augmented views (which resembles image models in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>). We conduct experiments on different input formats, architectures, and augmentations. It is found that much better representations can be learned by maximizing the agreement between two views from the same audio represented by the raw waveform and log-mel filterbanks. As a result, our single-modal model has a test mAP of 0.376 on AudioSet, outperforming the previous best multi-modal score of 0.309 <ref type="bibr" target="#b27">[28]</ref> by a large margin. Moreover, it generalizes to the ESC-50 downstream classification task with a new state-of-the-art accuracy of 90.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning framework</head><p>The multi-format contrastive audio learning framework is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The input audio in the waveform format is first cropped into two shorter clips x i and x j . One of them can be further transformed into the spectral representation. Waveform or spectrogram augmentations are then applied accordingly, creating the positive pair ( x i , x j ). There are different ways to form the distractors or negatives including sampling from the same or other data samples. In this work we choose the negative pairs composing views from two different samples the same way as in SimCLR <ref type="bibr" target="#b5">[6]</ref>. Then it learns by maximizing the similarity between the encoded representations of the positive pair (z i , z j ). The loss function is defined as</p><formula xml:id="formula_0">L i,j = −log exp (sim (z i , z j ) /τ ) k =i exp (sim (z i , z k ) /τ )<label>(1)</label></formula><p>where τ denotes the temperature parameter, and sim (·, ·) is the nonlinear cosine similarity measure with the form of sim</p><formula xml:id="formula_1">(u, v) = g (u) · g (v) / g (u) g (v)</formula><p>, in which the projector g is a multi-layer perceptron (MLP) model with 1 hidden layer and ReLU nonlinearity. It is shared by both branches. The summation in the denominator over k is computed from 2N − 1 crops in the batch (excluding z i ). Both L i,j and L j,i are computed and summed up as the overall loss for the positive pair (</p><formula xml:id="formula_2">x i , x j ).</formula><p>The final loss is computed across all positive pairs in the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>For the spectral input format (including spectrograms, log-mel, and MFCCs), we adopt the stateof-the-art supervised models from <ref type="bibr" target="#b18">[19]</ref> by removing the last two linear layers, and directly use the outputs from the global pooling layer. These models include CNN6, CNN10, CNN14, ResNet22, Resnet38, and ResNet54. For convenience, in this paper we use their original names even though there are two less layers.</p><p>When raw audio is presented, we employ networks previously used as the encoders in various unsupervised speech models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. The building block is a 1D convolutional layer followed by Group Normalization and ReLU activation. The first layer has a kernel size of 10 and stride 5, followed by 5 to 9 layers of kernel size of 4 and stride 2. These models shrink the temporal dimension by 160 to 2560 times, so that we refer them as Conv160, Conv320, ..., Conv2560 in this work. The number of filters in each layer is 512. Global average pooling is applied on the time dimension at the end. Besides, we consider the Res1dNet-31 and Res1dNet-51 model from <ref type="bibr" target="#b18">[19]</ref>. We remove the last two linear layers and find it is important to use Group Normalizations for these models.</p><p>The CNN6, CNN10 and Conv160 to Conv2560 model output a feature space of 512 dimensions, and the rest, namely, CNN14, ResNet22, ResNet38, ResNet54, Res1dNet-31, and Res1dNet-51, result in 2048 dimensions. The latent sizes need to be matched for the loss function when two different models are used. Note that the features from the encoders are pooled from the time and/or frequency dimension. In the ablation studies, if only one audio format is used, we let the two branches share the same model. We have also experimented training two models and concatenate the output features but found it yields similar results. If two different formats are presented, we use two networks and concatenate the features for the downstream tasks. In the final results, we also report the performance of each network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Audio augmentations</head><p>It is observed previously that augmentations are important to the contrastive learning framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. We consider different types of augmentations for raw audio and spectrograms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>As the pitch shift requires additional STFTs on the fly, in this work we consider the following less expensive augmentations:</p><p>Audio mixing Small additive noise of any sort will not alter the original categories of the audio. Given two audio clips x 1 and x 2 , the mixed-up version iŝ</p><formula xml:id="formula_3">x 1 = αx 1 + (1 − α)x 2<label>(2)</label></formula><p>wherex 1 inheritances labels from x 1 . In this work, α is samples from β(5, 2) distribution. This simulates various realistic noise conditions.</p><p>Time masking t consecutive time steps [t 0 , t 0 + t) of the audio can be dropped out and it should not change the event classes, where t 0 is randomly sampled. This can be applied both to raw audio and spectrograms.</p><p>Frequency masking A small amount of f frequency components [f 0 , f 0 + f ) on the spectrogram can be masked out without losing semantic information.</p><p>Frequency shift One can apply the truncated shift in frequency to the spectrograms by an integer number sampled from [−F, F ], where F is the maximum shift size. Missing values after the shift are set to zero energy. Intuitively, this is a less expensive alternative of changing the pitch of the audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We use the audio segments sampled at 16k Hz from AudioSet <ref type="bibr" target="#b14">[15]</ref> for both training and evaluation. We split the original training set into training and validation subset by 95% and 5%, respectively. We evaluate the representations in the downstream task of training shallow fully connected audio classifiers following the same setup as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>, where a 1-hidden-layer MLP with 512 units is used and the parameters in the pretrained network are fixed. In this section we detail some of key factors that affect the model performance based on the development set. Then we show how the proposed method compares to the state of the art on the test set.</p><p>Unless noted, the models are trained up to 400k steps with a batch size of 1024. Adam optimizer is used, starting from an initial learning rate of 10 −4 , and follows a cosine learning rate decay down to 10 −6 . We randomly crop two windows of 3 seconds from each data sample during training. On evaluation we equally split the data into overlapped subclips with the stride of half of the crop  size, and average the logits from the subclips to obtain the overall score of the clip. The loss has a temperature of 0.1. When the spectral representations are used, they are extracted by a window size of 20 ms and stride of 10 ms. The spectrogram and log-mel features have 80 dimensions. For MFCCs we follow the convention and take 13 features <ref type="bibr" target="#b30">[31]</ref>. The default small model uses CNN10 for spectrograms and Conv320 for raw input, and the base model employs CNN14 and Res1dNet-31.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Audio formats</head><p>We compare the performances of learning with different combinations of audio formats in <ref type="figure" target="#fig_1">Figure 2</ref>. It is noticed that both the small and base model benefit from using two kinds of input formats. Meanwhile, maximizing agreement between raw waveforms and the frequency representations outperforms combinations with two spectrograms by a large margin. In particular, the combination of raw audio and log mel spectrograms really stands out: on the base model, it improves relatively upon the raw-audio-only and log-mel-only counterpart by 15% and 41%, respectively. The MFCC-based models have the lowest scores, possibly because they are low in feature dimensions. We think the reason why this works well is because taking another format of audio can be viewed as an aggressive way of transforming or augmenting the data to create semantically related but vastly different views, such that the contrastive learning framework can not leverage the trivial cues to solve the proxy task without learning meaningful representations. <ref type="table" target="#tab_0">Table 1</ref> shows that the model is better trained when taking two randomly cropped clips of 3 to 5 seconds to create the views. Taking the full length (10 seconds) results in the worst performance. This is possibly due to the multi-instance nature of AudioSet. Because our models output features averaged on the time dimension, and some class may only last a very short duration within the clip, taking a long temporal scale may completely bury the short-lasting classes, resulting in a lower validation score. In addition, it also shows that the maximum frequency shift is optimal around half of the frequency dimension size.  In <ref type="figure" target="#fig_2">Figure 3</ref> it is seen that the frequency shift has the biggest impact on both the small and base model. The base model benefits more from the audio mixing, possibly because the small model does not have enough capacity to account for it. In our experiments, masking on either time or frequency does not improve the downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Creation of the views</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model architectures</head><p>We run ablations on the choice of model architectures and the results are shown in <ref type="table" target="#tab_1">Table 2</ref>. For smaller models, we observe gains by increasing the model capacity. However, the same does not hold for large models -the performance does not further improve when the model size goes beyond Res1dNet-31 or CNN14. The same behavior has also been documented in the supervised setting <ref type="bibr" target="#b18">[19]</ref>.</p><p>We leave how to scale it further for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">More ablations</head><p>We observe trends similar to the SimCLR image model <ref type="bibr" target="#b5">[6]</ref> on the choices of the temperature parameter <ref type="table" target="#tab_0">(Table 1)</ref>, projection MLP latent size <ref type="table" target="#tab_0">(Table 1)</ref>, batch size <ref type="figure" target="#fig_3">(Figure 4)</ref>. In particular, we also find that it is better to train the contrastive framework with a very large batch size, possibly because it needs a large pool of negatives for the softmax loss in Equation 1 to pick out the hard ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison to the state of the art</head><p>Based on the findings from the ablations above, we train our final model with CNN14 and Res1dNet-31 using a large batch size of 32768. Frequency shift, audio mixing, time and frequency masking are applied to the log-mel branch (CNN14), and audio mixing is used for the raw waveform (Res1dNet-31). It takes 3-second crops and runs up to 700k steps and save the model with the best validation score. The latent size in the projection head is 1024.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Train inputs Eval inputs Test mAP</head><p>Triplet <ref type="bibr" target="#b19">[20]</ref> log-mel log-mel 0.244 L 3 <ref type="bibr" target="#b21">[22]</ref> log-mel + video log-mel 0.249 CPC <ref type="bibr" target="#b20">[21]</ref> waveform waveform 0.277 C 3 <ref type="bibr" target="#b25">[26]</ref> log-mel + video log-mel 0.285 MMV <ref type="bibr" target="#b27">[28]</ref> log-mel + video + text log-mel 0.309 We report the test scores on AudioSet in <ref type="table" target="#tab_2">Table 3</ref>. When only one audio format is used in the contrastive learning framework, it is observed that the results are already better than the previous best score trained with multiple modalities. Specifically, our log-mel-only model has a test mAP of 0.329 and the waveform-only one scores 0.336, outperforming the multimodal versatile network at 0.309, which is trained with audio, video, and texts <ref type="bibr" target="#b27">[28]</ref>. This is counter-intuitive because previous works have shown multi-modal learning is better than the single-modal ones. We think this is at least partially because using only the audio modality allows one to train models using the contrastive loss with a very large batch size and more training steps given the same computation budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>When two audio formats are presented and both the log-mel and waveform network are trained simultaneously, it is seen that each individual network performs better than when trained with only one format. Note that the log-mel network, performing at 0.368 mAP, employs the CNN14 architecture. The same network trained under the supervised setting is reported with a mAP of 0.375 in <ref type="bibr" target="#b18">[19]</ref> (without class balancing). If we use the concatenated features from the two networks, the performance further increases to 0.376. The current supervised state of the art of 0.439 mAP is achieved by the Wavegram model trained with both log-mel spectrograms and waveforms <ref type="bibr" target="#b18">[19]</ref>. However, it is noted that class balancing is crucial to this model, which requires the access to class labels. The self-supervised framework used in this work does not require any label and has no assumption about the class distribution. This ensures that this self-supervised method can scale well with the large amount of unlabelled data. We leave this for future work.</p><p>We also present in <ref type="table" target="#tab_4">Table 4</ref> the results on generalization to a smaller downstream dataset ESC-50 <ref type="bibr" target="#b33">[34]</ref>. It is widely used for evaluating audio representations trained by various cross-modal frameworks.</p><p>Our multi-format model achieves a new SOTA accuracy of 90.5% without requiring any additional modality or dataset for pre-training. Moreover, without feature concatenation, the raw audio network of this model alone has an accuracy of 89.3%, while the log mel network achieves 89.7%, which are higher than the corresponding results of 84.9% and 86.3% using single-format training, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we study the use of multiple formats of the audio for contrastive learning. We observe a significant advantage when training with both raw waveforms and log-mel spectrograms. Our model improves the state of the art on the AudioSet benchmark relatively by 21.7%, bridging the gap between unsupervised and supervised learning. Our work shows that multi-format training is promising to fully unlock the potential of large-scale and audio-only unsupervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the multi-format contrastive audio learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Validation mAP of the small (left) and base (right) models with different combinations of audio formats as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Validation mAP of the raw-audio-vs-log-mel models with different combinations of raw audio (along rows) and spectrogram (along columns) augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Validation mean average precision of different batch sizes on the base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effects of different crop sizes, maximum frequency shifts, temperature parameters, and projection latent sizes on the base model when trained with both raw audios and log mel spectrograms.</figDesc><table><row><cell>Crop size (s)</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell>Val mAP</cell><cell cols="9">0.310 0.336 0.340 0.344 0.341 0.328 0.305 0.262</cell></row><row><cell>Max freq shift</cell><cell>0</cell><cell>2</cell><cell></cell><cell>4</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell></row><row><cell>Val mAP</cell><cell cols="9">0.331 0.329 0.333 0.340 0.340 0.342 0.338 0.326</cell></row><row><cell cols="4">Temperature 0.05</cell><cell>0.1</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell><cell>1</cell></row><row><cell cols="2">Val mAP</cell><cell cols="7">0.330 0.340 0.326 0.312 0.297 0.285</cell></row><row><cell></cell><cell cols="2">Latent size</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell cols="2">1024 2048</cell><cell></cell></row><row><cell></cell><cell>Val mAP</cell><cell cols="6">0.331 0.338 0.340 0.342 0.345</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impacts of network architectures when trained with both raw waveforms and log mel spectrograms.</figDesc><table><row><cell cols="6">Small model Conv160 Conv320 Conv640 Conv1280 Conv2560</cell></row><row><cell>CNN6</cell><cell>0.300</cell><cell>0.305</cell><cell>0.313</cell><cell>0.314</cell><cell>0.314</cell></row><row><cell>CNN10</cell><cell>0.303</cell><cell>0.313</cell><cell>0.320</cell><cell>0.320</cell><cell>0.322</cell></row><row><cell></cell><cell cols="4">Base model Res1dNet-31 Res1dNet-51</cell><cell></cell></row><row><cell></cell><cell>CNN14</cell><cell>0.340</cell><cell></cell><cell>0.340</cell><cell></cell></row><row><cell></cell><cell>ResNet-22</cell><cell>0.335</cell><cell></cell><cell>0.336</cell><cell></cell></row><row><cell></cell><cell>ResNet-38</cell><cell>0.332</cell><cell></cell><cell>0.333</cell><cell></cell></row><row><cell></cell><cell>ResNet-54</cell><cell>0.335</cell><cell></cell><cell>0.340</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test performance of shallow model classification on AudioSet with fixed representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy of linear classification on ESC-50 with fixed audio representations. Hyperparameters of the classifier are selected with split 1 and the average accuracy over 5 splits is reported.</figDesc><table><row><cell>Model</cell><cell>Train inputs</cell><cell>Eval inputs</cell><cell>Test accuracy (%)</cell></row><row><cell>L 3 [22]</cell><cell>log-mel + video</cell><cell>log-mel</cell><cell>79.3</cell></row><row><cell>AVTS [24]</cell><cell>log-mel + video</cell><cell>log-mel</cell><cell>82.3</cell></row><row><cell>XDC [27]</cell><cell>log-mel + video</cell><cell>log-mel</cell><cell>84.8</cell></row><row><cell>GDT [30]</cell><cell>log-mel + video</cell><cell>log-mel</cell><cell>88.5</cell></row><row><cell>MMV [28]</cell><cell>log-mel + video + text</cell><cell>log-mel</cell><cell>88.9</cell></row><row><cell>AVID [29]</cell><cell>log-mel + video</cell><cell>log-mel</cell><cell>89.2</cell></row><row><cell>Ours</cell><cell>log-mel</cell><cell>log-mel</cell><cell>86.3</cell></row><row><cell>Ours</cell><cell>waveform</cell><cell>waveform</cell><cell>84.9</cell></row><row><cell>Ours</cell><cell>waveform + log-mel</cell><cell>log-mel</cell><cell>89.7</cell></row><row><cell>Ours</cell><cell>waveform + log-mel</cell><cell>waveform</cell><cell>89.3</cell></row><row><cell>Ours</cell><cell>waveform + log-mel</cell><cell>waveform + log-mel</cell><cell>90.5</cell></row><row><cell>Supervised [19]</cell><cell>waveform + log-mel</cell><cell>log-mel</cell><cell>90.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Yan Wu for fruitful discussions. We also appreciate the feedback from the anonymous reviewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Phil Blunsom, and Aäron van den Oord. Learning robust and multilingual speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11128</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7414" to="7418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7669" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep residual network for large-scale acoustic scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2568" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of semantic audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheet</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayang</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive predictive coding of audio with an adversary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="826" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selfsupervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16228</idno>
		<title level="m">Self-supervised multimodal versatile networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-modal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and language processing</title>
		<imprint>
			<publisher>Pearson Education</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00991</idno>
		<title level="m">Data augmenting contrastive learning of speech representations in the time domain</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
