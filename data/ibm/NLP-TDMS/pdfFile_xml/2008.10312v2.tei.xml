<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
							<email>evgeniizh@campus.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Baskin</surname></persName>
							<email>chaimbaskin@campus.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Mendelson</surname></persName>
							<email>avi.mendelson@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning for Large-Scale Unsupervised Image Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Neural Networks</term>
					<term>Unsupervised Deep Learning</term>
					<term>Represen- tation learning</term>
					<term>Self-Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at https://github.com/ Randl/kmeans_selfsuper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning has become the primary tool in various computer vision tasks, being especially successful in image classification, detection, and segmentation. However, along with massive computing resources required to train state-of-the-art neural networks (NNs), massive datasets with millions of labeled samples are a necessary part of its success. Since creating those datasets is a costly procedure, researchers have recently started looking at the methods of training NNs without labeled data. Those methods commonly referred to as self-supervised learning recently has become a powerful instrument for large-scale computer vision.</p><p>A result of training a network in a self-supervised manner is usually a representation: a vector in a latent space. Evaluation of those representations proceeds mainly by two following approaches: fine-tuning the network as a feature extractor for some task (common choices are segmentation tasks or ImageNert classification on a small amount of data, e.g., 1% of labels) or training a linear classifier on the arXiv:2008.10312v2 [cs.CV] 9 Nov 2020 extracted features. A variation of the latter is to train a -nearest neighbor classifier instead of a linear classifier. While linear classification directly evaluates the learned representation, it is not always capable of predicting performance on downstream tasks <ref type="bibr">(Resnick et al.,</ref> ). On the other hand, performance of a fine-tuned network strongly depends on the training procedure which is hard to separate from the quality of the representation itself.</p><p>In computer vision, the main approaches to training a network in a selfsupervised manner are contrastive losses, pretext tasks, and generative models. Contrastive methods <ref type="bibr">(van den Oord et al., ; Ye et al., ; Ermolov et al., )</ref> try to create different views of the same image and bring a representation of different views closer and representations of different images farther apart. Alternatively, it is possible to train the network to perform some label-free pretext task, such as predicting context <ref type="figure">(</ref> The overview of recent methods of self-supervised and unsupervised approaches is given in <ref type="table">Table .</ref> Table : Brief review of selected self-supervised methods, including links to code and linear-evaluation performance. Clustering from pretext <ref type="bibr">(Van Gansbeke et al.,</ref> ) is an unsupervised method. First plus in each row is a hyperlink. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>In this paper, we propose an additional way of evaluating selfsupervised learning: training a clustering algorithm on extracted features in an unsupervised manner. While this method suffers from similar disadvantages as linear evaluation, it can provide additional insights and a benchmark for unsupervised learning on large-scale datasets, such as ImageNet. We also show that self-supervised learning provides a strong baseline for unsupervised computer vision and mentions some possible direction for the current self-supervised methods performance improvement. Thanks to the increasing trend of publishing pre-trained models and code, we were able to test the existing approaches on the proposed benchmark. In particular, we show that the best-performing self-supervised algorithm achieves almost 40% top-accuracy on ImageNet without any supervision. Those results are on par with a specialized clustering approach by <ref type="bibr">Van Gansbeke et al. (</ref> ). We also evaluate ObjectNet (Barbu et al.,</p><p>), a dataset created for testing image classification algorithms in conditions closer to real-life, and conclude that it is hard to achieve generalization in unsupervised settings.</p><p>This benchmark provides a more challenging task for future self-supervised learning approaches, allowing them to better track their progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method . Metrics</head><p>The evaluation of unsupervised learning methods is a complicated topic, and many different metrics were developed. In this section, we briefly review the metrics we utilized for clustering evaluation.</p><p>Accuracy In the presence of ground truth labels, it is possible to evaluate the prediction accuracy by assigning classes to predicted clusters. Similarly to previous works (Xie et al., <ref type="bibr">; Jiang et al., ; Van Gansbeke et al., )</ref> we use linear assignment (Kuhn, ; Crouse, ) for assignment of clusters to the classes. In cases when the number of clusters is larger than the number of classes (overclustering), we assign one cluster to each class, while the rest is assigned greedily to maximize accuracy.</p><p>Normalized Mutual Information (V-measure) For a partition of the instances, , we define entropy as</p><formula xml:id="formula_0">( ) = − ∑︁ =1 ( ) log( ( ))</formula><p>, <ref type="bibr">( )</ref> and mutual information between two partitions as</p><formula xml:id="formula_1">MI( , ) = ∑︁ =1 ∑︁ =1 ( , ) log ( , ) ( ) ( ) , ( ) ( , ) = | | ( ) ( ) = | | . ( )</formula><p>To be able to compare mutual information in different cases, it is usually normalized (Kvalseth, ):</p><formula xml:id="formula_2">NMI( , ) = MI( , ) avg( ( ), ( )) , ( )</formula><p>where avg is some function, in our case the arithmetic mean.</p><p>Adjusted Mutual Information Since mutual information tends to have larger values when the number of clusters is large, mutual information should be adjusted for random chance (Vinh et al., )</p><formula xml:id="formula_3">AMI( , ) = MI( , ) − [MI( , )] [avg( ( ), ( )) − [ ( , )] . ( ) Adjusted Rand Index Rand index (Rand,</formula><p>) is another measure of clustering quality. It can be viewed as an accuracy measure over pairs of instances: denoting the number of pairs as = 2 , the number of pairs of instances that belong to the same set in both partitions as TP, and the number of pairs of instances that belong to the different sets in both partitions as TN, we define Rand index as RI = TP + TN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>We also adjust the index for chance in the usual manner (Hubert and Arabie, ):</p><formula xml:id="formula_4">ARI = RI( , ) − [RI( , )] [1 − [RI( , )]<label>, ( )</label></formula><p>where 1 is the maximal value of Rand index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Evaluation</head><p>To train a clustering model, we extract features of both the training and the validation set with a pre-trained model. We do not apply any augmentations during feature extraction. As opposed to the existing clustering approaches, e.g., DeepCluster (Caron et al., b), our method does not utilize a clustering objective as a part of feature extractor training, but merely uses a feature extractor pre-trained in a self-supervised manner.</p><p>Modern clustering approaches are usually based on some distance between different samples. Unfortunately, if the dimension of space is high, the distance between samples provides a little information. In our case, since most of the methods provide at least 1000-dimensional embeddings, we apply dimensional reduction.</p><p>In particular, we train incremental PCA model with batch size max 4096, 2 · , where is the dimension of extracted features. After applying dimensional reduction, we train mini-batch variation of k-means with the transformed features. Since the features are extracted only once, the training clustering model is relatively cheap. Depending on the model, it takes a couple of hours on CPU. Using augmentation and training first PCA and then clustering models can boost performance but is much more resource-demanding.</p><p>While by default, we set the number of clusters to be 1000 (number of ImageNet classes), we also experiment with overclustering, following Van Gansbeke et al. <ref type="bibr">(</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>We evaluate recent state-of-the-art approaches with the proposed protocol: ). We trained k-means for epochs, but even epoch often gets decent results.</p><p>ImageNet Experimental results for ImageNet are shown in <ref type="table">Table .</ref> During accuracy calculation, we used training labels for cluster assignment. In addition, we visualize different metrics in <ref type="figure">Fig.</ref> . We note a strong correlation between linear evaluation accuracy and k-means accuracy, except for SimCLRv (ResNet-×, SK) and</p><p>SwAV. We note that SCAN (Van Gansbeke et al., ) gives significantly larger ARI for similar accuracy values. We also note that both supervised and self-supervised methods with high-dimensional embeddings ( ResNet-× and EfficientNet-L )</p><p>show weaker results than their counterparts.</p><p>ObjectNet To access the generalization of acquired clustering, in addition to ImageNet, we evaluate the proposed method on ObjectNet. ObjectNet is a test set for vision tasks, created to control the performance of vision algorithms in settings close to real life.</p><p>Table shows results for k-means trained on ImageNet training set. In this case, we evaluated only on intersecting classes between ImageNet and ObjectNet. We show accuracy both for cluster assignment based on ImageNet training set (ACC-tr) and ObjectNet itself (ACC-val). Note that since some ObjectNet classes are mapped to two different ImageNet classes, assigning all images to a single class will result We employ evaluation code by Wightman ( ).  in ∼1.77% accuracy. By manually inspecting the predictions of the k-means, we conclude that in many cases, assignment of a large part of instances to a single class indeed happens.</p><p>When the ImageNet cluster assignment is used, no network, including supervised ones, show better-than-random performance. For ObjectNet assignment, only BigBiGAN (as well as supervised networks) is significantly better than assigning all the instances to a single class. Moreover, ResNet-shows better results than larger networks among different self-supervised networks. We advise that, as for now, AMI should be used as a metric for tracking progress on this task. for classes not in ImageNet. For self-supervised, the difference is much smaller: for InfoMin, performance on classes not included in ImageNet is the same (6.53%).     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Ablation study</head><p>Dimensionality reduction <ref type="figure">Fig. a</ref> shows the effect of the number of dimensions used for clustering. As expected, an increasing number of dimensions provide diminishing returns and might harm the results for more than dimensions.</p><p>Overclustering Since some classes in ImageNet may contain fairly different images, increasing the number of clusters beyond improves not only accuracy (since this metric uses real labels, its calculation inevitably involves passing information), but also ARI, as shown in <ref type="figure">Fig. b</ref>. For that reason, we add 1.5× overclustering version of the best-performing model to comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we study the applications of self-supervised learning for unsupervised classification. We establish competitive baselines by just applying PCA dimensional reduction and k-means clustering to features extracted by existing self-supervised methods. Thanks to a practice of publishing both code and pre-trained models, we were able to evaluate multiple state-of-the-art approaches and achieve as high as 39% accuracy on ImageNet in an unsupervised manner and 46% with overclustering.</p><p>Also, we propose an unsupervised clustering of extracted features as an additional way to evaluate self-supervised training approaches, along with linear evaluation and transfer learning.</p><p>Finally, we raise several issues and possible directions for future work. First, the question of whether severe underperformance of models with higher-dimensional feature space, such as ResNet 3×, remains open. Is it the weakness of the proposed clustering method or rather a property of the model? Can the reduction of the dimension of the embeddings improve performance on other tasks? Second, the models' poor performance transferred to ObjectNet, even among supervised models, with a prominent exception of BigBiGAN, is of great interest. Is it possible to achieve high performance on ObjectNet and ImageNet simultaneously, at least at the linear classification level? What is the reason BigBiGAN is the only model showing better-than-random results on ObjectNet? What is performance on other ImageNet-related datasets such as ReaL labels <ref type="figure">(Beyer et al.,</ref> ), ImageNetV <ref type="bibr">(Recht et al.,</ref> ), ImageNet-R (Hendrycks et al., ), etc.? Last, can the approach itself be improved? Can we take into account the method during the self-supervised training without significant performance degradation in other tasks? What is the better approach for dimensional reduction and clustering itself? What is the effect of augmentation in the clustering training phase?</p><p>We hope this paper will raise an interest in self-supervised approach to largescale image clustering. Hinton. Big self-supervised models are strong semi-supervised learners. arXiv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Visualization of different metrics: (a) unsupervised accuracy and linear evaluation accuracy; (b) ARI and unsupervised accuracy. Green points are outliers (SimCLRv (ResNet-×, SK) and SwAV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Accuracy and ARI as a function of dimensions after dimensional reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Accuracy and ARI as a function of number of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. :</head><label>:</label><figDesc>Ablation study for the best-performing model, SimCLRv (ResNet-, SK).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table :</head><label>:</label><figDesc>Experimental results on ImageNet in form mean±std of runs. Overclustering denoted as "over.", supervised models denoted as "super." Bold denotes highest results among our experiments, and red denotes results within one standard deviation of best results. Results for self-label are taken from the paper's official repository.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Linear (super.)</cell><cell>ACC</cell><cell>ARI</cell><cell>AMI</cell><cell>NMI</cell></row><row><cell cols="3">MoCo v (ResNet-)</cell><cell></cell><cell></cell><cell>71.1</cell><cell></cell><cell>23.09 ± 0.16 11.99 ± 0.13 37.04 ± 0.10 63.22 ± 0.05</cell></row><row><cell cols="3">InfoMin (ResNet-)</cell><cell></cell><cell></cell><cell>73.0</cell><cell></cell><cell>33.17 ± 0.32 14.71 ± 0.38 48.25 ± 0.27 68.80 ± 0.17</cell></row><row><cell cols="3">SwAV (ResNet-)</cell><cell></cell><cell></cell><cell>75.3</cell><cell></cell><cell>15.04 ± 0.77 7.72 ± 0.33 32.33 ± 0.16 55.34 ± 0.62</cell></row><row><cell cols="3">SimCLRv (ResNet-)</cell><cell></cell><cell></cell><cell>71.7</cell><cell></cell><cell>22.40 ± 0.19 10.97 ± 0.20 34.85 ± 0.29 61.52 ± 0.18</cell></row><row><cell cols="3">BigBiGAN (RevNet-</cell><cell>×)</cell><cell></cell><cell>61.3</cell><cell></cell><cell>3.00 ± 0.09 1.01 ± 0.04 8.81 ± 0.27 35.99 ± 0.69</cell></row><row><cell cols="3">InfoMin (ResNeXt-)</cell><cell></cell><cell></cell><cell>75.2</cell><cell></cell><cell>38.60 ± 0.67 22.15 ± 0.52 52.56 ± 0.11 72.17 ± 0.13</cell></row><row><cell cols="4">SimCLRv (ResNet-, SK)</cell><cell></cell><cell>77.2</cell><cell></cell><cell>39.07 ± 0.61 22.80 ± 0.60 52.03 ± 0.19 71.83 ± 0.13</cell></row><row><cell cols="3">SimCLRv (ResNet-</cell><cell>×, SK)</cell><cell></cell><cell>79.8</cell><cell></cell><cell>31.15 ± 0.74 13.84 ± 0.84 46.64 ± 0.25 65.79 ± 0.58</cell></row><row><cell cols="5">SimCLRv (ResNet-, SK, . × over.)</cell><cell>77.2</cell><cell></cell><cell>46.03 ± 0.21 23.94 ± 0.16 50.77 ± 0.25 73.14 ± 0.06</cell></row><row><cell cols="4">SCAN (Van Gansbeke et al.,</cell><cell>)</cell><cell>−</cell><cell></cell><cell>39.9</cell><cell>27.5</cell><cell>51.2</cell><cell>72.0</cell></row><row><cell cols="3">Self-label (Asano et al.,</cell><cell>)</cell><cell></cell><cell>63.5</cell><cell></cell><cell>30.5</cell><cell>16.2</cell><cell>42.0</cell><cell>75.4</cell></row><row><cell cols="4">Self-label 3× over. (Asano et al.,</cell><cell>)</cell><cell>68.8</cell><cell></cell><cell>38.1</cell><cell>27.6</cell><cell>52.8</cell><cell>75.7</cell></row><row><cell>ResNet-</cell><cell cols="2">(super.)</cell><cell></cell><cell></cell><cell>81.0</cell><cell></cell><cell>65.60 ± 0.93 53.02 ± 0.76 74.02 ± 0.22 84.97 ± 0.17</cell></row><row><cell cols="2">IG-ResNeXt-</cell><cell cols="2">× d (super.)</cell><cell></cell><cell>85.4</cell><cell></cell><cell>72.39 ± 0.52 63.31 ± 0.40 81.17 ± 0.08 89.23 ± 0.05</cell></row><row><cell cols="3">EfficientNet-L (super.)</cell><cell></cell><cell></cell><cell>88.2</cell><cell></cell><cell>59.08 ± 0.67 46.32 ± 0.60 69.35 ± 0.26 82.33 ± 0.18</cell></row><row><cell>0</cell><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="4">Linear evaluation accuracy, %</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table shows</head><label>shows</label><figDesc>results for clustering trained directly on ObjectNet. For pre-trained models, the performance on classes that are part of ImageNet is much better: for example, IG-ResNeXt-× d, has 41.36% accuracy as compared to 15.81%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table :</head><label>:</label><figDesc>Experimental results on ObjectNet in form mean±std of runs, using clusters acquired from ImageNet training.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>ACC-tr</cell><cell>ACC-val</cell><cell>ARI</cell><cell>AMI</cell><cell>NMI</cell></row><row><cell cols="3">MoCo v (ResNet-)</cell><cell></cell><cell cols="3">0.11 ± 0.19 1.76 ± 0.20 0.02 ± 0.04 0.42 ± 0.49 0.82 ± 0.66</cell></row><row><cell cols="3">InfoMin (ResNet-)</cell><cell></cell><cell cols="3">0.12 ± 0.26 2.18 ± 0.25 0.08 ± 0.07 1.81 ± 0.57 2.34 ± 0.56</cell></row><row><cell cols="3">SwAV (ResNet-)</cell><cell></cell><cell cols="3">0.21 ± 0.33 1.85 ± 0.12 0.06 ± 0.08 0.65 ± 0.34 1.30 ± 0.48</cell></row><row><cell cols="3">SimCLRv (ResNet-)</cell><cell></cell><cell cols="3">0.00 ± 0.00 2.14 ± 0.30 0.06 ± 0.06 1.47 ± 0.76 2.43 ± 0.75</cell></row><row><cell cols="3">BigBiGAN (RevNet-</cell><cell>×)</cell><cell cols="3">0.10 ± 0.01 4.92 ± 0.20 0.10 ± 0.01 1.00 ± 0.06 15.98 ± 0.69</cell></row><row><cell cols="3">InfoMin (ResNeXt-)</cell><cell></cell><cell cols="3">0.67 ± 0.39 1.96 ± 0.39 0.01 ± 0.01 0.70 ± 0.37 1.26 ± 0.58</cell></row><row><cell cols="4">SimCLRv (ResNet-, SK)</cell><cell cols="3">0.00 ± 0.00 1.69 ± 0.28 0.03 ± 0.07 0.55 ± 0.82 0.86 ± 0.94</cell></row><row><cell cols="3">SimCLRv (ResNet-</cell><cell>×, SK)</cell><cell cols="3">0.00 ± 0.00 1.72 ± 0.19 0.01 ± 0.01 0.44 ± 0.42 0.94 ± 0.61</cell></row><row><cell cols="7">SimCLRv (ResNet-, SK, . × over.) 0.04 ± 0.10 1.75 ± 0.19 0.01 ± 0.01 0.45 ± 0.38 0.99 ± 0.50</cell></row><row><cell>ResNet-</cell><cell cols="2">(super.)</cell><cell></cell><cell cols="3">0.36 ± 0.48 1.75 ± 0.35 0.03 ± 0.07 0.53 ± 0.97 0.76 ± 1.19</cell></row><row><cell cols="2">IG-ResNeXt-</cell><cell cols="2">× d (super.)</cell><cell cols="3">0.04 ± 0.08 2.15 ± 0.84 0.14 ± 0.21 2.12 ± 2.91 2.51 ± 3.31</cell></row><row><cell cols="3">EfficientNet-L (super.)</cell><cell></cell><cell cols="3">0.36 ± 0.44 2.10 ± 0, 41 0.13 ± 0.14 1.95 ± 1.37 2.34 ± 1.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table :</head><label>:</label><figDesc>Experimental results on ObjectNet using clusters acquired by training k-means on ObjectNet itself.</figDesc><table><row><cell>16</cell><cell>64</cell><cell></cell><cell>256</cell><cell>1024</cell></row><row><cell></cell><cell></cell><cell cols="2">PCA dimensions</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>ACC</cell><cell>ARI</cell><cell>AMI</cell><cell>NMI</cell></row><row><cell cols="3">MoCo v (ResNet-)</cell><cell></cell><cell cols="2">4.30 ± 0.05 0.77 ± 0.02 8.08 ± 0.10 20.57 ± 0.39</cell></row><row><cell cols="3">InfoMin (ResNet-)</cell><cell></cell><cell cols="2">4.96 ± 0.08 0.92 ± 0.22 8.85 ± 0.08 21.49 ± 0.17</cell></row><row><cell cols="3">SwAV (ResNet-)</cell><cell></cell><cell cols="2">3.44 ± 0.11 0.60 ± 0.04 6.77 ± 0.11 16.20 ± 0.54</cell></row><row><cell cols="3">SimCLRv (ResNet-)</cell><cell></cell><cell cols="2">3.67 ± 0.22 0.63 ± 0.02 6.72 ± 0.09 18.75 ± 0.24</cell></row><row><cell cols="3">BigBiGAN (RevNet-</cell><cell>×)</cell><cell cols="2">2.30 ± 0.03 0.116 ± 0.001 1.75 ± 0.03 14.93 ± 0.22</cell></row><row><cell cols="3">InfoMin (ResNeXt-)</cell><cell></cell><cell cols="2">6.53 ± 0.19 1.59 ± 0.04 12.49 ± 0.13 24.97 ± 0.24</cell></row><row><cell cols="4">SimCLRv (ResNet-, SK)</cell><cell cols="2">5.34 ± 0.20 1.15 ± 0.07 9.24 ± 0.23 22.08 ± 0.17</cell></row><row><cell cols="3">SimCLRv (ResNet-</cell><cell>×, SK)</cell><cell cols="2">4.20 ± 0.19 1.00 ± 0.08 8.62 ± 0.16 17.53 ± 0.27</cell></row><row><cell cols="6">SimCLRv (ResNet-, SK, . × over.) 6.47 ± 0.07 1.32 ± 0.05 9.46 ± 0.08 23.62 ± 0.28</cell></row><row><cell>ResNet-</cell><cell cols="2">(super.)</cell><cell></cell><cell cols="2">14.36 ± 1.80 6.09 ± 1.20 23.93 ± 0.26 32.60 ± 2.55</cell></row><row><cell cols="2">IG-ResNeXt-</cell><cell cols="2">× d (super.)</cell><cell cols="2">25.25 ± 0.46 14.03 ± 0.18 36.30 ± 0.11 44.72 ± 0.24</cell></row><row><cell cols="3">EfficientNet-L (super.)</cell><cell></cell><cell cols="2">7.70 ± 0.57 2.07 ± 0.30 17.78 ± 0.44 22.60 ± 0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>-large-scale-bias-controlled-dataset-for-pushing-the-limits-of-object-rec (cited on pp. and ) Lucas Beyer, Olivier J Hénaff, AlexanderKolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done with ImageNet? arXiv preprint arXiv: . , . URL https://arxiv.org/abs/2006.07159. (cited on p. ) Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In The European Conference on Computer Vision (ECCV), September a. (cited on p. ) Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), September b. URL https://openaccess.thecvf.com/content_ECCV_2018/html/ Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.html. (cited on p. ) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv: . , . URL https://arxiv.org/ abs/2006.09882. (cited on pp. and ) Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Proceedings of Machine Learning and Systems , pages -. a. URL https://openai. com/blog/image-gpt/. (cited on p. ) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv: . , b. URL https://arxiv.org/abs/2002.05709. (cited on p. ) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey</figDesc><table><row><cell cols="5">Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling</cell></row><row><cell cols="5">via simultaneous clustering and representation learning. arXiv preprint</cell></row><row><cell>arXiv:</cell><cell>.</cell><cell>,</cell><cell cols="2">. URL https://arxiv.org/abs/1911.05371. (cited on</cell></row><row><cell cols="2">pp. and )</cell><cell></cell><cell></cell></row><row><cell cols="5">Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations</cell></row><row><cell cols="5">by maximizing mutual information across views. arXiv preprint arXiv:</cell><cell>.</cell><cell>,</cell></row><row><cell cols="5">. URL https://arxiv.org/abs/1906.00910. (cited on p. )</cell></row><row><cell cols="5">Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang,</cell></row><row><cell cols="5">Dan Gutfreund, Josh Tenenbaum, and Boris Katz. ObjectNet: A large-scale</cell></row><row><cell cols="5">bias-controlled dataset for pushing the limits of object recognition models.</cell></row><row><cell cols="5">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and</cell></row><row><cell cols="5">R. Garnett, editors, Advances in Neural Information Processing Systems , pages</cell></row><row><cell>-</cell><cell cols="3">. Curran Associates, Inc.,</cell><cell>. URL http://papers.nips.cc/paper/</cell></row><row><cell cols="3">9142-objectnet-a</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>arXiv: .</idno>
		<ptr target="https://arxiv.org/abs/2006.16241" />
		<title level="m">out-of-distribution generalization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
		<ptr target="https://link.springer.com/article/10.1007%2FBF01908075" />
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
		<idno>arXiv: .</idno>
		<ptr target="https://arxiv.org/abs/1611.05148" />
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/abstract/document/8354196" />
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entropy and correlation: Some comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tarald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kvalseth</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/4309069" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/1553374.1553453</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.1145/1553374.1553453" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the th annual international conference on machine learning</title>
		<meeting>the th annual international conference on machine learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_ECCV_2018/html/Dhruv_Mahajan_Exploring_the_Limits_ECCV_2018_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">https:/www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356</idno>
		<ptr target="https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the th International Conference on Machine Learning, volume of Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the th International Conference on Machine Learning, volume of Machine Learning Research</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
