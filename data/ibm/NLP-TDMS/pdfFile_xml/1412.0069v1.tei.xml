<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian Detection aided by Deep Learning Semantic Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pedestrian Detection aided by Deep Learning Semantic Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples ( <ref type="figure">Fig.1 (a)</ref>), which have large ambiguity, e.g. the shape and appearance of 'tree trunk' or 'wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by highlevel representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. 'carrying backpack') and scene attributes (e.g. 'road', 'tree', and 'horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech [11] and ETH [12] datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection has attracted broad attentions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. This problem is challenging because of large variation and confusion in human body and background scene, as shown in <ref type="figure">Fig.1 (a)</ref>, where the positive and hard negative patches have large ambiguity. * For more technical details of this work, please send email to pluo. lhi@gmail.com <ref type="figure">Figure 1</ref>: Separating positive samples (pedestrians) from hard negative samples is challenging due to the visual similarity. For example, the first and second row of (a) represent pedestrians and equivocal background samples (hard negatives), respectively. (b) shows that our TA-CNN rejects more hard negatives than detectors using handcrafted features (HOG <ref type="bibr" target="#b7">[8]</ref> and ACF <ref type="bibr" target="#b8">[9]</ref>) and the bestperforming deep model (JointDeep <ref type="bibr" target="#b22">[23]</ref>).</p><p>Current methods for pedestrian detection can be generally grouped into two categories, the models based on handcrafted features <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13]</ref> and deep models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref>. In the first category, conventional methods extracted Haar <ref type="bibr" target="#b32">[33]</ref>, HOG <ref type="bibr" target="#b7">[8]</ref>, or HOG-LBP <ref type="bibr" target="#b33">[34]</ref> from images to train SVM <ref type="bibr" target="#b7">[8]</ref> or boosting classifiers <ref type="bibr" target="#b9">[10]</ref>. The learned weights of the classifier (e.g. SVM) can be considered as a global template of the entire human body. To account for more complex pose, the hierarchical deformable part models (DPM) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17]</ref> learned a mixture of local templates for each body part. Although they are sufficient to certain pose changes, the feature representations and the classifiers cannot be jointly optimized to improve performance. In the second category, deep neural networks achieved promising results <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref>, owing to their capacity to learn middle-level representation. For example, Ouyang et al. <ref type="bibr" target="#b22">[23]</ref> learned features by designing specific hidden layers for the Convolutional Neural Network (CNN), such that features, deformable parts, and pedestrian classification can be jointly optimized. However, previous deep models treated pedestrian detection as a single binary classification task, they can mainly learn middle-level features, which are not able to capture rich pedestrian variations, as shown in <ref type="figure">Fig.1 (a)</ref>.</p><p>To learn high-level representations, this work jointly optimizes pedestrian detection with auxiliary semantic tasks, including pedestrian attributes (e.g. 'backpack', 'gender', and 'views') and scene attributes (e.g. 'vehicle', 'tree', and 'vertical'). To understand how this work, we provide an example in <ref type="figure" target="#fig_0">Fig.2</ref>. If only a single detector is used to classify all the positive and negative samples in <ref type="figure" target="#fig_0">Fig.2 (a)</ref>, it is difficult to handle complex pedestrian variations. Therefore, the mixture models of multiple views were developed in <ref type="figure" target="#fig_0">Fig.2 (b)</ref>, i.e. pedestrian images in different views are handled by different detectors. If views are treated as one type of semantic tasks, learning pedestrian representation by multiple attributes with deep models actually extends this idea to extreme. As shown in <ref type="figure" target="#fig_0">Fig.2 (c)</ref>, more supervised information enriches the learned features to account for combinatorial more pedestrian variations. The samples with similar configurations of attributes can be grouped and separated in the high-level feature space.</p><p>Specifically, given a pedestrian dataset (denoted by P), the positive image patches are manually labeled with several pedestrian attributes, which are suggested to be valuable for surveillance analysis <ref type="bibr" target="#b20">[21]</ref>. However, as the number of negatives is significantly larger than the number of positives, we transfer scene attributes information from  <ref type="figure">Figure 3</ref>: Comparisons of the feature spaces of HOG, channel features, CNN that models pedestrian detection as binary classification, and TA-CNN, using the Caltech-Test set <ref type="bibr" target="#b10">[11]</ref>. The positive and hard negative samples are represented by red and green, respectively.</p><p>existing background scene segmentation databases (each one is denoted by B) to the pedestrian dataset, other than annotating them manually. A novel task-assistant CNN (TA-CNN) is proposed to jointly learn multiple tasks using multiple data sources. As different B's may have different data distributions, to reduce these discrepancies, we transfer two types of scene attributes that are carefully chosen, comprising the shared attributes that appear across all the B's and the unshared attributes that appear in only one of them. The former one facilitates the learning of shared representation among B's, whilst the latter one increases diversity of attribute. Furthermore, to reduce gaps between P and B's, we first project each sample in B's to a structural space of P and then the projected values are employed as input to train TA-CNN. Learning TA-CNN is formulated as minimizing a weighted multivariate cross-entropy loss, where both the importance coefficients of tasks and the network parameters can be iteratively solved via stochastic gradient descent <ref type="bibr" target="#b15">[16]</ref>. This work has the following main contributions. (1) To our knowledge, this is the first attempt to learn high-level representation for pedestrian detection by jointly optimizing it with semantic attributes, including pedestrian attributes and scene attributes. The scene attributes can be transferred from existing scene datasets without annotating manually.</p><p>(2) These multiple tasks from multiple sources are trained using a single task-assistant CNN (TA-CNN), which is carefully designed to bridge the gaps between different datasets. A weighted multivariate cross-entropy loss is proposed to learn TA-CNN, by iterating among two steps, updating network parameters with tasks' weights fixed and updating weights with network parameters fixed. (3) We systematically investigate the effectiveness of attributes in pedestrian detection. Extensive experiments on both challenging Caltech <ref type="bibr" target="#b10">[11]</ref> and ETH <ref type="bibr" target="#b11">[12]</ref> datasets demonstrate that TA-CNN outperforms state-of-the-art methods. It reduces miss rates of existing deep models on these datasets by 17 and 5.5 percent, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>We review recent works in two aspects. Models based on Hand-Crafted Features The handcrafted features, such as HOG, LBP, and channel features, achieved great success in pedestrian detection. For example, Wang et al. <ref type="bibr" target="#b33">[34]</ref> utilized the LBP+HOG features to deal with partial occlusion of pedestrian. Chen et al. <ref type="bibr" target="#b6">[7]</ref> modeled the context information in a multi-order manner. The deformable part models <ref type="bibr" target="#b12">[13]</ref> learned mixture of local templates to account for view and pose variations. Moreover, Dollár et al. proposed Integral Channel Features (ICF) <ref type="bibr" target="#b9">[10]</ref> and Aggregated Channel Features (ACF) <ref type="bibr" target="#b8">[9]</ref>, both of which consist of gradient histogram, gradients, and LUV, and can be efficiently extracted. Benenson et al. <ref type="bibr" target="#b1">[2]</ref> combined channel features and depth information. However, the representation of hand-crafted features cannot be optimized for pedestrian detection. They are not able to capture large variations, as shown in <ref type="figure">Fig.3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) and (b).</head><p>Deep Models Deep learning methods can learn features from raw pixels to improve the performance of pedestrian detection. For example, ConvNet <ref type="bibr" target="#b28">[29]</ref> employed convolutional sparse coding to unsupervised pre-train CNN for pedestrian detection. Ouyang et al. <ref type="bibr" target="#b21">[22]</ref> jointly learned features and the visibility of different body parts to handle occlusion. The JointDeep model <ref type="bibr" target="#b22">[23]</ref> designed a deformation hidden layer for CNN to model mixture poses information. Unlike the previous deep models that formulated pedestrian detection as a single binary classification task, TA-CNN jointly optimizes pedestrian detection with related semantic tasks, and the learned features are more robust to large variations, as shown in <ref type="figure">Fig.3</ref> (c) and (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head><p>Method Overview <ref type="figure" target="#fig_4">Fig.4</ref> shows our pipeline of pedestrian detection, where pedestrian classification, pedestrian attributes, and scene attributes are jointly learned by a single TA-CNN. Given a pedestrian dataset P, for example Caltech <ref type="bibr" target="#b10">[11]</ref>, we manually label the positive patches with nine pedestrian attributes, which are listed in <ref type="figure" target="#fig_2">Fig.5</ref>. Most of them are suggested by the UK Home Office and UK police to be valuable in surveillance analysis <ref type="bibr" target="#b20">[21]</ref>. Since the number of negative patches in P is significantly larger than the number of positives, we transfer scene attribute </p><formula xml:id="formula_0">Caltech (P) CamVid (B a ) Stanford (B b ) LM+SUN (B c ) √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √</formula><p>Pedestrian Attributes Scene Attributes Shared Unshared information from three public scene segmentation datasets to P, as shown in <ref type="figure" target="#fig_4">Fig.4</ref> (a), including CamVid (B a ) <ref type="bibr" target="#b4">[5]</ref>, Stanford Background (B b ) <ref type="bibr" target="#b13">[14]</ref>, and LM+SUN (B c ) <ref type="bibr" target="#b30">[31]</ref>, where hard negatives are chosen by applying a simple yet fast pedestrian detector <ref type="bibr" target="#b8">[9]</ref> on these datasets. As the data in different B's are sampled from different distributions, we carefully select two types of attributes, the shared attributes (outlined in orange) that present in all B's and the unshared attributes (outlined in red) that appear only in one of them. This is done because the former one enables the learning of shared representation across B's, while the latter one enhances diversity of attribute. All chosen attributes are summarized in <ref type="figure" target="#fig_2">Fig.5</ref>, where shows that data from different sources have different subset of attribute labels. For example, pedestrian attributes only present in P, shared attributes present in all B's, and the unshared attributes present in one of them, e.g.'traffic light' of B a . We construct a training set D by combing patches cropped from both P and B's. Let D = {(x n , y n )} N n=1 be a set of image patches and their labels, where each y n = (y n , o p n , o s n , o u n ) is a four-tuple 1 . Specifically, y n denotes a binary label, indicating whether an image patch is pedestrian or not.</p><formula xml:id="formula_1">o p n = {o pi n } 9 i=1 , o s n = {o si n } 4 i=1 , and o u n = {o ui n } 4 i=1</formula><p>are three sets of binary labels, representing the pedestrian, shared scene, and unshared scene attributes, respectively. As shown in <ref type="figure" target="#fig_4">Fig.4 (b)</ref>, TA-CNN employs image patch x n as input and predicts y n , by stacking four convolutional layers (conv1 to conv4), four maxpooling layers, and two fully-connected layers (fc5 and fc6). This structure is inspired by the AlexNet <ref type="bibr" target="#b15">[16]</ref> for large-scale general object categorization. However, as the difficulty of pedestrian detection is different from general object categorization, we remove one convolutional layer of AlexNet and reduce the number of parameters at all remaining layers. The subsequent structure of TA-CNN is specified in <ref type="figure" target="#fig_4">Fig.4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>Formulation of TA-CNN Each hidden layer of TA-CNN from conv1 to conv4 is computed recursively by  convolution and max-pooling, which are formulated as</p><formula xml:id="formula_2">h v(l) n = relu(b v(l) + u k vu(l) * h u(l−1) n ), (1) h v(l) n(i,j) = max ∀(p,q)∈Ω (i,j) {h v(l) n(p,q) }.<label>(2)</label></formula><p>In Eqn.(1), relu(x) = max(0, x) is the rectified linear function <ref type="bibr" target="#b18">[19]</ref> and * denotes the convolution operator applied on every pixel of the feature map h u(l−1) n , where h u(l−1) n and h v(l) n stand for the u-th input channel at the l − 1 layer and the v-th output channel at the l layer, respectively. k vu(l) and b v(l) denote the filters and bias. In Eqn.(2), the feature map h v(l) n is partitioned into grid with overlapping cells, each of which is denoted as Ω (i,j) , where (i, j) indicates the cell index. The max-pooling compares value at each location (p, q) of a cell and outputs the maximum value of each cell.</p><p>Each hidden layer in fc5 and fc6 is obtained by</p><formula xml:id="formula_3">h (l) n = relu(W (l) T h (l−1) n + b (l) ),<label>(3)</label></formula><p>where the higher level representation is transformed from lower level with a non-linear mapping. W (l) and b (l) are the weight matrixes and bias vector at the l-th layer. TA-CNN can be formulated as minimizing the log posterior probability with respect to a set of network parameters</p><formula xml:id="formula_4">W W * = arg min W − N n=1 log p(y n , o p n , o s n , o u n |x n ; W), (4) where E = − N n=1 log p(y n , o p n , o s n , o u n |x n )</formula><p>is a complete loss function regarding the entire training set. Here, we illustrate that the shared attributes o s n in Eqn.(4) are crucial to learn shared representation across multiple scene datasets B's.</p><p>For clarity, we keep only the unshared scene attributes o u n in the loss function, which then becomes E = − N n=1 log p(o u n |x n ). Let x a denote the sample of B a . A shared representation can be learned if and only if all the samples share at least one target (attribute). Since the samples are independent, the loss function can be expanded</p><formula xml:id="formula_5">as E = − I i=1 log p(o u1 i |x a i )− J j=1 log p(o u2 j , o u3 j |x b j )− K k=1 log p(o u4 k |x c k ), where I + J + K = N ,</formula><p>implying that each dataset is only used to optimize its corresponding unshared attribute, although all the datasets and attributes are trained in a single TA-CNN. For instance, the classification model of o u1 is learned by using B a without leveraging the existence of the other datasets. In other words, the probability of p(o u1 |x a , x b , x c ) = p(o u1 |x a ) because of missing labels. The above formulation is not sufficient to learn shared features among datasets, especially when the data have large differences. To bridge multiple scene datasets B's, we introduce the shared attributes o s , the </p><formula xml:id="formula_6">, i.e. p(o s1 , o s2 , o s3 , o s4 |x a , x b , x c ).</formula><p>Now, we reconsider Eqn. <ref type="formula">(4)</ref>, where the loss function can be decomposed similarly,</p><formula xml:id="formula_7">E = − I i=1 log p(o s i , o u1 i |x a i )− J j=1 log p(o s j , o u2 j , o u3 j |x b j ) − K k=1 log p(o s k , o u4 k |x c k ) − L =1 log p(y , o p |x p ), with I + J + K + L = N .</formula><p>Even though the discrepancies among B's can be reduced by o s , this decomposition shows that gap remains between datasets P and B's. To resolve this issue, we compute the structure projection vectors z n for each sample x n , and Eqn.(4) turns into</p><formula xml:id="formula_8">W * = arg min W − N n=1 log p(y n , o p n , o s n , o u n |x n , z n ; W).</formula><p>(5) For example, the first term of the above decomposition can be written as</p><formula xml:id="formula_9">p(o s i , o u1 i |x a i , z a i ),</formula><p>where z a i is attained by projecting the corresponding x a i in B a on the feature space of P. This procedure is explained below. Here z a i is used to bridge multiple datasets, because samples from different datasets are projected to a common space of P. TA-CNN adopts a pair of data (x a i , z a i ) as input (see <ref type="figure" target="#fig_4">Fig.4 (b)</ref>). All the remaining terms can be derived in a similar way.</p><p>Structure Projection Vector As shown in <ref type="figure" target="#fig_5">Fig.6</ref>, to close the gap between P and B's, we calculate the structure projection vector (SPV) for each sample by organizing the positive (+) and negative (-) data of P into two tree structures, respectively. Each tree has depth that equals three and partitions the data top-down, where each child node groups the data of its parent into clusters, for example C 1 1 and C 10 5 . Then, SPV of each sample is obtained by concatenating the distance between it and the mean of each leaf node. Specifically, at each parent node, we extract HOG feature for each sample and apply k-means to group the data. We partition the data into five clusters (C 1 to C 5 ) in the first level, and then each of them is further partitioned into ten clusters, e.g. C 1 1 to C 10 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Task-Assistant CNN</head><p>To learn network parameters W, a natural way is to reformulate Eqn.(5) as the softmax loss functions similar to the previous methods. We have 2</p><formula xml:id="formula_10">E − y log p(y|x, z) − 9 i=1 α i o pi log p(o pi |x, z) − 4 j=1 β j o sj log p(o sj |x, z) − 4 k=1 γ k o uk log p(o uk |x, z),<label>(6)</label></formula><p>where the main task is to predict the pedestrian label y and the attribute estimations, i.e. o pi , o sj , and o uk , are auxiliary semantic tasks. α, β, and γ denote the importance coefficients to associate multiple tasks. Here, p(y|x, z), p(o pi |x, z), p(o sj |x, z), and p(o uk |x, z) are modeled by softmax functions, for example, p(y = 0|x, z) =</p><formula xml:id="formula_11">exp(W m ·1 T h (L) ) exp(W m ·1 T h (L) )+exp(W m ·2 T h (L) )</formula><p>, where h (L) and W m indicate the top-layer feature vector and the parameter matrix of the main task y respectively, as shown in <ref type="figure" target="#fig_4">Fig.4</ref> </p><formula xml:id="formula_12">(b), and h (L) is obtained by h (L) = relu(W (L) h (L−1) + b (L) + W z z + b z ).</formula><p>Eqn. <ref type="formula" target="#formula_10">(6)</ref> optimizes eighteen loss functions together. It has two main drawbacks. First, since different tasks have different convergence rates, training many tasks together suffers from over-fitting. Previous works prevented overfitting by adjusting the importance coefficients. However, they are determined in a heuristic manner, such as early stopping <ref type="bibr" target="#b37">[38]</ref>, other than estimating in the learning procedure. Second, if the dimension of the features h (L) is high, the number of parameters at the top-layer increases exponentially. For example, if the feature vector h (L) has H dimensions, the weight matrix of each two-state variable (e.g. W m of the main task) has 2 × H parameters, whilst the weight matrix of the four-state variable 'viewpoint' has 4 × H parameters 3 . As we have seventeen two-state variables and one four-state variable, the total number of parameters at the top-layer is 17 × 2 × H + 4 × H = 38H.</p><p>To resolve the above issues, we cast learning multiple tasks in Eqn. <ref type="bibr" target="#b5">(6)</ref> as optimizing a single weighted multivariate cross-entropy loss, which can not only learn a compact weight matrix but also iteratively estimate the importance coefficients,</p><formula xml:id="formula_13">E − y T diag(λ) log p(y|x, z) − (1 − y) T diag(λ)(log 1 − p(y|x, z),<label>(7)</label></formula><p>where λ denotes a vector of importance coefficients and diag(·) represents a diagonal matrix. </p><formula xml:id="formula_14">1 1+exp(−W y T h (L) ) , where h (L)</formula><p>is achieved in the same way as in Eqn. <ref type="bibr" target="#b5">(6)</ref>. The optimization of Eqn. <ref type="bibr" target="#b6">(7)</ref> iterates between two steps, updating network parameters with the importance coefficients fixed and updating coefficients with the network parameters fixed.</p><p>Learning Network Parameters The network parameters are updated by minimizing Eqn. <ref type="bibr" target="#b6">(7)</ref> using stochastic gradient descent <ref type="bibr" target="#b15">[16]</ref> and back-propagation (BP) <ref type="bibr" target="#b27">[28]</ref>, where the error of the output layer is propagated top-down to update filters or weights at each layer. For example, the weight matrix of the L-th layer in the t + 1-th iteration, W y t+1 , is attained by</p><formula xml:id="formula_15">W y t+1 = W y t + ∆ t+1 , ∆ t+1 = 0.9 · ∆ t − 0.001 · · W y t − · ∂E ∂W y t .<label>(8)</label></formula><p>Here, t is the index of training iteration. ∆ is the momentum variable, is the learning rate, and ∂E ∂W y t = h (L) e (L) T is the derivative calculated by the outer product of the backpropagation error e (L) and the hidden features h (L) . The BP procedure is similar to <ref type="bibr" target="#b15">[16]</ref>. The main difference is how to compute error at the L-th layer. In the traditional BP algorithm, the error e (L) at the L-th layer is obtained by the gradient of Eqn. <ref type="bibr" target="#b6">(7)</ref>, indicating the loss, i.e. e (L) = y − y, where y denotes the predicted labels. However, unlike the conventional BP where all the labels are observed, each of our dataset only covers a subset of attributes. Let o signify the unobserved labels. The posterior probability of Eqn. <ref type="bibr" target="#b6">(7)</ref> becomes p(y \ o , o|x, z), where y \ o specifies the labels y excluding o. Here we demonstrate that o can be simply marginalized out, since the labels are independent. We</p><formula xml:id="formula_16">have o p(y \ o , o|x, z) = p(y \ o |x, z) · o1 p( o 1 |x, z) · o2 p( o 2 |x, z) · ... · oj p( o j |x, z) = p(y \ o |x, z).</formula><p>Therefore, the error e (L) of Eqn. <ref type="formula" target="#formula_13">(7)</ref> can be computed as</p><formula xml:id="formula_17">e (L) = y − y, if y ∈ y \ o , 0, otherwise,<label>(9)</label></formula><p>which demonstrates that the errors of the missing labels will not be propagated no matter whether their predictions are correct or not. Learning Importance Coefficients We update the importance coefficients with the network parameters fixed, by minimizing the posterior probability p(λ|x, y) = as introduced in <ref type="bibr" target="#b5">[6]</ref>. Taking the negative logarithm of the posterior, the problem develops into arg min λ − log p(x, y|λ) − log p(λ) + log p(x, y), <ref type="bibr" target="#b9">(10)</ref> where the first term, log p(x, y|λ), is a log likelihood similar to Eqn. <ref type="bibr" target="#b6">(7)</ref>, measuring the evidence of selecting importance coefficients λ. The second term specifies a log prior of λ. To avoid trivial solution, i.e. exists λ i ∈ λ equals zero, we have log p(λ) = i=1 − 1 σ 2 λ i − 1 2 2 , showing that each coefficient is regularized by a Gaussian prior with mean '1' and standard deviation σ. This implies that each λ i ∈ λ should not deviate too much from one, because we assume all tasks have equal contributions at the very beginning. Let λ 1 be the coefficient of the main task. We fix λ 1 = 1 through out the learning procedure, as our goal is to optimize the main task with the help of the auxiliary tasks. The third term is a normalization constant, which can be simply modeled as a constant scalar. In this work, we adopted the restricted Boltzmann machine (RBM) <ref type="bibr" target="#b14">[15]</ref> to learn p(x, y), because RBM can well model the data space. In other words, we can measure the predictions of the coefficients with respect to the importance of each sample. Note that RBM can be learned off-line and p(x, y) can be stored in a probability table for fast indexing.</p><p>Intuitively, coefficient learning is similar to the process below. At the very beginning, all the tasks have equal importance. In the training stage, for those tasks whose values of the loss function are stable but large, we decrease their weights, because they may not relate to the main task or begin to over-fit the data. However, we penalize the coefficient that is approaching zero, preventing the corresponding task from suspension. For those tasks have small values of loss, their weights could be increased, since these tasks are highly related to the main task, i.e. whose error rates are synchronously decreased with the main task. In practice, all the tasks' coefficients in our experiments become 0.1 ∼ 0.2 when training converges, except the main task whose weight is fixed and equals one. Learning of TA-CNN is summarized in Algorithm 1. Typically, we run the first step for sufficient number of iterations to reach a local minima, and then perform the second step to update the coefficients. This strategy can help avoid getting stuck at local minima.</p><p>Here, we explain the third term in details. With the RBM, we have</p><formula xml:id="formula_18">log p(x, y) = log h exp − E(x, y, h) ,</formula><p>which represents the free energy <ref type="bibr" target="#b14">[15]</ref> of RBM. Specifically,</p><formula xml:id="formula_19">E(x, y, h) = −x T W xh h − x T b x − y T W yh h − y T b y − h T b h</formula><p>is the energy function, which learns the latent binary representation h that models the shared hidden space of x and y. W xh and W yh are the projection matrixes capturing the relations between x and h, and y and h, respectively, while b x , b y , and b h are the biases. The RBM can be solved by the contrastive divergence <ref type="bibr" target="#b14">[15]</ref>. Since the latent variables h are independent given x and y, log p(x, y) can be rewritten by integrating over h, i.e. log p(x, y) =</p><formula xml:id="formula_20">i log 1 + exp(b h i + x T W xh ·i + y T W yh ·i ) + x T b x + y T b y .</formula><p>Combining all the above definitions, Eqn.(10) is an unconstrained optimization problem, where the importance coefficients can be efficiently updated by using the L-BFGS algorithm <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed TA-CNN is evaluated on the Caltech-Test <ref type="bibr" target="#b10">[11]</ref> and ETH datasets <ref type="bibr" target="#b11">[12]</ref>. We strictly follow the evaluation protocol proposed in <ref type="bibr" target="#b10">[11]</ref>, which measures the log average miss rate over nine points ranging from 10 −2 to 10 0 False-Positive-Per-Image. We compare TA-CNN with the best-performing methods as suggested by the Caltech and ETH benchmarks 4 on the reasonable subsets, where pedestrians are larger than 49 pixels height and have 65 percent visible body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effectiveness of TA-CNN</head><p>We systematically study the effectiveness of TA-CNN in four aspects as follows. In this section, TA-CNN is trained on Caltech-Train and tested on Caltech-Test.</p><p>Effectiveness of Hard Negative Mining To save computational cost, We employ ACF <ref type="bibr" target="#b8">[9]</ref>    at the testing stage. Two main adjustments are made in ACF. First, we compute the exact feature pyramids at each scale instead of making an estimated aggregation. Second, we increase the number of weak classifiers to enhance the recognition ability. Afterwards, a higher recall rate is achieved by ACF and it obtains 37.31 percent miss rate on Caltech-Test. Then the TA-CNN with only the main task (pedestrian classification) achieved 31.45 percent miss rate by cascading on ACF, obtaining more than 5 percent improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Pedestrian Attributes</head><p>We investigate how different pedestrian attributes can help improve the main task. To this end, we train TA-CNN by combing the main task with each of the pedestrian attributes, and the miss rates are reported in <ref type="table" target="#tab_4">Table 1</ref>, where shows that 'viewpoint' is the most effective attribute, which improves the miss rate by 3.25 percent, because 'viewpoint' captures the global information of pedestrian. The attribute capture the pose information also attains significant improvement, e.g. 2.62 percent by 'riding'. Interestingly, among those attributes modeling local information, 'hat' performs the best, reducing the miss rate by 2.56 percent. We observe that this result is consistent with previous works, SpatialPooling <ref type="bibr" target="#b25">[26]</ref> and InformedHaar <ref type="bibr" target="#b36">[37]</ref>, which showed that head is the most informative body parts for pedestrian detection. When combining all the pedestrian attributes, TA-CNN achieved 25.64 percent miss rate, improving the main task by 6 percent.</p><p>Effectiveness of Scene Attributes Similarly, we study how different scene attributes can improve pedestrian detection. We train TA-CNN combining the main task with each scene attribute. For each attribute, we select 5, 000 hard negative samples from its corresponding dataset. For  example, we crop five thousand patches for 'vertical' from the Stanford Background dataset. We test two settings, denoted as "Neg." and "Attr.". In the first setting, we label the five thousand patches as negative samples. In the second setting, these patches are assigned to their original attribute labels. The former one uses more negative samples compared to the TA-CNN (main task), whilst the latter one employs attribute information.</p><p>The results are reported in <ref type="table" target="#tab_5">Table 2</ref>, where shows that 'traffic-light' improves the main task by 2.53 percent, revealing that the patches of 'traffic-light' are easily confused with positives. This is consistent when we exam the hard negative samples of most of the pedestrian detectors. Besides, the 'vertical' background patches are more effective than the 'horizontal' background patches, corresponding to the fact that hard negative patches are more likely to present vertically.</p><p>Attribute Prediction We also consider the accuracy of attribute prediction and find that the averaged accuracy of all the attributes exceeds 75 percent. We select the pedestrian attribute 'viewpoint' as illustration. In <ref type="table" target="#tab_7">Table 3</ref>, we report the confusion matrix of 'viewpoint', where the number of detected pedestrians of 'front', ''back', ''left', and 'right' are 283, 276, 220, 156 respectively. We observed that 'front' and 'back' information are relatively easy to capture, rather than the 'left' and 'right', which are more likely to confuse with each other, e.g. 21 + 40 = 61 misclassified samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Performance on Caltech</head><p>We report overall results in two parts. All the results of TA-CNN are obtained by training on Caltech-Train and evaluating on Caltech-Test. In the first part, we analyze the performance of different components of TA-CNN. As shown in <ref type="figure" target="#fig_7">Fig.7a</ref>, the performances show clear increasing patterns when gradually adding more components. For example, TA-CNN (main task) cascades on ACF and reduces the miss rate of it by more than 5 percent. TA-CNN (PedAttr.+SharedScene) reduces the result of TA-CNN (PedAttr.) by 2.2 percent, because it can bridge the gaps among multiple scene datasets. After modeling the unshared attributes, the miss rate is further decreased by 1.5 percent, since more attribute information is incorporated.   The final result of 20.86 miss rate is obtained by using the structure projection vector as input to TA-CNN. Its effectiveness has been demonstrated in <ref type="figure" target="#fig_7">Fig.7a</ref>.</p><p>In the second part, we compare the result of TA-CNN with all existing best-performing methods, including VJ <ref type="bibr" target="#b31">[32]</ref>, HOG <ref type="bibr" target="#b7">[8]</ref>, ACF-Caltech <ref type="bibr" target="#b8">[9]</ref>, MT-DPM <ref type="bibr" target="#b34">[35]</ref>, MT-DPM+Context <ref type="bibr" target="#b34">[35]</ref>, JointDeep <ref type="bibr" target="#b22">[23]</ref>, SDN <ref type="bibr" target="#b17">[18]</ref>, ACF+SDT <ref type="bibr" target="#b26">[27]</ref>, InformedHaar <ref type="bibr" target="#b36">[37]</ref>, ACF-Caltech+ <ref type="bibr" target="#b19">[20]</ref>, SpatialPooling <ref type="bibr" target="#b25">[26]</ref>, LDCF <ref type="bibr" target="#b19">[20]</ref>, Katamari <ref type="bibr" target="#b3">[4]</ref>, SpatialPooling+ <ref type="bibr" target="#b24">[25]</ref>. These works used various features, classifiers, deep networks, and motion and context information. We summarize them as below. Note that TA-CNN dose not employ motion and context information.</p><p>Features: Haar (VJ), HOG (HOG, MT-DPM), Channel-Feature (ACF+Caltech, LDCF); Classifiers: latent-SVM (MT-DPM), boosting (VJ, ACF+Caltech, SpatialPooling);    <ref type="figure" target="#fig_7">Fig.7b</ref> reports the results. TA-CNN achieved the smallest miss rate compared to all existing methods. Although it only outperforms the second best method (SpatialPooling+ <ref type="bibr" target="#b24">[25]</ref>) by 1 percent, it learns 200 dimensions high-level features with attributes, other than combining LBP, covariance features, channel features, and video motion as in <ref type="bibr" target="#b24">[25]</ref>. Also, the Katamari <ref type="bibr" target="#b3">[4]</ref> method integrates multiple types of features and context information.</p><p>Hand-crafted Features The learned high-level representation of TA-CNN outperforms the conventional handcrafted features by a large margin, including Haar, HOG, HOG+LBP, and channel features, shown in <ref type="figure" target="#fig_9">Fig.8 (a)</ref>. For example, it reduced the miss rate by 16 and 9 percent compared to DPM+Context and Spatial Pooling, respectively. DPM+Context combined HOG feature with pose mixture and context information, while SpatialPooling combined multiple features, such as LBP, covariance, and channel features.</p><p>Deep Models <ref type="figure" target="#fig_9">Fig.8 (b)</ref> shows that TA-CNN surpasses other deep models. For example, TA-CNN outperforms two state-of-the-art deep models, JointDeep and SDN, by 18 and 17 percent, respectively. Both SDN and JointDeep treated pedestrian detection as a single task and thus cannot learn high-level representation to deal with the challenging hard negative samples.</p><p>Time Complexity Training TA-CNN on Caltech-Train with a single GPU takes 3 hours. At the testing stage, the running time of hard negative mining is 10 frames per second (FPS) on Matlab with CPU, while TA-CNN runs at 100 FPS on GPU. In summary, the entire system detects pedestrians from raw 640 × 480 images at around 5 FPS. The bottleneck is the step of hard negative mining. We consider to migrate it to GPU platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overall Performance on ETH</head><p>We compare TA-CNN with the existing best-performing methods (see Sec.4.2) on ETH <ref type="bibr" target="#b11">[12]</ref>. TA-CNN is trained   . This setting aims at evaluating the generalization capacity of the TA-CNN. As shown in <ref type="figure" target="#fig_11">Fig.9</ref>, TA-CNN achieves the lowest miss rate, which outperforms the second best method by 2.5 percent. It also outperforms the best deep model by 5.5 percent.</p><p>Effectiveness of different Components The analysis of the effectiveness of different components of TA-CNN is displayed in <ref type="figure" target="#fig_12">Fig.10</ref>, where the log-average miss rates show clear decreasing patterns as follows, while gradually accumulating more components.</p><p>• TA-CNN (main task) cascades on ACF and reduces the miss rate by 5.4 percent.</p><p>• With pedestrian attributes, TA-CNN (PedAttr.) reduces the result of TA-CNN (main task) by 5.5 percent.</p><p>• When bridging the gaps among multiple scene datasets with shared scene attributes, TA-CNN (Pe-dAttr.+SharedScene) further lower the miss rate by 1.8 percent.</p><p>• After incorporating unshared attributes, the miss rate is further decreased by another 1.2 percent.</p><p>• TA-CNN finally achieves 34.99 percent log-average miss rate with the structure projection vector.</p><p>Comparisons with Hand-crafted Features <ref type="figure">Fig.11</ref> shows that the learned representation of TA-CNN outperforms the conventional handcrafted features in a large margin, including Haar, HOG, HOG+LBP, and channel features. For instance, it reduces the miss rate by 9.8 and 8.5 percent compared to FisherBoost <ref type="bibr" target="#b29">[30]</ref> and Roerei <ref type="bibr" target="#b2">[3]</ref>, respectively. FisherBoost combined HOG and covariance features, and trained the detector in a complex model, while Roerei carefully designed the feature pooling, feature selection, and preprocessing methods based on channel features.</p><p>Comparisons with Deep Models <ref type="figure" target="#fig_0">Fig.12</ref> shows that TA-   <ref type="figure">Figure 11</ref>: Comparison with hand-crafted feature based models on ETH dataset CNN surpasses other deep models on ETH dataset. For example, TA-CNN outperforms other two best-performing deep models, SDN <ref type="bibr" target="#b17">[18]</ref> and DBN-Mul <ref type="bibr" target="#b23">[24]</ref>, by 5.5 and 6 percent, respectively. Besides, TA-CNN even reduces the miss rate by 12.7 compared to MultiSDP <ref type="bibr" target="#b35">[36]</ref>, which carefully designed multiple classification stages to recognize hard negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of Detection Results</head><p>We visualize the results of TA-CNN and compare with HOG <ref type="bibr" target="#b7">[8]</ref>, ACF <ref type="bibr" target="#b8">[9]</ref>, and JointDeep <ref type="bibr" target="#b22">[23]</ref>. <ref type="figure" target="#fig_15">Fig.13</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel Task-Assistant CNN (TA-CNN) to learn features from multiple tasks (pedestrian and scene attributes) and datasets, showing superiority over hand-crafted features and features learned by other deep models. This is because high-level representation can be learned by employing sematic tasks and multiple data sources. Extensive experiments demonstrate its effectiveness. The proposed model can be further improved by incorporating more attributes. Future work will explore more attribute configurations. The proposed approach also has potential for scene parsing, because it predicts background attributes.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparisons between different schemes of pedestrian detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Attribute summarization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The proposed pipeline for pedestrian detection (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The computation of the structural projection vector (SPV). loss function develops into E = − N n=1 log p(o s n , o u n |x n ), such that TA-CNN can learn a shared representation across B's because the samples share common targets o s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>TA-CNN (main task) 25.64% TA-CNN (PedAttr.) 23.45% TA-CNN (PedAttr.+SharedScene) 21.94% TA-CNN (PedAttr.+AllScene) 20.86% TA-CNN (PedAttr.+AllScene+SPV) (a) Log-average miss rate reduction procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Results under standard evaluation settings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Results on Caltech-Test: (a) comparison with hand-crafted feature based models; (b) comparison with other deep models Deep Models: JointDeep, SDN; Motion and context: MT-DPM+Context, ACF+SDT, Katamari, SpatialPooling+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Results on ETHon INRIA-Train<ref type="bibr" target="#b7">[8]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>TA-CNN (main task) 39.18% TA-CNN (PedAttr.) 37.32% TA-CNN (PedAttr.+SharedScene) 36.11% TA-CNN (PedAttr.+AllScene) 34.99% TA-CNN (PedAttr.+AllScene+SPV) Log-average miss rate reduction procedure on ETH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>and Fig.14 show the detection examples on Caltech reasonable subset, while Fig.15 shows samples on ETH reasonable subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Comparison with other deep models on ETH dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Detection examples of a series of continuous crossroad scenes on reasonable subset of Caltech-Test (only consider pedestrians that are larger than 49 pixels in height and that have 65 percent visible body parts). Green and red bounding boxes represent true positives and false positives, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Detection examples on reasonable subset of Caltech-Test (only consider pedestrians that are larger than 49 pixels in height and that have 65 percent visible body parts). Green and red bounding boxes represent true positives and false positives, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Detection examples on reasonable subset of ETH (only consider pedestrians that are larger than 49 pixels in height and that have 65 percent visible body parts). Green and red bounding boxes represent true positives and false positives, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Here, y = (y, o p , o s , o u ) is a vector of binary labels, concatenating the pedestrian label and all attribute labels. Note that each two-state (four-state) variable can be described by one bit (two bits). Since we have seventeen two-state variables and one four-state variable, the weight matrix at the top layer, denoted as W y in this case, has 17 × H + 2 × H = 19H parameters, which reduces the number of parameters by half, i.e. 19H compared to 38H of Eqn.<ref type="bibr" target="#b5">(6)</ref>. Moreover, p(y|x, z) is modeled by sigmoid function, i.e. p(y|x, z) =</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Data: Training set D = {(x n , y n )} N n=1 ; Result: Network parameters W and importance coefficients λ;</figDesc><table><row><cell>Train RBM of p(x, y), and calculate and store the</cell></row><row><cell>probability table of p(x, y);</cell></row><row><cell>while not stopping criterion do</cell></row><row><cell>1. update W with λ fixed: repeat the below</cell></row><row><cell>process until a local minima is reached,</cell></row><row><cell>for a minibatch of x n do</cell></row><row><cell>forward propagation by using Eqn.(1), (2), and</cell></row><row><cell>(3);</cell></row><row><cell>backward propagation to update network</cell></row><row><cell>filters and weights by BP;</cell></row><row><cell>end</cell></row><row><cell>2. update λ with W fixed by solving Eqn.(10);</cell></row><row><cell>end</cell></row><row><cell>Algorithm 1: Learning TA-CNN</cell></row><row><cell>p(x,y|λ)p(λ)</cell></row><row><cell>p(x,y)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>All 31.45 30.44 29.83 28.89 30.77 30.70 29.36 28.83 30.22 28.20 25.64</figDesc><table><row><cell>main task</cell><cell>backpack</cell><cell>dark-trousers</cell><cell>hat</cell><cell>bag</cell><cell>gender</cell><cell>occlusion</cell><cell>riding</cell><cell>white-cloth</cell><cell>viewpoint</cell></row><row><cell>for mining hard nega-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tives at the training stage and pruning candidate windows</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 http://www.vision.caltech.edu/Image_</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets/CaltechPedestrians/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Log-average miss rate (%) on Caltech-Test with pedestrian attribute learning tasks.</figDesc><table><row><cell>main task</cell><cell>sky</cell><cell>tree</cell><cell>building</cell><cell>road</cell><cell>vehicle</cell><cell>traffic-light</cell><cell>vertical</cell><cell>horizontal</cell></row><row><cell>Neg. 31.45 Attr.</cell><cell cols="8">31.07 30.92 31.16 31.02 30.75 30.85 30.91 30.96 30.79 30.50 30.90 30.54 29.41 28.92 30.03 30.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Log-average miss rate (%) on Caltech-Test with scene attribute learning tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>View-point estimation results on Caltech-Test.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, scalar variable is denoted by normal letter, while set, vector, or matrix is denoted as boldface letter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We drop the sample index n in the remaining derivation for clarity.<ref type="bibr" target="#b2">3</ref> All tasks are binary classification (i.e. two states) except the pedestrian attribute 'viewpoint', which has four states, including 'front', 'back', 'left', and 'right'.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable training of l 1-regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2903" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection evolution with multi-order contextual co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1798" to="1805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 2005. 1, 8</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shape-based human detection and segmentation via hierarchical part-template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="618" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">People analysis cctv investigator handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nortcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Home Office Centre of Applied Science and Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling mutual visibility relationship in pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="546" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2882" to="2889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 2013. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training effective node classifiers for cascade classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust multiresolution pedestrian detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3033" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Informed haarlike features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>ECCV. 2014. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning a hierarchical deformable template for rapid deformable object parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1029" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
