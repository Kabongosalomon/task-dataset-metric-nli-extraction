<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncen</forename><surname>Li</surname></persName>
							<email>juncenli@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
							<email>robinjia@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
							<email>hehe@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing "screen is just the right size" to "screen is too small"). Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., "too small"). Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b18">Shen et al., 2017;</ref><ref type="bibr" target="#b3">Fu et al., 2018)</ref><p>. In each of these cases, the goal is to convert a sentence with one attribute (e.g., negative sentiment) to one with a different attribute (e.g., positive sentiment), while preserving all attribute-independent content 1 (e.g., what properties of a restaurant are being discussed). Typically, aligned sentences with the same content but different attributes are not available; systems must learn to disentangle attributes and content given only unaligned sen-tences labeled with attributes. Previous work has attempted to use adversarial networks <ref type="bibr" target="#b18">(Shen et al., 2017;</ref><ref type="bibr" target="#b3">Fu et al., 2018)</ref> for this task, but-as we demonstrate-their outputs tend to be low-quality, as judged by human raters. These models are also difficult to train <ref type="bibr" target="#b16">(Salimans et al., 2016;</ref><ref type="bibr" target="#b0">Arjovsky and Bottou, 2017;</ref><ref type="bibr" target="#b2">Bousmalis et al., 2017)</ref>.</p><p>In this work, we propose a set of simpler, easierto-train systems that leverage an important observation: attribute transfer can often be accomplished by changing a few attribute markerswords or phrases in the sentence that are indicative of a particular attribute-while leaving the rest of the sentence largely unchanged. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example in which the sentiment of a sentence can be altered by changing a few sentiment-specific phrases but keeping other words fixed.</p><p>With this intuition, we first propose a simple baseline that already outperforms prior adversarial approaches. Consider a sentiment transfer (negative to positive) task. First, from unaligned corpora of positive and negative sentences, we identify attribute markers by finding phrases that occur much more often within sentences of one attribute than the other (e.g., "worst" and "very disppointed" are negative markers). Second, given a sentence, we delete any negative markers in it, and regard the remaining words as its content. Third, we retrieve a sentence with similar content from the positive corpus.</p><p>We further improve upon this baseline by incorporating a neural generative model, as shown in Figure 1. Our neural system extracts content words in the same way as our baseline, then generates the final output with an RNN decoder that conditions on the extracted content and the target attribute. This approach has significant benefits at training time, compared to adversarial networks: having already separated content and attribute, we simply train our neural model to reconstruct sentences in the training data as an auto-encoder.</p><p>We test our methods on three text attribute transfer datasets: altering sentiment of Yelp reviews, altering sentiment of Amazon reviews, and altering image captions to be more romantic or humorous. Averaged across these three datasets, our simple baseline generated grammatical sentences with appropriate content and attribute 23% of the time, according to human raters; in contrast, the best adversarial method achieved only 12%. Our best neural system in turn outperformed our baseline, achieving an average success rate of 34%. Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at https://github.com/lijuncen/ Sentiment-and-Style-Transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>We assume access to a corpus of labeled sen-</p><formula xml:id="formula_0">tences D = {(x 1 , v 1 ), . . . , (x m , v m )}, where</formula><p>x i is a sentence and v i ∈ V, the set of possible attributes (e.g., for sentiment, V = {"positive", "negative"}). We define D v = {x : (x, v) ∈ D}, the set of sentences in the corpus with attribute v. Crucially, we do not assume access to a parallel corpus that pairs sentences with different attributes and the same content.</p><p>Our goal is to learn a model that takes as input (x, v tgt ) where x is a sentence exhibiting source (original) attribute v src , and v tgt is the target attribute, and outputs a sentence y that retains the content of x while exhibiting v tgt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>As a motivating example, suppose we wanted to change the sentiment of "The chicken was delicious." from positive to negative. Here the word "delicious" is the only sentiment-bearing word, so we just need to replace it with an appropriate negative sentiment word. More generally, we find that the attribute is often localized to a small fraction of the words, an inductive bias not captured by previous work.</p><p>How do we know which negative sentiment word to insert? The key observation is that the remaining content words provide strong cues: given "The chicken was . . . ", one can infer that a tasterelated word like "bland" fits, but a word like "rude" does not, even though both have negative sentiment. In other words, while the deleted sentiment words do contain non-sentiment information too, this information can often be recovered using the other content words.</p><p>In the rest of this section, we describe our four systems: two baselines (RETRIEVEONLY and TEMPLATEBASED) and two neural models (DELETEONLY and DELETEANDRETRIEVE). An overview of all four systems is shown in <ref type="figure">Figure</ref>  : i have had this mount for about a year and it .</p><p>: i have had it for a while but barely used it .</p><p>: barely used</p><p>(1) Delete attribute markers <ref type="figure">Figure 2</ref>: Our four proposed methods on the same sentence, taken from the AMAZON dataset. Every method uses the same procedure (1) to separate attribute and content by deleting attribute markers; they differ in the construction of the target sentence. RETRIEVEONLY directly returns the sentence retrieved in (2). TEMPLATEBASED combines the content with the target attribute markers in the retrieved sentence by slot filling. DELETEANDRETRIEVE generates the output from the content and the retrieved target attribute markers with an RNN. DELETEONLY generates the output from the content and the target attribute with an RNN.</p><p>1. Delete: All 4 systems use the same procedure to separate the words in x into a set of attribute markers a(x, v src ) and a sequence of content words c(x, v src ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Retrieve: 3 of the 4 systems look through the corpus and retrieve a sentence x tgt that has the target attribute v tgt and whose content is similar to that of x.</p><p>3. Generate: Given the content c(x, v src ), target attribute v tgt , and (optionally) the retrieved sentence x tgt , each system generates y, either in a rule-based fashion or with a neural sequence-to-sequence model.</p><p>We describe each component in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Delete</head><p>We propose a simple method to delete attribute markers (n-grams) that have the most discriminative power. Formally, for any v ∈ V, we define the salience of an n-gram u with respect to v by its (smoothed) relative frequency in D v :</p><formula xml:id="formula_1">s(u, v) = count(u, D v ) + λ v ∈V,v =v count(u, D v ) + λ ,<label>(1)</label></formula><p>where count(u, D v ) denotes the number of times an n-gram u appears in D v , and λ is the smoothing parameter. We declare u to be an attribute marker for v if s(u, v) is larger than a specified threshold γ. The attributed markers can be viewed as discriminative features for a Naive Bayes classifier.</p><p>We define a(x, v src ) to be the set of all source attribute markers in x, and define c(x, v src ) as the sequence of words after deleting all markers in a(x, v src ) from x. For example, for "The chicken was delicious," we would delete "delicious" and consider "The chicken was. . . " to be the content <ref type="figure">(Figure 2,</ref> Step 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieve</head><p>To decide what words to insert into c(x, v src ), one useful strategy is to look at similar sentences with the target attribute. For example, negative sentences that use phrases similar to "The chicken was. . . " are more likely to contain "bland" than "rude." Therefore, we retrieve sentences of similar content and use target attribute markers in them for insertion.</p><p>Formally, we retrieve x tgt according to:</p><formula xml:id="formula_2">x tgt = argmin x ∈D v tgt d(c(x, v src ), c(x , v tgt )),<label>(2)</label></formula><p>where d may be any distance metric comparing two sequences of words. We experiment with two options: (i) TF-IDF weighted word overlap and (ii) Euclidean distance using the content embeddings in Section 3.3 <ref type="figure">(Figure 2,</ref> Step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generate</head><p>Finally, we describe how each system generates y <ref type="figure">(Figure 2,</ref> Step 3). RETRIEVEONLY returns the retrieved sentence x tgt verbatim. This is guaranteed to produce a grammatical sentence with the target attribute, but its content might not be similar to x.</p><p>TEMPLATEBASED replaces the attribute markers deleted from the source sentence a(x, v src ) with those of the target sentence a(x tgt , v tgt ). 2 This strategy relies on the assumption that if two attribute markers appear in similar contexts , they are roughly syntactically exchangeable. For example, "love" and "don't like" appear in similar contexts (e.g., "i love this place." and "i don't like this place."), and exchanging them is syntactically valid. However, this naive swapping of attribute markers can result in ungrammatical outputs.</p><p>DELETEONLY first embeds the content c(x, v src ) into a vector using an RNN. It then concatenates the final hidden state with a learned embedding for v tgt , and feeds this into an RNN decoder to generate y. The decoder attempts to produce words indicative of the source content and target attribute, while remaining fluent.</p><p>DELETEANDRETRIEVE is similar to DELE-TEONLY, but uses the attribute markers of the retrieved sentence x tgt rather than the target attribute v tgt . Like DELETEONLY, it encodes c(x, v src ) with an RNN. It then encodes the sequence of attribute markers a(x tgt , v tgt ) with another RNN. The RNN decoder uses the concatenation of this vector and the content embedding to generate y.</p><p>DELETEANDRETRIEVE combines the advantages of TEMPLATEBASED and DELE-TEONLY. Unlike TEMPLATEBASED, DELETE-ANDRETRIEVE can pick a better place to insert the given attribute markers, and can add or remove function words to ensure grammaticality. Compared to DELETEONLY, DELETEANDRETRIEVE has a stronger inductive bias towards using target attribute markers that are likely to fit in the current context. <ref type="bibr" target="#b7">Guu et al. (2018)</ref> showed that retrieval strategies like ours can help neural generative models. Finally, DELETEANDRETRIEVE gives us finer control over the output; for example, we can control the degree of sentiment by deciding whether to add "good" or "fantastic" based on the retrieved sentence x tgt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>We now describe how to train DELETEAN-DRETRIEVE and DELETEONLY. Recall that at training time, we do not have access to ground truth outputs that express the target attribute. Instead, we train DELETEONLY to reconstruct the sentences in the training corpus given their content and original attribute value by maximizing:</p><formula xml:id="formula_3">L(θ) = (x,v src )∈D log p(x | c(x, v src ), v src ); θ).</formula><p>(3) For DELETEANDRETRIEVE, we could similarly learn an auto-encoder that reconstructs x from c(x, v src ) and a(x, v src ). However, this results in a trivial solution: because a(x, v src ) and c(x, v src ) were known to come from the same sentence, the model merely learns to stitch the two sequences together without any smoothing. Such a model would fare poorly at test time, when we may need to alter some words to fluently combine a(x tgt , v tgt ) with c(x, v src ). To address this train/test mismatch, we adopt a denoising method similar to the denoising auto-encoder <ref type="bibr" target="#b19">(Vincent et al., 2008)</ref>. During training, we apply some noise to a(x, v src ) by randomly altering each attribute marker in it independently with probability 0.1. Specifically, we replace an attribute marker with another randomly selected attribute marker of the same attribute and word-level edit distance 1 if such a noising marker exists, e.g., "was very rude" to "very rude", which produces a (x, v src ).</p><p>Therefore, the training objective for DELETE-ANDRETRIEVE is to maximize:</p><formula xml:id="formula_4">L(θ) = (x,v src )∈D log p(x | c(x, v src ), a (x, v src ); θ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated our approach on three domains: flipping sentiment of Yelp reviews (YELP) and Amazon reviews (AMAZON), and changing image captions to be romantic or humorous (CAPTIONS). We compared our four systems to human references and three previously published adversarial approaches. As judged by human raters, both of our two baselines outperform all three adversarial methods. Moreover, DELETEANDRETRIEVE outperforms all other automatic approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>First, we describe the three datasets we use, which are commonly used in prior works too. All datasets are randomly split into train, development, and test sets <ref type="table" target="#tab_3">(Table 1)</ref>.</p><p>YELP Each example is a sentence from a business review on Yelp, and is labeled as having either positive or negative sentiment.</p><p>AMAZON Similar to YELP, each example is a sentence from a product review on Amazon, and is labeled as having either positive or negative sentiment <ref type="bibr" target="#b8">(He and McAuley, 2016)</ref>.</p><p>CAPTIONS In the CAPTIONS dataset <ref type="bibr" target="#b4">(Gan et al., 2017)</ref>, each example is a sentence that describes an image, and is labeled as either factual, romantic, or humorous. We focus on the task of converting factual sentences into romantic and humorous ones. Unlike YELP and AMAZON, CAP-TIONS is actually an aligned corpus-it contains captions for the same image in different styles. Our systems do not use these alignments, but we use them as gold references for evaluation. CAPTIONS is also unique in that we reconstruct romantic and humorous sentences during training, whereas at test time we are given factual captions. We assume these factual captions carry only content, and therefore do not look for and delete factual attribute markers; The model essentially only inserts romantic or humorous attribute markers as appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human References</head><p>To supply human reference outputs to which we could compare the system outputs for YELP and AMAZON, we hired crowdworkers on Amazon Mechanical Turk to write gold outputs for all test sentences. Workers were instructed to edit a sentence to flip its sentiment while preserving its content.</p><p>Our delete-retrieve-generate approach relies on the prior knowledge that to accomplish attribute transfer, a small number of attribute markers should be changed, and most other words should be kept the same. We analyzed our human reference data to understand the extent to which humans follow this pattern. We measured whether humans preserved words our system marks as content, and changed words our system marks as attribute-related (Section 3.1). We define the content word preservation rate S c as the average fraction of words our system marks as content that were preserved by humans, and the attributerelated word change rate S a as the average fraction of words our system marks as attribute-related that were changed by humans:</p><formula xml:id="formula_5">S c = 1 |D test | (x,v src ,y * )∈Dtest |c(x, v src ) ∩ y * | |c(x, v src )| S a = 1 − 1 |D test | (x,v src ,y * )∈Dtest |a(x, v src ) ∩ y * | |a(x, v src )| ,<label>(5)</label></formula><p>where D test is the test set, y * is the human reference sentence, and | · | denotes the number of nonstopwords. Higher values of S c and S a indicate that humans preserve content words and change attribute-related words, in line with the inductive bias of our model. S c is 0.61, 0.71, and 0.50 on YELP, AMAZON, and CAPTIONS, respectively; S a is 0.72 on YELP and 0.54 on AMAZON (not applicable on CAPTIONS).</p><p>To understand why humans sometimes deviated from the inductive bias of our model, we randomly sampled 50 cases from YELP where humans changed a content word or preserved an attribute-related word. 70% of changed content words were unimportant words (e.g., "whole" was deleted from "whole experience"), and another 18% were paraphrases (e.g., "charge" became "price"); the remaining 12% were errors where the system mislabeled an attribute-related word as a content word (e.g., "old" became "new"). 84% of preserved attribute-related words did pertain to sentiment but remained fixed due to changes in the surrounding context (e.g., "don't like" became "like", and "below average" became "above average"); the remaining 16% were mistagged by our system as being attribute-related (e.g., "walked out").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Previous Methods</head><p>We compare with three previous models, all of which use adversarial training. STYLEEMBED-  Human evaluation results on all three datasets. We show average human ratings for grammaticality (Gra), content preservation (Con), and target attribute match (Att) on a 1 to 5 Likert scale, as well as overall success rate (Suc). On all three datasets, DELETEANDRETRIEVE is the best overall system, and all four of our methods outperform previous work.</p><p>DING <ref type="bibr" target="#b3">(Fu et al., 2018)</ref> learns an vector encoding of the source sentence such that a decoder can use it to reconstruct the sentence, but a discriminator, which tries to identify the source attribute using this encoding, fails. They use a basic MLP discriminator and an LSTM decoder. MULTIDE-CODER <ref type="bibr" target="#b3">(Fu et al., 2018)</ref> is similar to STYLEEM-BEDDING, except that it uses a different decoder for each attribute value. CROSSALIGNED <ref type="bibr" target="#b18">(Shen et al., 2017)</ref> also encodes the source sentence into a vector, but the discriminator looks at the hidden states of the RNN decoder instead. The system is trained so that the discriminator cannot distinguish these hidden states from those obtained by forcing the decoder to output real sentences from the target domain; this objective encourages the real and generated target sentences to look similar at a population level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Details</head><p>For our methods, we use 128-dimensional word vectors and a single-layer GRU with 512 hidden units for both encoders and the decoder. We use the maxout activation function <ref type="bibr" target="#b6">(Goodfellow et al., 2013)</ref>. All parameters are initialized by sampling from a uniform distribution between −0.1 and 0.1. For optimization, we use Adadelta <ref type="bibr" target="#b21">(Zeiler, 2012)</ref> with a minibatch size of 256. For attribute marker extraction, we consider spans up to 4 words, and the smoothing parameter λ is set to 1. We set the attribute marker threshold γ, which controls the precision and recall of our attribute markers, to 15, 5.5 and 5 for YELP, AMAZON, and CAPTIONS. These values were set by manual inspection of the resulting markers and tuning slightly on the dev set. For retrieval, we used the TF-IDF weighted word overlap score for DELETEANDRETRIEVE and TEMPLATEBASED, and the Euclidean distance of content embeddings for RETRIEVEONLY. We find the two scoring functions give similar results.</p><p>For all neural models, we do beam search with a beam size of 10. For DELETEANDRETRIEVE, similar to <ref type="bibr" target="#b7">Guu et al. (2018)</ref>, we retrieve the top-10 sentences and generate results using markers from each sentence. We then select the output with the lowest perplexity given by a separately-trained neural language model on the target-domain training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation</head><p>We hired workers on Amazon Mechanical Turk to rate the outputs of all systems. For each source sentence and target attribute, the same worker was shown the output of each tested system. Workers were asked to rate each output on three criteria on a Likert scale from 1 to 5: grammaticality, similarity to the target attribute, and preservation of the source content. Finally, we consider a generated output "successful" if it is rated 4 or 5 on all three criteria. For each dataset, we evaluated 400 randomly sampled examples (200 for each target attribute). <ref type="table" target="#tab_5">Table 2</ref> shows the human evaluation results. On all three datasets, both of our baselines have a higher success rate than the previously published models, and DELETEANDRETRIEVE achieves the best performance among all systems. Additionally, we see that human raters strongly preferred the human references to all systems, suggesting there is still significant room for improvement on this task.</p><p>We find that a human evaluator's judgment of a sentence is largely relative to other sentences being evaluated together and examples given in the instruction (different for each dataset/task). There-fore, evaluating all system outputs in one batch is important and results on different datasets are not directly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis</head><p>We analyze the strengths and weaknesses of the different systems. <ref type="table" target="#tab_6">Table 3</ref> show typical outputs of each system on the YELP and CAPTIONS dataset.</p><p>We first analyze the adversarial methods. CROSSALIGNED and MULTIDECODER tend to lose the content of the source sentence, as seen in both the example outputs and the overall human ratings. The decoder tends to generate a frequent but only weakly related sentence with the target attribute. On the other hand, STYLEEM-BEDDING almost always generates a paraphrase of the input sentence, implying that the encoder preserves some attribute information. We conclude that there is a delicate balance between preserving the original content and dropping the original attribute, and existing adversarial models tend to sacrifice one or the other.</p><p>Next, we analyze our baselines. RE-TRIEVEONLY scores well on grammaticality and having the target attribute, since it retrieves sentences with the desired attribute directly from the corpus. However, it is likely to change the content when there is no perfectly aligned sentence in the target domain. In contrast, TEMPLATE-BASED is good at preserving the content because the content words are guaranteed to be kept. However, it makes grammatical mistakes due to the unsmoothed combination of content and attribute words.</p><p>DELETEANDRETRIEVE and DELETEONLY achieve a good balance among grammaticality, preserving content, and changing the attribute. Both have strong inductive bias on what words should be changed, but still have the flexibility to smooth out the sentence. The main difference is that DELETEONLY fills in attribute words based on only the target attribute, whereas DELETE-ANDRETRIEVE conditions on retrieved attribute words. When there is a diverse set of phrases to fill in-for example in CAPTIONS-conditioning on retrieved attribute words helps generate longer sentences with more specific attribute descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Automatic Evaluation</head><p>Following previous work <ref type="bibr" target="#b9">(Hu et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017)</ref>, we also compute automatic evalua-tion metrics, and compare these numbers to our human evaluation results.</p><p>We use an attribute classifier to assess whether outputs have the desired attribute <ref type="bibr" target="#b9">(Hu et al., 2017;</ref><ref type="bibr" target="#b18">Shen et al., 2017)</ref>. We define the classifier score as the fraction of outputs classified as having the target attribute. For each dataset, we train an attribute classifier on the same training data. Specifically, we encode the sentence into a vector by a bidirectional LSTM with an average pooling layer over the outputs, and train the classifier by minimizing the logistic loss.</p><p>We also compute BLEU between the output and the human references, similar to <ref type="bibr" target="#b4">Gan et al. (2017)</ref>. A high BLEU score primarily indicates that the system can correctly preserve content by retaining the same words from the source sentence as the reference. One might also hope that it has some correlation with fluency, though we expect this correlation to be much weaker. <ref type="table" target="#tab_7">Table 4</ref> shows the classifier and BLEU scores. In <ref type="table" target="#tab_8">Table 5</ref>, we compute the system-level correlation between classifier score and human judgments of attribute transfer, and between BLEU and human judgments of content preservation and grammaticality. We also plot scores given by the automatic metrics and humans in <ref type="figure">Figure 4</ref>. While the scores are sometimes well-correlated, the results vary significantly between datasets; on AMAZON, there is no correlation between the classifier score and the human evaluation. Manual inspection shows that on AMAZON, some product genres are associated with either mostly positive or mostly negative reviews. However, our systems produce, for example, negative reviews about products that are mostly discussed positively in the training set. Therefore, the classifier often gives unreliable predictions on system outputs. As expected, BLEU does not correlate well with human grammaticality ratings. The lack of automatic fluency evaluation artificially favors systems like TEMPLATEBASED, which make more grammatical mistakes. We conclude that while these automatic evaluation methods are useful for model development, they cannot replace human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Trading off Content versus Attribute</head><p>One advantage of our methods is that we can control the trade-off between matching the target attribute and preserving the source content. To achieve different points along this trade-off curve, From negative to positive (YELP) SOURCE we sit down and we got some really slow and lazy service . CROSSALIGNED</p><p>we went down and we were a good , friendly food . STYLEEMBEDDING we sit down and we got some really slow and prices suck . MULTIDECODER we sit down and we got some really and fast food . TEMPLATEBASED we sit down and we got some the service is always great and even better service . RETRIEVEONLY i got a veggie hoagie that was massive and some grade a customer service . DELETEONLY we sit down and we got some great and quick service . DELETEANDRETRIEVE we got very nice place to sit down and we got some service .</p><p>From factual to romantic (CAPTIONS) SOURCE two dogs play by a tree . CROSSALIGNED a dog is running through the grass . STYLEEMBEDDING two dogs play against a tree . MULTIDECODER two dogs play by a tree . TEMPLATEBASED two dogs play by a tree loving . RETRIEVEONLY two dogs are playing in a pool as best friends . DELETEANDRETRIEVE two dogs play by a tree , enjoying the happiness of childhood . DELETEONLY two dogs in love play happily by a tree . From negative to positive (AMAZON) SOURCE this is the worst game i have come across in a long time . CROSSALIGNED this is the best thing i ve had for a few years . STYLEEMBEDDING this is the worst game i have come across in a long time . MULTIDECODER this is the best knife i have no room with a long time . TEMPLATEBASED this is the best come across in a long time . RETRIEVEONLY the customer support is some of the best i have come across in a long time . DELETEONLY this is the best game i have come across in a long time . DELETEANDRETRIEVE this is the best game i have come across in a long time .  we simply vary the threshold γ (Section 3.1) at test time to control how many attribute markers we delete from the source sentence. In contrast, other methods <ref type="bibr" target="#b18">(Shen et al., 2017;</ref><ref type="bibr" target="#b3">Fu et al., 2018)</ref> would require retraining the model with different hyperparameters to achieve this effect. <ref type="figure" target="#fig_1">Figure 3</ref> shows this trade-off curve for DELETEANDRETRIEVE, DELETEONLY, and TEMPLATEBASED on YELP, where target attribute match is measured by the classifier score and content preservation is measured by BLEU. 3 3 RETRIEVEONLY is less affected by what content words are preserved, especially when no good output sentence exists in the target corpus. Therefore, we found that it did not exhibit a clear content-attribute trade-off.</p><p>We see a clear trade-off between changing the attribute and retaining the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work and Discussion</head><p>Our work is closely related to the recent body of work on text attribute transfer with unaligned data, where the key challenge to disentangle attribute and content in an unsupervised way. Most existing work <ref type="bibr" target="#b18">(Shen et al., 2017;</ref><ref type="bibr" target="#b3">Fu et al., 2018;</ref><ref type="bibr" target="#b12">Melnyk et al., 2017)</ref> uses adversarial training to separate attribute and content: the content encoder aims to fool the attribute discriminator by removing attribute information from the content embedding. However, we find that empirically it is often easy to fool the discriminator without ac-Classifier BLEU Attribute Content Grammaticality All data 0.810 (p &lt; 0.01) 0.876 (p &lt; 0.01) −0.127 (p = 0.58) YELP 0.991 (p &lt; 0.01) 0.935 (p &lt; 0.01) 0.119 (p = 0.80) CAPTIONS 0.982 (p &lt; 0.01) 0.991 (p &lt; 0.01) −0.631 (p = 0.13) AMAZON −0.036 (p = 0.94) 0.857 (p &lt; 0.01) 0.306 (p = 0.50)  tually removing the attribute information. Therefore, we explicitly separate attribute and content by taking advantage of the prior knowledge that the attribute is localized to parts of the sentence.</p><p>To address the problem of unaligned data, <ref type="bibr" target="#b9">Hu et al. (2017)</ref> relies on an attribute classifier to guide the generator to produce sentences with a desired attribute (e.g. sentiment, tense) in the Variational Autoencoder (VAE) framework. Similarly,  used a regularized autoencoder in the adversarial training framework; however, they also find that these models require extensive hyperparameter tuning and the content tends to be changed during the transfer. <ref type="bibr" target="#b18">Shen et al. (2017)</ref> used a discriminator to align target sentences and sentences transfered to the target domain from the source domain. More recently, unsupervised machine translation models <ref type="bibr" target="#b1">(Artetxe et al., 2017;</ref><ref type="bibr" target="#b11">Lample et al., 2017)</ref> used a cycle loss similar to <ref type="bibr" target="#b10">Jun-Yan et al. (2017)</ref> to ensure that the content is preserved during the transformation. These methods often rely on bilinguial word vectors to provide word-for-word translations, which are then finetune by back-translation. Thus they can be used to further improve our results.</p><p>Our method of detecting attribute markers is reminiscent of Naive Bayes, which is a strong baseline for tasks like sentiment classification <ref type="bibr" target="#b20">(Wang and Manning, 2012)</ref>. Deleting these attribute markers can be viewed as attacking a Naive Bayes classifier by deleting the most informative features <ref type="bibr" target="#b5">(Globerson and Roweis, 2006)</ref>, similarly to how adversarial methods are trained to fool an attribute classifier. One difference is that our classifier is fixed, not jointly trained with the model.</p><p>To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training. The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases. While many prior works on linguistic style analysis confirm our observation that attributes often manifest in idiosyncratic phrases <ref type="bibr" target="#b15">(Recasens et al., 2013;</ref><ref type="bibr" target="#b17">Schwartz et al., 2017;</ref><ref type="bibr" target="#b13">Newman et al., 2003)</ref>, we recognize the fact that in some problems (e.g., <ref type="bibr" target="#b14">Pavlick and Tetreault (2017)</ref>), content and attribute cannot be so cleanly separated along phrase boundaries. Looking forward, a fruitful direction is to develop a notion of attributes more general than n-grams, but with more inductive bias than arbitrary latent vectors.</p><p>Reproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0xe3eb416773ed4883bb737662b31b4948/. From factual to humorous (CAPTIONS) SOURCE a black and white dog is running through shallow water . CROSSALIGNED two dogs are playing on a field to win the water . STYLEEMBEDDING a black and white dog is running through shallow water . MULTIDECODER a black and white dog is running through grassy water . TEMPLATEBASED a black and white dog is running through shallow water looking for . RETRIEVEONLY a black and white dog is slowly running through a snowy field . DELETEANDRETRIEVE a black and white dog is running through shallow water to search for bones . DELETEONLY a black and white dog is running through shallow water like a fish . From positive to negative (AMAZON) SOURCE i would definitely recommend this for a cute case . CROSSALIGNED i would not recommend this for a long time . STYLEEMBEDDING i would definitely recommend this for a cute case . MULTIDECODER i would definitely recommend this for a bra does it . TEMPLATEBASED skip this one for a cute case . RETRIEVEONLY cute while it lasted . . . so if you want to have a NUM night stand case , this is your case . DELETEONLY i would not recommend it for a cute case . DELETEANDRETRIEVE i would not recommend this for a cute case .   <ref type="figure">Figure 4</ref>: Scatter plots of humans scores vs. automatic metric scores on attribute transfer, content preservation, and grammaticality. The automatic metrics have some correlation with the attribute transfer and content preservation ratings, though results vary across datasets; the metrics do not measure grammaticality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our approach. (a) We identify attribute markers from an unaligned corpus. (b) We transfer attributes by removing markers of the original attribute, then generating a new sentence conditioned on the remaining words and the target attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Trade-off curves between matching the target attribute (measured by classifier scores) and preserving the content (measured by BLEU). Bigger points on the curve correspond to settings used for both training and our official evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1</head><label></label><figDesc>Introduction great food but horrible staff and very very rude workers ! target=positive great food staff and very workers ! great food , awesome staff , very personable and very efficient atmosphere !</figDesc><table><row><cell>Delete attribute markers</cell></row><row><cell>Run system</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2. Formally, the main components of these systems are as follows:</figDesc><table><row><cell>Inputs</cell><cell></cell><cell></cell></row><row><cell>: positive</cell><cell></cell><cell>DeleteOnly</cell></row><row><cell>: negative : i have had this mount for</cell><cell>RNN</cell><cell>i have had this mount for about a year and it still works .</cell></row><row><cell cols="2">about a year and it works great .</cell><cell></cell></row><row><cell></cell><cell></cell><cell>DeleteAndRetrieve</cell></row><row><cell></cell><cell>RNN</cell><cell>i have had this mount for about a year and barely used it .</cell></row><row><cell></cell><cell></cell><cell>TemplateBased</cell></row><row><cell></cell><cell></cell><cell>i have had this mount for</cell></row><row><cell></cell><cell></cell><cell>about a year and it barely used .</cell></row><row><cell>(2) Retrieve similar sentence with target attribute</cell><cell>Extract target attribute markers</cell><cell></cell></row><row><cell></cell><cell></cell><cell>RetrieveOnly</cell></row><row><cell></cell><cell></cell><cell>i have had it for a while</cell></row><row><cell></cell><cell></cell><cell>but barely used it .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Example outputs on YELP, CAPTIONS, and AMAZON. Additional examples for transfer from opposite directions are given inTable 6. Added or changed words are in italic. Attribute markers are colored.</figDesc><table><row><cell></cell><cell>YELP</cell><cell></cell><cell cols="2">CAPTIONS</cell><cell>AMAZON</cell><cell></cell></row><row><cell></cell><cell cols="6">Classifier BLEU Classifier BLEU Classifier BLEU</cell></row><row><cell>CROSSALIGNED</cell><cell>73.7%</cell><cell>3.1</cell><cell>74.3%</cell><cell>0.1</cell><cell>74.1%</cell><cell>0.4</cell></row><row><cell>STYLEEMBEDDING</cell><cell>8.7%</cell><cell>11.8</cell><cell>54.7%</cell><cell>6.7</cell><cell>43.3%</cell><cell>10.0</cell></row><row><cell>MULTIDECODER</cell><cell>47.6%</cell><cell>7.1</cell><cell>68.5%</cell><cell>4.6</cell><cell>68.3%</cell><cell>5.0</cell></row><row><cell>TEMPLATEBASED</cell><cell>81.7%</cell><cell>11.8</cell><cell>92.5%</cell><cell>17.1</cell><cell>68.7%</cell><cell>27.1</cell></row><row><cell>RETRIEVEONLY</cell><cell>95.4%</cell><cell>0.4</cell><cell>95.5%</cell><cell>0.7</cell><cell>70.3%</cell><cell>0.9</cell></row><row><cell>DELETEONLY</cell><cell>85.7%</cell><cell>7.5</cell><cell>83.0%</cell><cell>9.0</cell><cell>45.6%</cell><cell>24.6</cell></row><row><cell>DELETEANDRETRIEVE</cell><cell>88.7%</cell><cell>8.4</cell><cell>96.8%</cell><cell>7.3</cell><cell>48.0%</cell><cell>22.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Automatic evaluation results. "Classifier" shows the percentage of sentences labeled as the target attribute by the classifier. BLEU measures content similarity between the output and the human reference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Spearman correlation between two automatic evaluation metrics and related human evaluation scores. While some correlations are strong, the classifier exhibits poor correlation on AMAZON, and BLEU only measures content, not grammaticality.</figDesc><table><row><cell>BLEU</cell><cell>15 20 25</cell><cell></cell><cell cols="2">TemplateBased Retrieval CrossAligned MultiDecoder StyleEmbedding DeleteAndRetrieve DeleteOnly</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0 0</cell><cell>0.2</cell><cell>0.4 Classifier accuracy 0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>appetizer sandwich , she was it wrong . STYLEEMBEDDING my husband got a greatest sandwich , he loved it . MULTIDECODER my husband got a beginning house with however i ignored . TEMPLATEBASED my husband got a ruben sandwich , don't care RETRIEVEONLY i got the club sandwich and my mom got the chicken salad sandwich . DELETEONLY my husband got a ruben sandwich , and it was horrible ! DELETEANDRETRIEVE my husband got a ruben sandwich , it was too dry .</figDesc><table><row><cell></cell><cell>From positive to negative (YELP)</cell></row><row><cell>SOURCE</cell><cell>my husband got a ruben sandwich , he loved it .</cell></row><row><cell>CROSSALIGNED</cell><cell>my husband got a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Additional example outputs on YELP, CAPTIONS, and AMAZON. Added or changed words are in italic. Attribute markers are colored.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">The success of natural language generation (NLG) systems depends on their ability to carefully control not only the topic of produced utterances, but also attributes such as sentiment and style. The desire for more sophisticated, controllable NLG has led to increased interest in text attribute transferthe task of editing a sentence to alter specific attributes, such as style, sentiment, and tense(Hu   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Henceforth, we refer to attribute-independent content as simply content, for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(3) Generate output sentence</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Markers are replaced from left to right, in order. If there are not enough markers in x tgt , we use an empty string.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462. J.L. is supported by Tencent. R.J. is supported by an NSF Graduate Research Fellowship under Grant No. DGE-114747.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Style transfer in text: Exploration and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stylenet: Generating attractive visual captions with styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nightmare at test time: robust learning by feature deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with oneclass collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jun-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taesung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09395</idno>
		<title level="m">Improved neural text attribute transfer with non-parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lying words: Predicting deception from linguistic styles. Personality and Social</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical analysis of formality in online communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic models for analyzing and detecting biased language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by crossalignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarially regularized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
