<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Unit Memory Network for Weakly Supervised Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tzzhang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
							<email>yangwf@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
							<email>jingenliu@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@live.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
							<email>fengwu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Action Unit Memory Network for Weakly Supervised Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without framelevel annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an action unit memory bank. In the proposed AUMN, two attention modules are designed to update the memory bank adaptively and learn action units specific classifiers. Furthermore, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the first work to explicitly model the action units with a memory network. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs favorably against stateof-the-art methods. Specifically, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly improved from 47.0% to 52.1%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action localization (TAL) is an important yet challenging task for video understanding. Its goal is to localize temporal boundaries of actions with specific categories in untrimmed videos <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref>. Because of its broad applications in high-level tasks such as video surveillance <ref type="bibr" target="#b39">[40]</ref>, video summarization <ref type="bibr" target="#b16">[17]</ref>, and event detection <ref type="bibr" target="#b14">[15]</ref>, TAL has recently drawn increasing attentions from the community. Up to now, deep learning based methods have made impressive progresses in this area. However, most of them handle this task in a fully supervised way, requiring massive temporal boundary annotations for actions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36]</ref>. Such manual annotations are ex- <ref type="bibr">Figure 1</ref>. (a) Illustration of "Sharing Units" characteristic. The running (red box) is a shared action unit among high-jump, longjump and cricket-bowling. (b) Illustration of "Sparsity" characteristic. An action usually occupies a small portion of untrimmed videos. (c) Illustration of "Smoothness" characteristic. CAS1 is more suitable for the action localization task because the CAS2 tends to divide a continuous action into multiple instances.</p><p>pensive to obtain, which limits the development potential of fully-supervised methods in real-world scenarios.</p><p>To relieve this problem, the weakly supervised setting that only requires video-level category labels is proposed <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. It can be formulated as a multiple instance learning problem, where a video is treated as a bag of multiple segments and fed into a video-level classifier to get a class activation sequence (CAS). There are two primary challenges, named localization completeness and background interference. To solve the first challenge, previous works usually adopt a well-designed erasing strategy <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b52">53]</ref> or a multi-branch architecture <ref type="bibr" target="#b20">[21]</ref>. Both of them aim to force the model to concentrate on different parts of videos and hence discover the whole action without missing any relevant segments. To handle the second challenge, some methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12]</ref> employ an attention-based per-class feature aggregation scheme, where the class-specific attention is obtained by normalizing the CAS along the temporal axis. This scheme helps learn a compact intra-class feature, which enables action segments to be more discriminative than the background. Furthermore, one popular way to handle both challenges is to learn a class-agnostic attention mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref>, to highlight action segments and suppress background segments.</p><p>By studying all previous TAL methods, we sum up the following three important observations (i.e., TAL Properties): (1) Sharing Units. An action to be detected generally consists of some primary action units, which can be shared with other action classes. For example, as shown in <ref type="figure">Figure 1 (a)</ref>, a high-jump contains running and jumping upward while a long-jump consists of running and jumping forward, so running is a shared action unit. (2) Sparsity. In general, only a sparse set of video segments contains the meaningful target actions. As we can see from <ref type="figure">Figure 1</ref> (b), an action only occupies a small portion of the video.</p><p>(3) Smoothness. A smooth CAS is required for localization, because an action is continuous, as shown in <ref type="figure">Figure 1</ref> (c). These three characteristics are critical for the success of action localization. Unfortunately, they have not been thoroughly addressed by previous studies. To achieve accurate and complete action localization, these three observations should be taken into consideration when designing an action localization model. However, with only video-level labels, it is difficult to model them jointly in a unified model.</p><p>To fully leverage the above three characteristics for action localization, we propose a novel end-to-end framework, called Action Unit Memory Network (AUMN), for more effective weakly supervised action localization. Our framework starts with the action unit templates learning. According to the "Sharing Units" characteristic, we design a sub-network as a memory bank of action unit templates, which serve as our learning primary for action localization. To exploit the templates for action classification, we further design a Multi-Layer Perceptron (MLP) network to embed each template into the action class space. Basically, the MLP network helps connect templates to action classes. Intuitively speaking, action unit templates will be projected onto a set of action classifiers. Afterwards, a cross-attention module is proposed to compute the relationships between a video segment and all templates. And according to the "Smoothness" characteristic, we introduce a self-attention module to compute the relationships between different segments in a video for aggregating context information. Leveraging both of the attention mechanisms, we can get refined segment features and be able to dynamically select action classifiers for each video segment, which in turn, simultaneously contribute the adaptive learning of the memory bank.</p><p>However, the video-level ground-truth supervision alone is not enough for memory updating. Based on the property of action units and "Sparsity" characteristic, we further design three effective mechanisms to guide the updating of the memory bank: (1) Since action units are different from each other, each template in the memory bank should be unique. To achieve this goal, we design a diversity mechanism to encourage the differences among the templates in the memory. (2) While the diversity mechanism can encourage each template in the memory to be unique, it does not guarantee that no template is useless, which means that a template may have low similarities with all video segments. To avoid this, we design a homogeneity mechanism to encourage a uniform distribution for the occurring probability of templates. <ref type="formula" target="#formula_3">(3)</ref> In an untrimmed video, action segments only occupy a small portion of the whole video, and most of the video segments are background. Thus we design a sparsity mechanism to encourage that only a sparse set of video segments can have high similarities with the templates in the memory. These three mechanisms together with the supervision of video-level category label can guide the network to learn meaningful action units.</p><p>To sum up, the main contributions of our work are threefold: (1) To the best of our knowledge, we are the first to model the action units with a memory network for the weakly supervised TAL task. (2) We propose two attention modules to ensure our memory to update adaptively and learn action units specific classifiers. Further, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating. (3) Extensive experimental results on two challenging benchmarks including THUMOS14 <ref type="bibr" target="#b12">[13]</ref> and ActivityNet <ref type="bibr" target="#b2">[3]</ref> demonstrate that the proposed AUMN performs favorably against state-of-theart weakly supervised TAL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we overview methods that are related to fully and weakly supervised temporal action localization and memory networks.</p><p>Fully Supervised Temporal Action Localization. Temporal action localization (TAL) aims to not only recognize actions in untrimmed videos but also give an accurate temporal proposal for each action, which makes it very challenging. To tackle this problem, fully supervised based methods have been extensively studied recently, where the frame-level annotations are required during training <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b50">51]</ref>. Most of these methods borrow intuitions from the object detection frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>. In specific, many methods adopt a two-stage pipeline, i.e., action proposals are generated first and then fed into a classification module. For proposal generation, some methods adopt the sliding window <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref> and others predict temporal boundaries of action instances directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref>. In addition to the two-stage methods, one-stage methods are proposed to predict action category and temporal boundaries from raw data directly, which are more flexible and efficient <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Weakly Supervised Temporal Action Localization. Weakly supervised methods tackle the same problem but with less supervision, e.g., video-level category labels. This pipeline can alleviate the requirement for expensive action boundary annotations, but raise two challenges named localization completeness and background interference. To handle the two problems, existing methods can be divided into three types. The first type of works attempt to solve the localization completeness by applying a well-designed erasing strategy <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b52">53]</ref> or a multi-branch architecture <ref type="bibr" target="#b20">[21]</ref>. For example, Zhong et al. <ref type="bibr" target="#b54">[55]</ref> design a stepby-step erasion approach to train the one-by-one classifiers, via collecting detection results from these classifiers, more action segments are found. And in CMCS <ref type="bibr" target="#b20">[21]</ref>, a multibranch network with a diversity loss is proposed to make the model focus on different parts of videos. The second type of works aim to tackle the background interference via a intra-class feature compactness scheme <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. They first compute the class-specific attention by applying the softmax function to CAS then use this attention to get an aggregated video-level feature. By devising different mechanisms to learn a compact intra-class feature, action and background segments tend to be separated. For example, to decrease the intra-class variance, 3C-Net <ref type="bibr" target="#b27">[28]</ref> and A2CL-PT <ref type="bibr" target="#b26">[27]</ref> maintain a set of class center and RPN <ref type="bibr" target="#b11">[12]</ref> learns class-specific prototypes. The third type of works are based on a class-agnostic attention mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref>, which can consider both the challenges simultaneously. Unlike the second type of works, the attention here is generated in a bottom-up way from the raw data and trained for highlighting foreground segments. It is first proposed by STPN <ref type="bibr" target="#b28">[29]</ref> and then inspires many following methods. Some of them introduce an auxiliary category to focus on modeling background <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>. And based on the observation that background features differ from action features, DGAM <ref type="bibr" target="#b33">[34]</ref> adopts a conditional variation auto-encoder to construct different feature distributions conditioned on the attention. Recently, TSCN <ref type="bibr" target="#b51">[52]</ref> and EM-MIL <ref type="bibr" target="#b24">[25]</ref> fuse the output of different modalities (RGB and optical flow) to generate pseudo labels for guidance of the attention.</p><p>Memory Networks. Memory networks typically involve an internal memory implicitly updated in a recurrent process, e.g., LSTM <ref type="bibr" target="#b10">[11]</ref>, or an explicit memory bank that can be read or written with an attention based mechanism. Memory networks that can be trained end-to-end are first proposed in the natural language processing research like question answering <ref type="bibr" target="#b25">[26]</ref> and sentiment analysis <ref type="bibr" target="#b5">[6]</ref>. Recently, in the temporal action localization task, a popular use of memory is exploring the temporal structure based on the LSTM <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b37">38]</ref>. The ability of LSTM to learn from long sequences with unknown size of background is wellsuited for fine-grained action localization from untrimmed videos. Instead of exploiting the temporal relationships in a video, we propose an attention-based memory mechanism to model the action units which are shared among all the videos. This mechanism helps us to deal with the large intra-class variations, so that we can get more complete localization results by discovering various action units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Approach</head><p>In this section, we first formulate the task of weakly supervised Temporal Action Localization. Then we describe each composition of the proposed Action Unit Memory Network (AUMN) in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and Preliminaries</head><formula xml:id="formula_0">Assume we have N untrimmed training videos {v i } N i=1 . Each video v i has its ground-truth label y i ∈ R C , where</formula><p>C is the number of action categories. y i (j) = 1 if the action category j is present in the video and y i (j) = 0 otherwise <ref type="bibr" target="#b0">1</ref> . During testing, the goal of the temporal action localization is to generate a set of action proposals {(c, s, e, q)} for each video, where c and q denote the predicted category and the confidence score, s and e represent the start and the end time respectively. In this paper, we follow previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> to extract features for both RGB and optical flow streams. Given an untrimmed video v i , we first divide it into non-overlapping 16-frame segments and apply the I3D pretrained on the Kinetics dataset to extract features for each segment. Then we get two segment-wise features X RGB i ∈ R li×D and X F LOW i ∈ R li×D , where l i denotes the number of segments in video v i and D is the dimension of features. Because the RGB and FLOW streams are trained independently, we use X i to represent them in the rest of this paper for simplicity. Since the extracted features from I3D are learned for the action recognition task originally, it is desired to add a task-adaption layer to refine the extracted features. In specific, we adopt a temporal convolutional layer with the ReLU activation as</p><formula xml:id="formula_1">X e i = ReLU(W emb * X i + b emb ),<label>(1)</label></formula><p>where the * represents the convolution operation, W emb and b emb are the weights and bias of temporal filters, X e i ∈ R li×F is the learned embedding feature, and F is the dimension of learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Unit Memory Network</head><p>The overall architecture of our action unit memory network is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. It consists of three parts including feature extraction, memory bank construction, memory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attention Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Bank Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention Module Sparsity Loss Homogeneity Loss Diversity Loss</head><p>Class Activation Sequence bank for classification and updating. The details are introduced as follows.</p><p>Memory Bank Construction. The action unit memory bank stores multiple templates M ∈ R K×F , and each template represents an action unit, where K and F are the number and dimension of templates respectively. We adopt two encoders named Enc K and Enc V to embed the templates into pairs of keys and values respectively. The Enc K is designed to reduce the dimension of the templates for efficient reading from the memory, which is implemented as a fully connected layer (FC). And the Enc V is designed to encode each template into a template-specific classifier, which is a MLP network consisted of two FC layers with a bottleneck structure among them to reduce parameters. In this way, the keys store appearance and motion related information for the templates and can be used for template matching during memory reading, and the values store templates specific classifiers and can be used for segment classification. Formally, we formulate the encodings as follows:</p><formula xml:id="formula_2">K M = Enc K (M),<label>(2)</label></formula><formula xml:id="formula_3">V M = Enc V (M),<label>(3)</label></formula><p>where K M ∈ R K×F/m and V M ∈ R K×CF are keys and values, M denotes the memory, and m is a hyper-parameter to control memory reading efficiency. Given the memory bank and an input video, we introduce how to perform video classification and memory updating next. Memory Bank for Classification. For video classification, we use an encoder Enc Q which is implemented as a FC layer to encode video feature X e i into a set of queries Q i ∈ R li×F/m , and then feed the segment features and queries into a self-attention module and a cross attention module to generate classification results. In the self-attention module, we first calculate the similarity scores among video segments with queries and then use these scores to refine the segment features by aggregating context information, which can be formulated as</p><formula xml:id="formula_4">X s i = (softmax( Q i Q T i F/m ) + I)X e i ,<label>(4)</label></formula><p>where I is the identity matrix used to preserve the original information, and X s i keeps the same dimension with X e i . Via this message passing between segments, we can extract global context information and get more discriminative features for both classification and localization.</p><p>In the cross-attention module, we read from the memory and get a set of segment-wise classifiers. To achieve this goal, we first calculate the similarity scores S i ∈ R li×K between video segments and memory templates with the scaled dot-product</p><formula xml:id="formula_5">S i = sigmoid( Q i (K M ) T F/m ).<label>(5)</label></formula><p>Based on the similarity scores, we can obtain a set of segment-wise classifiers by using the similarity scores to aggregate memory values as</p><formula xml:id="formula_6">V O i = S i V M ,<label>(6)</label></formula><p>where V O i ∈ R li×CF . Later, to perform classification, we reshape V O i into a set of segment classifiers W cls</p><formula xml:id="formula_7">i = {W cls i (t) ∈ R F ×C } li t=1</formula><p>, which is adaptive to the appearance or motion variations of each segment.</p><p>With the refined features of the self-attention module and the segment-wise classifiers of the cross-attention module, we can obtain the segment-level classification results by applying each classifier on the corresponding segment. Since we only have video-level ground-truth supervision, we need to aggregate these segment-level classification results into a video-level prediction. In specific, since the second dimension of the similarity matrix S i denotes the similarity between a segment and a template, we can apply the maxpooling operation along the second dimension of the S i to get the foreground attention weight a i ∈ R li as a i = MaxPool(S i ),</p><p>The video-level classification resultŷ i is then obtained as an attention weighted poolinĝ</p><formula xml:id="formula_9">y i = softmax( 1 l i li t=1 a i (t)(X s i (t)W cls i (t))),<label>(8)</label></formula><p>whereŷ i ∈ R C . Then the classification loss is defined as the cross-entropy loss between the prediction and the video label y i</p><formula xml:id="formula_10">L cls = − 1 B B i=1 C j=1 y i (j) logŷ i (j).<label>(9)</label></formula><p>Memory Bank Updating. For the memory updating, we find that the above classification loss alone is not enough to learn a satisfying memory bank. Thus we design three mechanisms (diversity, homogeneity and sparsity) to guide the updating of the memory bank. The diversity mechanism encourages that each template in the memory bank is different from other templates, the homogeneity mechanism encourages that each template in the memory bank is meaningful, and the sparsity mechanism encourages that the templates in the memory bank can suppress background segments. In specific, in the diversity mechanism, we design a diversity loss to ensure the uniqueness of each template in the memory as</p><formula xml:id="formula_11">L d = MM T − I F ,<label>(10)</label></formula><p>where I is the identity matrix and · F is the Frobenius norm of a matrix. While the diversity loss encourages the templates in the memory bank to be unique, it does not guarantee that each template in the memory bank is useful. For example, a template may not represent an action unit and have low similarities with all video segments during training. To deal with this issue, we design a homogeneity loss to encourage a uniform distribution for the occurring probability of templates in the homogeneity mechanism. In specific, we first pool the similarity matrix S i over time by a sum operation and then use a softmax function to obtain the occurring probability of each template as</p><formula xml:id="formula_12">p O i = softmax( li t=1 S i (t)),<label>(11)</label></formula><p>where p O i ∈ R K . Then the homogeneity loss can be formulated as</p><formula xml:id="formula_13">L h = 1 B B i=1 p O i 2 ,<label>(12)</label></formula><p>where B is the mini-batch size. And based on the obser-vation that an action usually occupies a small portion of a untrimmed video, we design a sparsity loss in the sparsity mechanism to relieve the interference of background segments. And the sparsity loss is designed as</p><formula xml:id="formula_14">L s = 1 B B i=1 a i 1 ,<label>(13)</label></formula><p>which encourages background segments to have low similarities with all the templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training and Inference</head><p>Training. For training the whole network, we compose the classification loss and three auxiliary losses as</p><formula xml:id="formula_15">L = L cls + αL d + βL h + γL s .<label>(14)</label></formula><p>where α, β and γ are hyper-parameters to balance the contribution of each loss function. To summarize, we maintain a set of templates in representation of various action units and update them in the video-level classification task. To better guide the learning of templates, a diversity loss and a homogeneity loss are devised to keep the variety and effectiveness of each template, and a sparsity loss is introduced to relieve background interference.</p><p>Inference. After modeling action units by the AUMN, we can localize actions by examining whether a segment belongs to a kind of action unit. In specific, we take a two-step approach to perform localization. First, we threshold on video-level prediction scoresŷ i and discard categories which have confidence scores below a threshold η cls . Thereafter, for each of the remaining action categories, we apply the threshold η act on the foreground attention weight to generate action proposals. To assign a confidence for each proposal, we compute the class activation sequence</p><formula xml:id="formula_16">(CAS) C i ∈ R li×C first, where C i (t, ; ) = X s i (t)W cls i (t),<label>(15)</label></formula><p>then C i is passed through a softmax function along the category dimension to get class scores at each time location, denoted asC i . And the confidence score q in the proposal {(c, s, e, q)} is computed as</p><formula xml:id="formula_17">q = e t=s θa R i (t)C R i (t, c) + (1 − θ)a F i (t)C F i (t, c) s − e + 1 ,<label>(16)</label></formula><p>where the superscripts R and F denote RGB or F LOW streams respectively, θ is a scalar denoting the relative importance between the two modalities and is set to 0.3 in this work. To remove proposals with a high overlap, the classwise Non-Maximal Suppression (NMS) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. The proposed AUMN is evaluated on two benchmark datasets including THUMOS14 <ref type="bibr" target="#b12">[13]</ref> and Ac-tivityNet <ref type="bibr" target="#b2">[3]</ref>. THUMOS14 dataset contains 200 validation videos and 213 testing videos annotated with temporal action boundaries belonging to 20 categories. This dataset is <ref type="table">Table 1</ref>. Localization performance comparison with state-of-the-art methods on the THUMOS14 test set. Note that weak + represents methods that utilize external supervision information besides from video labels, i.e., frequency of action instances.  <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52]</ref>, we train our model on the training set and evaluate it on the validation set. Evaluation Metrics. Following the standard evaluation protocol, we evaluate the TAL performance with the mean Average Precision (mAP) values under different intersection over union (IoU) thresholds. Implementation Details. We use the two-stream I3D networks <ref type="bibr" target="#b3">[4]</ref> pre-trained on Kinetics as our feature extractor. Note that for fair comparison, we do not finetune the I3D network. We apply the TV-L1 algorithm to extract optical flow from RGB data. Then we divide both streams into non-overlapping 16 frames segments as the input to the I3D network, the dimension D of the output feature for each segment is 1024. We train separate AUMNs for RGB and FLOW streams and collect the generated propos-als from both networks during inference. In AUMN, the embedding layer is composed of a temporal convolutional layer with 1024 input channels and 512 output channels. The number of templates K is 7 if not mentioned specifically. In Eq. (14), the loss function weights α = 0.01, β = 0.02, and γ is set to 0.05 and 0.03 for the RGB stream and the FLOW stream, respectively. During inference, the threshold η cls is 0.1 and η act is the mean value of the corresponding foreground attention a i for video v i . And we use the class-wise NMS with a threshold 0.3 to remove highly overlapped proposals. Our model is trained using Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with the learning rate 10 −4 and batch size 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>Experiments on THUMOS14. <ref type="table">Table 1</ref> summarizes the performance comparison between the proposed AUMN and state-of-the-art TAL methods on the THUMOS14 test set. Weakly + denotes methods that adopt additional supervision during training, e.g., the number of action instances in a video, and AVG indicates the average mAP for IoU thresholds 0.1:0.1:0.5. From the results, we can see that the proposed AUMN outperforms all the previous weakly supervised models and achieves a new state-of-the-art performance (33.3% mAP at IoU0.5). And an absolute gain of 5.1% is achieved in terms of the average mAP when compared to the best previous method (TSCN <ref type="bibr" target="#b51">[52]</ref>). It is worth noting that EM-MIL <ref type="bibr" target="#b24">[25]</ref> gets a higher mAP at IoU thresholds 0.6 and 0.7 than ours. However, we get 7% improvement than EM-MIL at average mAP. Besides, EM- MIL adopts a pseudo label scheme to relieve background interference while we adopt a simple sparsity prior. We believe the performance of our approach can be promoted further when equipped with a more effective background suppression techniques. Compared to the weakly + methods, our method outperforms 3C-Net <ref type="bibr" target="#b27">[28]</ref> at all IoU thresholds and achieves 5.1% improvement over STAR <ref type="bibr" target="#b42">[43]</ref> in average mAP. When compared with fully supervised methods, we note that the performance of AUMN drops faster than fully supervised methods as the IoU threshold increases. However, we can also get a comparable result at low IoU thresholds, e.g., AUMN outperforms TAL-Net at IoU thresholds 0.1, 0.2 and 0.3. Experiments on ActivityNet. On the ActivityNet dataset, we follow the standard evaluation protocol <ref type="bibr" target="#b2">[3]</ref> by reporting the average mAP scores at different thresholds (0.5:0.05:0.95). The performance comparisons on the Ac-tivityNet1.2 and ActivityNet1.3 are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>, respectively. The results are consistent with those on the THUMOS14 dataset, and our AUMN outperforms all previous weakly supervised models in average mAP on both ActivityNet1.2 and ActivityNet1.3, with 25.5% and 23.5% average mAP respectively. It is worth noting THUMOS14 dataset and ActivityNet dataset have different characteristics. For the THUMOS14 dataset, the most important thing is the background suppression. While for the ActivityNet dataset, the most important thing is the localization completeness. At high IoU thresholds, EM-MIL has better performance on the THUMOS14 dataset while worse performance on the ActivityNet dataset. This is because EM-MIL mainly considers background suppression while ignores the localization completeness. Different from existing methods, our AUMN takes both background suppression and localization completeness into consideration, and can achieve favorable performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we conduct a series of ablation studies on the THUMOS14 dataset to evaluate the influence of each  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>design.</head><p>Influence of Each Loss Function. As introduced in Section 3.2, we design three auxiliary losses (diversity loss L d , homogeneity loss L h and sparsity loss L s ) to guide the memory updating. To explore the influence of each loss function, we conduct experiments with different loss combinations, and the results are shown in <ref type="table" target="#tab_3">Table 4</ref>. From the results, we have the following observations: (1) The sparsity loss L s can bring a significant performance improvement at all IoU thresholds. Because there is no frame-level label as the supervision of the foreground attention, L s can serve as a prior to guide the action unit templates to focus on the action related segments. (2) The diversity loss L d is designed to encourage the action unit templates in the memory bank to be different from each other. Without the diversity loss, we can only rely on the random initialization to achieve our goal. From the results, we can see that the diversity loss can bring a 1.0% performance gain in average mAP, which indicates that the diversity loss is necessary. (3) The homogeneity loss L h is designed to guarantee that each learned action unit template is useful. Without the L h , some templates in the memory bank may be useless, which may decrease the representative ability of the memory bank. When equipped with this loss, we observe a 0.6% performance gain in average mAP. (4) These three losses can promote each other. For example, the L d can keep the difference between templates, but cannot ensure each learned template is useful. On the other hand, although L d can ensure no template is redundant, it may lead to learning a set of identical memory templates. By combining them together, the mAP is increased by 3% at IoU = 0.5, which is much more significant than applying them independently.  Influence of the Self-attention Module. As introduced in Section 3.2, the self-attention module is designed to incorporate context information so as to encourage a smoother temporal classification score, which is important for complete action localization. From the results in Table 4, the self-attention module can consistently improve the performance at all IoU thresholds. And it is worth noting that the performance gain at IoU = 0.4, 0.5, 0.6 is more significant than that at IoU = 0.1, 0.2, 0.3. To further verify the self-attention design, we show several visualization results in <ref type="figure" target="#fig_1">Figure 3</ref>. With the self-attention module, some less discriminative action segments can be assigned higher confidence scores, and it means that the self-attention module can indeed help to improve localization completeness.</p><p>Influence of the Template Number. To explore the influence of the template number, we conduct experiments on the THUMOS14 dataset and report the average mAP at IoU 0.1:0.1:0.5 of AUMN with different template numbers. The results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the average mAP can be consistently improved as the template number K grows from 1 to 7, which means 7 templates are sufficient to model all the action units on the THUMOS14 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>To better understand our method, the qualitative results of our AUMN on three videos from the ActivityNet1.2 validation set are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. The action instances from left to right are javelin-throw, long-jump and highjump respectively. We visualize the similarities between video segments with different templates in third to fifth rows. And the 6th row is the foreground attention a. We find that different templates attend to model different visual patterns. For example, the first template has a high similarity to the segments which contain the action unit running while the second is similar to jumping. The third template tends to focus on throwing, which is an important action unit in javelin-throw. Interestingly, some segments of throwing are a little similar to the first template, because the man still keeps running while throwing the javelin. It is worth noting that long-jump and high-jump both contain segments about jumping, to distinguish them from each other, the segmentwise classifiers defined in Eq (6) are desired. In summary, by finding action units in untrimmed videos via the templates from memory and utilizing the segment-wise classifiers, we can correctly recognize an action and obtain robust foreground attentions for complete action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an Action Unit Memory Network (AUMN) to model action units for weakly supervised temporal action localization. We design a memory bank to store the appearance and motion information of action units and their corresponding classifiers. We further introduce a cross-attention module to read segment-wise classifiers from the memory and a self-attention module for refining features by aggregating temporal context information. Then we can get segment-level predictions and update the memory in an adaptive way with three auxiliary mechanisms (diversity, homogeneity and sparsity). With a meaningful memory bank, we can achieve more complete localization results by finding action units in untrimmed videos. Extensive experimental results on two benchmarks demonstrate the effectiveness of the proposed AUMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of our proposed Action Unit Memory Network (AUMN), which consists of three parts: feature extraction, memory bank construction, memory bank for classification and updating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the class activation sequence with (W) and without (W/O) the self-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Performance comparison of different template numbers. The average mAP is computed at IoU thresholds 0.1:0.1:0.5. Note that the L d and L h are removed when K = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on ActivityNet1.2. The action instances from left to right are javelin-throw, long-jump and highjump respectively. The six rows in each example are input video, ground truth action instance, three different subsets of similarities scores S in Eq. (5) and the foreground attention a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Localization performance comparison with state-of-theart methods on the ActivityNet1.2 validation set.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell cols="3">mAP@IoU 0.75 0.95 AVG</cell></row><row><cell>UntrimmedNet [39]</cell><cell>7.4</cell><cell>3.9</cell><cell>1.2</cell><cell>3.6</cell></row><row><cell>Zhong et al. [55]</cell><cell cols="2">27.3 14.7</cell><cell>2.9</cell><cell>15.6</cell></row><row><cell>AutoLoc [35]</cell><cell cols="2">27.3 15.1</cell><cell>3.3</cell><cell>16.0</cell></row><row><cell>WTALC [31]</cell><cell cols="2">37.0 14.6</cell><cell>-</cell><cell>18.0</cell></row><row><cell>TSM [49]</cell><cell cols="2">28.3 17.0</cell><cell>3.5</cell><cell>-</cell></row><row><cell>CMCS [21]</cell><cell cols="2">36.8 22.0</cell><cell>5.6</cell><cell>22.4</cell></row><row><cell>Clean-Net [23]</cell><cell cols="2">37.1 20.3</cell><cell>5.0</cell><cell>21.6</cell></row><row><cell>3C-Net [28]</cell><cell cols="2">37.2 23.7</cell><cell>-</cell><cell>21.7</cell></row><row><cell>Bas-Net [16]</cell><cell cols="2">38.5 24.2</cell><cell>5.6</cell><cell>24.3</cell></row><row><cell>Huang et al. [12]</cell><cell cols="2">37.6 23.9</cell><cell>5.4</cell><cell>23.3</cell></row><row><cell>TCAM [10]</cell><cell cols="2">40.0 25.0</cell><cell>4.6</cell><cell>24.6</cell></row><row><cell>DGAM [34]</cell><cell cols="2">41.0 23.5</cell><cell>5.3</cell><cell>24.4</cell></row><row><cell>EM-MIL [25]</cell><cell cols="2">37.4 23.1</cell><cell>2.0</cell><cell>20.3</cell></row><row><cell>TSCN [52]</cell><cell cols="2">37.6 23.7</cell><cell>5.7</cell><cell>23.6</cell></row><row><cell>AUMN (Our's)</cell><cell cols="2">42.0 25.0</cell><cell>5.6</cell><cell>25.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Localization performance comparison with state-of-theart methods on the ActivityNet1.3 validation set.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell cols="3">mAP@IoU 0.75 0.95 AVG</cell></row><row><cell>STPN [29]</cell><cell cols="2">29.3 16.9</cell><cell>2.6</cell><cell>16.3</cell></row><row><cell>ASSG [53]</cell><cell cols="2">32.3 20.1</cell><cell>4.0</cell><cell>18.8</cell></row><row><cell>CMCS [21]</cell><cell cols="2">34.0 20.9</cell><cell>5.7</cell><cell>21.2</cell></row><row><cell>STAR [43]</cell><cell cols="2">31.1 18.8</cell><cell>4.7</cell><cell>18.2</cell></row><row><cell>TSM [49]</cell><cell cols="2">30.3 19.0</cell><cell>4.5</cell><cell>-</cell></row><row><cell>Nguyen et al. [30]</cell><cell cols="2">36.4 19.2</cell><cell>2.9</cell><cell>19.5</cell></row><row><cell>Bas-Net [16]</cell><cell cols="2">34.5 22.5</cell><cell>4.9</cell><cell>22.2</cell></row><row><cell>TSCN [52]</cell><cell cols="2">35.3 21.4</cell><cell>5.3</cell><cell>21.7</cell></row><row><cell>A2CL-PT [27]</cell><cell cols="2">36.8 22.0</cell><cell>5.2</cell><cell>22.5</cell></row><row><cell>AUMN</cell><cell cols="2">38.3 23.5</cell><cell>5.2</cell><cell>23.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies on the THUMOS14 dataset, where Ls, L d , L h denote the sparsity loss, the diversity loss and the homogeneity loss. Here, S denotes the self-attention module.</figDesc><table><row><cell cols="4">Ls L d L h S</cell><cell>mAP@IoU 0.1 0.2 0.3 0.4 0.5 0.6 0.7 AVG</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-58.5 53.1 45.1 34.9 24.6 14.4 6.8 43.2</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">-65.1 59.8 51.5 40.9 28.8 16.3 7.3 49.2</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="2">-65.8 61.0 52.6 42.2 29.5 17.0 7.6 50.2</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell cols="2">-65.5 60.9 51.7 41.3 29.4 17.1 7.7 49.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">-66.1 61.5 54.4 43.3 31.8 19.1 8.9 51.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.2 61.9 54.9 44.4 33.3 20.5 9.0 52.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If there are multiple action categories in one video, y i is normalized with the 1 normalization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning temporal co-attention models for unsupervised video action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9819" to="9828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Relational prototypical network for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The thumos challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-agent event detection: Localization and role assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11320" to="11327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
	<note>Nanning Zheng, and Gang Hua</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Weakly-supervised action localization with expectation-maximization multiinstance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00163</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial background-aware loss for weakly-supervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shabaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08216</idno>
		<title level="m">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Phuc Xuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06552</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wtalc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1529" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716</idno>
		<title level="m">A pursuit of temporal accuracy in general activity detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07460</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Exploring temporal preservation networks for precise temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohe</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03280</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncertainty guided collaborative training for weakly supervised temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Local correspondence network for weakly supervised temporal sentence grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal structure mining for weakly supervised action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5522" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03252</idno>
		<title level="m">Graph convolutional networks for temporal action localization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Two-stream consensus networks for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial seeded sequence growing for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="738" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Step-by-step erasion, one-by-one collection: A weakly supervised temporal action detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
