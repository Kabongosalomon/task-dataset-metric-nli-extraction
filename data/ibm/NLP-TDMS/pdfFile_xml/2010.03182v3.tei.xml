<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyeon</forename><forename type="middle">Caren</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>1 Cleveland Street</addrLine>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqu</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>1 Cleveland Street</addrLine>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwen</forename><surname>Luo</surname></persName>
							<email>siwen.luo@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>1 Cleveland Street</addrLine>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunze</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>1 Cleveland Street</addrLine>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Poon</surname></persName>
							<email>josiah.poon@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>1 Cleveland Street</addrLine>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image multimodal tasks, generating/retrieving an image from a given text description, are extremely challenging tasks since raw text descriptions cover quite limited information in order to fully describe visually realistic images. We propose a new visual contextual text representation for text-to-image multimodal tasks, VICTR, which captures rich visual semantic information of objects from the text input. First, we use the text description as initial input and conduct dependency parsing to extract the syntactic structure and analyse the semantic aspect, including object quantities, to extract the scene graph. Then, we train the extracted objects, attributes, and relations in the scene graph and the corresponding geometric relation information using Graph Convolutional Networks, and it generates text representation which integrates textual and visual semantic information. The text representation is aggregated with word-level and sentencelevel embedding to generate both visual contextual word and sentence representation. For the evaluation, we attached VICTR to the state-of-the-art models in text-to-image generation.VICTR is easily added to existing models and improves across both quantitative and qualitative aspects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decade, deep learning has achieved remarkable success in multimodal research problems, such as visual question answering, visual dialog, and image captioning. However, it is quite challenging to solve the text-to-vision multimodal tasks that deal with a text input and produce a visual output, such as text to image generation, text to video generation, or text to image retrieval. Generally, natural language (incl. text) is a more natural medium for a human to describe the image that they want to generate or retrieve. However, raw text includes only limited information to fully describe and represent an image.</p><p>For example, text to image generation tasks aim to generate photo-realistic images according to the given text descriptions. The current state-of-the-art (SOTA) text-to-image generation models <ref type="bibr" target="#b14">(Xu et al., 2018;</ref><ref type="bibr" target="#b16">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Zhu et al., 2019)</ref> mainly focus on generating the high resolution images by applying generative adversarial networks (GAN) <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> and rather neglect understanding input text descriptions. The most common text encoding approach in those SOTA models applies Recurrent Neural Networks (RNN) to extract global sentence-level and word-level embedding, and the last hidden state of RNN cells is used as a direct input to the GAN image generation model. However, RNNbased text encoding approaches are not capable of fully representing the rich visual semantics of input text descriptions in order to describe or generate photo-realistic visual output (e.g. image). For example, from the given sample text caption, A man on a skateboard with a brown dog outside, the last hidden state of RNN cells holds semantics and order of words and stores words' importance in the sentence. However, those information do not include most of the required information to describe/generate the image; for example, what objects are in the image (aspect of objects)? Where are those objects (position of objects)? How to represent the relations between objects (relation between objects)? OR for example, what objects are in the image (aspect of objects); where are those objects (position of objects); how to represent the relations between objects (relation between objects)?</p><p>The question is: What would be the best approach to extract the rich visual semantics of input text descriptions in order to describe/generate an image? In this paper, we introduce a new Visual Contextual Text Representation (VICTR: Visual Information Captured Text Representation), which represents the input text description with its visual semantics, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The proposed model, VICTR, can be applied to diverse text-to-image multimodal tasks.</p><p>VICTR has five different modules: 1) Text to Scene Graph Parsing, 2) Scene Graph Embedding, 3) Positional Graph Embedding, 4) Visual Semantic embedding, 5) Visual Contextual Text Representation. First, we extract scene graphs from input text descriptions in order to define what objects, attributes and relations should be in the image. The scene graph is initially proposed by <ref type="bibr" target="#b5">Johnson et al. (2015)</ref> in order to represent the objects, its attributes and their relations in the image. Inspired by this, we generate scene graphs from the raw text description by using dependency parsing and transformer-based object-attribute-relation classification. Then, we train the extracted object, attribute and relation nodes via Graph Convolutional Networks (GCN) in order to generate the visual contextual text representation, a word-level representation which incorporates textual syntactic and visual semantic information. Finally, it aggregates with word-level and sentence-level embedding respectively in order to generate a visual contextual word representation and visual contextual sentence representation.</p><p>For evaluating quantitative and qualitative aspects of our proposed model, we attach VICTR to the SOTA models in text-to-image generation. Thorough experiments on the COCO benchmark dataset demonstrate the superiority of VICTR with respect to both semantic consistency and visual reality. The main contribution is summarised in the end of the Sec. 2.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works and Contributions</head><p>We explore research trends in diverse text-to-vision multimodal tasks, which use text information as an only input to produce a visual output, including text to image generation and text to video generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text to Image Generation</head><p>From 2016, text to image generation tasks have been explored by applying conditional GANs <ref type="bibr" target="#b10">(Reed et al., 2016;</ref><ref type="bibr" target="#b16">Zhang et al., 2017;</ref> with the text caption as input. AttnGAN <ref type="bibr" target="#b14">(Xu et al., 2018)</ref> is the first method that utilized a word-level embedding and fused it with image vectors in an attention mechanism to identify the contributing words of sub-regions in the generated images. Extended from AttnGAN, MirrorGAN <ref type="bibr" target="#b9">(Qiao et al., 2019)</ref> applied the same attention mechanism on both sentence and word embedding to capture the global semantic consistency between generated images and input texts. SEGAN <ref type="bibr" target="#b12">(Tan et al., 2019)</ref> proposed an adaptive attention mechanism on word-level embeddings to ensure only relevant words on the generated images would obtain attention-weight. SD-GAN  with a Siamese structure is utilized to guarantee the semantic alignment between generated images and captions. DM-GAN <ref type="bibr" target="#b18">(Zhu et al., 2019)</ref> proposed a dynamic memory network to fuse word embedding and image representations for image generation. Most SOTA models applied bidirectional LSTM (RNN)-based text encoding, which contains only information that represents the order of the words and the words' importance in the given text caption. The output of the RNN-based text encoding is not enough to represent rich visual semantics in order to directly generate the image, hence, the images from current SOTA models are not successful in aligning with the given text description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text to Video Generation</head><p>Similar to text to image tasks, most text to video generation models learn the caption via RNN cells as conditional input representation to be used with Variational Autoencoders (VAE) or GANs in order to generate image frames. The main difference is that video generation considers how to model the temporal dependency by conditioning on the corresponding text captions. Sync-DRAW <ref type="bibr" target="#b7">(Mittal et al., 2017)</ref> applies the recurrent attention-based VAE to create a temporally dependent sequence of frames but it still applies LSTM for input text encoding. GAN-based text to video generation approaches also apply RNN-based encoding to handle the input text captions. TGANs-C <ref type="bibr" target="#b8">(Pan et al., 2017)</ref> utilises GANs with three discriminators to generate video based on input text captions encoded by Bi-LSTM. IRC- GAN <ref type="bibr" target="#b2">(Deng et al., 2019)</ref> proposed a Mutual-information Introspection (MI) that measures the semantic similarity between text and generated video through a two-stage process. The conditional text input is represented through Bi-LSTM network. TFGAN <ref type="bibr" target="#b0">(Balaji et al., 2019)</ref> applies a scheme via generating discriminative convolutional filters from text features and then convolves them with image features in the discriminator. It applies a CNN (Convolutional neural network)-based text encoder but it still does not represent the sufficient visual semantics from text captions in order to generate the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Main contribution</head><p>Most existing models for text-to-image and text-to-video generation tend to have a RNN or CNN-based sentence feature from the raw text for modeling the cross-model relation with the generated visual output. Hence, we now present our model, VICTR, the successful approach to extract the rich visual semantics of input text descriptions in order to describe/generate an image. The proposed VICTR is evaluated with text-to-image generation tasks. The model with VICTR outperforms the performance of original SOTA models in photo-realistic image generation based on text input. The major contributions of this work are summarised as follows: 1) The paper provides an example of capturing rich visual semantic and geometric relation information from raw text input. 2) The paper proposes a new visual information captured text representation for text-to-image generation tasks, which has not been reported before. The proposed text representation model can be usable with any text-to-vision multimodal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the proposed visual contextual text representation, VICTR (Visual Information Captured Text Representation), mainly focuses on capturing and representing the visual semantic information (i.e. location or aspect of the object in the image, positional relation between objects) from raw text descriptions. This is crucial for text-to-image generation tasks. In summary, the architecture of the proposed VICTR is composed into five modules: 1) Text to Scene Graph Parsing, 2) Scene Graph Embedding, 3) Positional Graph Embedding, 4) Visual Semantic Embedding, 5) Visual Contextual Text Representation. Note that the first module text to scene graph can be considered as a pre-processing step to generate visual information embedded text representation for the rest of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text to Scene Graph Parsing</head><p>Based on the given raw text description, including image caption or scene description, we firstly extract a graph-based semantic representation, called scene-graph <ref type="bibr" target="#b5">(Johnson et al., 2015)</ref>, which explicitly repre-sents object instances, their attributes, and the relation between objects. This simple graph representation describes visual scenes/images in great detail. Inspired by this idea, we generate scene graphs based on input text descriptions. Like the nature of text-to-image generation tasks, we use only text descriptions in order to extract the scene graph with rich visual semantics (objects, attributes, relations of the image). In order to parse scene graphs from the given text caption, we firstly recognise the syntactic structure of the text descriptions by applying a universal dependency parser; in this research, we applied the Stanford enhanced dependency Parser <ref type="bibr" target="#b1">(Chen and Manning, 2014)</ref>. However, the output of a dependency parser would not be enough to directly represent the number of objects (as well as its attributes and relations between objects) that should be drawn in the scene graph. Hence, we have a semantic enhancement processing component, quantity checker. The quantity checker aims to detect the number of objects that the scene graphs need to include. For example, the following two text captions two men are riding brown horses and two men are riding a brown horse include different semantic information: the former would have two man objects and two brown horse objects but the latter could contain two man objects and one brown horse object. We duplicate the individual nodes in the dependency graph according to the value of their quantificational modifiers. In addition to this, we also cover some quantificational determiners by using the quantifier expression rule list, such as both of, a dozen of, or a lot of. From the syntactic and semantic integrated graph, we extract all nouns to classify into object classes, and retrieve all adjectives as attribute types of the specific object (pairwise classification). The relation between objects is detected if the word is the predicate or preposition of two different objects.</p><p>As a result, each text caption of an image can derive one scene graph</p><formula xml:id="formula_0">G (O, R, A), where O = {o 1 , o 2 , ..., o n } is a set of objects, R = {r 1 , r 2 , ..., r m } is a set of relations, and A = {a 1 , a 2 , ..., a k } is a set of attributes. Especially, each object o i ∈ O is assigned with a super-class 1 t ∈ T = {t 1 , t 2 , ..., t µ }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scene Graph Embedding</head><p>We convert the extracted scene graph to a vectorised graph representation to produce useful feature representations of nodes and edges in the object-attributes-relation networks. We apply GCNs to model the relative nearness of nodes and edges in the scene graph and preserve the visual semantics.</p><p>The basic relational graph G b represents visual semantic alignment between object and relation as well as object and attribute in scene graphs. We train the graph using GCNs to produce scene graph embeddings. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, each object o i ∈ O, relation r i ∈ R, and attribute a i ∈ A is made as a node of the graph G b . Then the object-to-relation connection and relation-to-object connection are represented as edges e o i →r j and edges e r j →o i respectively. Similarly, edge e o i →at indicates the objectattribute alignment. For edge e o i →r j , e r j →o i , and e o i →at , the weight is calculated based on the equations:</p><formula xml:id="formula_1">W eo i →r j = number of e o i →r j number of e o i →R<label>(1)</label></formula><p>W er j →o i = number of e r j →o i number of e r j →O (2)</p><formula xml:id="formula_2">W eo i →a t = number of e at→o i number of e at→O<label>(3)</label></formula><p>The edge weight to the node itself would be 1. The edge weights are compiled into an adjacency matrix combined with the graph degree matrix and are passed into a 2-layer GCN to be trained through mapping each object to its corresponding super-class. We denote node embeddings for an object, an attribute and a relation as EB o , EB a , EB r ∈ R B .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Positional Graph Embedding</head><p>In section 3.2, the scene graph-based basic relational graphs mainly focus on the semantic relations (predicates or preposition) between objects, e.g. ride from the text description man rides a horse. It provides the lingual semantics of objects and relations but it is not still enough to fully describe the image with geographical information, such as location of objects or the relative position (e.g. left to) between objects, which includes an indicative and explicit location of one object in relation to another. Hence, we propose a position-enhanced relational graph G p that focuses on visual semantics of relations between objects. Six relative geometric relations are chosen and denoted as p ∈ {left of, right of, above, below, inside, surrounding} to represent edges e o i →r j and edges e r j →o i . To train these edges, the geometric relation is detected based on the gap between bounding boxes of one to another object. Considering that one object may correspond to multiple geometric relations, we generate individual graphs for each type of geometric relations. The weights of edges e o i →r j and edges e r j →o i in a graph G p of six geometric indicators are calculated as those in the basic relational graph. For each graph, the edge weights are compiled into an adjacency matrix combined with the graph degree matrix, and passed into a 2-layer GCN to train each object with its corresponding super-class. The object-level and relation-level node embedding in each of the six position-enhanced relational graphs are concatenated and produces the positional object-level node embedding EP o ∈ R P and relation-level node embedding EP r ∈ R P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visual Semantic Embedding</head><p>We now integrate object, relation, attribute-level embeddings from basic relational graphs and position enhanced graphs in order to produce the comprehensive visual semantic embedding for scene graphs. The visual semantic embedding is composed to three aspects: 1) Object-level embedding E o ∈ R B+P : concatenate EB o and EP o of object o i ∈ O, 2) Relation-level embedding E r ∈ R B+P : combine EB r and EP r for each relation r j ∈ R, 3) Attribute-level embedding E a = EB a ∈ R B . For each object in one scene graph, the object embedding is concatenated with its attribute embedding as well as the corresponding relation embedding to produce the final visual semantic embedding E vs ∈ R 2 * (B+P )+B . Based on the produced final visual semantic embedding, we now visualise the ability of the proposed embedding model to automatically organise different aspects of objects and learn implicitly the relations between them. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the visual semantic vectors of diverse objects and the positionenhanced relation vectors that appear frequently with those objects from the COCO2014. In <ref type="figure" target="#fig_1">Figure 2(a)</ref>, animal objects cat, dog are close to each other while being far away from the electronics objects, such as mouse and TV. These electronics objects are close to the relation place because it is commonly used with them instead of the relation sit or stand. Similarly, the relations park, dock are close to the group of vehicle objects truck, boat, train but far from the kitchenware object cluster in <ref type="bibr">Figure2(b)</ref>. This pattern can be also found in Figure2(c) as it is shown that the food objects are gathered together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Visual Contextual Text Representation</head><p>The proposed visual semantic embedding strongly integrates the semantic information of an object with its attributes and the relations attached to it as well as the positional (geometric) relations between the object and others. In order to seamlessly grain this visual semantic information into the text represen-tation, we integrate it into the word and sentence representation from raw text using attention mechanism. The attention mechanism is applied between the E vs and the corresponding text word embedding E word ∈ R L×D , which is derived from the text encoder with the sequence length L and the dimension of word embedding D. The attention is inspired by <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref>, and we made E vs as both K and V while taking E word as Q:</p><formula xml:id="formula_3">Attention (Q(E word ), K(E vs ), V (E vs )) = softmax (W T E word )E T vs E vs (4)</formula><p>Here W is a learnable weight to map the word representation to the visual semantics space. The attended visual semantic embedding E vs ∈ R L×2 * (B+P )+B represents the importance of each object-based visual semantic information to each word in the sequence of a text caption. The E vs is concatenated with the word embedding E word to get the visual contextual word representation E V ICT R−W . Similarly, the object-information over all the words are summed up via E vs , and then concatenated with the sentence embedding E sent ∈ R D derived from the text encoder to get the visual contextual sentence representation E V ICT R−S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Setup</head><p>Baselines Three text-to-image generation SOTA models, StackGAN <ref type="bibr" target="#b16">(Zhang et al., 2017)</ref>, AttnGAN <ref type="bibr" target="#b14">(Xu et al., 2018)</ref>, and DM-GAN <ref type="bibr" target="#b18">(Zhu et al., 2019)</ref> were selected as baselines, which all used text representation as the only source for image generation. We replaced the original text representation with our proposed VICTR and compared the generated images.</p><p>Dataset We evaluated the model performance on COCO2014 <ref type="bibr" target="#b6">(Lin et al., 2014)</ref> 2 , which is the most common benchmark and contains photo-realistic images with diverse objects and relations. Detailed dataset statistics are shown in <ref type="table" target="#tab_1">Table 1</ref>. Each image has 5 corresponding image descriptions and we selected the caption which generates the richest scene graph. We used bounding box features to train the geometric relations of multiple objects.   <ref type="table" target="#tab_2">Table 2</ref>: Inception score (IS), Fréchet Inception Distance (FID), R-precision results of VICTR-based models comparing to all baseline models. Evaluation Metrics We use Inception Score (IS), Frechét Inception Distance (FID) and R-precision to quantitatively evaluate the model performance on 30,000 generated images 3 . IS <ref type="bibr" target="#b11">(Salimans et al., 2016)</ref> uses Kullback-Leiber (KL) divergence to compare the similarity between each generated image label probability distribution and the marginal probability distribution of all generated images, the higher the IS, the better the model is to generate diverse and distinct images. FID <ref type="bibr" target="#b4">(Heusel et al., 2017)</ref> is an improved version of IS, comparing the Frechét distance between the maximum entropy distribution of the generated images and the real images. The lower the FID, the more similar the generated images to the real images. R-precision measures the consistency between the generated image and the input text. We followed <ref type="bibr" target="#b14">Xu et al. (2018)</ref> and set R = 1, comparing the cosine similarity between generated image vector and input text embedding to find the top r captions that are relevant to images and calculate R-precision as r/R. The final score is taken as the average of R-precision of all images, the higher the score, the better consistency between generated images and captions. For AttnGAN and DM-GAN, we applied the VICTR-S feature in the initial image generation and VICTR-W feature for the iterative refinement. There is a clear improvement of all three metrics for both models with VICTR. The improvement in FID shows that using visual semantic relations between objects actually helped to form a group of objects in the geographically simi- lar position to those in the ground truth images. Moreover, VICTR was mined from the original text and aligned the lingual semantics and visual semantics in the image captions. It helps the model to generate images which are better aligned to the text captions and leads to the increase of R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Comparison</head><p>The visual comparisons between three SOTA baselines, and those with the proposed VICTR are presented in <ref type="figure" target="#fig_2">Figure 3</ref>. There are several findings from the visual comparison: firstly, with VICTR, images show a clearer structure (appearance of objects and their relative positions) and are also closer to the ground-truth than those generated by the original SOTA model. For example, the column 3 in <ref type="figure" target="#fig_2">Figure  3</ref>(a) generated by StackGAN with VICTR has the similar structure of ground truth image that each object beach, ocean, and sky are positioned from the bottom to top, as well as a kite flying in the sky and a man standing in the beach. Secondly, compared to images from original models, the VICTR-based images provide clearer object shapes so the objects are relatively easier to be recognised, (i.e. food, plate, sheep, cat and human from column 1,2 in <ref type="figure" target="#fig_2">Figure 3</ref>(a), column 1,2 in <ref type="figure" target="#fig_2">Figure 3</ref>(b) and column 1 in <ref type="figure" target="#fig_2">Figure 3</ref>(c)). Moreover, VICTR supports the model to well-understand contents in the text caption: 1) more objects from the text are identified in the image (e.g. the object cat from column 2 in <ref type="figure" target="#fig_2">Figure 3</ref>(b) and the man/parasail from column 1 in <ref type="figure" target="#fig_2">Figure 3</ref>(c) are completely missing without VICTR). 2) VICTR is good at handling quantifiers into individual objects. a flock of sheep are well captured by VICTR at column 1 in <ref type="figure" target="#fig_2">Figure 3(b)</ref> where the original model failed to identify the number of objects. 3) even when the ground-truth image does not match with the caption, the VICTR-based models can generate images that are consistent with the caption, shown in column 3 and 2 in in <ref type="figure" target="#fig_2">Figure 3(b)</ref> and (c) respectively. <ref type="figure" target="#fig_3">Figure 4</ref> indicates that VICTR-based model is able to generate better initialised images and refine them to be more related to the given text caption. In the baseline models, the initial stage image generation with the sentence-level feature captures the major frame or very rough appearance of objects identified from the text, whereas the image refinement process only focuses on the word-feature to polish the initial image but makes no major scene changes. From the images generated by original models, we found that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study -Cascaded Generators</head><p>(1) the sentence-level feature from the Bi-LSTM encoder at the initial stage is not enough to produce the precise main image structure as described in the original text caption, so the models tend to create mistakes; and (2) the word feature in the following refinement process is not enough to amend these mistakes (from the initial stage), which limits the quality of final images. For example, the 2nd caption in <ref type="figure" target="#fig_3">Figure 4</ref>(a) and 4(b) describes two kites sailing in the sky. However, both the initial images generated by the original AttnGAN and DM-GAN capture only one kite in the sky and this error propagates to the final image. In comparison, in the images generated with VICTR, two kites appeared from the initial stages, which matches the caption well and this persists all the way to the final image generation. Similar pattern can be found from the 1st caption in <ref type="figure" target="#fig_3">Figure 4(a)</ref> where the original AttnGAN fails to well capture the positional relationship in the background between the object street and buildings as well as the 1st caption in <ref type="figure" target="#fig_3">Figure 4</ref>(b) from which the object train and water are not drawn clearly and not well positioned in relation to each other in the initial image, leading to the low quality of final image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Refinement Attention Inspection</head><p>We visualise the parsed scene graph, and the intermediate images and attention maps of each refinement stage in <ref type="figure" target="#fig_4">Figure 5</ref>. Several improvements can be observed in the word-image attention that better reflects the visual-linguistic alignment of objects and their positions: 1) The model with VICTR can focus on the more relevant and important object region in the image while using the corresponding word feature for the refinement. For example, in <ref type="figure" target="#fig_4">Figure 5</ref>(a), the model with VICTR highlights the words a, couple and elephant to generate two elephants in the image whereas the original models do not. The similar pattern can be found with the words flower and tree in <ref type="figure" target="#fig_4">Figure 5(b)</ref>.</p><p>2) The positional relation attention represents a semantically meaningful visual context alignment on the linguistic relation expressed in the text description. This can be easily observed by the attention of words standing in from <ref type="figure" target="#fig_4">Figure 5(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Human Evaluation</head><p>We conducted a human evaluation with 50 participants to qualitatively evaluate VICTR in the consistency between generated images and captions. The results and examples are in the Appendix 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a new visual contextual text representation for text-to-image multimodal tasks, called VICTR, which extracts rich visual semantic information from input text descriptions. We have shown improvement across both quantitative and qualitative aspects when applying VICTR to diverse SOTA models in text-to-image generation. We also present an analysis showing the ability of VICTR to automatically organise different aspects of objects and learn the relations between them. The human evaluation results show that VICTR produces images that are highly aligned with text captions and very realistic. It is hoped that VICTR provides the insight into future integration of text handling in text-tovision multimodal tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of the proposed visual contextual text representation (VICTR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two-dimensional PCA projection of 1200-dimensional visual semantic vectors of objects and 500-dimensional position-enhanced vectors of relations. The figures illustrate ability of the model to automatically organise concepts and learn implicitly the similarity between them, as during the training we did not provide any supervised information but only the text description of images in COCO2014.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) StackGAN VS StackGAN-VICTOR (b) AttnGAN VS AttnGAN-VICTOR (c) DMGAN VS DMGAN-VICTOR Examples of images generated by (1st row) original StackGAN, AttnGAN, DM-GAN models, (2nd row) each model with VICTR, and (3rd row) the corresponding ground truth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The result of 3 different stages of the original AttnGAN/DM-GAN and the AttnGAN-VICTR/DM-GAN-VICTR, including initial 64x64 image generation (1st row), the iterative refinement 128x28 images (2nd row) and 256x256 images (3rd row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Parsed Scene Graphs and Attention visualisation on the COCO2014. The first row shows the output 64x64, 128x128, and 256x256 images. The following rows show the attention map generated in stage 1 and 2 by the original AttnGAN/DM-GAN and with VICTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>COCO2014 Statistics<ref type="bibr" target="#b6">(Lin et al., 2014)</ref> Implementation Details We used 2-layer GCN with the hidden layer of dimension 200 and 50 for each basic relational graph and position-enhanced graph, and trained the GCNs with learning rate 0.02 for 200 epoch. The number of trainable parameters for the basic relational graph GCN is 17,258,800 while for the six position-enhanced GCN are 492,200 (left of), 488,400 (right of), 632,000 (above), 657,200 (below), 63,800 (inside) and 54,600 (surrounding) respectively. The final hidden state is used as embedding and the concatenated E vs is 1200d. For AttnGAN and DM-GAN, we used their word feature E word (256d) and the sentence feature E sent (256d) extracted by the pre-trained DAMSM text encoder and applied the attention with E vs . We replaced the derived E V ICT R−W and E V ICT R−S with the initial word feature E word and sentence feature E sent respectively in the original models. We kept all the best-performance configuration as it is from the three original models for comparison. We trained the StackGAN with VICTR on stage-I for 600 epoch as the same in the stackGAN paper and trained the StackGAN with VICTR on stage-II and the other two models (AttnGAN VICTR and DM-GAN VICTR) for 150 epoch. StackGAN-VICTR has the same number of trainable parameters as StackGAN, which are 32,735,457 for stage-I and 197,327,218 for stage-II. For AttnGAN-VICTR and DM-GAN-VICTR, there are 256,168,838 and 44,154,260 respectively for the GAN part and 614,400 for the weight W . All experiments are conducted on 24 GB NVIDIA TITAN RTX GPU with 10.2 CUDA. With our environment,</figDesc><table><row><cell>Methods</cell><cell>IS↑</cell><cell>FID↓</cell><cell>R-precision↑</cell></row><row><cell>StackGAN</cell><cell>8.45 ± .03</cell><cell>-</cell><cell>-</cell></row><row><cell>StackGAN with VICTR</cell><cell>10.38 ± .20</cell><cell>-</cell><cell>-</cell></row><row><cell>AttnGAN</cell><cell>25.89 ± .47</cell><cell>35.49</cell><cell>85.47 ± 3.69</cell></row><row><cell>AttnGAN with VICTR</cell><cell>28.18 ± .51</cell><cell>29.26</cell><cell>86.39 ± .0039</cell></row><row><cell>DM-GAN</cell><cell>30.49 ± .57</cell><cell>32.64</cell><cell>88.56 ± 0.28</cell></row><row><cell>DM-GAN with VICTR</cell><cell>32.37 ± .31</cell><cell>32.37</cell><cell>90.37 ± .0063</cell></row></table><note>it took around 24(21)min/epoch on stage-I and 45(40)min/epoch on stage-II training for StackGAN with VICTR (StackGAN), 57(51)min/epoch for AttnGAN with VICTR (AttnGAN) and 60(56)min/epoch for DM-GAN with VICTR (DM-GAN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows the performance of IS, FID and R-precision of the SOTA models, and the corresponding improvement with VICTR. Applying the VICTR-S feature in StackGAN improved the overall IS by around 1.93, which indicates the higher quality of final generated images. Specifically, the original StackGAN achieved 8.45 on IS with 600 epochs on stage-II, while the model with VICTR outperformed the original model at only 130 epochs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Super-classes are from the COCO or Mediawiki(https://wiki.dbpedia.org/) can be extracted the parent class of each object (a) animals and electronics (b) kitchen-wares and vehicles (c) foods and sports</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">COCO2014 http://cocodataset.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code of three quantitative evaluation metrics are from: https://github.com/MinfengZhu/DM-GAN (a) AttnGAN VS AttnGAN-VICTR (b) DMGAN VS DMGAN-VICTR</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) AttnGAN VS AttnGAN-VICTR (b) DMGAN VS DMGAN-VICTR</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Available at: https://usydnlp.info/victr_coling2020_appendix/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional gan with discriminative filter generation for text-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1995" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Irc-gan: introspective recurrent convolutional gan for text-to-video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangle</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2216" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sync-draw: automatic video generation using deep recurrent attentive architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">To create what you tell: Generating videos from captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantics-enhanced adversarial nets for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10501" to="10510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchicallynested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6199" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
