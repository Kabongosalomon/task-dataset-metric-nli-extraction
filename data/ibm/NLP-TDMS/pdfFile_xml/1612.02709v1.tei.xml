<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Ground-Level Scene Layout from Aerial Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Bessinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
							<email>jacobs@cs.uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Ground-Level Scene Layout from Aerial Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image.</p><p>We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geoorientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning-based methods for pixel-level labeling of aerial imagery have long relied on manually annotated training data. Unfortunately, such data is expensive to create. Furthermore, its value is limited because a method trained on one dataset will typically not perform well when applied to another source of aerial imagery. The difficulty in obtaining datasets of sufficient scale for all modalities has hampered progress in applying deep learning techniques to aerial imagery. There have been a few notable exceptions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>, but these have all used fairly coarse grained semantic classes, covered a small spatial area, and are limited to modalities in which human annotators are able to manually assign labels.</p><p>We propose a novel strategy for obtaining semantic la-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Entropy Loss</head><p>Label Extract Transform Transfer Semantics <ref type="figure">Figure 1</ref>. We learn to predict the ground-image segmentation directly from an aerial image of the same location, thereby transferring the semantics from the ground to the aerial image domain. bels for aerial image segmentation. See <ref type="figure">Figure 1</ref> for a schematic overview of the approach. Our idea is to use existing methods for semantic image segmentation, which are tailored for ground images, and apply these to a large dataset of geo-tagged ground images. We use these semantically labeled images as a form of weak supervision and attempt to predict these semantic labels from an aerial image centered around the location of the ground image. We do not use a parametric transformation between the aerial and ground-level viewpoints. Instead, we use a dense representation, similar in spirit to the general representation, dubbed filter flow, described by Seitz and Baker <ref type="bibr" target="#b25">[26]</ref>.</p><p>There has been significant interest recently in predicting ground image features from aerial imagery for the task of ground image geolocalization <ref type="bibr" target="#b32">[33]</ref>. Our work is unique in that it is the first to attempt to predict a dense pixel-level segmentation of the ground image. We demonstrate the value of this approach in several ways.</p><p>Main Contributions: The main contributions of this work are: (1) a novel convolutional neural network (CNN) architecture that relates the appearance of a aerial image appearance to the semantic layout of a ground image of the same location, <ref type="bibr" target="#b1">(2)</ref> demonstrating the value of our training strategy for pre-training a CNN to understand aerial imagery, (3) extensions of the proposed technique to the tasks of ground image localization, orientation estimation, and synthesis, and (4) an extensive evaluation of each of these techniques on large, real-wold datasets. Together these represent an important step in enabling deep learning techniques to be extended to the domain of aerial image understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning Viewpoint Transformations Many methods have been proposed to represent the relationship between the appearance of two viewpoints. Seitz and Baker <ref type="bibr" target="#b25">[26]</ref> model image transformations using a space-variant linear filter, similar to a convolution but varying per-pixel. They highlight that a linear transformation of a vectorized representation of all the pixels in an image is very general; it can represent all standard parametric transformations, such as similarity, affine, perspective, and more. More recently, Jaderberg et al. <ref type="bibr" target="#b11">[12]</ref> describe an end-to-end learnable module for neural networks, the spatial transformer, which allows explicit spatial transformations (e.g. scaling, cropping, rotation, non-rigid deformation) of feature maps within the network that are conditioned on individual data samples. Practically, including a spatial transformer allows a network to select regions of interest from an input and transform them to a canonical pose. Similarly, Tinghui et al. <ref type="bibr" target="#b33">[34]</ref> address the problem of novel view synthesis. They observe that the visual appearance of different views is highly correlated and propose a CNN architecture for estimating appearance flows, a representation of which pixels in the input image can be used for reconstruction.</p><p>Relating Aerial and Ground-Level Viewpoints Several methods have been recently proposed to jointly reason about co-located aerial and ground image pairs. Luo et al. <ref type="bibr" target="#b18">[19]</ref> demonstrate that aerial imagery can aid in recognizing the visual content of a geo-tagged ground image. Máttyus et al. <ref type="bibr" target="#b19">[20]</ref> perform joint inference over monocular aerial imagery and stereo ground images for fine-grained road segmentation. Wegner et al. <ref type="bibr" target="#b29">[30]</ref> build a map of street trees. Given the horizon line and the camera intrinsics, Ghouaiel and Lefèvre <ref type="bibr" target="#b6">[7]</ref> transform geo-tagged groundlevel panoramas to a top-down view to enable comparisons with aerial imagery for the task of change detection. Recent work on cross-view image geolocalization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> has shown that convolutional neural networks are capable of extracting features from aerial imagery that can be matched to features extracted from ground imagery. Vo et al. <ref type="bibr" target="#b28">[29]</ref> extend this line of work, demonstrating improved geolocalization performance by applying an auxiliary loss function to regress the ground-level camera orientation with respect to the aerial image. To our knowledge, our work is the first work to explore predicting the semantic layout of a ground image from an aerial image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation of Aerial/Satellite Imagery</head><p>There is a long tradition of using computer vision techniques for aerial and satellite image understanding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4]</ref>. Historically these two domains were distinct. Satellite imagery was typically lower-resolution, from a strictly topdown view, and with a diversity of spectral bands. Aerial imagery was typically higher-resolution, with a greater diversity of viewing angles, but with only RGB and NIR sensors. Recently these two domains have converged; we will use the term aerial imagery as we are primarily working with high-resolution RGB imagery. However, our approach could be applied to many types of aerial and satellite imagery. Kluckner et al. <ref type="bibr" target="#b15">[16]</ref> address the task of semantic segmentation using a random forest to combine color and height information. More recent work has explored the use of CNNs for aerial image understanding. Mnih and Hinton propose a CNN for detecting roads in aerial imagery <ref type="bibr" target="#b20">[21]</ref> using GIS data as ground truth. They extend their approach to handle omission noise and misregistration between the imagery and the labels <ref type="bibr" target="#b21">[22]</ref>. These approaches require either extensive pixel-level manual annotation or existing GIS data. Our work is the first to demonstrate the ability to transfer a dense pixel-level labeling of ground imagery to aerial imagery.</p><p>Visual Domain Adaptation Domain adaptation addresses the misalignment of source and target domains <ref type="bibr" target="#b5">[6]</ref>. A significant amount of work has explored domain adaptation for visual recognition <ref type="bibr" target="#b23">[24]</ref>. Jhuo et al. <ref type="bibr" target="#b12">[13]</ref> propose a low-rank reconstruction approach where the source features are transformed to an intermediate representation in which they can be linearly reconstructed by the target samples. Our work is most similar to that of Sun et al. <ref type="bibr" target="#b27">[28]</ref>, who propose a method for transferring scene categorizations and attributes from ground images to aerial imagery. Similar to our approach, they learn a transformation matrix which minimizes the distance between a source feature and the target feature. Our work differs in several ways: 1) we carry out the linear transformation not only in the semantic di-  <ref type="figure">Figure 2</ref>. A visual overview of our network architecture. We extract features from an aerial image using the VGG16 architecture and form a hypercolumn using the PixelNet approach. These features are processed by three networks that consist of 1 × 1 convolutions: network A converts the hypercolumn into semantic features; network S extracts useful features from the aerial image for controlling the transformation; and network F defines the transformation between viewpoints. The transformation is applied, T , to the aerial semantic features to create a ground-level semantic labeling.</p><p>mensions but also in the spatial dimensions, 2) we constrain the transformation matrix such that the semantic meaning of the source feature and the target feature remains the same, 3) our transformation matrix is input dependent, and 4) we learn the transformation matrix as well as the source feature at the same time, in an end-by-end manner, which simplifies training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross-view Supervised Training</head><p>We propose a novel training strategy for learning to extract useful features from aerial imagery. The idea is to predict the semantic scene layout, L g , of a ground image, I g , using only an aligned aerial image, I a , from the same location. This strategy leverages existing methods for ground image understanding at training time, but does not require any ground imagery at testing time.</p><p>We represent semantic scene layout, L g , as a pixel-level probability distribution over classes, such as road, vegetation, and building. We construct a training pair by collecting a georegistered ground panorama and an aerial image of the same location, orienting the panorama to the aerial image (panoramas are originally aligned with the road direction), and then extracting the semantic scene layout, L g , of the panorama using an off-the-shelf method <ref type="bibr" target="#b1">[2]</ref> with four semantic classes. We then use an end-to-end training strategy to learn to extract pixel-level features from the aerial image and transform them to the ground-level viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Our proposed network architecture is composed of four modules. A convolutional neural network (CNN), L a = A(I a ; Θ A ), is used to extract semantic labels from the aerial imagery. Another CNN, S(I a ; Θ S ), uses features extracted from aerial imagery to help estimate the transformation matrix, M = F (x r , y r , i c , j c , S(I a ; Θ S ); Θ F ), based on aerial image features and the pixel location in the respective images. Finally, we have a transformation module, L g = T (L a , M ), that converts from the aerial viewpoint to the ground-level using the estimated transformation matrix, M . There are many choices for these components, and the remainder of this section describes the particular choices we made for this study. See <ref type="figure">Figure 2</ref> for a visual overview of the architecture.</p><p>Aerial Image Feature Extraction For A(I a ; Θ A ), we use the VGG16 <ref type="bibr" target="#b26">[27]</ref> base architecture and convert it to a pixel-level labeling method using the PixelNet approach <ref type="bibr" target="#b2">[3]</ref>. The core idea is to interpolate intermediate feature maps of the base network to a uniform size, then concatenate them along the channels dimension to form a hypercolumn. In our experiments, we form the hypercolumn from conv-{1 2 , 2 2 , 3 3 , 4 3 } of the VGG16 network. The hypercolumn, which is now 256 × 256 × 960, is followed by three 1 × 1 convolutional layers, with 512, 512, and 4 output channels respectively. The first two 1 × 1 convolutions have ReLU activations, the final is linear. We designate the output of the final convolution as L a = A(I a ; Θ A ). The output of this stage is transformed from a aerial viewpoint to a ground viewpoint by final stage of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-view Semantic Transformation</head><p>We represent the transformation between the aerial and ground-level viewpoints as a linear operation applied channel-wise to L a . To transform from the h a × w a × 4 aerial label, L a , to the h g × w g × 4 ground label, L g , we need to estimate a h g w g × h a w a matrix, M . Given M , the transformation process is as follows: reshape the aerial label, L a , into a h a w a × 4 matrix, l a ; multiply it by M to get l g ; then reshape l g to the size of the ground label, L g , to form our estimate of the ground label, L g . We constrain M such that the sum of each row is unit. To account for the expected layout of the scene, and to handle the sky class (which is not visible from the aerial image), we carry out the transformation on the logits of l a , f a , and add a bias term, b to get the logits of l g , f g :</p><formula xml:id="formula_0">f g = M f a + b.</formula><p>There are many ways of representing the transformation matrix, M . The naïve approach is to treat M as a matrix of learnable variables. However, this approach has two downsides: (1) the transformation does not depend on the content of the aerial image and (2) the number of parameters scales quadratically with the number of pixels in L a and L g .</p><p>We represent each element, M rc , in the transformation matrix, M , as the output of a neural network, F , which is conditioned on the aerial image, I a , and the location in the input and output feature maps. More precisely, each element M rc = F (x r , y r , i c , j c , S(I a ; Θ S )), where (i c , j c ) ∈ [0, 1], is the aerial image pixel of the corresponding element, (y r , x r ) ∈ [0, 1] is the ground image pixel of the corresponding element.</p><p>We now define the architecture of the transformation estimation neural network, F . The value of the transformation matrix at location (r, c) is computed through a neural network,F , followed by a softmax function to normalize the impact of all pixels sampled from the aerial image:</p><formula xml:id="formula_1">M rc = F (r, c, S(I a ; Θ S )) = eF r,c c eF r,c ,</formula><p>where:F r,c =F (i, j, y, x, S(I a ; Θ S )) , and i = c/w a /h a , j = mod(c, w a )/w a , y = r/w g /h g , x = mod(r, w g )/w g .</p><p>The base network,F , is a multi-layer perceptron, with ReLU activation functions, that takes as input a 293element vector. The network has three layers, with 128, 64 and 1 output channels respectively (refer to the lower part of <ref type="figure">Figure 2</ref>). The naïve approach can be considered a special case of this representation where we ignore the aerial image and use a one-hot encoding representation of rows and columns.</p><p>As described above, there are two main advantages of our approach of representing the transformation matrix: a reduction in the number of parameters when M is large and the ability to adapt to different aerial image layouts. An additional benefit is that if we change the resolution of our input and output feature maps it is easy to create a new transformation matrix, M , without needing to resort to interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset</head><p>We collect our training and testing dataset from the CVUSA dataset <ref type="bibr" target="#b32">[33]</ref>. CVUSA contains approximately 1.5 million geo-tagged pairs of ground and aerial images from across the United States. We use the Google Street View panoramas of CVUSA as our ground images. For each panorama, we also download an aerial image at zoom level 19 from Microsoft Bing Maps in the same location. We filter out panoramas with no available corresponding aerial imagery. Using the camera's extrinsic information, we then warp the panoramas to align with the aerial images. We also crop the panoramas vertically to reduce the portion of the sky and ground pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>We implemented the proposed architecture using Google's TensorFlow framework <ref type="bibr" target="#b0">[1]</ref>. We train our networks for 10 epochs with the Adam optimizer <ref type="bibr" target="#b14">[15]</ref>. We enable batch normalization <ref type="bibr" target="#b10">[11]</ref> with decay 0.9 in all convolutional and fully-connected layers (except for the output layers) to accelerate the training process. Our implementation is available at https://github.com/viibridges/ crossnet.</p><p>The training procedure is as follows: for a given cross-view image pair, (I a , I g ), we first compute for the ground semantic pixel label: I g → L g , using SegNet <ref type="bibr" target="#b1">[2]</ref>. We then minimize the cross entropy between L g and T (A(I a ; Θ A ); Θ T ) with respect to the model parameters, Θ A and Θ T . The resulting architecture requires a significant amount of memory to output the full final feature map, which would normally result in very small batch sizes for GPU training. Due to the PixelNet approach of using interpolation to scale the feature maps, we are able to perform sparse training. Instead of outputting the full-size feature map, we only extract a dense grid of points, the resulting feature map is 17 × 17 × 4. Despite this, at testing time, we can provide an aerial image and generate a full-resolution, semantically meaningful feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and Applications</head><p>In this section, we will show that our network architecture can be used in four different tasks: 1) weakly supervised semantic learning, 2) aerial imagery labeling, 3) orientation regression and geocalibration, and 4) cross-view image synthesis. Additional qualitative results and the complete network structure used for cross-view image synthesis can be found in our supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Weakly Supervised Learning</head><p>We trained our full network architecture (with randomly initialized weights) to predict ground-level semantic labeling using the dataset described in Section 3.2. <ref type="figure" target="#fig_3">Figure 4</ref> shows example output, L a , from the aerial image understanding CNN. This demonstrates that the resulting network has learned to extract semantic features from an aerial image, all without any manual annotated aerial imagery.</p><p>While these results are compelling, they could be better with a higher quality ground-image segmentation method. The method we use, SegNet <ref type="bibr" target="#b1">[2]</ref>, was trained on mostly urban scenes, but many of our images are from rural and suburban areas. The end result is that certain classes are often mislabeled in the ground imagery, including dirt and building. In addition, because the panoramas and aerial images were not captured at the same time, we are unable to accurately model transient objects, such as vehicles and pedestrians. All these factors make the dataset very challenging for training. Given these limitations, it is, perhaps, surprising that the resulting aerial image segmentation method works so well. In the following section, we show using this network as a starting point for strongly supervised aerial image segmentation outperforms two standard initialization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-view for Pre-training</head><p>We evaluate our proposed technique as a pre-training strategy for the task of semantic-pixel labeling of aerial imagery. Starting from the optimal weights from the previous section, we finetune and evaluate using the ISPRS dataset <ref type="bibr" target="#b24">[25]</ref>. This dataset contains 33 true orthophotos captured over Vaihingen, Germany. The ground sampling distance is 9 cm/px and there are over 168 million pixels in total. Ground truth is provided for 16 photos; each pixel is assigned one of six categories: Impervious surfaces, Building, Low vegetation, Tree, Car, and Clutter/background. Image Processing Compared to the Bing Maps imagery we used for pre-training, images in the ISPRS dataset are at a different spatial scale and the color channels represent different frequency bands (the R channel is actually a near infrared channel). To ensure that the pre-trained network weights are appropriate for the new dataset, we adjusted the scale and color channels as follows. We first resize the IS-PRS images to the equivalent of Bing Maps zoom level 19. We then label a pixel as vegetation if R R+G+B is greater than 0.4. For each pixel labeled as vegetation, we halve the R channel intensity and swap the R and G channels. The resulting images, shown in <ref type="figure" target="#fig_4">Figure 5</ref>, are much closer in appearance to the Bing Maps imagery than the raw imagery.</p><p>Evaluation Protocol We split the 16 annotated images into training (images <ref type="bibr">5, 7, 11, 13, 15, 17, and 21)</ref>, validation sets (images 1 and 3) and testing (images <ref type="bibr">23, 26, 28, 30, 32, 37, and 40)</ref>. From each set we extracted a set of 224 × 224 subwindows (respectively 82, 12, and 34 from training, validation, and testing respectively). We then compared performance with different numbers of training images: 1, 2, 7, 20, 54, and 82. We evaluated the performance in terms of the average precision for all pixels. We ignore the Cluster/background pixels because of the low number of assigned pixels.</p><p>Training and Testing We used the same architecture with the aerial feature extractor, A(I a ; Θ A ), defined in Section 3.1, to do the semantic labeling on ISPRS. During training, we use the Adam optimizer to minimize the cross entropy between the network outputs and the labels. We use a batch size of 8, randomly sample 1,000 pixels per image for sparse training, and train the network till convergence. We run the validation set every 1,000 training iterations and save the optimal network weights for testing. During testing, we sample all pixels on the image to generate the dense labeling.</p><p>We experiment using three different initializations of the VGG16 convolutional layers and finetune the remaining layers of the network: 1) Ours: initialize with model pretrained using our framework; 2) Random: initialize using Xavier initialization <ref type="bibr" target="#b7">[8]</ref>; 3) VGG16: initialize with model pre-trained on ImageNet.</p><p>Since the VGG16 model we used in this experiment is trained without batch normalization, it may be less competitive. To achieve a fair comparison, we turned off batch normalization in this experiment and re-trained the network for 15 epochs to get the pre-trained model.</p><p>Our results ( <ref type="figure">Figure 6)</ref> show that finetuning from the VGG16 model performs poorly on the aerial image labeling task. We think that the patterns it learned mostly from the ground image may hinder pattern learning for aerial imagery. Our method outperforms both of the other initialization strategies. We also present the prediction precision per class in <ref type="table" target="#tab_0">Table 1</ref>. We highlight that our method does better especially on the Building, Low Vegetation, and Tree classes, which can also be found in pre-training annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cross-view for Geocalibration</head><p>We show how the ground-level feature maps we estimate from aerial imagery can be used to estimate the orientation Ours Random VGG16 <ref type="figure">Figure 6</ref>. Performance comparison of different initialization methods on the ISPRS segmentation task. The x-axis is the number of training images and the y-axis is average precision.  and location of a ground image. We show quantitative results for the orientation estimation task and qualitative results for simultaneous orientation and location estimation. We use the following datasets for all experiments: The discrete PDFs of the ground camera orientation are visualized with red arrows, whose lengths indicate the magnitudes. In the CVUSA results, the ground truth (green) and the optimal prediction (blue) are also shown with the orientation PDF. The last prediction result is a typical failure case of our method, where the scene is symmetric from the aerial point of view.</p><p>• CVUSA: We use the test set introduced in Section 3.2 to create this dataset, it has two parts for orientation estimation and geocalibration, respectively. For the orientation regression task, we rotate the aerial image to a random angle. For the fine-grained geocalibration experiment, we center-crop the aerial image around a random x, y offset, then rotate the image to a random angle. In both experiments, the heading direction of the ground images are the same. We center crop a 224 × 448 cutout from each ground image as the query image.</p><p>• Cityscapes: The Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> is a recently released benchmark dataset designed to support the task of urban scene understanding through semantic pixel labeling. It consists of stereo video from 50 different cities and fine pixel-level annotations for 5,000 frames and coarse pixel-level annotations for 20,000 frames.</p><p>Orientation Estimation For this task, we assume the location and focal length of the ground image, I g , is known but the orientation is not. The intuition behind our method is that the semantic labeling of the ground image will be most similar to the feature map of the aerial image at the actual orientation. For a query ground image, I g , the first step is to download the corresponding aerial image, I a . We then infer the semantic labeling of the query image, I g → L g , and predict the ground image label from the aerial image using our learned network, I a → L g . We assign an energy to each possible orientation by computing the cross entropy between L g and L g in a sliding window fashion across all possible orientations. We select the orientation with the lowest energy. We present sample results in <ref type="figure" target="#fig_7">Figure 7</ref> and a histogram of the orientation errors on the CVUSA dataset in <ref type="figure">Figure 8</ref>.</p><p>Fine-grained Geocalibration For this task, we assume that we know the focal length of the camera and have a rough estimate of the camera location (i.e., with 100 meters). We extract 256 × 256 aerial images from the area around our rough estimate and extract the corresponding ground-level feature maps. We apply our orientation estimation procedure to each feature map. The result is a distribution over orientations for each location. <ref type="figure">Figure 9</ref> shows several example results, including the most likely direction for each location, as well as the most likely location and orientation pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Synthesizing Ground Images from Aerial Images</head><p>We propose a novel application to infer a ground image by using features extracted from our network. We begin by describing our network structure and then show qualitative results for different generated ground-level scenes.</p><p>Our network architecture is based on the deep, directed generative model proposed by Kim et al. <ref type="bibr" target="#b13">[14]</ref>. Their model consists of two parts: a deep generator, G, which generates images that try to minimize a deep energy model, E. A low energy implies the image is real and high energy implies the image is fake. The architecture and training methods are inspired by generative adversarial networks <ref type="bibr" target="#b8">[9]</ref>, however it provides a energy-based formulation of the discriminator to address common instabilities of adversarial training.</p><p>A complete description of the architecture used to design the deep generator and deep energy model is provided in our supplemental materials. We begin by extracting an 8 × 40 × 512 cross-view feature map, f , that has been learned to relate an aerial and ground image pair. The generator is given f along with random noise, z, as input. The generator outputs a 64 × 320 panorama, Iĝ, that represents the predicted ground image. The cross-view feature, predicted panorama, and the ground truth panorama, I g , are <ref type="figure">Figure 9</ref>. Fine-grained geocalibration results on CVUSA. (left) From top to bottom are the Ig, Lg, and L g respectively. We visualize three classes on the labels: road (red), vegetation (green), and man-made (blue). (right) Orientation flow map (red), where the arrow direction indicates the optimal direction at that location and length indicates the magnitude. We also show the optimal prediction and the ground-truth frustums in blue and green respectively. then passed into the energy model which returns an energy function,</p><formula xml:id="formula_2">E Θ (f , I g * )) = 1 σ 2 f T f − b T f − i log(1 + e W T i I g * +bi ),</formula><p>similar to the free energy of a Gaussian Restricted Boltzmann Machine (RBM). Batch normalization <ref type="bibr" target="#b10">[11]</ref> is applied in every layer of both models, except for the final layers.</p><p>ReLU activations are used throughout the generator and Leaky ReLU, with leak parameter α = 0.2, are used in the energy model. The models are updated in an alternating fashion, where the generator is updated twice for every update of the energy model. Both the generator and energy model are optimized using the Adam optimizer, with moment parameters β 1 = 0.5 and β 2 = 0.999. We train using batch sizes of 32 for 30 epochs. Example outputs generated by our network are shown in <ref type="figure" target="#fig_8">Figure 10</ref>. Each row contains an aerial image (left), its respective ground panorama (top-right), and our prediction of the ground scene layout (bottom-right), which would ideally be the same as its above image. The network has learned the most common features, such as roads and their orientations, as well as trees and grass. However, it has difficulty hallucinating buildings and the sky, which is likely caused by highly variable appearance factors.</p><p>We note that the resolution of the synthesized groundlevel panoramas is much lower than the original panorama, however adversarial generation of high-resolution images is an active area of research. We expect that in the near future we will be able to use our learned features in a similar manner to generate full-resolution panoramas. Additionally, algorithmic improvements to our ground image segmentation method would provide more photo-realistic predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a novel strategy for using automatically labeled ground images as a form of weak supervision for learning to understand aerial images. The key is to simultaneously learn to extract features from the aerial image and learn to map from the aerial to the ground image. We demonstrated that by using this process we are able to automatically extract semantically meaningful features from aerial imagery, refine these to obtain more accurate pixellevel labeling of aerial imagery, estimate the location and orientation of a ground image, and synthesize novel groundlevel views. The proposed technique is equally applicable to other forms of imagery, including NIR, multispectral, and hyperspectral. For future work, we plan to explore richer ground image annotation methods to explore the limits of what is predictable about a ground-level view from an aerial view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples of aligned aerial/ground image pairs from our dataset. (row 1) In the aerial images, north is the up direction. In the ground images, north is the central column. (row 2-4) Image dependent receptive fields estimated by our algorithm as follows: 1) fix ground locations (y, x) (locations in squares); 2) select all (i, j) (locations in contours) with highF (i, j, y, x, S(Ia; ΘS)) values. Corresponding fields between the aerial image and the ground image are shown in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>In total, we collected 35,532 image pairs for training and 8,884 image pairs for testing. Some examples aerial/ground image pairs in our dataset are shown Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Example outputs from our weakly supervised learning method on test images. For each aerial image (top), we show the pixel-level labeling inferred by our model, which uses only noisy ground image segmentation as labels. We visualize three classes: road (red), vegetation (green), and man-made (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>An example from the ISPRS dataset [25]. (left) Near infrared image; (middle) The same image after pre-processing; (right) Ground-truth annotation of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Ours 0 .Figure 8 .</head><label>08</label><figDesc>67 0.74 0.63 0.64 0.66 0.64 Random 0.70 0.70 0.54 0.62 0.61 0.73 VGG16 0.60 0.55 0.55 0.61 0.70 0.59 Bldg Ours 0.72 0.76 0.76 0.80 0.75 0.78 Random 0.56 0.62 0.63 0.64 0.82 0.71 VGG16 0.78 0.72 0.71 0.69 0.70 0.75 Low. Ours 0.37 0.43 0.51 0.65 0.67 0.67 Random 0.29 0.29 0.29 0.37 0.67 0.64 VGG16 0.25 0.25 0.29 0.44 0.53 0.57 Tree Ours 0.68 0.54 0.71 0.71 0.74 0.74 Random 0.42 0.46 0.49 0.56 0.71 0.69 VGG16 0.36 0.44 0.50 0.55 0.65 0.74 Car Ours 0.13 0.46 0.67 0.48 0.48 0.49 Random 0.05 0.08 0.10 0.25 0.45 0.57 VGG16 0.05 0.11 0.20 0.20 0.25 0.23 Histogram of the orientation errors on the CVUSA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of orientation predictions on Cityscapes dataset (top) and CVUSA (bottom). The Ig, Lg and L g are stacked vertically on the left side of the aerial image. We visualize three classes on the labels: road (red), vegetation (green), and man-made (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Synthesized ground-level views. Each row shows an aerial image (left), its corresponding ground-level panorama (topright), and predicted ground-level panorama (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Per-Class Precision on the ISPRS Segmentation Task</figDesc><table><row><cell>Class Init.</cell><cell>1</cell><cell>Number of training samples 2 7 20 54</cell><cell>82</cell></row><row><cell>Imp.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NSF CA-REER grant (IIS-1553116), a Google Faculty Research Award, and an AWS Research Education grant.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The appendix contains additional details and qualitative results from our experiments. <ref type="figure">Figure 11</ref> demonstrates how we visualize the transformation matrix in different ways. <ref type="figure">Figure 12</ref> shows randomly selected qualitative results from our weakly supervised learning task. <ref type="figure">Figure 13</ref> shows additional fine-grained geocalibration results. <ref type="table">Table 2 and Table 3</ref> describe the complete network structure for the ground image synthesis application and additional qualitative results are shown in <ref type="figure">Figure 14</ref>.   , where the arrow direction indicates the optimal direction at that location and length indicates the magnitude. We also show the optimal prediction and the ground-truth frustums in blue and green respectively.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06694</idno>
		<title level="m">Towards a General Pixel-level Architecture</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey on object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1603.06201</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coupling ground-level panoramas and aerial imagery for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghouaiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geo-spatial Information Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting buildings in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huertas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="131" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust visual domain adaptation with low-rank reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic classification in aerial imagery by integrating appearance and height information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kluckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event recognition: viewing the world with a third eye</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hd maps: Fine-grained road segmentation by parsing ground and aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Máttyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3611" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to detect roads in highresolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ISPRS Workshop: Looking From Above: When Earth Observation Meets Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Isprs test project on urban classification and 3d building reconstruction. Commission III-Photogrammetric Computer Vision and Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Group III/4-3D Scene Analysis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Filter flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised crossview semantic transfer for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cataloging public objects using aerial and street-level images-urban trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6014" to="6023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A rule-based system for house reconstruction from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Willuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the location dependence of convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ISPRS Workshop: Looking From Above: When Earth Observation Meets Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03557</idno>
		<title level="m">View synthesis by appearance flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
