<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Samples Synthesizing for Robust Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MReaL Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Samples Synthesizing for Robust Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite Visual Question Answering (VQA) has realized impressive progress over the last few years, today's VQA models tend to capture superficial linguistic correlations in the train set and fail to generalize to the test set with different QA distributions. To reduce the language biases, several recent works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on VQA-CP. However, since the complexity of design, current methods are unable to equip the ensemble-based models with two indispensable characteristics of an ideal VQA model: 1) visual-explainable: the model should rely on the right visual regions when making decisions. 2) question-sensitive: the model should be sensitive to the linguistic variations in question. To this end, we propose a model-agnostic Counterfactual Samples Synthesizing (CSS) training scheme. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (i.e., the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. In return, the performance of these models is further boosted. Extensive ablations have shown the effectiveness of CSS. Particularly, by building on top of the model LMH <ref type="bibr" target="#b13">[14]</ref>, we achieve a record-breaking performance of 58.95% on VQA-CP v2, with 6.5% gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering (VQA), i.e., answering natural language questions about the visual content, is one of the core techniques towards complete AI. With the release of multiple large scale VQA datasets (e.g., VQA v1 <ref type="bibr" target="#b5">[6]</ref> and v2 <ref type="bibr" target="#b16">[17]</ref>), VQA has received unprecedented attention and to predict correct answer (e.g., "surfing"), but also relies on the right reference regions when making this prediction. (b) questionsensitive ability: the model should be sensitive to the linguistic variations, e.g., after replacing the critical word "luggage" with "bus", the predicted answers of two questions should be different.</p><p>hundreds of models have been developed. However, since the inevitable annotation artifacts in the real image datasets, today's VQA models always over-rely on superficial linguistic correlations (i.e., language biases) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>For example, a model answering "2" for all "how many X" questions can still get satisfactory performance regardless of the X. Recently, to disentangle the bias factors and clearly monitor the progress of VQA research, a diagnostic benchmark VQA-CP (VQA under Changing Priors) <ref type="bibr" target="#b2">[3]</ref> has been proposed. The VQA-CP deliberately has different questionanswer distributions in the train and test splits. The performance of many state-of-the-art VQA models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4]</ref> drop significantly on VQA-CP compared to other datasets. Currently, the prevailing solutions to mitigate the bias issues are ensemble-based methods: they introduce an aux- iliary question-only model to regularize the training of targeted VQA model. Specifically, these methods can further be grouped into two sub-types: 1) adversary-based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref>: they train two models in an adversarial manner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12]</ref>, i.e., minimizing the loss of VQA model while maximizing the loss of question-only model. Since the two models are designed to share the same question encoder, the adversarybased methods aim to reduce the language biases by learning a bias-neutral question representation. Unfortunately, the adversarial training scheme brings significant noise into gradients and results in an unstable training process <ref type="bibr" target="#b17">[18]</ref>. 2) fusion-based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>: they late fuse the predicted answer distributions of the two models, and derive the training gradients based on the fused answer distributions. The design philosophy of the fusion-based methods, is to let the targeted VQA model focuses more on the samples, which cannot be answered correctly by the question-only model. Although the ensemble-based methods have dominated the performance on VQA-CP, it is worth noting that current methods fail to equip them with two indispensable characteristics of an ideal VQA model: 1) visual-explainable: the model should rely on the right visual regions when making decisions, i.e., right for the right reasons <ref type="bibr" target="#b33">[34]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a), although both two models can predict the correct answer "surfing", they actually refer to totally different reference regions when making this answer prediction. 2) question-sensitive: the model should be sensitive to the linguistic variations in question. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), for two questions with similar sentence structure (e.g., only replacing word "luggage" with "bus"), if the meanings of two questions are different, the model should perceive the discrepancy and make corresponding predictions.</p><p>In this paper, we propose a novel model-agnostic Coun-terfactual Samples Synthesizing (CSS) training scheme. The CSS serves as a plug-and-play component to improve the VQA models' visual-explainable and question-sensitive abilities, even for complex ensemble-based methods. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, CSS consists of two different types of samples synthesizing mechanisms: V-CSS and Q-CSS. For V-CSS, it synthesizes a counterfactual image by masking critical objects in the original image. By "critical", we mean that these objects are important in answering a certain question (e.g., object for the question "what color is the man's tie"). Then, the counterfactual image and original question compose a new image-question (VQ) pair. For Q-CSS, it synthesizes a counterfactual question by replacing critical words in the original question with a special token "[MASK]". Similarly, the counterfactual querstion and original image compose a new VQ pair. Given a VQ pair (from V-CSS or Q-CSS), a standard VQA training sample triplet still needs the corresponding ground-truth answers. To avoid the expensive manual annotations, we design a dynamic answer assigning mechanism to approximate ground-truth answers for all synthesized VQ pairs (e.g., "not green" in <ref type="figure" target="#fig_1">Figure 2</ref>). Then, we train the VQA models with all original and synthesized samples. After training with numerous complementary samples, the VQA models are forced to focus on critical objects and words.</p><p>Extensive ablations including both qualitative and quantitative results have demonstrated the effectiveness of CSS. The CSS can be seamlessly incorporated into the ensemblebased methods, which not only improves their both visualexplainable and question-sensitive abilities, but also consistently boosts the performance on VQA-CP. Particularly, by building of top on model LMH <ref type="bibr" target="#b13">[14]</ref>, we achieve a new record-breaking performance of 58.95% on VQA-CP v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Language Biases in VQA. Despite VQA is a multimodal task, a large body of research <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b16">17]</ref> has shown the existence of language biases in VQA. There are two main solutions to reduce the language biases: 1. Balancing Datasets to Reduce Biases. The most straightforward solution is to create more balanced datasets. For example, Zhang et al. <ref type="bibr" target="#b41">[42]</ref> collected complementary abstract scenes with opposite answers for all binary questions. And Goyal et al. <ref type="bibr" target="#b16">[17]</ref> extended this idea into real images and all types of questions. Although these "balanced" datasets have reduced biases to some extent, the statistical biases from questions still can be leveraged <ref type="bibr" target="#b2">[3]</ref>. As shown in the benchmark VQA-CP, the performance of numerous models drop significantly compared to these "balanced" datasets. In this paper, we follow the same spirit of dataset balancing and train VQA models with more complementary samples. Especially, CSS doesn't need any extra manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Designing Models to Reduce Biases. Another solution is</head><p>to design specific debiasing models. So far, the most effective debiasing models for VQA are ensemble-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. In this paper, we propose a novel CSS training scheme, which can be seamlessly incorporated into the ensemble-based models to further reduce the biases.</p><p>Visual-Explainable Ability in VQA Models. To improve visual-explainable ability, early works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref> directly apply human attention as supervision to guide the models' attention maps. However, since the existence of strong biases, even with appropriate attention maps, the remaining layers of network may still disregard the visual signal <ref type="bibr" target="#b35">[36]</ref>. Thus, some recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref> utilize Grad-CAM <ref type="bibr" target="#b34">[35]</ref> to obtain private contribution of each object to correct answers, and encourage the rank of all object contributions to be consistent with human annotations. Unfortunately, these models have two drawbacks: 1) They need extra human annotations. 2) The training is not end-to-end.</p><p>Question-Sensitive Ability in VQA Models. If VQA systems really "understand" the question, they should be sensitive to the linguistic variations in question. Surprisingly, to the best of our knowledge, there is only one work <ref type="bibr" target="#b36">[37]</ref> has studied the influence of linguistic variations in VQA. Specifically, it designs a cycle-consistent loss between two dual tasks, and utilizes sampled noises to generate diverse questions. However, Shah et al. <ref type="bibr" target="#b36">[37]</ref> only considers the robustness to different rephrasings of questions. In contrast, we also encourage the model to perceive the difference of questions when changing some critical words.</p><p>Counterfactual Training Samples for VQA. Some concurrent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> also try to synthesize counterfactual samples for VQA. Different from these works that all resort to GAN <ref type="bibr" target="#b15">[16]</ref> to generate images, CSS only mask critical objects or words, which is easier and more adoptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We consider the common formulation of VQA task as a multi-class classification problem. Without loss of generality, given a dataset D = {I i , Q i , a i } N i consisting of triplets of images I i ∈ I, questions Q i ∈ Q and answers a i ∈ A, VQA task learns a mapping f vqa : I ×Q → [0, 1] |A| , which produces an answer distribution given image-question pair. For simplicity, we omit subscript i in the following sections.</p><p>In this section, we first introduce the base bottom-up topdown model <ref type="bibr" target="#b3">[4]</ref>, and the ensemble-based methods for debiasing in Section 3.1. Then, we introduce the details of the Counterfactual Samples Synthesizing (CSS) in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Bottom-Up Top-Down (UpDn) Model. For each image I, the UpDn uses an image encoder e v to output a set of object features: V ← e v (I) <ref type="bibr">3:</ref> Q ← e q (Q) <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_0">V = {v 1 , ..., v nv }, where v i is i-th object feature.</formula><formula xml:id="formula_1">P vqa (a) ← f vqa (V , Q) 5: P q (a) ← f q (Q) question-only model 6:P vqa (a) ← M (P vqa (a), P q (a)) 7:</formula><p>Loss ← XE(P vqa (a), a) update parameters <ref type="bibr">8:</ref> if cond then 9:</p><p>return V , Q, P vqa (a) <ref type="bibr">10:</ref> end if 11: end function w j is j-th word feature. Then both V and Q are fed into the model f vqa to predict answer distributions:</p><formula xml:id="formula_2">P vqa (a|I, Q) = f vqa (V , Q).</formula><p>(1)</p><p>Model f vqa typically contains an attention mechanism <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>, and it is trained with cross-entropy loss <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11]</ref>. Ensemble-Based Models. As we discussed in Section 1, the ensemble-based models can be grouped into two subtypes: adversary-based and fusion-based. Since adversarybased models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref> suffer severe unstable training and relatively worse performance, in this section, we only introduce the fusion-based models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. As shown in Algorithm 1, they introduce an auxiliary question-only model f q which takes Q as input and predicts answer distribution:</p><formula xml:id="formula_3">P q (a|Q) = f q (Q).<label>(2)</label></formula><p>Then, they combine the two answer distributions and obtain a new answer distributionP vqa (a) by a function M :</p><formula xml:id="formula_4">P vqa (a|I, Q) = M (P vqa (a|I, Q), P q (a|Q)).<label>(3)</label></formula><p>In the training stage, the XE loss is computed based on the fused answer distributionP vqa (a) and the training gradients are backpropagated through both f vqa and f q . In test stage, only model f vqa is used as the plain VQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Counterfactual Samples Synthesizing (CSS)</head><p>The overall structure of CSS training scheme is shown in Algorithm 2. Specifically, for any VQA model, given a training sample (I, Q, a), CSS consists of three main steps:</p><p>1. Training VQA model with original sample (I, Q, a); 2. Synthesizing a counterfactual sample (I − , Q, a − ) by V-CSS or (I, Q − , a − ) by Q-CSS; 3. Training VQA model with the counterfactual sample.</p><p>In the following, we introduce the details of V-CSS and Q-CSS (i.e., the second step). As shown in Algorithm 2, for each training sample, we only use one certain synthesizing mechanism, and δ is the trade-off weight (See <ref type="figure" target="#fig_6">Figure 4</ref> (c) for more details about the influence of different δ). V , Q, P vqa (a) ← VQA(I, Q, a, True) <ref type="bibr">3:</ref> cond ∼ U [0, 1]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>if cond ≥ δ then execute V-CSS 5:</p><formula xml:id="formula_5">I ← IO SEL(I, Q) 6: s(a, v i ) ← S(P vqa (a), v i ) 7: I + , I − ← CO SEL(I, {s(a, v i )}) 8: a − ← DA ASS(I + , Q, VQA, a) 9:</formula><p>VQA(I − , Q, a − , False) <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_6">else execute Q-CSS 11: s(a, w i ) ← S(P vqa (a), w i ) 12: Q + , Q − ← CW SEL({s(a, w i )}) 13:</formula><p>a − ← DA ASS(I, Q + , VQA, a) <ref type="bibr" target="#b13">14</ref>:</p><formula xml:id="formula_7">VQA(I, Q − , a − , False) 15:</formula><p>end if 16: end function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">V-CSS</head><p>We sequentially introduce all steps of V-CSS following its execution path (line 5 to 8 in Algorithm 2), which consists of four main steps: initial objects selection (IO SEL), object local contributions calculation, critical objects selection (CO SEL), and dynamic answer assigning (DA ASS).</p><p>1. Initial Objects Selection (IO SEL). In general, for any specific QA pair (Q, a), only a few objects in image I are related. To narrow the scope of critical objects selection, we first construct a smaller object set I, and assume all objects in I are possibly important in answering this question. Since we lack annotations about the critical objects for each sample, we followed <ref type="bibr" target="#b38">[39]</ref> to extract the objects which are highly related with the QA. Specifically, we first assign POS tags to each word in the QA using the spaCy POS tagger <ref type="bibr" target="#b18">[19]</ref> and extract nouns in QA. Then, we calculate the cosine similarity between the GloVe <ref type="bibr" target="#b30">[31]</ref> embedding of object categories and the extracted nouns, the similarity scores between all objects in I and the QA are denoted as SIM. We select |I| objects with the highest SIM scores as I.</p><p>2. Object Local Contributions Calculation. After obtaining the object set I, we start to calculate the local contribution of each object to the predicted probability of groundtruth answer. Following recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> which utilize the modified Grad-CAM <ref type="bibr" target="#b34">[35]</ref> to derive the local contribution of each participant, we calculate the contribution of i-th object feature to the ground-truth answer a as:</p><formula xml:id="formula_8">s(a, v i ) = S(P vqa (a), v i ) := (∇ vi P vqa (a)) T 1,<label>(4)</label></formula><p>where P vqa (a) is the predicted answer probability of ground truth answer a, v i is i-th object feature, and 1 is an allones vector. Obviously, if the score s(a, v i ) is higher, the contributions of object v i to answer a is larger. VQA.eval() don't update parameters <ref type="bibr">3:</ref> , , P + vqa (a) ← VQA(I + , Q + , a, True) <ref type="bibr">4:</ref> a + ← top-N(argsort ai∈A (P + vqa (a i ))) 5:</p><formula xml:id="formula_9">a − := {a i |a i ∈ a, a i / ∈ a + }</formula><p>a is gt answer set <ref type="bibr">6:</ref> return a − 7: end function  3. Critical Objects Selection (CO SEL). After obtaining the private contribution scores s(a, v i ) for all objects in I, we select the top-K objects with highest scores as the critical object set I + . The K is a dynamic number for each image, which is the smallest number meets Eq. <ref type="formula">(5)</ref>: <ref type="bibr" target="#b4">(5)</ref> where η is a constant, we set η = 0.65 in all experiments (See <ref type="figure" target="#fig_6">Figure 4</ref> for more details about the dynamic K setting).</p><formula xml:id="formula_10">vi∈I + exp(s(a, v i ))/ vj ∈I exp(s(a, v j )) &gt; η,</formula><p>Then, the counterfactual visual input I − is the absolute complement of set I + in set I, i.e., I − = I\I + . We show an example of I, I + , and I − in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>4. Dynamic Answer Assigning (DA ASS). Given the counterfactual visual input I − and original question Q, we compose a new VQ pair (I − , Q). To assign ground truth answers for VQ pair (I − , Q), we design a dynamic answer assigning (DA ASS) mechanism. The details of DA ASS are shown in Algorithm 3. Specifically, we first feed another VQ pair (I + , Q) into the VQA model, and obtain the predicted answer distribution P + vqa (a). Based on P + vqa (a), we select the top-N answers with highest predicted probabilities as a + . Then we define a − := {a i |a i ∈ a, a i / ∈ a + }. In an extreme case, if the model predicts all ground truth answer correctly for VQ pair (I + , Q), i.e., a ⊂ a + , then a − is a ∅, i.e., zero for all answer candidates. The basic motivation is that if current model can predict ground truth answer for (I + , Q) (i.e., I + contains critial objects and I − not), the ground truth for (I − , Q) should not contain original ground truth answers anymore, e.g., "not green" in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Q-CSS</head><p>All steps in Q-CSS are similar to V-CSS. Following its execution path (line 11 to 13 in Algorithm 2), it consists of word local contribution calculation, critical words selection (CW SEL), and dynamic answer assigning (DA ASS).</p><p>1. Word Local Contribution Calculation. Similar with the V-CSS (cf. Eq. (4)), we calculate the contribution of i-th word feature to the ground-truth answer a as: s(a, w i ) = S(P vqa (a), w i ) := (∇ wi P vqa (a)) T 1. <ref type="formula">(6)</ref> 2. Critical Words Selection (CW SEL.) In this step, we first extract question-type words for each question Q 2 (e.g., "what color" in <ref type="figure" target="#fig_4">Figure 3</ref>). Then, we select top-K words with highest scores from the remaining sentence (except the question-type words) as critical words. The counterfactual question Q − is the sentence by replacing all critical words in Q with a special token "[MASK]". Meanwhile, the Q + is the sentence by replacing all other words (except question-type and critical words) with "[MASK]". We show an example of Q, Q + , and Q − in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>3. Dynamic Answer Assigning (DA ASS.) This step is identical to the DA ASS in V-CSS, i.e., Algorithm 3. For Q-CSS, the input for DA ASS is the VQ pair (I, Q + ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Settings. We evaluated the proposed CSS for VQA mainly on the VQA-CP test set <ref type="bibr" target="#b2">[3]</ref>. We also presented experimental results on the VQA v2 validation set <ref type="bibr" target="#b16">[17]</ref> for completeness. For model accuracies, we followed the standard VQA evaluation metric <ref type="bibr" target="#b5">[6]</ref>. For fair comparisons, we did all the same data preprocessing steps with the widely-used UpDn model <ref type="bibr" target="#b3">[4]</ref> using the publicly available reimplementation 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablative Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Hyperparameters of V-CSS and Q-CSS</head><p>We run a number of ablations to analyze the influence of different hyperparameters of V-CSS and Q-CSS. Specifically, we conducted all ablations by building on top of ensemblebased model LMH <ref type="bibr" target="#b13">[14]</ref>. Results are illustrated in <ref type="figure" target="#fig_6">Figure 4</ref>. The size of I in V-CSS. The influence of different size of I is shown in <ref type="figure" target="#fig_6">Figure 4 (a)</ref>. We can observe that the model's performance gradually decreases with the increase of |I|. The size of critical objects in V-CSS. The influence of masking different numbers of critical objects is shown in <ref type="figure" target="#fig_6">Figure 4 (a)</ref>. We compared the dynamic K (Eq. (5)) with some fixed constants (e.g., <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5)</ref>. From the results, we can observe that the dynamic K achieves the best performance. The size of critical words in Q-CSS. The influence of replacing different sizes of critical words is shown in <ref type="figure" target="#fig_6">Figure 4</ref>  <ref type="bibr" target="#b1">2</ref> We use the default question-type annotations in VQA-CP dataset. <ref type="bibr" target="#b2">3</ref>  (b). From the results, we can observe that replacing only one word (i.e., top-1) achieves the best performance.</p><p>The proportion δ of V-CSS and Q-CSS. The influence of different δ is shown in <ref type="figure" target="#fig_6">Figure 4 (c)</ref>. From the results, we can observe that the performance is best when δ = 0.5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Architecture Agnostic</head><p>Settings. Since the proposed CSS is a model-agnostic training scheme, which can be seamlessly incorporated into different VQA architectures. To evaluate the effectiveness of CSS to boost the debiasing performance of different backbones, we incorporated the CSS into multiple architectures including: UpDn <ref type="bibr" target="#b3">[4]</ref>, PoE (Product of Experts) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>, RUBi <ref type="bibr" target="#b9">[10]</ref>, LMH <ref type="bibr" target="#b13">[14]</ref>. Especially, PoE, RUBi, LMH are ensemble-based methods. All results are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Results. Compared to these baseline models, the CSS can consistently improve the performance for all architectures. The improvement is more significant in the ensemble-based models (e.g., 6.50% and 9.79% absolute performance gains in LMH and PoE). Furthermore, when both two types of CSS are used, models often achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-Arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance on VQA-CP v2 and VQA v2</head><p>Settings. We incorporated the CSS into model LMH <ref type="bibr" target="#b13">[14]</ref>, which is dubbed as LMH-CSS, and compared it with the state-of-the-art models on both VQA-CP v2 and VQA v2. According to the backbone of these models, we group them into: 1) AReg <ref type="bibr" target="#b32">[33]</ref>, MuRel <ref type="bibr" target="#b8">[9]</ref>, GRL <ref type="bibr" target="#b17">[18]</ref>, RUBi <ref type="bibr" target="#b9">[10]</ref>, SCR <ref type="bibr" target="#b38">[39]</ref>, LMH <ref type="bibr" target="#b13">[14]</ref>, HINT <ref type="bibr" target="#b35">[36]</ref>. These models utilize the UpDn <ref type="bibr" target="#b3">[4]</ref> as their backbone. 2) HAN <ref type="bibr" target="#b27">[28]</ref>, GVQA <ref type="bibr" target="#b2">[3]</ref>, ReGAT <ref type="bibr" target="#b24">[25]</ref>, NSM <ref type="bibr" target="#b19">[20]</ref>. These models utilize other different backbones, e.g., BLOCK <ref type="bibr" target="#b7">[8]</ref>, BAN <ref type="bibr" target="#b23">[24]</ref> etc. Especially, the AReg, GRL, RUBi, LMH are ensemble-based models.</p><p>Results. The results are reported in <ref type="table" target="#tab_4">Table 3</ref>. When trained and tested on the VQA-CP v2 dataset (i.e., left side of <ref type="table" target="#tab_4">Table 3</ref>), the LMH-CSS achieves a new state-of-the-art performance over all question categories. Particularly, CSS improves the performance of LMH with a 6.50% absolution performance gains (58.95% vs. 52.45%). When trained and tested on the VQA v2 dataset (i.e., middle side of <ref type="table" target="#tab_4">Table 3</ref>), the CSS results in a minor drop in the performance by 1.74% for LMH. For completeness, we further compared the performance drop between the two benchmarks. Different from previous models that suffer severe performance drops (e.g., <ref type="bibr" target="#b22">23</ref>.74% in UpDn, and 9.19% in LMH), the LMH-CSS can significantly decrease the performance drop into 0.96%, which demonstrate that the effectiveness of CSS to further reduce the language biases in VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance on VQA-CP v1</head><p>Settings. We further compared the LMH-CSS with state-ofthe-art models on VQA-CP v1. Similarly, we group these baseline models into: 1) GVQA with SAN [40] backbone, 2) AReg, GRL, RUBi, and LMH with UpDn backbone.</p><p>Results. Results are reported in <ref type="table" target="#tab_3">Table 2</ref>. Compared to these baseline models, the LMH-CSS achieves a new state-of-theart performance on VQA-CP v1. Particularly, the CSS improves the performance of LMH with a 5.68% absolution performance gains (60.95% vs. 55.27%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Improving Visual-Explainable Ability</head><p>We will validate the effectiveness of CSS to improve the visual-explainable ability by answering the following questions: Q1: Can existing visual-explainable models be incorporated into the ensemble-based framework? Q2: How    For qualitative results, we illustrated in <ref type="figure" target="#fig_7">Figure 5 (a)</ref>.</p><p>Results. From <ref type="table" target="#tab_5">Table 4</ref> (b), we can observe that CSS dramatically improves the AI scores, which means the actually influential objects are more related to the QA pair. <ref type="figure" target="#fig_7">From Figure 5 (a)</ref>, we can find that the CSS helps the model to make predictions based on critical objects (i.e., green boxes), and suppress the influence of irrelevant objects (i.e., red boxes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Improving Question-Sensitive Ability</head><p>We will validate the effectiveness of CSS to improve the question-sensitive ability by answering the following questions: Q3: Does CSS helps to improve the robustness to diverse rephrasings of questions? Q4: How does CSS improve the model's question-sensitive abilities?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Robustness to Rephrasings of Questions (Q3)</head><p>Settings. As discussed in previous work <ref type="bibr" target="#b36">[37]</ref>, being robust to diverse rephrasing of questions is one of key behaviors of a question-sensitive model. To more accurately evaluate the robustness, we re-splited the existing dataset VQA-Rephrasings <ref type="bibr" target="#b36">[37]</ref> with the same splits as VQA-CP, and denoted it as VQA-CP-Rephrasings. For evaluation, we used the standard metric Consensus Score CS(k). Results are reported in <ref type="table" target="#tab_5">Table 4</ref> (c) (left). We refer readers to <ref type="bibr" target="#b36">[37]</ref> for more details about the VQA-Rephrasings and metric CS(k). <ref type="table" target="#tab_5">Table 4</ref> (c), we can observe that Q-CSS dramatically improves the robustness to diverse rephrasings of questions. Furthermore, V-CSS can help to further improve the robustness, i.e., CSS achieves the best performance. whereâ is the model predicted answer for sample (I, Q), 1 is an indicator function. The results are reported in <ref type="table" target="#tab_5">Table 4</ref> (c). For qualitative results, we illustrated in <ref type="figure" target="#fig_7">Figure 5 (b)</ref>. <ref type="table" target="#tab_5">Table 4</ref> (c), we can observe that CSS helps the model to benefit more from the critical words, i.e., removing critical words results in more confidence drops for the ground-truth answers. From <ref type="figure" target="#fig_7">Figure 5</ref> (b), we can find that CSS helps the model to make predictions based on critical words (e.g., "stove" or "lasagna"), i.e., forcing model to understand the whole questions before making predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a model-agnostic Counterfactual Samples Synthesizing (CSS) training scheme to im-prove the model's visual-explainable and question-sensitive abilities. The CSS generates counterfactual training samples by masking critical objects or words. Meanwhile, the CSS can consistently boost the performance of different VQA models. We validate the effectiveness of CSS through extensive comparative and ablative experiments. Moving forward, we are going to 1) extend CSS to other visuallanguage tasks that suffer severe language biases; 2) design a specific VQA backbone to benefits from CSS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>color of the luggage What is the color of the bus What is the color of the luggage The two indispensable characteristics of an ideal VQA model. (a) visual-explainable ability: the model not only needs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>arXiv:2003.06576v1 [cs.CV] 14 Mar 2020 What color is the man's tie Image Question Answer green What color is the man's tie NOT green What color is the man's [MASK] NOT green (a): A training sample from the VQA-CP. (b): The synthesized training sample by V-CSS. It masks ciritcal objects (e.g., "tie") in image and assigns different ground-truth answers ("not green"). (c): The synthesized training sample by Q-CSS. It replaces critical words (e.g., "tie") with special token "[MASK]" in question and assigns different ground-truth answers ("not green").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For each question Q, the UpDn uses a question encoder e q to output a set of word features: Q = {w 1 , ..., w nq }, where Algorithm 1 Ensemble-based Model (fusion-based) 1: function VQA(I, Q, a, cond) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2</head><label>2</label><figDesc>Counterfactual Samples Synthesizing 1: function CSS(I, Q, a) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3</head><label>3</label><figDesc>Dynamic Answer Assigning 1: function DA ASS(I + , Q + , VQA, a) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>An informal illustration example of the I + , I − , Q + , and Q − in CSS. For I + and I − , they are two mutual exclusive object sets. For Q + and Q − , we show the example when word "kite" is selected as critical word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Ablations. Accuracies (%) on VQA-CP v2 test set of different hyperparameters settings of V-CSS or Q-CSS. (a) The results of different size of I and critical objects in V-CSS. All results come from model LMH+V-CSS. (b) The results of different size of critical words in Q-CSS. All results come from model LMH+Q-CSS. (c) The results of different δ. All results come from model LMH+V-CSS+Q-CSS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>(a) visual-explainable ability: The green boxes denote their scores s(â, v)&gt;0, i.e., positive contributions to final predictions; The red boxes denote their scores s(â, v)&lt;0, i.e., negative contributions to final predictions. Only objects which are highly related to the QA pair are shown (i.e., SIM ≥ 0.6). (b) question-sensitive ability: The different shades of green color in the question denotes the relative values of s(â, w). Thus, the word with darker green denotes the word has larger contribution to final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>https://github.com/hengyuan-hu/bottom-up-attention-vqa Baseline 39.74 42.27 11.93 46.05 Baseline † 39.68 41.93 12.68 45.91 +Q-CSS 40.05 42.16 12.30 46.56 +V-CSS 40.98 43.12 12.28 46.86 +CSS 41.16 43.96 12.78 47.48</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell>All</cell><cell>Y/N</cell><cell>Num Other</cell></row><row><cell>Plain Models</cell><cell>UpDn [4]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell>39.93</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="4">Baseline  † 39.86 41.96 12.59 46.25</cell></row><row><cell></cell><cell>PoE [14, 27]</cell><cell>+Q-CSS</cell><cell cols="3">40.73 42.99 12.49 47.28</cell></row><row><cell>Ensemble-Based Models</cell><cell>RUBi [10]</cell><cell cols="4">+V-CSS +CSS Baseline Baseline  † 45.23 64.85 11.83 44.11 49.65 74.98 16.41 45.50 48.32 70.44 13.84 46.20 44.23 ---+Q-CSS 46.31 68.70 12.15 43.95 +V-CSS 46.00 62.08 11.84 46.95 +CSS 46.67 67.26 11.62 45.13 Baseline 52.05 ---Baseline  † 52.45 69.81 44.46 45.54</cell></row><row><cell></cell><cell>LMH [14]</cell><cell>+Q-CSS</cell><cell cols="3">56.66 80.82 45.83 46.98</cell></row><row><cell></cell><cell></cell><cell>+V-CSS</cell><cell cols="3">58.23 80.53 52.48 48.13</cell></row><row><cell></cell><cell></cell><cell>+CSS</cell><cell cols="3">58.95 84.37 49.42 48.21</cell></row></table><note>Accuracies (%) on VQA-CP v2 test set of different VQA architectures. CSS denotes the model with both V-CSS and Q- CSS.† represents these results are based on our reimplementation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracies (%) on VQA-CP v1 test set of state-of-the-art models. † represents the ensemble-based methods. * indicates the results from our reimplementation using offical released codes.</figDesc><table><row><cell>does CSS improve the model's visual-explainable ability?</cell></row><row><cell>4.3.1 CSS vs. SCR (Q1)</cell></row><row><cell>Settings. We equipped the existing state-of-the-art visual-</cell></row><row><cell>explainable model SCR [39] into the LMH framework, and</cell></row><row><cell>compared it with CSS. Results are reported in Table 4 (a).</cell></row><row><cell>Results. Since the training of all SOTA visual-explainable</cell></row><row><cell>models (e.g., SCR, HINT) are not end-to-end, for fair com-</cell></row><row><cell>parisons, we used a well-trained LMH (i.e., 52.45% accu-</cell></row><row><cell>racies on VQA-CP v2) as the initial model. However, we</cell></row><row><cell>observe that its performance continues to decrease from the</cell></row><row><cell>start, which shows that the existing visual-explainable mod-</cell></row><row><cell>els can not be easily incorporated into the ensemble-based</cell></row><row><cell>framework. In contrast, CSS can improve the performance.</cell></row><row><cell>4.3.2 Evaluations of Visual-Explainable Ability (Q2)</cell></row><row><cell>Settings. We evaluate the effectiveness of CSS to improve</cell></row><row><cell>the visual-explainable ability on both quantitative and qual-</cell></row><row><cell>itative results. For quantitative results, since we lack human</cell></row><row><cell>annotations about the critical objects for each question, we</cell></row></table><note>regard the SIM score (Section 3.2.1 IO SEL) as pseudo ground truth. Thus, we design a new metric Average Impor- tance (AI): the average SIM score of the top-K objects with highest |s(a, v)|. The results are shown in Table 4 (b).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracies (%) on VQA-CP v2 test set and VQA v2 val set of state-of-the-art models. The gap represents the accuracy difference between VQA v2 and VQA-CP v2. † represents the ensemble-based methods. Expl. denotes the model has used extra human annotations, e.g., human attention (HAT) or explanations (VQA-X). * indicates the results from our reimplementation using official released codes.<ref type="bibr" target="#b38">39</ref>.84 33.38 29.11 7.44 LMH+Q-CSS 54.83 42.34 35.48 31.02 9.02 LMH+CSS 55.04 42.78 35.63 31.17 9.03</figDesc><table><row><cell>Model SCR LMH LMH+SCR LMH+CSS 58.95 84.37 49.42 48.21 All Yes/No Num Other 48.47 70.41 10.42 47.29 52.45 69.81 44.46 45.54 continued decrease</cell><cell>LMH+CSS LMH+V-CSS 30.24 28.53 27.51 33.43 31.27 29.86 Model Top-1 Top-2 Top-3 UpDn 22.70 21.58 20.89 SCR 27.58 26.29 25.38 LMH 29.67 28.06 27.04</cell><cell>Model UpDn LMH</cell><cell>k=1 49.94 38.80 31.55 28.08 6.01 k=2 k=3 k=4 CI 51.68</cell></row><row><cell>(a) Accuracies (%) on VQA-CP v2 test set.</cell><cell>(b) AI score (%) on VQA-CP v2 test set.</cell><cell></cell><cell></cell></row></table><note>(c) Left: CS(k) (%) on VQA-CP-Rephrasing; Right: CI score (%) on VQA-CP v2 test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results about the evaluation of the VQA models' visual-explainable and question-sensitive abilities.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Evaluations of Question-Sensitive Ability (Q4)</head><p>Settings. We evaluate the effectiveness of CSS to improve the question-sensitive ability on both quantitative and qualitative results. For quantitative results, since there is no standard evaluation metric, we design a new metric Confidence Improvement (CI): Given a test sample (I, Q, a), we remove a critical noun in question Q, and obtain a new test sample (I, Q * , a) 4 . Then we feed both two samples into evaluated model, and calcluate the confidence decreses of the ground-truth answer. We formally define CI in Eq. 7: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedika</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno>arXiv, 2019. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Don&apos;t take the premise for granted: Mitigating artifacts in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rubi: Reducing unimodal biases in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counterfactual critic multiagent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition using semanticspreserving adversarial embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relationaware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple but effective techniques to reduce biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karimi</forename><surname>Rabeeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning visual question answering by bootstrapping hard attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Questionconditioned counterfactual image generation for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv, 2019. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring human-like attention supervision in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Taking a hint: Leveraging explanations to make vision and language models more grounded</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-critical reasoning for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video question answering via attributeaugmented attention network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interpretable visual question answering by visual grounding from attention supervision mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
