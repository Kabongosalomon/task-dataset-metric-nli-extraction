<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Video Object Segmentation Using Global and Local Transfer Modules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>Jun Koh 2</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><surname>Heo</surname></persName>
							<email>yukheo@mcl.korea.ac.krchangsukim@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Chungnam National University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive Video Object Segmentation Using Global and Local Transfer Modules</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">Jun Koh 2</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video object segmentation</term>
					<term>interactive segmentation</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An interactive video object segmentation algorithm, which takes scribble annotations on query objects as input, is proposed in this paper. We develop a deep neural network, which consists of the annotation network (A-Net) and the transfer network (T-Net). First, given user scribbles on a frame, A-Net yields a segmentation result based on the encoder-decoder architecture. Second, T-Net transfers the segmentation result bidirectionally to the other frames, by employing the global and local transfer modules. The global transfer module conveys the segmentation information in an annotated frame to a target frame, while the local transfer module propagates the segmentation information in a temporally adjacent frame to the target frame. By applying A-Net and T-Net alternately, a user can obtain desired segmentation results with minimal efforts. We train the entire network in two stages, by emulating user scribbles and employing an auxiliary loss. Experimental results demonstrate that the proposed interactive video object segmentation algorithm outperforms the state-of-the-art conventional algorithms. Codes and models are available at https://github.com/yuk6heo/IVOS-ATNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video object segmentation (VOS) aims at separating objects of interest from the background in a video sequence. It is an essential technique to facilitate many vision tasks, including action recognition, video retrieval, video summarization, and video editing. Many researches have been carried out to perform VOS, and it can be categorized according to the level of automation. Unsupervised VOS segments out objects with no user annotations, but it may fail to detect objects of interest or separate multiple objects. Semi-supervised VOS extracts target objects, which are manually annotated by a user in the first frame or only a few frames in a video sequence. However, semi-supervised approaches require timeconsuming pixel-level annotations (at least 79 seconds per instance as revealed in <ref type="bibr" target="#b4">[5]</ref>) to delineate objects of interest.  Therefore, as an alternative approach, we consider interactive VOS, which allows users to interact with segmentation results repeatedly using simple annotations, e.g. scribbles, point clicks, or bounding boxes. In this regard, the objective of interactive VOS is to provide reliable segmentation results with minimal user efforts. A work-flow to achieve this objective was presented in the 2018 DAVIS Challenge <ref type="bibr" target="#b4">[5]</ref>. This work-flow employs scribble annotations as supervision, since it takes only about 3 seconds to draw a scribble on an object instance. In this scenario, a user provides scribbles on query objects in a selected frame and the VOS algorithm yields segment tracks for the objects in all frames. We refer to this turn of user-algorithm interaction as a segmentation round. Then, we repeat segmentation rounds to refine the segmentation results until satisfactory results are obtained as illustrated in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>.</p><p>In this paper, we propose a novel approach to achieve interactive VOS using scribble annotations with the work-flow in <ref type="bibr" target="#b4">[5]</ref>. First, we develop the annotation network (A-Net), which produces a segmentation mask for an annotated frame using scribble annotations for query objects. Next, we propose the transfer network (T-Net) to transfer the segmentation result to other target frames subsequently to obtain segment tracks for the query objects. We design the global transfer module and the local transfer module in T-Net to convey segmentation information reliably and accurately. We train A-Net and T-Net in two stages by mimicking scribbles and employing an auxiliary loss. Experimental results verify that the proposed algorithm outperforms the state-of-the-art interactive VOS algorithms on the DAVIS2017 <ref type="bibr" target="#b34">[35]</ref>. Also, we perform a user study to demonstrate the effectiveness of the proposed algorithm in real-world applications. This paper has three main contributions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised VOS: Unsupervised VOS is a task to segment out primary objects <ref type="bibr" target="#b21">[22]</ref> in a video without any manual annotations. Before the advance of deep learning, diverse information, including motion, object proposals, and saliency, was employed to solve this problem <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47]</ref>. Recently, many deep learning algorithms with different network architectures have been developed to improve VOS performance using big datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref>. Tokmakov et al . <ref type="bibr" target="#b42">[43]</ref> presented a fully convolutional model to learn motion patterns from videos. Jain et al . <ref type="bibr" target="#b16">[17]</ref> merged appearance and motion information to perform unsupervised segmentation. Song et al . <ref type="bibr" target="#b40">[41]</ref> proposed an algorithm using LSTM architecture <ref type="bibr" target="#b10">[11]</ref> with atrous convolution layers <ref type="bibr" target="#b5">[6]</ref>. Wang et al . <ref type="bibr" target="#b47">[48]</ref> also adopted LSTM with a visual attention module to simulate human attention. Semi-supervised VOS: Semi-supervised VOS extracts query objects using accurately annotated masks at the first frames. Early methods for semi-supervised VOS were developed using hand-crafted features based on random walkers, trajectories, or super-pixels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, deep neural networks have been adopted for semi-supervised VOS. Some deep learning techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref> are based on a time-consuming online learning, which fine-tunes a pre-trained network using query object masks at the first frame. Without the fine-tuning, the algorithms in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">54]</ref> propagate segmentation masks, which are estimated in the previous frame, to the current target frame sequentially for segmenting out query objects. Jang et al . <ref type="bibr" target="#b18">[19]</ref> warped segmentation masks in the previous frame to the target frame and refined the warped masks through convolution trident networks. Yang et al . <ref type="bibr" target="#b53">[54]</ref> encoded object location information from a previous frame and combined it with visual appearance features to segment out the query object in the target frame. Also, the algorithms in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> perform matching between the first frame and a target frame in an embedding space to localize query objects. Chen et al . <ref type="bibr" target="#b7">[8]</ref> dichotomized each pixel into either object or background using features from the embedding network. Voigtlaender et al . <ref type="bibr" target="#b43">[44]</ref> trained their embedding network to perform the global and local matching. Interactive image segmentation: Interactive image segmentation aims at extracting a target object from the background using user annotations. As annotations, bounding boxes were widely adopted in early methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50]</ref>. Recently, point-interfaced techniques have been developed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52]</ref>. Maninis et al . <ref type="bibr" target="#b26">[27]</ref> used four extreme points as annotations to inform their network about object boundaries. Jang and Kim <ref type="bibr" target="#b19">[20]</ref> corrected mislabeled pixels through the backpropagating refinement scheme. Interactive VOS: Interactive VOS allows users to interact with segmentation results repeatedly using various input types, e.g. points, scribbles, and bounding boxes. Users can refine segmentation results until they are satisfied. Some interactive VOS algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref> build graph models using the information in user strokes and segment out target objects via optimization. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>, patch matching between target and reference frames is performed to localize query objects. Box interactions can be provided to correct box positions. Benard and Gygli <ref type="bibr" target="#b1">[2]</ref> employed two deep learning networks to achieve interactive VOS. They first obtained object masks from point clicks or scribbles using an interactive image segmentation network and then segmented out the objects using a semisupervised VOS network. Chen et al . <ref type="bibr" target="#b7">[8]</ref> demanded only a small number of point clicks based on pixel-wise metric learning. Oh et al . <ref type="bibr" target="#b29">[30]</ref> achieved interactive VOS by following the work-flow in <ref type="bibr" target="#b4">[5]</ref>. They used two segmentation networks to obtain segmentation masks from user scribbles and to propagate the segmentation masks to neighboring frames by exploiting regions of interest. However, their networks may fail to extract query objects outside the regions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>We segment out one or more objects in a sequence of video frames through user interactions. To this end, we develop two networks: 1) annotation network (A-Net) and 2) transfer network (T-Net). <ref type="figure" target="#fig_2">Fig. 2</ref> is an overview of the proposed algorithm. In the first segmentation round, a user provides annotations (i.e. scribbles) for a query object to A-Net, which then yields a segmentation mask for the annotated frame. Then, T-Net transfers the segmentation mask bi-directionally to both ends of the video to compose a segment track for the object. From the second round, the user selects the poorest segmented frame, and then provides positive and negative scribbles so that A-Net corrects the result. Then, T-Net again propagates the refined segmentation mask to other frames until a previously annotated frame is met. This process is repeated until satisfactory results are obtained. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the architecture of the proposed algorithm, which is composed of A-Net and T-Net. First, we segment out query objects in an annotated frame I a via A-Net. Then, to achieve segmentation in a target frame I t , we develop T-Net, which includes the global and local transfer modules. A-Net: Through user interactions, A-Net infers segmentation results in an annotated frame I a . There are two types of interactions according to iteration rounds. In the first round, a user draws scribbles on target objects. In this case, A-Net accepts four-channel input: RGB channels of I a and one scribble map. In subsequent rounds, the user supplies both positive and negative scribbles after examining the segmentation results in the previous rounds, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. Hence, A-Net takes six channels: RGB channels, segmentation mask map in the previous round, and positive and negative scribble maps. We design A-Net to take 6-channel input, but in the first round, fill in the segmentation mask map with 0.5 and the negative scribble map with 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture</head><p>A-Net has the encoder-decoder architecture, as specified in <ref type="figure" target="#fig_4">Fig. 4</ref>(a). We adopt SE-ResNet50 <ref type="bibr" target="#b13">[14]</ref> as the encoder to extract features and employ skip connections to consider both low-level and high-level features. We perform dilated convolution and exclude max-pooling in the R5 convolution layer. Then, we use two parallel modules: an ASPP module <ref type="bibr" target="#b6">[7]</ref>, followed by up-sampling with bilinear interpolation, and a bottom-up module. ASPP analyzes multi-scale context features using dilated convolution with varying rates. The bottom-up module consists of two refine modules <ref type="bibr" target="#b28">[29]</ref>. The output signals of the ASPP and bottomup modules are concatenated and then used to predict a probability map of a query object through three sets of convolutional layers, ReLU, and batch normalization. Finally, the estimated probability map is up-sampled to be of the same size as the input image using bilinear interpolation.  T-Net: We develop T-Net, which consists of shared encoders, a global transfer module, a local transfer module, and a decoder, as shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. The encoders and decoder in T-Net have the same structures as those of A-Net in <ref type="figure" target="#fig_4">Fig. 4(a)</ref>. The T-Net decoder yields a probability map for query objects in a target frame I t using the features from the encoder, the global transfer module, and the local transfer module. Let us describe these two transfer modules.</p><formula xml:id="formula_0">1/4 ×4 Upsample (a) (b) (c) Local Affinity Computation R R ෨ ෨ ෨ ෨ R R Normalization L L Normalization L L L L L R : Reshape</formula><p>Global transfer module: We design the global transfer module to convey the segmentation information of the annotated frame I a to the target frame I t . <ref type="figure" target="#fig_4">Fig. 4</ref>(b) shows its structure, which adopts the non-local model in <ref type="bibr" target="#b48">[49]</ref>. It takes two feature volumes F t and F a for I t and I a , respectively. Each volume contains C-dimensional feature vectors for H × W pixels. We then construct an affinity matrix W between I t and I a , by computing the inner products between all possible pairs of feature vectors in F t and F a . Specifically, letF t ∈ R HW ×C and F a ∈ R HW ×C denote the feature volumes reshaped into matrices. We perform the matrix multiplication to obtain the affinity matrix</p><formula xml:id="formula_1">W =F t ×F T a .<label>(1)</label></formula><p>Its element W(i, j) represents the affinity of the ith pixel inF t to the jth pixel inF a . Then, we obtain the transition matrix A by applying the softmax normalization to each column in W.</p><p>The transition matrix A contains matching probabilities from pixels in I a to those in I t . Therefore, it can transfer query object probabilities in I a to I t . To approximate these probabilities in I a , we extract a mid-layer feature from the A-Net decoder, down-sample it using the converter, which includes two sets of SE-Resblock <ref type="bibr" target="#b13">[14]</ref> and max-pooling layer. Then, its channels are halved by 1×1 convolutions after it is concatenated to the output of the A-Net encoder, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The concatenated feature F o is fed into the global transfer module, as shown in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>. Then, it is reshaped intoF o , which represents the query object feature distribution in I a . Finally, the global transfer module produces the transferred distributioñ</p><formula xml:id="formula_2">F g = AF o ,<label>(2)</label></formula><p>which can be regarded as an inter-image estimate of the query object feature distribution in I t . Then the distribution is reshaped into F g ∈ R H×W ×C to be input to the T-Net decoder. From the second round, there are N annotated frames, where N is the ordinal index for the round. To obtain reliable segmentation results, we use all information in the N annotated frames. Specifically, we compute the transition matrix A (i) from the ith annotated frame to I t and the query object distribution F (i) o in the ith annotated frame. Then, we obtain the average of the multiple inter-image estimates of the query object distribution in I t bỹ</p><formula xml:id="formula_3">F g = 1 N N i=1 A (i)F(i) o .<label>(3)</label></formula><p>Local transfer module: The segmentation information in an annotated frame is propagated bidirectionally throughout the sequence. Thus, during the propagation, when a target frame I t is to be segmented, there is the previous frame I p that is already segmented. We design the local transfer module to convey the segmentation information in I p to I t . The local transfer module is similar to the global one, but it performs matching locally since I t and I p are temporally adjacent. In other words, object motions between I t and I p , which tend to be smaller than those between I t and I a , are estimated locally. Furthermore, since I t and I p are more highly correlated, motions between them can be estimated more accurately. Therefore, the local module uses higher-resolution features than the global one does. Specifically, the local module takes features from the R2 convolution layer in the encoder in <ref type="figure" target="#fig_4">Fig. 4(a)</ref>, instead of the R5 layer. F L t and F L p , which denote these feature volumes from I t and I p , are provided to the local transfer module, as shown in <ref type="figure" target="#fig_4">Fig. 4(c)</ref>. Then, we compute the local affinity matrix W L , whose (i, j)th element indicates the similarity between the ith pixel I t and the jth pixel in I p . Specifically, W L (i, j) is defined as</p><formula xml:id="formula_4">W L (i, j) = f T t,i f p,j j ∈ N i , 0 otherwise,<label>(4)</label></formula><p>where f t,i and f p,j are the feature vectors for the ith pixel in F L t and the jth pixel in F L p , respectively. Also, the local region N i is the set of pixels, which are sampled from (2d + 1) × (2d + 1) pixels around pixel i with stride 2 to reduce the computations. In this work, d is set to 4. Then, the affinity is computed for those pixels in the local region only, and set to be zeros for the other pixels.</p><p>As in the global module, W L is normalized column-by-column to the transition matrix A L . Also, a segmentation mask map P p in the previous frame I p is down-sampled and vectorized to obtain a probability vector p L . Then, we obtain A L p L , which is another estimate of the query object distribution in I t . It has a higher resolution than the estimate in the global module, and thus is added to the corresponding mid-layer in the T-Net decoder, as shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>.</p><p>Computing global and local similarities in the proposed global and local transfer modules is conceptually similar to <ref type="bibr" target="#b43">[44]</ref>, but their usage is significantly different. Although <ref type="bibr" target="#b43">[44]</ref> also computes global and local distances, it transforms those distances into a single channel by taking the minimum distance at each position. Thus, it loses a substantial amount of distance information. In contrast, the proposed algorithm computes global and local affinity matrices and uses them to transfer object probabilities from annotated and previous frames to a target frame. In Section 4.3, we verify that the proposed global and local modules are more effective than the best matching approach in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training phase</head><p>We train the proposed interactive VOS networks in two stages, since T-Net should use A-Net output; we first train A-Net and then train T-Net using the trained A-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A-Net training:</head><p>To train A-Net, we use the image segmentation dataset in <ref type="bibr" target="#b11">[12]</ref> and the video segmentation datasets in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref>. Only a small percentage of videos in the DAVIS2017 dataset <ref type="bibr" target="#b34">[35]</ref> provide user scribble data. Hence, we emulate user scribbles via two schemes: 1) point generation and 2) scribble generation in <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the first round, A-Net yields a segmentation mask for a query object using positive scribbles only. We perform the point generation to imitate those positive scribbles. We produce a point map by sampling points from the ground-truth mask for the query object. Specifically, we pick one point randomly for every 100 ∼ 3000 object pixels. We vary the sampling rate to reflect that users provide scribbles with different densities. Then, we use the generated point map as the positive scribble map.</p><p>In each subsequent round, A-Net should refine the segmentation mask in the previous round using both positive and negative scribbles. To mimic an inaccurate segmentation mask, we deform the ground-truth mask using various affine transformations. Then, we extract positive and negative scribbles using the scribble generation scheme in <ref type="bibr" target="#b4">[5]</ref>, by comparing the deformed mask with the ground-truth. Then, I a , the deformed mask, and the generated positive and negative scribble maps are fed into A-Net for training.</p><p>We adopt the pixel-wise class-balanced cross-entropy loss <ref type="bibr" target="#b50">[51]</ref> between A-Net output and the ground-truth. We adopt the Adam optimizer to minimize this loss for 60 epochs with a learning rate of 1.0 × 10 −5 . We decrease the learning rate by a factor of 0.2 every 20 epochs. In each epoch, the training is iterated for 7,000 mini-batches, each of which includes 6 pairs of image and ground-truth. For data augmentation, we apply random affine transforms to the pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-Net training:</head><p>For each video, we randomly pick one frame as an annotated frame, and then select seven consecutive frames, adjacent to the annotated frame, in either the forward or backward direction. Among those seven frames, we randomly choose four frames to form a mini-sequence. Thus, there are five frames in a mini-sequence: one annotated frame and four target frames. For each target frame in the mini-sequence, we train T-Net using the features from the trained A-Net, which takes the annotated frame as input.</p><p>We compare an estimated segmentation mask with the ground-truth to train T-Net, by employing the loss function</p><formula xml:id="formula_5">L = L c + λL aux (5)</formula><p>where L c is the pixel-wise class-balanced cross-entropy loss between the T-Net output and the ground-truth. The auxiliary loss L aux is the pixel-wise mean square loss between the transferred probability map, which is the output of the local transfer module, and the down-sampled ground-truth. The auxiliary loss L aux enforces the front encoders of T-Net in <ref type="figure" target="#fig_3">Fig. 3</ref> to generate appropriate features for transferring the previous segmentation mask successfully. Also, λ is a balancing hyper-parameter, which is set to 0.1. We also employ the Adam optimizer to minimize the loss function for 40 epochs with a learning rate of 1.0 × 10 −5 , which is decreased by a factor of 0.2 every 20 epochs. The training is iterated 6,000 mini-batches, each of which contains 8 mini-sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference phase</head><p>Suppose that there are multiple target objects. In the first round, for each target object in an annotated frame, A-Net accepts the user scribbles on the object and produces a probability map for the object. To obtain multiple object segmentation results, after zeroing probabilities lower than 0.8, each pixel is assigned to the target object class, corresponding to the highest probability. Then, T-Net transfers the multiple segmentation masks in the annotated frame bi-directionally to both ends of the sequence. T-Net also compares the multiple probability maps and determines the target object class of each pixel, as done in A-Net. From the second round, the user selects the frame with the poorest segmentation results and then provides additional positive and negative scribbles. The scribbles are then fed into A-Net to refine the segmentation results. Then, we transfer segmentation results bidirectionally with T-Net. In each direction, the transmission is carried out until another annotated frame is found. During the transfer, we superpose the result of segmentation mask P r t for frame I t in the current round r with that P r−1 t in the previous round. Specifically, the updated resultP r t in round r is given bỹ</p><formula xml:id="formula_6">P r t = 1 2 (1 + t − t b t r − t b )P r t + t r − t 2(t r − t b ) P r−1 t<label>(6)</label></formula><p>where t r is the annotated frame in round r and t b is one of the previously annotated frames, which is the closest to t in the direction of the transfer. By   employing this superposition scheme, we can reduce drifts due to a long temporal distance between annotated and target frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We first compare the proposed interactive VOS algorithm with conventional algorithms. Second, we conduct a user study to assess the proposed algorithm in real-world applications. Finally, we do various ablation studies to analyze the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparative assessment</head><p>In this test, we follow the interactive VOS work-flow in <ref type="bibr" target="#b4">[5]</ref>. The work-flow first provides a manually generated scribble for each target object in the first round, and then automatically generates additional positive and negative scribbles to refine the worst frames in up to 8 subsequent rounds. There are three different scribbles provided in the first round. In other words, three experiments are performed for each video sequence. The region similarity (J) and contour accuracy (F) metrics are employed to assess VOS algorithms. For the evaluation of interactive VOS, we measure the area under the curve for J score (AUC-J) and for joint J and F scores (AUC-J&amp;F) to observe the overall performance according over the 8 segmentation rounds. Also, we measure the J score at 60 seconds (J@60s), and the joint J and F score at 60 seconds (J&amp;F@60s) to evaluate how much performance is achieved within the restricted time. <ref type="figure" target="#fig_6">Fig. 5</ref> shows the J&amp;F performances of the proposed algorithm on the validation set in DAVIS2017 <ref type="bibr" target="#b34">[35]</ref> according to the time and the number of rounds. The performances increase quickly and saturate at around 40s or in the third round. Also, we observe that the 8-round experiment is completed within 60 seconds. <ref type="table" target="#tab_2">Table 1</ref> compares the proposed algorithm with recent state-of-the-art algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>. The scores of the conventional algorithms are from the respective papers. The proposed algorithm outperforms the conventional algorithms by significant margins in all metrics. <ref type="figure">Fig. 6</ref> presents examples of segmentation results of the proposed algorithm after 8 rounds. We see that multiple primary objects are segmented out faithfully.  <ref type="figure">Fig. 8</ref>: Examples of scribbles and segmentation results during the user study.</p><p>Positive and negative scribbles are depicted in green and red, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User study</head><p>We conduct a user study, by recruiting 10 off-line volunteers and asking them to provide scribbles repeatedly until they are satisfied. We measure the average time in seconds per video (SPV), including the interaction time to provide scribbles and the running time of the algorithm, and the average round number in rounds per video (RPV) until the completion. Also, we report the J and F means of all sequences when the interactive process is completed. We perform the user study for the proposed algorithm and the state-of-theart interactive VOS algorithm <ref type="bibr" target="#b29">[30]</ref>. For this comparison, we use the validation set (20 sequences) in DAVIS2016 <ref type="bibr" target="#b33">[34]</ref>, in which each video contains only a single query object. This is because the provided source code of <ref type="bibr" target="#b29">[30]</ref> works on a singleobject case only. <ref type="figure">Fig. 7</ref> plots the average time and the average round number for each user. We observe that all users, except user 3, spend less time and conduct fewer rounds using the proposed algorithm. <ref type="table" target="#tab_3">Table 2</ref> summarizes the user study results. The proposed algorithm is faster than <ref type="bibr" target="#b29">[30]</ref> in terms of both SPV and RPV. It is worth pointing out that the proposed algorithm yields better segmentation results within shorter times. <ref type="figure">Fig. 8</ref> shows examples of segmentation results in the user study. For the "Libby," "Horsejump-High," and "Parkour" sequences, the proposed algorithm deals with occlusions and scale changes of query objects effectively, and completes the segmentation in just a single round. Please see the supplemental video to see how the evaluation works.  <ref type="table">Table 4</ref>: Ablation study to validate the proposed probability transfer approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC-J J@60s AUC-J&amp;F J&amp;F@60s</head><p>Matching approach <ref type="bibr" target="#b43">[44]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>We analyze the efficacy of the proposed global and local transfer modules through two ablation studies. First, we verify that the structure and the training method of the local transfer module are effective. In <ref type="table" target="#tab_4">Table 3</ref>, we report the J scores on the validation set in DAVIS2017, by varying the configurations of the local transfer module. In method I, we assess the proposed algorithm without the local transfer module. Note that the J scores in early rounds degrade severely. The local model is hence essential for providing satisfactory results to users quickly in only a few rounds. Method II uses the features of rear TE, instead of those of front TE to compute the affinity matrix of the local transfer module. The features of the front TE are more effective than those of rear TE because of their higher spatial resolution. In method III, without the auxiliary loss L aux (i.e. λ = 0 in (5)), the local transfer module becomes ineffective and the performances degrade significantly. Methods IV, V, and VI vary the parameter λ. We see that λ = 0.1 performs the best by balancing the two losses in <ref type="bibr" target="#b4">(5)</ref>.</p><p>Next, we verify that the proposed global and local transfer modules are more effective for interactive VOS than the global and local matching in <ref type="bibr" target="#b43">[44]</ref>. Note that <ref type="bibr" target="#b43">[44]</ref> is a semi-supervised VOS algorithm, which estimates matching maps between a target frame and the target object region. We plug its matching modules into the proposed interactive system. More specifically, we compute a global similarity map between a target frame and the target object region in an annotated frame to perform the global matching in <ref type="bibr" target="#b43">[44]</ref>. We determine the target object region in two ways: 1) the region predicted by A-Net or 2) the set of scribble-annotated pixels. We then transform the similarity map into a single channel by taking the maximum similarity at each position. Then, we replace F g , which is the output of the proposed global transfer module, with the single-channel similarity. For the local matching, we obtain a local similarity map between the target frame and the segmentation region in the previous frame to compose another single-channel similarity. We then feed the local matching result, instead of A L p L , to the T-Net decoder. We train these modified networks using the same training set as the proposed networks. The implementation details of the modified networks can be found in the supplemental document. Table 4 compares the performances of the proposed transfer modules with those of the matching modules in <ref type="bibr" target="#b43">[44]</ref> on the validation set in DAVIS2017. We observe that the proposed probability transfer approach outperforms the best matching approach <ref type="bibr" target="#b43">[44]</ref> significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a novel interactive VOS algorithm using A-Net and T-Net. Based on the encoder-decoder architecture, A-Net processes user scribbles on an annotated frame to generate a segmentation result. Then, using the global and local transfer modules, T-Net conveys the segmentation information to the other frames in the video sequence. These two modules are complementary to each other. The global module transfers the information from an annotated frame to a target frame reliably, while the local module conveys the information between adjacent frames accurately. In the training process, we introduced the pointgeneration method to compensate for the lack of scribble-annotated data. Moreover, we incorporated the auxiliary loss to activate the local transfer module and make it effective in T-Net. By employing A-Net and T-Net repeatedly, a user can obtain satisfactory segmentation results. Experimental results showed that the proposed algorithm performs better than the state-of-the-art algorithms, while requiring fewer interaction rounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Three different levels of supervision in (a) unsupervised VOS, (b) semisupervised VOS, and (c) interactive VOS. Unsupervised VOS demands no user interaction. Semi-supervised VOS needs pixel-level annotations of an object. Interactive VOS uses quick scribbles and allows interactions with a user repeatedly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed interactive VOS algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Architecture of the proposed networks. A target object in an annotated frame I a is extracted by A-Net in (a), and the result is sequentially propagated to the other frames, called target frames, by T-Net in (b). In this diagram, skip connections are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) The encoder-decoder architecture, adopted by the proposed A-Net and T-Net. Each fraction is the ratio of the output feature resolution to the input image resolution. (b) Global transfer module. (c) Local transfer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>J&amp;F performances of the proposed algorithm on the validation set in DAVIS2017 according to the time and the number of rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Results of the proposed interactive VOS algorithm after 8 rounds. Comparison of the average times and average round numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Net with the global and local transfer modules. 2. Training strategy with the scribble imitation and the auxiliary loss to activate the local transfer module and make it effective in T-Net. 3. Remarkable performance on the DAVIS dataset in various conditions.</figDesc><table /><note>1. Architecture of A-Net and T-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the proposed algorithm with the conventional algorithms on the DAVIS2017 validation set. The best results are boldfaced.</figDesc><table><row><cell></cell><cell>AUC-J</cell><cell cols="3">J@60s AUC-J&amp;F J&amp;F@60s</cell></row><row><cell>Najafi et al . [28]</cell><cell>0.702</cell><cell>0.548</cell><cell>−</cell><cell>−</cell></row><row><cell>Heo et al . [13]</cell><cell>0.704</cell><cell>0.725</cell><cell>0.734</cell><cell>0.752</cell></row><row><cell>Ren et al . [37]</cell><cell>−</cell><cell>−</cell><cell>0.766</cell><cell>0.780</cell></row><row><cell>Oh et al . [30]</cell><cell>0.691</cell><cell>0.734</cell><cell>−</cell><cell>−</cell></row><row><cell>Proposed</cell><cell>0.771</cell><cell>0.790</cell><cell>0.809</cell><cell>0.827</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summary of the user study results.</figDesc><table><row><cell></cell><cell></cell><cell>SPV</cell><cell>RPV</cell><cell>J Mean</cell><cell>F Mean</cell></row><row><cell></cell><cell>Oh et al . [30]</cell><cell>37.9</cell><cell>2.77</cell><cell>0.823</cell><cell>0.817</cell></row><row><cell></cell><cell>Proposed</cell><cell>29.8</cell><cell>1.90</cell><cell>0.832</cell><cell>0.822</cell></row><row><cell>libby</cell><cell>1st round</cell><cell></cell><cell></cell><cell></cell></row><row><cell>horsejump-high</cell><cell>1st round</cell><cell></cell><cell></cell><cell></cell></row><row><cell>parkour</cell><cell>1st round</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the local transfer module (J scores on the validation set in DAVIS2017).</figDesc><table><row><cell>Round</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video SnapCut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00269</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">SegFlow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">JumpCut: nonsuccessive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Interactive video object segmentation using sparseto-dense networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised video object segmentation using multiple random walkers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">POD: Discovering primary objects in videos based on evolutionary refinement of object recurrence, background, and primary object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequential clique optimization for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Image segmentation with a bounding box prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Similarity learning for dense label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">LIVEcut: Learning-based interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robust multiple object mask propagation with efficient object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GrabCut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">SeedNet: Automatic seed generation with deep reinforcement learning for robust interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">GrabCut in one cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">FEELVOS: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactive video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">MILCut: A sweeping line multiple instance learning paradigm for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<title level="m">YouTube-VOS: A large-scale video object segmentation benchmark</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
