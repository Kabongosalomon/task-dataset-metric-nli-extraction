<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Outpainting and Harmonization using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basile</forename><forename type="middle">Van</forename><surname>Hoorick</surname></persName>
							<email>basile.vanhoorick@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Outpainting and Harmonization using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although the inherently ambiguous task of predicting what resides beyond all four edges of an image has rarely been explored before, we demonstrate that GANs hold powerful potential in producing reasonable extrapolations. Two outpainting methods are proposed that aim to instigate this line of research: the first approach uses a context encoder inspired by common inpainting architectures and paradigms, while the second approach adds an extra post-processing step using a single-image generative model. This way, the hallucinated details are integrated with the style of the original image, in an attempt to further boost the quality of the result and possibly allow for arbitrary output resolutions to be supported.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>When presented with an incomplete image, humans are excellent at filling in the blanks and producing a realistic explanation for what could be missing. Image inpainting is a well-studied problem that replicates this behaviour, often tasking deep neural networks with trying to understanding the semantic content of natural images in order to recover the missing regions of a photo. However, the spatially inverted variant of this problem is even more challenging and, with a small play on words, can be denoted outpainting. The problem statement is shown in <ref type="figure" target="#fig_0">Figure 1</ref>; essentially, the task is to extrapolate the image content rather than to interpolate within an image. More formally, we must design a generator G that converts an image x with dimensions n * n into a larger image G(x) with dimensions m * m, such that the center part of G(x) looks the same as x, while the complete outpainted image G(x) should be a plausible hypothesis of what could encompass the original image. In particular, the generated parts should look realistic, despite the fact that it is often impossible to know precisely what the scene contains outside of the pictured boundaries. <ref type="figure">Figure 2</ref> gives a practical illustration of what is accomplished in this work. Possible applications of outpainting include the opportunity to experience and explore multimedia more immersively, enabling technologies similar to Ambilight to produce much richer and/or realistic surroundings, or merely as a form of computer-generated art that can be appreciated aesthetically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Deep neural networks have recently shown great performance in image completion. This section discusses previous work related to the proposed methods for outpainting.</p><p>Self-supervised learning The automatic generation of a supervision signal, by systematically omitting certain parts of the input data, grants the ability to capitalize on the vast amounts of unlabeled images and videos that are available on the Web. For example, both colorization <ref type="bibr" target="#b0">[1]</ref> and predicting the relative spatial position of two patches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> have been proven to be successful pretext tasks to representation learning. These methods do not require any labellings of the dataset, since the inputs (gray-scale images resp. patches) as well as their corresponding ground truths (color images resp. locations) can be extracted procedurally.</p><p>Generative Adversarial Networks Image or video generation using convolutional neural networks with an adversarial loss <ref type="bibr" target="#b3">[4]</ref> has attracted significant research interest. The idea is to let a generator network G create samples according to some distribution, and subsequently have an auxiliary network called the discriminator D try to distinguish whether a given sample is valid or was actually generated by G. These two components are trained in a quick alternation, where G trying to fool D forces D to become better at telling real from fake, which in turn motivates G to synthesize increasingly convincing outputs <ref type="bibr" target="#b4">[5]</ref>. From the perspective of game theory, GANs can be viewed as a zero-sum two-player game; as such, they are comparatively hard to train. In fact, it turns out that the conditions for their convergence is still an open research problem <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inpainting and Context Encoders</head><p>Recovering holes within images has important applications for the restoration of damaged media, and can be achieved by training GANs in a self-supervised way. Specifically, a generator can learn to 'fill in the blanks' by using an encoder-decoder network that encodes the surrounding context in order to understand the image content, and subsequently produces a plausible hypothesis for the missing square <ref type="bibr" target="#b6">[7]</ref>. More recent improvements include attempts to complete images of arbitrary resolutions by filling in regions of any shape, and employing two types of discriminators (global and local ) in order to enforce both overall consistency as well as realistic details <ref type="bibr" target="#b4">[5]</ref>.</p><p>Outpainting</p><p>The problem opposite to inpainting comprises predicting which pixels reside beyond the borders of a fully intact photo, and has to our knowledge been explored only few times before by the academic research community <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. One approach geometrically extrapolates the field of view of an image using another panoramic reference image of the same scene category using old-school computer vision techniques <ref type="bibr" target="#b7">[8]</ref>, while a more recent relevant paper uses a GAN to perform horizontal outpainting <ref type="bibr" target="#b8">[9]</ref>. Results look promising, although there is room for visual improvement in terms of making the hallucinations omnidirectional and increasing the output resolution. <ref type="bibr" target="#b9">[10]</ref> achieves impressive outcomes but tends to focus on limited domain datasets (such as faces only), and notes that generative models experience difficulties trying to fit datasets as diverse as Places2. Lastly, Google's Snapseed application has a proprietary 'Expand' functionality that seems to select patches from the image and copy them to the edges <ref type="bibr" target="#b10">[11]</ref>, but a limited number of experiments suggest that this tool fails to capture the local structure of most scenes from region to region; see <ref type="figure">Figure 3</ref>.</p><p>Single-image generative models The recently introduced SinGAN framework can train an unconditional generative model on a single natural photo, captur-  ing and reproducing image statistics across various scales of the image <ref type="bibr" target="#b11">[12]</ref>. It allows for the creation of random samples with new object configurations by starting from a low-resolution sample at the coarsest scale, and then progressively upsampling and refining the result through the pyramid of generators. By carefully modifying the initial sample, SinGAN can be used to perform several image manipulation tasks such as editing photos, splicing foreign objects into the scene and subsequently harmonizing their style with the environment, or even superresolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>During training, we decide to crop images first before feeding them into the generator. Consequently, learning can be done in a self-supervised way thanks to the fact that we are now able to enforce the output to approximate the original, uncropped image. Any sufficiently large dataset of unlabelled, natural photos will therefore suffice. We continue with the MIT CSAIL Places365-Standard dataset, which contains millions of images with landscapes, buildings and other everyday scenarios intended for scene recognition <ref type="bibr" target="#b12">[13]</ref>. A second set of experiments was performed with a dataset consisting of images scraped from WikiArt <ref type="bibr" target="#b13">[14]</ref>. This website holds a database of visual artwork belonging to many different categories.</p><p>The photos will first be resized to 192x192 as a preprocessing step, and the generator will then be tasked with expanding a crop of 128x128 back into a 192x192 image. Note that adding 32-pixels at every boundary might not seem significant, but we consider this configuration to be quite ambitious already: the total output size is 2.25 times that of the input, meaning that over half of all pixels will be hallucinated. In practice, the generator G maps a partially masked 192x192 color image to an outpainted variant of the same dimensions, with the masked part replaced by the model's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIG. 4:</head><p>Context encoder trained with joint reconstruction and adversarial loss for semantic outpainting, based on <ref type="bibr" target="#b6">[7]</ref>. Note that as opposed to inpainting, the decoder is almost an exact mirroring of the encoder, since both input and output have the same dimensions.  Many architectural aspects and ideas can be naturally adopted from inpainting. The context encoder part of the generator network G repeatedly downsamples the masked input through six convolutional layers, in order to efficiently capture the image content and object semantics within some embedding space. Next, the decoder consists of a special kind of layers called up-convolutional or deconvolutional, which can be understood as having a fractional stride in order to 'undo' the downsampling performed by the encoder <ref type="bibr" target="#b6">[7]</ref>.</p><p>The discriminator D is another deep neural network that estimates the probability of the ground truth or the hallucinated image being real. In inpainting, D typically sees the generated part only <ref type="bibr" target="#b6">[7]</ref>, although in this project we decide to operate on the full outpainted image in order to discourage G from introducing obvious perceptual discontinuities or other kinds of structural inconsistencies. The architecture we use for D results in a 24 * 24 grid of probabilities whose errors are averaged during the training process.</p><p>See <ref type="figure">Figure 4</ref> as well as <ref type="table" target="#tab_2">Tables I and II</ref> for a detailed overview of the system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>Training is done for 200 epochs, with a fixed learning rate of α = 0.0003 and two Adam optimizers with β 1 = 0.5, β 2 = 0.999. The loss functions are as follows:</p><formula xml:id="formula_0">L rec = ||x − G(x)|| 1 (2.1) L adv = ||D(G(x)) − 1|| 2 2 (2.2) L G = λ rec L rec + λ adv L adv (2.3) L D = ||D(x) − 1|| 2 2 + ||D(G(x)) − 0|| 2 2 (2.4)</formula><p>Using an L 1 reconstruction loss instead of L 2 helps produce less blurry images <ref type="bibr" target="#b14">[15]</ref>. The weight of the adversarial loss λ adv relative to the reconstruction loss λ rec = 1 − λ adv turned out to be particularly tricky to adjust; this factor was initially set to λ adv = 0.001 as in various existing works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>, although the GAN kept collapsing into a failure mode where the adversarial loss did not move away from 1. This means that G was unable to fool D, so D was ahead of G and could always tell real from fake successfully. A working remedy involved varying λ adv (n) throughout time as a function of the epoch n as follows: This will punish the generator more heavily for producing unrealistic outputs as time progresses, rather than just  enforcing an accurate pixel-wise reconstruction.</p><formula xml:id="formula_1">λ adv (n) =         </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Harmonization</head><p>Due to the visual fuzziness of our initial experiments, we came up with the idea of leveraging SinGAN's harmonization capabilities in an attempt to improve the fidelity of the hallucinated outputs, serving as a second possible approach to the outpainting problem. To this end, we first train a SinGAN model on the original high-quality image, then propagate it forward through the outpainting generator (which produces a low-resolution output), and then try to super-resolve this result by injecting it into one of the coarser scales of SinGAN. The hope is that the model will harmonize the outpainted parts with the style of the original image that it was trained on, while simultaneously synthesizing a finer-scale, higher-resolution variant by pushing it up through the hierarchy of multiscale generators. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments</head><p>Three models were trained and evaluated, differing in the following aspects:</p><p>• 200k images of Places365, with L G = L rec , so only the L 1 reconstruction loss is considered.</p><p>• 200k images of Places365, with fully functional joint reconstruction and adversarial loss functions.</p><p>• 50k images of WikiArt, with fully functional joint loss functions.  Since training takes a long time, the second model's training process was momentarily interrupted and resumed in order to upgrade the adversarial loss weight to the better performing Equation 2.5. Selected learning curves are plotted in <ref type="figure" target="#fig_2">Figure 5</ref>. Note that the discriminator D seems to have a harder time deciding for the WikiArt dataset than for Places365, both in terms of adversarial loss L adv and regular discriminator loss L D . This suggests a priori that the artwork domain is inherently more diverse than natural scenery in a visual sense, making it harder to reliably classify the authenticity of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluations</head><p>In the final step of the outpainting pipeline, the input is pasted on top of and blended with the output using a simple 16-pixel wide gradient alpha mask. Compression artefacts arising from the autoencoder-like operation at the center of the image are thus resolved by simply hiding them. However, two quantitative performance metrics, the mean squared error and the discriminator's output values, are calculated from the original, unmodified result. See <ref type="table" target="#tab_2">Table III</ref> for an overview of these metrics for the three trained models as calculated on the respective test sets. Including the adversarial loss increases the reconstruction loss, and outpainted artwork fools the discriminator more often than when we stay within the domain of natural images.</p><p>Non-adversarial versus adversarial models See <ref type="figure" target="#fig_3">Figure 6</ref> for samples of generators trained with λ adv = 0 and λ adv = 0.040 respectively. If only the L 1 reconstruction loss is used, then the hallucinated part looks fuzzy and unrealistic. Enabling the discriminator clearly has a sharpening effect on the output, although the results start to look excessively decorated at times. It seems that the trade-off between the reconstruction and adversarial loss functions should be more finely investigated.</p><p>Variation in reconstruction quality See <ref type="figure">Figure 7</ref> for samples where the MSE between the output and ground truth is either particularly low or high, when including the adversarial loss. It seems that 'smooth' images such as landscapes perform best, while highly detailed images including indoor scenes perform worst. This is understandable: if the dimensions of objects within the image are smaller than what is being omitted, they become impossible for the outpainter to predict.</p><p>Artwork outpainting See <ref type="figure">Figure 8</ref> for a few examples of generative outpainting performed on the WikiArt validation-and test sets. In this application, we answer the question 'What would the artist have drawn if he/she   had a bigger canvas?'. The results look comparatively accurate and visually pleasing, for which we think the reason is that 'imperfections' in art (to the extent that term is even well-defined) are much less bothersome than in natural imagery. It is also interesting to observe that when the output's realism D(G(x)) is maximized, a bias towards specific types of artwork content and/or genres can be perceived as in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recursive outpainting</head><p>In contrast to inpainting, outpainting does not have to be performed just once; in theory, there is no limit as to how many times we can extrapolate a single image. <ref type="figure" target="#fig_0">Figure 10</ref> reveals an interesting aspect of the outpainting network: it seems that given enough iterations, the images eventually start to converge to some kind of 'eigenmode' of the generator. We moreover observe that the painting of a park with flowers seems to slowly morph into a landscape with clouds as we keep hallucinating outwards, suggesting that this scenery must appear (disproportionately) often in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Harmonization</head><p>Our second method for outpainting introduces an additional step that involves employing SinGAN as described earlier. The biggest drawback of this approach is that it takes around an hour to train SinGAN on a single input image, in contrast to the outpainter itself which is a model that can simply be applied anytime once it is trained on a dataset. Nevertheless, we tested out the joint harmonization and super-resolution process on several images that demonstrate its potential, as seen in <ref type="figure" target="#fig_0">Figure 11</ref>. When applied on the output of the reconstruction loss minimizer, it has a sharpening effect. Furthermore, despite the fact that the end result still lacks photorealism, we observe fewer glitches than the first approach which uses an adversarially trained GAN directly. Arbitrarily large resolutions could in principle be supported by SinGAN, although in practice a huge amount of GPU memory (&gt; 12GB) is needed to scale beyond a maximum image dimension of around 500 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>Image outpainting is a novel but exciting idea that holds promise, especially when cascaded with SinGAN to further increase the output fidelity. The models trained in this project still contain some glitches, but we believe these could be alleviated by closer investigations into the subject. Non-photorealistic images such as artwork seem to produce convincing results, which we largely attribute to human judgement becoming more permissive rather than to a better-performing model. V. FUTURE WORK Other than architectural improvements, possible directions for future research include:</p><p>• While enabling the discriminator in this project significantly sharpened the outpainting results, the training stability could be better and the model was not very successful in enforcing natural object semantics across the image. Instead of a single discriminator, utilizing both a global and local variant might increase the overall consistency of generated images as well as improve the realism of smallerscale structures <ref type="bibr" target="#b4">[5]</ref>.</p><p>• At the present time, it is unknown how the model reacts to slight variations in the input space. Hence, video outpainting presents a set of new interesting challenges, mostly related to temporal consistency of the hallucinations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 :</head><label>1</label><figDesc>Image outpainting idea. FIG. 2: Demonstration of the outpainting task, as achieved using an adversarially trained generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 5 :</head><label>5</label><figDesc>Learning curves for the outpainting GAN, where the difficulty of training GANs is observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 6 :</head><label>6</label><figDesc>Comparison between outpainting on Places365 without or with adversarial loss, demonstrating blurry results in the former case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Low reconstruction error. (b) High reconstruction error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 7 :FIG. 8 :FIG. 9 :</head><label>789</label><figDesc>Examples of outpainting on Places365 selected for varying MSEs (lower is better), revealing semantic differences within the images. Qualitative illustration of artwork outpainting on WikiArt. Examples of outpainting on WikiArt selected for high discriminator output values, indicating a bias towards highly decorated paintings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 10 :</head><label>10</label><figDesc>Recursive outpainting on both natural photos and artwork, revealing some glitches but also intriguing 'eigenmodes' of the network.FIG. 11: Qualitative comparison of outpainting without adversarial loss followed by SinGAN harmonization (which has an adversarial aspect by nature), versus direct adversarial outpainting with the proposed GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Layers of the generator network G.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Layers of the discriminator network D.</figDesc><table /><note>B. Architecture</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III :</head><label>III</label><figDesc>Mean square error and realism (i.e. probability of legitimacy) statistics.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that our definition of the term harmonization is slightly different from what is actually meant in the referred paper, but the underlying process is exactly what we intend to perform anyway.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>I would like to thank Prof. Peter Belhumeur for providing the exciting opportunity to conduct this research project, and for offering a great learning opportunity in his excellent course Deep Learning for Computer Vision. I am also grateful to John Daciuk for giving me access to his WikiArt dataset, thus saving me time otherwise spent scraping while also allowing for aesthetically appealing experiments to be conducted more easily.</p><p>This work was performed while being the recipient of a Belgian American Educational Foundation Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="9910" to="69" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Networks</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5589" to="5626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Framebreak: Dramatic image extrapolation by guided shift-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1171" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Painting Outside the Box: Image Outpainting with GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sabini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Rusak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Widecontext semantic image extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Snapseed -apps on google play</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SinGAN: Learning a Generative Model from a Single Natural Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Places: A 10 Million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Wikiart: Visual art encyclopedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An Introduction to Image Synthesis with Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-Aware Semantic Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
