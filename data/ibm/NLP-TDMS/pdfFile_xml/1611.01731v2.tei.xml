<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACCEPTED BY IEEE TIP 1 Deep Label Distribution Learning With Label Ambiguity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin-Bin</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xin</forename><surname>Geng</surname></persName>
						</author>
						<title level="a" type="main">ACCEPTED BY IEEE TIP 1 Deep Label Distribution Learning With Label Ambiguity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Label distribution</term>
					<term>deep learning</term>
					<term>age estima- tion</term>
					<term>head pose estimation</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation, head pose estimation, multi-label classification and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and groundtruth label distributions using deep ConvNets. The proposed DLDL (Deep Label Distribution Learning) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from over-fitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ONVOLUTIONAL Neural Networks (ConvNets) have achieved state-of-the-art performance on various visual recognition tasks such as image classification <ref type="bibr" target="#b0">[1]</ref>, object detection <ref type="bibr" target="#b1">[2]</ref> and semantic segmentation <ref type="bibr" target="#b2">[3]</ref>. The availability of a huge set of training images is one of the most important factors for their success. However, it is difficult to collect sufficient training images with unambiguous labels in domains such as age estimation <ref type="bibr" target="#b3">[4]</ref>, head pose estimation <ref type="bibr" target="#b4">[5]</ref>, multi-label classification and semantic segmentation. Therefore, exploiting deep learning methods with limited samples and ambiguous labels has become an attractive yet challenging topic.</p><p>Why is it difficult to collect a large and accurately labeled training set? Firstly, it is difficult (even for domain experts) to provide exact labels to some tasks. For example, the pixels close to object boundaries are very difficult to label for annotators in semantic segmentation. In addition, pixel labeling is a time-consuming task that may limit the amount of training samples. Another example is that people's apparent age and head pose is difficult to describe with an accurate number. Secondly, it is very hard to gather complete and sufficient data. For example, it is difficult to build an age dataset covering people from 1 to 85 years old, and ensure that every age in this range has enough associated images. Similar difficulties arise in head pose estimation, where head poses are usually collected at a small set of angles with a 10 • or 15 • increment. Thus, the publicly available age, head pose and semantic segmentation datasets are small scale compared to those in image classification tasks.</p><p>These aforementioned small datasets have a common characteristic, i.e., label ambiguity, which refers to the uncertainty among the ground-truth labels. On one hand, label ambiguity is unavoidable in some applications. We usually predict another person's age in a way like "around 25", which indicates using not only 25, but also neighboring ages to describe the face. And, different people may have different guesses towards the same face. Similar situations also hold for other types of tasks. The labels of pixels at object boundaries are difficult to annotate because of the inherent ambiguity of these pixels in semantic segmentation. On the other hand, label ambiguity can also happen if we are not confident in the labels we provide for an image. In the multi-label classification task, some objects are clearly visible but difficult to recognize. This type of objects are annotated as Difficult in the PASCAL Visual Object Classes (VOC) classification challenge <ref type="bibr" target="#b5">[6]</ref>, e.g., the chair in the third image of the first row in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>There are two main types of labeling methods: single-label recognition (SLR) and multi-label recognition (MLR). SLR assumes one image or pixel has one label and MLR assumes that one image or pixel may be assigned multiple labels. Both SLR and MLR aim to answer the question of which labels can be used to describe an image or pixel, but they can not describe the label ambiguity associated with it. Label ambiguity will help improve recognition performance if it can be reasonably exploited. In order to utilize label correlation (which may be considered as a consequence of label ambiguity in some applications), Geng et al. proposed a label distribution learning (LDL) approach for age estimation <ref type="bibr" target="#b3">[4]</ref> and head pose estimation <ref type="bibr" target="#b6">[7]</ref>. Recently, some improvements of LDL have been proposed. <ref type="bibr">Xing</ref>  LDL methods <ref type="bibr" target="#b7">[8]</ref>. Furthermore, He et al. generated age label distributions through weighted linear combination of the input image's label and its context-neighboring samples <ref type="bibr" target="#b8">[9]</ref>. However, these methods are suboptimal because they only utilize the correlation of neighboring labels in classifier learning, but not in learning the visual representations. Deep ConvNets have natural advantages in feature learning. Existing ConvNet frameworks can be viewed as classification and regression models based on different optimization objective functions. In many cases, the softmax loss and 2 loss are used in deep ConvNet models for classification <ref type="bibr" target="#b9">[10]</ref> and regression problems <ref type="bibr" target="#b10">[11]</ref>, respectively. The softmax loss maximizes the estimated probability of the ground-truth class without considering other classes, and the 2 loss minimizes the squared difference between the estimated values of the network and the ground-truth. These methods have achieved satisfactory performance in some domains such as image classification, human pose estimation and object detection. However, existing deep learning methods cannot utilize the label ambiguity information. Moreover, a well-known fact is that learning a good ConvNet requires a lot of images.</p><p>In order to solve the issues mentioned above, we convert both traditional SLR and MLR problems to label distribution learning problems. Every instance is assigned a discrete label distribution y according to its ground-truth. The label distribution can naturally describe the ambiguous information among all possible labels. Through deep label distribution learning, the training instances associated with each class label is significantly increased without actually increase the number of the total training examples. <ref type="figure" target="#fig_1">Fig. 1</ref> intuitively shows four examples of label distribution for different recognition tasks. Then, we utilize a deep ConvNet to learn the label distribution in both feature learning and classifier learning. Since we learn label distribution with deep ConvNets, we call our method DLDL: Deep Label Distribution Learning. The benefits of DLDL are summarized as follows:</p><p>• DLDL is an end-to-end learning framework which utilizes the label ambiguity in both feature learning and classifier learning; • DLDL not only achieves more robust performance than existing classification and regression methods, but also effectively relaxes the requirement for large amount of training images, e.g., a training face image with groundtruth label 25 is also useful for predicting faces at age 24 or 26; • DLDL (only single model without ensemble) achieves better performance than the state-of-the-art methods on age and head pose estimation tasks. DLDL also improves the performance for multi-label classification and semantic segmentation. The rest of this paper is organized as follows. We first review the related work in Section II. Then, Section III proposes the DLDL framework, including the DLDL problem definition, DLDL theory, label distribution construction and training details. After that, the experiments are reported in Section IV. Finally, Section V presents discussions and the conclusion is given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In the past two decades, many efforts have been devoted to visual recognition, including at least image classification, object detection, semantic segmentation, and facial attribute (apparent age and head pose) estimation. These works can be divided into two streams. Earlier research was mainly based on hand-crafted features, while more recent ones are usually deep learning methods. In this section, we briefly review these related approaches.</p><p>Methods based on hand-crafted features usually include two stages. The first stage is feature extraction. The second stage learns models for recognition, detection or estimation using these features. SVM, random forest <ref type="bibr" target="#b11">[12]</ref> and neural networks have commonly been used during the learning stage. In addition, Geng et al. proposed the label distribution learning approach to utilize the correlation among adjacent labels, which further improved performance on age estimation <ref type="bibr" target="#b3">[4]</ref> and head pose estimation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Although important progresses have been made with these features, the hand-crafted features render them suboptimal for particular tasks such as age or head pose estimation. More recently, learning feature representation has shown great advantages. For example, Lu et al. <ref type="bibr" target="#b12">[13]</ref> tried to learn costsensitive local binary features for age estimation.</p><p>Deep learning has substantially improved upon the stateof-the-art in image classification <ref type="bibr" target="#b9">[10]</ref>, object detection <ref type="bibr" target="#b1">[2]</ref>, semantic segmentation <ref type="bibr" target="#b2">[3]</ref> and many other vision tasks. In many cases, the softmax loss is used in deep models for classification <ref type="bibr" target="#b9">[10]</ref>. Besides classification, deep ConvNets have also been trained for regression tasks such as head pose estimation <ref type="bibr" target="#b13">[14]</ref> and facial landmark detection <ref type="bibr" target="#b14">[15]</ref>. In regression problems, the training procedure usually optimizes a squared 2 loss function. Satisfactory performance has also been obtained by using Tukey's biweight function in human pose estimation <ref type="bibr" target="#b10">[11]</ref>. In terms of model architecture, deep ConvNet models which use deeper architecture and smaller convolution filters (e.g., VGG-Nets <ref type="bibr" target="#b15">[16]</ref> and VGG-Face <ref type="bibr" target="#b16">[17]</ref>) are very powerful. Nevertheless, these deep learning methods do not make use of the presence of label ambiguity in the training set, and usually require a large amount of training data.</p><p>A latest approach, in Inception-v3 <ref type="bibr" target="#b17">[18]</ref>, is based on label smoothing (LS). Instead of only using the ground-truth label, they utilize a mixture of the ground-truth label and a uniform distribution to regularize the classifier. However, LS is limited to the uniform distribution among labels rather than mining labels' ambiguous information. We believe that label ambiguity is too important to ignore. If we make good use of the ambiguity, we expect the required number of training images for some tasks could be effectively reduced.</p><p>In this paper, we focus on how to exploit the label ambiguity in deep ConvNets. Age and head pose estimation from still face images are suitable applications of the proposed research. In addition, we also extend our works to multi-label classification and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED DLDL APPROACH</head><p>In this section, we firstly give the definition of the DLDL problem. Then, we present the DLDL theory. Next, we propose the construction methods of label distribution for different recognition tasks. Finally, we briefly introduce the DLDL architecture and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The deep label distribution learning problem</head><p>Given an input image, we are interested in estimating a category output y (e.g., age or head pose angles). For two input images X 1 and X 2 with ground-truth labels y 1 and y 2 , X 1 and X 2 are supposed to be similar to each other if the correlation of y 1 and y 2 is strong, and vice versa. For example, the correlation between faces aged 32 and 33 should be stronger than that between faces aged 32 and 64, in terms of facial details that reflect the age (e.g., skin smoothness). In other words, we expect high correlation among input images with similar outputs. The label distribution learning approach [4], <ref type="bibr" target="#b6">[7]</ref> exploited such correlations in the machine learning phase, but used features that are extracted ignoring these correlations. The proposed DLDL approach, however, is an end-to-end deep learning method which utilizes such correlation information in both feature learning and classifier learning. We will also extend DLDL to handle other types of label ambiguity beyond correlation.</p><p>To fulfill this goal, instead of outputting a single value y for an input X, DLDL quantizes the range of possible y values into several labels. For example, in age estimation, it is reasonable to assume that 0 &lt; y ≤ 85, and it is a common practice to estimate integer values for ages. Thus, we can define the set L = {1, 2, . . . , 85} as the ordered label set for age estimation. The task of DLDL is then to predict a label distribution y ∈ R 85 , where y i is the estimated probability that X should be predicted to be i years old. By estimating an entire label distribution, the deep learning machine is forced to take care of the ambiguity among labels.</p><p>Specifically, the input space of our framework is X = R h×w×d , where h, w and d are the height, width, and number of channels of the input image, respectively. DLDL predicts a label distribution vector y ∈ R |Y | , where Y = {l 1 , l 2 , . . . , l C } is the label set defined for a specific task (e.g., the L above). We assume Y is complete, i.e., any possible y value has a corresponding member in Y . A training data set with N instances is then denoted as D = {(X 1 , y 1 ), · · · , (X N , y N )}. We use boldface lowercase letters like y to denote vectors, and the i-th element of y is denoted as y i . The goal of DLDL is to directly learn a conditional probability mass functionŷ = p(y|X; θ) from D, where θ is the parameters in the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep label distribution learning</head><p>Given an instance X with label distribution y, we assume that x = φ(X; θ) is the activation of the last fully connected layer in a deep ConvNet. We use a softmax function to turn these activations into a probability distribution, that is,</p><formula xml:id="formula_0">y j = exp(x j ) t exp(x t ) .<label>(1)</label></formula><p>Given a training data set D, the goal of DLDL is to find θ to generate a distributionŷ that is similar to y. There are different criteria to measure the similarity or distance between two distributions. For example, if the Kullback-Leibler (KL) divergence is used as the measurement of the similarity between the ground-truth and predicted label distribution, then the best parameter θ * is determined by</p><formula xml:id="formula_1">θ * = argmin θ k y k ln y k y k = argmin θ − k y k lnŷ k . (2)</formula><p>Thus, we can define the loss function as:</p><formula xml:id="formula_2">T = − k y k lnŷ k .<label>(3)</label></formula><p>Stochastic gradient descent is used to minimize the objective function Eq. 3. For any k and j,</p><formula xml:id="formula_3">∂T ∂ŷ k = − y k y k ,<label>(4)</label></formula><p>and the derivative of softmax (Eq. 1) is well known, as</p><formula xml:id="formula_4">∂ŷ k ∂x j =ŷ k δ {k=j} −ŷ j ,<label>(5)</label></formula><p>where δ {k=j} is 1 if k = j, and 0 otherwise. According to the chain rule, for any fixed j, we have</p><formula xml:id="formula_5">∂T ∂x j = k ∂T ∂ŷ k ∂ŷ k ∂x j = −y j +ŷ j k y k = −y j +ŷ j . (6)</formula><p>Thus, the derivative of T with respect to θ is</p><formula xml:id="formula_6">∂T ∂θ = (ŷ − y) ∂x ∂θ .<label>(7)</label></formula><p>Once θ is learned, the label distributionŷ of any new instance X can be generated by a forward run of the network. If the expected class label is a single one, DLDL outputs</p><formula xml:id="formula_7">l i * ∈ Y , where i * = argmax iŷ i .<label>(8)</label></formula><p>Prediction with multiple labels is also allowed, which could be a set {l i |ŷ i &gt; ξ} where ξ ∈ [0, 1] is a predefined threshold. If the expected output is a real number, DLDL predicts the expectation ofŷ i , as</p><formula xml:id="formula_8">iŷ i l i ,<label>(9)</label></formula><p>where l i ∈ Y . This indicates that DLDL is suitable for both classification and regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Label distribution construction</head><p>The ground-truth label distribution y is not available in most existing datasets, which must be generated under proper assumptions. A desirable label distribution y = (y 1 , y 2 , . . . , y C ) must satisfy some basic principles: (1) y should be a probability distribution. Thus, we have y i ∈ [0, 1] and</p><formula xml:id="formula_9">C i=1 y i = 1. (2)</formula><p>The probability values y i should have difference among all possible labels associated with an image. In other words, a less ambiguous category must be assigned high probability and those more ambiguous labels must have low probabilities. In this section, we propose the way to construct label distributions for age estimation, head pose estimation, multi-label classification and semantic segmentation.</p><p>For age estimation, we assume that the probabilities should concentrate around the ground-truth age y. Thus, we quantize y to get y using a normal distribution. For example, the apparent age of a face is labeled by hundreds of users. The ground-truth (including a mean µ and a standard deviation σ) is calculated from all the votes. For this problem, we find the range of the target y (e.g., 0 &lt; y ≤ 85), quantize it into a complete and ordered label set L = {l 1 , l 2 , . . . , l C }, where C is the label set size and l i ∈ R are all possible predictions for y. A label distribution y is then (y 1 , y 2 , . . . , y C ), where y i is the probability that y = l i (i.e., y i = Pr(y = l i ) for 1 ≤ i ≤ C). Since we use equal step size in quantizing y, the normal p.d.f. (probability density function) is a natural choice to generate the ground-truth y from µ and σ:</p><formula xml:id="formula_10">y j = p(l j |µ, σ) k p(l k |µ, σ) ,<label>(10)</label></formula><p>where p(l j |µ, σ)</p><formula xml:id="formula_11">= 1 √ 2πσ exp − (lj −µ) 2 2σ 2</formula><p>. <ref type="figure" target="#fig_1">Fig. 1a</ref> shows a face and its corresponding label distribution. For problems where σ is unknown, we will show that a reasonably chosen σ also works well in DLDL.</p><p>For head pose estimation, we need to jointly estimate pitch and yaw angles. Thus, learning joint distribution is also necessary in DLDL. Suppose the label set is L = {l jk |j = 1, · · · , n 1 , k = 1, · · · , n 2 }, where l jk is a pair of values. That is, we want to learn the joint distribution of two variables. Then, the label distribution y can be represented by an n 1 ×n 2 matrix, whose (j, k)-th element is y jk . For example, when we use two angles (pitch and yaw) to describe a head pose, l jk is a pair of pitch and yaw angles. Given an instance X with ground-truth mean µ and covariance matrix Σ, we calculate its label distribution as</p><formula xml:id="formula_12">y jk = p(l jk ) j k p(l jk ) ,<label>(11)</label></formula><p>where p(l jk ) =</p><formula xml:id="formula_13">1 2π|Σ| 1 2 exp − 1 2 (l jk − µ) T Σ −1 (l jk − µ) .</formula><p>In the above, we assume Σ = σ 2 0 0 σ 2 , that is, the covariance matrix is diagonal. <ref type="figure" target="#fig_1">Fig. 1b</ref> shows a joint label distribution with head pose pitch = 0 • and yaw = 60 • .</p><p>For multi-label classification, a multi-label image always contains at least one object of the class of interest. There are usually multiple labels for an image. These labels are grouped into three different levels, including Positive, Negative and Difficult in the PASCAL VOC dataset <ref type="bibr" target="#b5">[6]</ref>. A label is Positive means an image contains objects from that category, and Negative otherwise. Difficult indicates that an object is clearly visible but difficult to recognize. Existing multi-label methods often view Difficult as Negative, which leads to the loss of useful information. It is not reasonable either if we simply treat Difficult as Positive. Therefore, a nature choice is to use label ambiguity. We define different probabilities for different types of labels, as</p><formula xml:id="formula_14">p P &gt; p D &gt; p N ,<label>(12)</label></formula><p>for Positive, Difficult and Negative labels, respectively. Furthermore, an 1 normalization is applied to ensure</p><formula xml:id="formula_15">C i=1 y i = 1: y j = p(l j ) k p(l k ) ,<label>(13)</label></formula><p>where p(l k ) equals p P , p D or p N if the label l k is Positive, Difficult or Negative, respectively. The label distribution is shown for a multi-label image in <ref type="figure" target="#fig_1">Fig. 1c</ref>. For semantic segmentation, we need to label a pixel as belonging to one class if it is a pixel inside an object of that class, or as the background otherwise. Let y ijk denote the annotation of the (i, j)-th pixel, where k = {0, 1, . . . , C} (assuming there are C categories and 0 for background). Fully Convolutional Networks (FCN) have been an effective solution to this task. In FCN <ref type="bibr" target="#b2">[3]</ref>, a ground-truth label l means that y ijl = 1 and y ijk = 0 for all k = l. However, it is very difficult to specify ground-truth labels for pixels close to object boundaries, because labels of these pixels are inherently ambiguous. We propose a mechanism to describe the label ambiguity in the boundaries. Considering a Gaussian kernel matrix f K×K , we replace the original label distribution y with y , as</p><formula xml:id="formula_16">y ijk = K i =1 K j =1 f i j × y i +(i−1)S−P,j +(j−1)S−P,k . (14)</formula><p>where f i j ≥ 0,</p><formula xml:id="formula_17">K i =1 K j =1 f i j = 1,</formula><p>K is the kernel size, P and S are padding and stride sizes. In our experiment, we set K = 5, P = 2 and S = 1, and the generated label distribution is <ref type="figure" target="#fig_1">Fig. 1d</ref> gives the semantic label distribution for a bird image which shows that the ambiguity is encoded in the label distributions.</p><formula xml:id="formula_18">y ijk = y ijk k y ijk .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The DLDL architecture and training details</head><p>We use a deep ConvNet and a training set D to learn aŷ as the estimation of y. The structure of our network is based on popular deep models such as ZF-Net <ref type="bibr" target="#b18">[19]</ref> and VGG-Nets <ref type="bibr" target="#b15">[16]</ref>. The ZF-Net consists five convolution layers, followed by three fully connected layers. The VGG-Nets architecture includes 16 or 19 layers. We modify the last fully connected layer's output based on the task and replace the original softmax loss function with the KL loss function. In addition, we use the parameter ReLU <ref type="bibr" target="#b19">[20]</ref> for ZF-Net. In our network, the input is an order three tensor X h×w×d and the outputŷ may be a vector (age estimation and multi-label classification), a matrix (head pose estimation) or a tensor (semantic segmentation).</p><p>In this paper, we train the deep models in two ways: Training from scratch. For ZF-Net, the initialization is performed randomly, based on a Gaussian distribution with zero mean and 0.01 standard deviation, and biases are initialized to zero. The coefficient of the parameter ReLU is initialized to 0.25. The dropout is applied to the last two fully connected layers with rate 0.5. The coefficient of weight decay is set to 0.0005. Optimization is done by Stochastic Gradient Descent (SGD) using mini-batches of 128 and the momentum coefficient is 0.9. The initial learning rate is set to 0.01. The total number of epochs is about 20.</p><p>Fine-tuning. Three pre-trained models including VGG-Nets (16-layers and 19-layers) and VGG-Face (16-layers) are used to fine-tune for different tasks. We remove these pretrained models' classification layer and loss layer, and put in our label distribution layer which is initialized by the Gaussian distribution N (0, 0.01) and the KL loss layer. The learning rates of the convolutional layers, the first two fully-connected layers and the label distribution layer are initialized as 0.001, 0.001 and 0.01, respectively. We fine-tune all layers by back propagation through the whole net using mini-batches of 32. The total number of epochs is about 10 for age estimation and 20 for multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate DLDL on four tasks, i.e., age estimation, head pose estimation, multi-label classification and semantic segmentation. Our implementation is based on MatConvNet <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b0">1</ref> All our experiments are carried out on a NVIDIA K40 GPU with 12GB of onboard memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Age estimation</head><p>Datasets. Two age estimation datasets are used in our experiments. The first is Morph <ref type="bibr" target="#b21">[22]</ref>, which is one of the largest publicly available age datasets. There are 55,134 face images from more than 13,000 subjects. Ages range from 16 to 77. Since no TRAIN/TEST split is provided, 10-fold crossvalidation is used for Morph.</p><p>The second dataset is from the apparent age estimation competition, the first competition track of the ICCV ChaLearn LAP 2015 workshop <ref type="bibr" target="#b22">[23]</ref>. Compared with Morph, this dataset (ChaLearn) consists of images collected in the wild, without any position, illumination or quality restriction. The only condition is that each image contains only one face. The dataset has 4,699 images, and is split into 2,476 training (TRAIN), 1,136 validation (VAL) and 1,087 testing (TEST) images. The apparent age (i.e., how old does this person look like) of each image is labeled by multiple individuals. The age of face images range from 3 to 85. For each image, its mean age and the corresponding standard deviation are given. Since the ground-truth for TEST images are not published, we train on the TRAIN split and evaluate on the VAL split of ChaLearn images.</p><p>Baselines. To demonstrate the effectiveness of DLDL, we firstly consider two related methods as baselines: Conv-Net+LS (KL) and ConvNet+LD (α-div). The former uses label smoothing (LS) <ref type="bibr" target="#b17">[18]</ref> as ground-truth and KL divergence as loss function. The latter uses label distribution (LD) as ground-truth and α divergence <ref type="bibr" target="#b23">[24]</ref> as loss function, which is</p><formula xml:id="formula_19">T = −2 k ( √ y k − ŷ k ) 2 .<label>(16)</label></formula><p>In addition, we also compare DLDL with the following baseline methods:  by the mapping 2(y−min) max − min − 1, where max and min are the maximum and minimum values in the training label set. During prediction, the R-ConvNet regression result is reverse mapped to getŷ. To speed up convergence, the last fully connected layer is followed a hyperbolic tangent activation function f (x) = tanh(x), which maps [−∞, +∞] to [−1, +1] <ref type="bibr" target="#b13">[14]</ref>. The squared 2 , 1 and -ins loss functions are used in R-ConvNet. Implementation details. We use the same preprocessing pipeline for all compared methods, including face detection, facial key points detection and face alignment, as shown in <ref type="figure" target="#fig_2">Fig 2.</ref> We employ the DPM model <ref type="bibr" target="#b25">[26]</ref> to detect the main facial region. Then, the detected face is fed into cascaded convolution networks <ref type="bibr" target="#b14">[15]</ref> to get the five facial key points, including the left/right eye centers, nose tip and left/right mouth corners. Finally, based on these facial points, we align the face to the upright pose. Data augmentation are only applied to the training images for ChaLearn. For one color input training image, we generate its gray-scale version, and left-right flip both color and gray-scale versions. Thus, every training image turns into 4 images.</p><formula xml:id="formula_20">• BFGS-LDL</formula><p>We define Y = {1, 2, . . . , 85} for both datasets. The label distribution of each image is generated using Eq. 10. The mean µ is provided in both Morph and ChaLearn. The standard deviation σ, however, is provided in ChaLearn but not in Morph. We simply set σ = 2 in Morph. Experiments for different methods are conducted under the same data splits.</p><p>Evaluation criteria. Mean Absolute Error (MAE) and Cumulative Score (CS) are used to evaluate the performance of age estimation. MAE is the average difference between the predicted and the real age:</p><formula xml:id="formula_21">M AE = 1 N N n=1 |l n − l n |,<label>(17)</label></formula><p>wherel n and l n are the estimated and ground-truth age of the n-th testing image, respectively. CS is defined as the accuracy rate of correct estimation:</p><formula xml:id="formula_22">CS g = C g N × 100%,<label>(18)</label></formula><p>where C g is the number of correct estimation, i.e., testing images that satisfy |l n − l n | ≤ g. In our experiment, g ∈ {1, 2, . . . , 30}. In addition, a special measurement (namederror) is defined by the ChaLearn competition, computed as Used additional external face images (i.e., IMDB-WIKI); <ref type="bibr" target="#b2">3</ref> Used pre-trained model (i.e., VGG-Nets or VGG-Face).</p><formula xml:id="formula_23">= 1 N N n=1 1 − exp − (l n − l n ) 2 2σ 2 n .<label>(19)</label></formula><p>Results. <ref type="table" target="#tab_1">Table I</ref> lists results on both datasets. The upper part shows results in the literature. The middle part shows the baseline results. The lower part shows the results of the proposed approach. The first term in the parenthesis behind each method is the loss function corresponding to the method. Max or Exp represent predicting according to Eq. 8 or 9, respectively. Since cross-validation is used in Morph, we also provide its standard deviations.</p><p>From <ref type="table" target="#tab_1">Table I</ref>, we can see that DLDL consistently outperforms baselines and other published methods. The difference between DLDL (KL, Max) and its competitor C-ConvNet (softmax, Max) is 0.51 on Morph. This gap is more than 6 times the sum of their standard deviations (0.03+0.05), showing statistically significant differences. The advantage of DLDL over R-ConvNet, C-ConvNet and ConvNet+LS suggests that learning label distribution is advantageous in deep end-to-end models. DLDL has much better results than BFGS-LDL, which shows that the learned deep features are more powerful than manually designed ones. Compared to ConvNet+LD (α-div), DLDL (KL) achieves lower MAE on both datasets. It indicates that KL-divergence is better than α-divergence for measuring the similarity of two distributions in this context.</p><p>We find that C-ConvNet and R-ConvNet are not stable. The R-ConvNet ( 1 ) method, although being the second best method for ChaLearn, is inferior to C-ConvNet (softmax, Exp) for Morph. In addition, we also find that Eq. 9 is better than Eq. 8 in many cases, which suggests that Eq. 9 is more suitable than Eq. 8 for age estimation.    Fine-tuning DLDL. Instead of training DLDL from scratch, we also fine-tune the network of VGG-Face <ref type="bibr" target="#b16">[17]</ref>. On the small scale ChaLearn dataset, the MAE of DLDL is reduced from 5.34 to 3.51, yielding a significant improvement. The -error of DLDL is reduced from 0.44 to 0.31, which is close to the best competition result 0.28 <ref type="bibr" target="#b29">[30]</ref> on the validation set. In <ref type="bibr" target="#b30">[31]</ref>, external training images (260,282 additional external training images with real age annotation) were used. DLDL only uses the ChaLearn dataset's 2,476 training images and is the best among ChaLearn teams that do not use external data <ref type="bibr" target="#b22">[23]</ref>. In the competition, the best external-data-free -error is 0.48, which is worse than DLDL's. However, the idea in <ref type="bibr" target="#b30">[31]</ref> to use external data is useful for further reducing DLDL's estimation error. <ref type="figure" target="#fig_3">Fig. 3a</ref> and <ref type="figure" target="#fig_3">Fig. 3b</ref> show the CS curves on ChaLearn and Morph datasets. At every error level, our DLDL finetuned VGG-Face always achieves the best accuracy among all methods. It is noteworthy that the CS curves of DLDL (KL, Max) and ConvNet (α-div, Max) are very close to that of the DLDL+VGG-Face (KL, Max) on Morph even without lots of external data and very deep model. This observation supports the idea that using DLDL can achieve competitive performance even with limited training samples.</p><p>In <ref type="figure" target="#fig_5">Fig. 4</ref>, we show some examples of face images from the ChaLearn validation set and predicted label distributions by DLDL (KL, Exp). In many cases, our solution is able to accurately predict the apparent age of faces. Failures may come from two causes. The first is the failure to detect or align the face. The second is some extreme conditions of face images such as occlusion, low resolution, heavy makeup and old photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Head pose estimation</head><p>Datasets. We use three datasets in head pose estimation: Pointing'04 <ref type="bibr" target="#b31">[32]</ref>, BJUT-3D <ref type="bibr" target="#b32">[33]</ref> and Annotated Facial Landmarks in the Wild (AFLW) <ref type="bibr" target="#b33">[34]</ref>. In them, head pose is determined by two angles: pitch and yaw. Pointing'04 discretizes the pitch into 9 angles</p><formula xml:id="formula_24">{0 • , ±15 • , ±30 • , ±60 • , ±90 • } and the yaw into 13 angles {0 • , ±15 • , ±30 • , ±45 • , ±60 • , ±75 • , ±90 • }.</formula><p>When the pitch angel is +90 • or −90 • , the yaw angle is always set to 0 • . Thus, there are 93 poses in total. The head images are taken from 15 different human subjects in two different time periods, resulting in 15×2×93 = 2, 790 images.</p><p>BJUT-3D contains 500 3D faces (250 male and 250 female people), acquired by a CyberWare Laser Scanner in an engineered environment. 9 pitch angles {0 • , ±10 • , ±20 • , ±30 • , ±40 • } and 13 yaw angles {0 • , ±10 • , ±20 • , ±30 • , ±40 • , ±50 • , ±60 • } are used. There are in total 93 poses in this dataset, similar to that in Pointing'04. Therefore, 500 × 93 = 46, 500 face images are obtained.</p><p>Unlike Pointing'04 and BJUT-3D, the AFLW is a real-world face database. Head pose is coarsely obtained by fitting a mean 3D face with the POSIT algorithm <ref type="bibr" target="#b34">[35]</ref>. The dataset contains about 24k faces in real-world images. We select 23,409 faces to ensure pitch and yaw angles within [−90 • , 90 • ]. Implementation details. The head region is provided by bounding box annotations in Pointing'04 and AFLW. The BJUT-3D does not contain background regions. Therefore, we will not perform any preprocessing.</p><p>In DLDL, we set σ = 15 • in Pointing'04 and σ = 5 • in BJUT-3D for constructing label distributions. For AFLW, ground-truth of head pose angles are given as real numbers. Ground-truth (pitch and yaw) angles are divided from −90 • to +90 • in steps of 3 • , so we get 61 × 61 = 3, 721 (pitch, yaw) pair category labels. We set σ = 3 • for AFLW. Since the discrete Jeffrey's divergence is used in LDL <ref type="bibr" target="#b6">[7]</ref>, we implement BFGS-LDL with the Kullback-Leibler divergence. All experiments are performed under the same setting, including data splits, input size and network architecture.</p><p>To validate the effectiveness of DLDL for head pose estimation, we use the same baselines as age estimation. Our experiments show that Eq. 9 has lower accuracy than Eq. 8. Hence, we use Eq. 8 in this section.</p><p>Evaluation criteria. Three types of prediction values are evaluated: pitch, yaw, and pitch+yaw, where pitch+yaw jointly estimates the pitch and yaw angles. Two different measurements are used, which is MAE (Eq. 17) and classification accuracy (Acc). When we treat different poses as different classes, Acc measures the pose class classification accuracy. In particular, the MAE of pitch+yaw is calculated as the Euclidean distance between the predicted (pitch, yaw) pair and the ground-truth pair; the Acc of pitch+yaw is calculated by regarding each (pitch, yaw) pair as a class. For R-ConvNet, we only report its MAE but not Acc, because its predicted value are continuous real numbers. All methods are tested with 5-fold cross validation for Pointing'04 and BJUT-3D Results <ref type="table" target="#tab_1">. Tables II, III and IV</ref> show results on Pointing'04, BJUT-3D and AFLW, respectively. Pointing'04 is small scale with only 2,790 images. We observe that BFGS-LDL (with hand-crafted features) has much lower MAE and much higher accuracy than deep learning methods C-ConvNet, R-ConvNet and ConvNet+LS. One reasonable conjecture is that C-ConvNet, R-ConvNet and ConvNet+LS are not well-learned with only small number of training images. DLDL, however, successfully learns the head pose. For example, its accuracy for pitch+yaw is 73.15% (and C-ConvNet is only 42.97%). That is, DLDL is able to perform deep learning with few training images, while C-ConvNet R-ConvNet and ConvNet+LS have failed for this task.</p><p>On BJUT-3D and AFLW which have enough training data, we observe that many deep learning methods show higher  performance than BFGS-LDL. DLDL achieves the best performance: it has much lower MAE and higher accuracy than other methods. Another observation is also worth mentioning. Although R-ConvNet is better than C-ConvNet when label is dense such as age estimation and head pose estimation on AFLW, it is obviously worse than C-ConvNet on BJUT-3D and pointing'04 for head pose estimation which have sparse labels. In other words, the performance of C-ConvNet and R-ConvNet are not very robust, while the proposed method consistently achieves excellent performance. <ref type="figure" target="#fig_3">Fig. 3c</ref> shows the pitch+yaw CS curves on the AFLW dataset. There is an obvious gap between DLDL and baseline methods at every error level. <ref type="figure" target="#fig_6">Fig. 5</ref> shows the predicted label distributions for different head poses on the AFLW testing set using the DLDL model. Our approach can estimate head pose with low errors but may fail under some extreme conditions. It is noteworthy that DLDL may produce more incorrect estimations when both yaw and pitch are large (e.g., ±90 • ). The reason might be that there are much fewer training examples for large angles than for other angles.</p><formula xml:id="formula_25">(+77 • ,-4 • ) (-16 • ,-1 • ) (-1 • ,-30 • ) (+30 • ,+8 • ) (+4 • ,-4 • ) (-36 • ,+13 • ) (-87 • ,-3 • ) (-61 • ,-58 • ) (+63 • ,+12 • ) (+80 • ,-27 • ) −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-label classification</head><p>Datasets. We evaluate our approach for multi-label classification on the PASCAL VOC dataset <ref type="bibr" target="#b5">[6]</ref>: PASCAL VOC2007 and VOC2012. There are 9,963 and 22,531 images in them, respectively. Each image is annotated with one or several labels, corresponding to 20 object categories. These images are divided into three subsets including TRAIN, VAL and TEST sets. We train on the TRAINVAL set and evaluate on the TEST set. The evaluation metric is average precision (AP) and mean average precision (mAP), complying with the PASCAL challenge protocols.</p><p>We denote our methods as Images-Fine-tuning-DLDL (IF-DLDL) and Proposals-Fine-tuning-DLDL (PF-DLDL) when ConvNets are fine-tuned by images and proposals of images, respectively. Details of these two variants are explained later in this section. We compare the proposed approaches with the following methods:</p><p>• VGG+SVM <ref type="bibr" target="#b15">[16]</ref>. This method densely extracted 4,096</p><p>dimensional ConvNet features at the penultimate layer of VGG-Nets pre-trained on ImageNet. These features from different scales (smallest image side Q ∈ {256, 384, 512, 640, 768}) were aggregated by average pooling. Then, these averaged features from two networks ("Net-D" containing 16 layers and "Net-E" containing 19 layers) were further fused by stacking. Finally, <ref type="bibr" target="#b15">[16]</ref> 2 normalized the resulting image features and used these features to train a linear SVM classifier for multi-label classification. • HCP <ref type="bibr" target="#b36">[37]</ref>. HCP proposed to solve the multi-label object recognition task by extracting object proposals from the images. The method used image label and square loss to fine-tune a pre-trained ConvNet. Then, BING <ref type="bibr" target="#b37">[38]</ref> or EdgeBoxes <ref type="bibr" target="#b38">[39]</ref> was used to extract object proposals, which were used to fine-tune the ConvNet again. Finally, scores of these proposals were max-pooled to obtain the prediction. • Fev+Lv <ref type="bibr" target="#b39">[40]</ref>. This approach transformed the multilabel object recognition problem into a multi-class multiinstance learning problem. Two views (label view and feature view) were extracted for each proposal of images. Then, these two views were encoded by a Fisher vector for each image. • IF-VGG-2 and IF-VGG-KL. We fine-tune the VGG-Nets with square loss and multi-label cross-entropy loss <ref type="bibr" target="#b40">[41]</ref> and use them as our IF-DLDL's baselines. They are trained using the same setting.</p><p>Implementation details. According to the ground-truth labels, we set different probabilities for all possible labels on PASCAL VOC dataset. In our experiments, p P = 1, p D = 0.3, p N = 0. Finally, similar to label smoothing, a uniform distribution u i = /20 is added to y, where = 0.01. IF-DLDL. Following <ref type="bibr" target="#b15">[16]</ref>, each training image is individually rescaled by randomly sampling in the range <ref type="bibr">[256,</ref><ref type="bibr">512]</ref>. We randomly crop 256 × 256 patches from these resized images. We also adjust the pooling kernel in the pool5 layer from 3 × 3 to 4 × 4. Max-pooling and Avg-pooling are used at pool5 to train two ConvNets. We obtain four ConvNet models thought fine-tuning "Net-D" and "Net-E". At the prediction stage, the smaller side of each image is scaled to a fixed length Q ∈ {256, 320, 384, 448, 512}. Each scaled image is fed to the fine-tuned ConvNets to obtain the 20-dim probability outputs. These probability outputs from different scales and different models are averaged to form the final prediction.</p><p>PF-DLDL. Following <ref type="bibr" target="#b41">[42]</ref>, we further fine-tune IF-DLDL models with proposals of images to boost performance. For each training image, we employ EdgeBoxes <ref type="bibr" target="#b38">[39]</ref> to produce a set of proposal bounding boxes which are grouped into m clusters by the normalized cut algorithm <ref type="bibr" target="#b45">[46]</ref>. For each cluster, the top k proposals with higher predictive scores generated by EdgeBoxes are resized into square shapes (i.e., 256 × 256). As a result, we can obtain mk proposals for an image. Finally, these mk resized proposals are fed into a fine-tuned IF-DLDL model to obtain prediction scores and these scores are fused by max-pooling to form the prediction distribution of the image. This process can be learned by using an end-to-end way. In our implementation, we set m = 15, k = 1 and m = 15, k = 30 at the training and the prediction stage, respectively. Similar to IF-DLDL, we also average fuse prediction scores of different models to generate the final prediction.</p><p>Results. In <ref type="table" target="#tab_6">Table V</ref>, we compare single model results (average AP of all classes) on VOC2007. Our PF-DLDL defeats all the other methods. Compared with Fev+Lv [40], 1.7% improvement can be achieved by PF-DLDL even without using the bounding box annotation. Compared with HCP-VGG <ref type="bibr" target="#b41">[42]</ref>, our PF-DLDL can achieve 92.3% mAP, which is significantly higher than their 90.9%. This further indicates that it is very important to learn a label distribution.</p><p>Table VI and VII report details of all experimental results on VOC2007 and VOC2012, respectively. It can be seen that IF-DLDL outperforms IF-VGG-2 by 1.1% for VOC2007 and 1.3% for VOC2012, which indicates that the KL loss function is more suitable than 2 loss for measuring the similarity of two label distributions. Furthermore, IF-DLDL improves IF-VGG-KL for about 0.2-0.3 points in mAP, which suggests that learning a label distribution is beneficial. More importantly, PF-DLDL can achieve 93.4% for VOC2007 and 92.4% for VOC2012 in mAP when we average fuse output scores of four PF-DLDL models.</p><p>Our framework shows good performance especially for scene categories such as "chair", 'table" and "sofa". Although PF-DLDL significantly outperforms IF-DLDL in mAP, PF-DLDL has higher computational cost than IF-DLDL on both training and testing stages. Since IF-DLDL does not need region proposals or bounding box information, it may be effectively and efficiently implemented for practical multilabel application such as multi-label image retrieval <ref type="bibr" target="#b46">[47]</ref>. It is also possible that by adopting new techniques (such as the region proposal method using gated unit in <ref type="bibr" target="#b47">[48]</ref>, which has higher accuracy that ours on VOC tasks), the accuracy of our DLDL methods can be further improved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Semantic segmentation</head><p>Datasets. We employ the PASCAL VOC2011 segmentation dataset and the Semantic Boundaries Dataset (SBD) for training the proposed DLDL. There are 2,224 images (1,112 for training and 1,112 for testing) with pixel labels for 20 semantic categories in VOC2011. SBD contains 11,355 annotated images (8,984 for training and 2,371 for testing) from Hariharan et al. <ref type="bibr" target="#b48">[49]</ref>. Following FCN <ref type="bibr" target="#b2">[3]</ref>, we train DLDL using the union set (8,825 images) of SBD and VOC2011 training images. We evaluate the proposed approach on VOC2011 (1,112) and VOC2012 (1,456) test images.</p><p>Evaluation criteria. The performance is measured in terms of mean IU (intersection over union), which is the most widely used metric in semantic segmentation.</p><p>We keep the same settings as FCN including training images and model structure. The main change is that we employ KL divergence as the loss function based on label distribution (Eq. 15). Note that although we transform the ground-truth to label distribution in the training process, our evaluation rely only on ground-truth label.</p><p>Recently, Conditional Random Field (CRF) has been broadly used in many state-of-the-art semantic segmentation systems. We optionally employ a fully connected CRF <ref type="bibr" target="#b49">[50]</ref> to refine the predicted category score maps using the default parameters of <ref type="bibr" target="#b50">[51]</ref>.</p><p>Results. <ref type="table" target="#tab_1">Table VIII</ref> gives the performance of DLDL-8s and DLDL-8s-CRF on the test images of VOC2011 and VOC2012 and compares it to the well-known FCN-8s. DLDL-8s improves the mean IU of FCN-8s form 62.7% to 64.9% on VOC2011. On VOC2012, DLDL-8s leads to an improvement of 2.3 points in mean IU. DLDL achieves better results than FCN, which suggests it is important to improve the segmentation performance using label ambiguity. In addition, the CRF further improve performance of DLDL-8s, offering a 2.6% absolute increase in mean IU both on VOC2011 and VOC2012. <ref type="figure" target="#fig_8">Fig. 7</ref> shows four semantic segmentation examples from the VOC2011 validation images using FCN-8s, DLDL-8s and DLDL-8s-CRF. We can see that DLDL-8s can successfully  segment some small objects (e.g., car and bicycle) and particularly improve the segmentation of object boundaries (e.g., horse's leg and plant's leaves), but FCN-8s does not. DLDL-8s may fail, e.g., it sees a flowerpot as a potted plant in the fourth row in <ref type="figure" target="#fig_8">Fig. 7</ref>. Furthermore, compared to DLDL-8s, DLDL-8s-CRF is able to refine coarse pixel-level label predictions to produce sharp boundaries and fine-grained segmentations (e.g., plant's leaves). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSIONS</head><p>In this section, we try to understand the generalization performance of DLDL through feature visualization, and to analyze why DLDL can achieve high accuracy with limited training data. In addition, a study of the hyper-parameter is also provided.</p><p>Feature visualization. We visualize the model features in a low-dimensional space. Early layers learn low-level features (e.g., edge and corner) and latter layers learn high level features (e.g., shapes and objects) in a deep ConvNet <ref type="bibr" target="#b18">[19]</ref>. Hence, we extract the penultimate layer features (4,096dimensional) on Morph, ChaLearn, Pointing'04 and AFLW validation sets. To obtain the 2-dimensional embeddings of the extracted high dimensional features, we employ a popular dimension reduction algorithm t-SNE <ref type="bibr" target="#b51">[52]</ref>. The low-dimensional embeddings of validation images from the above four datasets are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>. The first row shows the 2-dim embeddings of hand-crafted features (BIF for Morph and Chalearn, HOG for Pointing'04 and AFLW) and the second row shows that of the DLDL features. These figures are colored by their semantic category. It can be observed that clear semantic clusterings (old or young for age datasets, left or right, up or down for head pose datasets) appear in deep features but do not in handcrafted features.</p><p>Reduce over-fitting. DLDL can effectively reduce overfitting when the training set is small. This effect can be explained by the label ambiguity. Considering an input sample X with one single label l. In traditional deep ConvNet, y l = 1 and y k = 0 for all k = l. In DLDL, the label distribution y contains many non zeros elements. The diversity of labels helps reduce over-fitting. Moreover, the objective function (Eq. 3) of DLDL can be rewritten as T = −(y l lnŷ l + k =l y k lnŷ k ) .</p><p>In Eq. 20, the first term is the tradition ConvNet loss function.</p><p>The second term maximize the log-likelihood of the ambiguous labels. Unlike existing data augmentation techniques such as random cropping on the images, DLDL augments data on the label side. In <ref type="figure" target="#fig_9">Fig. 8</ref>, MAE is shown as a function of the number of epochs on two age datasets (ChaLearn and Morph) and two head pose datasets (BJUT-3D and AFLW). On ChaLearn Accelerate convergence. We further analyze the convergence performance of DLDL, C-ConvNet and R-ConvNet. We can observe that the training MAE is reduced very slowly at the beginning of training using C-ConvNet and R-ConveNet in many cases as shown in <ref type="figure" target="#fig_9">Fig. 8</ref>. On the contrary, the MAE of DLDL reduces quickly.</p><p>Robust performance. One notable observation is that C-ConvNet and R-ConveNet is unstable. <ref type="figure" target="#fig_9">Fig. 8c</ref> shows the MAE for pitch+yaw, a complicated estimation of the joint distribution. This is a very sparse label set because the interval of adjacent class (pitch or yaw) is 10 • . R-ConvNet has difficulty in estimating this output, yielding errors that are roughly 20 times higher than DLDL and C-ConvNet. On the other hand, C-ConvNet easily fall into over-fitting when there are not enough training data (e.g, <ref type="figure" target="#fig_9">Fig. 8a and Fig. 8d</ref>). The proposed DLDL is more amenable to small datasets or sparse labels than C-ConvNet and R-ConvNet.</p><p>Analyze the hyper-parameter. DLDL's performance may be affected by the label distribution. Here, we take age estimation (Morph) and head pose estimation (Pointing'04) for examples. σ is a common hyper-parameter in these tasks if it is not provided in the ground-truth. We have empirically set σ = 2 in Morph, and σ = 15 • in Pointing'04 in our experiments. In order to study the impact of σ, we test DLDL with different σ values, changing from 0 to 3σ with 0.5σ interval. <ref type="figure" target="#fig_10">Fig. 9</ref> shows the MAE performance on Morph and Pointing'04 with different σ. We can see that a proper σ is important for low MAE. But generally speaking, a σ value that is close to the interval between neighboring labels is a good choice. Because the shape of all curves are V-shape like, it is also very convenient to find an optimal σ value using the cross-validation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We observe that current deep ConvNets cannot successfully learn good models when there are not enough training data and/or the labels are ambiguous. We propose DLDL, a deep label distribution learning framework to solve this issue by exploiting label ambiguity. In DLDL, each image is labeled by a label distribution, which can utilize label ambiguity in both feature learning and classifier learning. DLDL consistently improves the network training process in our experiments, by preventing it from over-fitting when the training set is small. We empirically showed that DLDL produces robust and competitive performances than traditional classification or regression deep models on several popular visual recognition tasks.</p><p>However, constructing a reasonable label distribution is still challenging due to the diversity of label space for different recognition tasks. It is an interesting direction to extend DLDL to more recognition problems by constructing different label distributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported in part by the National Natural Science Foundation of China under Grant 61422203, Grant 61622203, and Grant 61232007, in part by the Jiangsu Natural Science Funds for Distinguished Young Scholar under Grant BK20140022, in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization, and in part by the Collaborative Innovation Center of Wireless Communications Technology. (Corresponding Author: Jianxin Wu.) B.-B. Gao, C.-W. Xie and J. Wu are with the National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: gaobb@lamda.nju.edu.cn; xiecw@lamda.nju.edu.cn; wujx@lamda. nju.edu.cn). C. Xing and X. Geng are with the MOE Key Laboratory of Computer Network and Information Integration, School of Computer Science and Engineering, Southeast University, Nanjing 211189, China (e-mail: xingchao@seu.edu.cn; xgeng@seu.edu.cn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>et al. proposed two algorithms named LDLogitBoost and AOSO-LDLogitBoost to learn general models to relax the maximum entropy model in traditional arXiv:1611.01731v2 [cs.CV] 10 May 2017 Different label distributions for different recognition tasks. The first row shows four images, with the first two images coming from ChaLearn 2015 and Pointing'04 and the last two images coming from the PASCAL VOC2007 classification task and the PASCAL VOC2011 segmentation challenge. The second row shows their corresponding label distributions (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The face image pre-processing pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>in years Accuracy (% of test images)BFGS−LDL (KL, Exp) C−ConvNet (softmax, Exp) R−ConvNet (L2) R−ConvNet (L1) R−ConvNet (ε−ins) ConvNet+LS (KL, Max) ConvNet+LD (α−div, Exp) DLDL (KL, Exp) DLDL+VGG−Face (KL, Exp) in years Accuracy (% of test images) BFGS−LDL (KL, Exp) C−ConvNet (softmax, Exp) R−ConvNet (L2) R−ConvNet (L1) R−ConvNet (ε−ins) ConvNet+LS (KL, Max) ConvNet+LD (α−div,Max) DLDL (KL, Max) DLDL+VGG−Face (KL, Max) in degrees Accuracy (% of test images) AVM BFGS−LDL (KL, Max) C−ConvNet (softmax, Max) R−ConvNet (L2) R−ConvNet (L1) R−Convnet (ε−ins) ConvNet+LS (KL, Max) ConvNet+LD (α−div, Max) DLDL (KL, Max) (c) AFLW Comparisons of CS curves on the ChaLearn, Morph and AFLW validation sets. Note that the CS cures are plotted using better estimation based on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Examples of face images and DLDL results. The first row shows ten cropped and aligned faces from the apparent age estimation challenge and their corresponding ground-truth apparent ages. The second row shows their predicted label distributions and predicted ages. The left seven columns show good age estimations and the right three columns are failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Examples of face images and DLDL results. The first row shows ten cropped faces from the AFLW dataset and their corresponding ground-truth labels (yaw angle, pitch angle). The second row shows their predicted label distributions and predicted head poses. The left seven columns are the good examples and the right three columns are the failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Visualizations of hand-crafted and DLDL features using the t-SNE algorithm on Morph, ChaLearn and AFLW validation sets. The first row shows the embeddings of hand-crafted features (BIF or HOG). The second row shows the embeddings of the DLDL features derived from the penultimate fully connected layer of DLDL (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Semantic segmentation examples using FCN-8s, DLDL-8s and DLDL-8s-CRF on PASCAL VOC2011 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>(softmax, Max) train C−ConvNet (softmax, Max) val R−ConvNet (L2) train R−ConvNet (L2) val R−ConvNet (L1) train R−ConvNet (L1) val R−ConvNet (ε−ins) train R−ConvNet (ε−ins) val ConvNet+LS (KL, Max) train ConvNet+LS (KL, Max) val ConvNet+LD (α−div, Max) train ConvNet+LD (α−div, Max) val DLDL (KL, Max) train DLDL (KL, Max) val (softmax, Max) train C−ConvNet (softmax, Max) val R−ConvNet (L2) train R−ConvNet (L2) val R−ConvNet (L1) train R−ConvNet (L1) val R−ConvNet (ε−ins) train R−ConvNet (ε−ins) val ConvNet+LS (KL, Max) train ConvNet+LS (KL, Max) val ConvNet+LD (α−div, Max) train ConvNet+LD (α−div, Max) val DLDL (KL, Max) train DLDL (KL, Max) val (softmax, Max) train C−ConvNet (softmax, Max) val R−ConvNet (L2) train R−ConvNet (L2) val R−ConvNet (L1) train R−ConvNet (L1) val R−ConvNet (ε−ins) train R−ConvNet (ε−ins) val ConvNet+LS (KL, Max) train ConvNet+LS (KL, Max) val ConvNet+LD (α−div, Max) train ConvNet+LD (α−div, Max) val DLDL (KL, Max) train DLDL (KL, Max) val (softmax, Max) train C−ConvNet (softmax, Max) val R−ConvNet (L2) train R−ConvNet (L2) val R−ConvNet (L1) train R−ConvNet (L1) val R−ConvNet (ε−ins) train R−ConvNet (ε−ins) val ConvNet+LS (KL, Max) train ConvNet+LS (KL, Max) val ConvNet+LD (α−div, Max) train ConvNet+LD (α−div, Max) val DLDL (KL, Max) train DLDL (KL, Max) val (d) AFLW Comparisons of training and validation MAE of DLDL and all baseline methods on the ChaLearn, Morph, BJUT-3D and AFLW datasets (lower is better, best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>The performance (MAE) of DLDL with different label distributions (different parameter σ). The left figure is for the Morph dataset, while the right figure is for the Pointing'04 dataset (lower is better). and AFLW, C-ConveNet (softmax) achieves the lowest training MAE, but produces the highest validation MAE. In particular, the validation MAE increases after the 8th epoch on ChaLearn. Similar phenomenon is observed on AFLW. This fact shows that over-fitting happens in C-ConvNet when the number of training images is small. Although there are 15,561 training images in AFLW, each category contains on averagely 4 training images since there are 3,721 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Geng et al.  proposed the label distribution learning approach (IIS-LLD) for age and head pose estimation. They used traditional image features. To further improve IIS-LLD, Geng et al.<ref type="bibr" target="#b24">[25]</ref> proposed a BFGS-LDL algorithm by using the effective quasi-Newton optimization method BFGS. C-ConvNet Classification ConvNets have obtained very competitive performance in various computer vision tasks. ZF-Net<ref type="bibr" target="#b18">[19]</ref> and VGG-Net are popular models which use the softmax loss. We replace the ImageNetspecific 1000-way classification in these modes with the label set Y .</figDesc><table /><note>•• R-ConvNet ConvNets are also successively trained for regression tasks. In R-ConvNet, the ground-truth label y (age and pose angle) is projected into the range [−1, 1]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I COMPARISONS</head><label>I</label><figDesc>OF DIFFERENT METHODS FOR AGE ESTIMATION.</figDesc><table><row><cell>Description</cell><cell>Morph MAE</cell><cell cols="2">ChaLearn MAE -error</cell></row><row><cell>IIS-LDL [4]</cell><cell>5.67±0.15</cell><cell>-</cell><cell>-</cell></row><row><cell>CPNN [4]</cell><cell>4.87±0.31</cell><cell>-</cell><cell>-</cell></row><row><cell>ST+CSHOR [27] 1</cell><cell>3.82</cell><cell>-</cell><cell>-</cell></row><row><cell>M-S ConvNets [28]</cell><cell>3.63</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvNets [29] 1</cell><cell>3.31</cell><cell>-</cell><cell>-</cell></row><row><cell>VGG (softmax, Exp) [30] 3</cell><cell>-</cell><cell>6.08</cell><cell>0.51</cell></row><row><cell>VGG (softmax, Exp) [30] 2,3</cell><cell>-</cell><cell>3.22</cell><cell>0.28</cell></row><row><cell>VGG (softmax, Exp) [31] 2,3</cell><cell>2.68</cell><cell>3.25</cell><cell>0.28</cell></row><row><cell>BFGS-LDL (KL, Max)</cell><cell>3.94±0.05</cell><cell>7.81</cell><cell>0.57</cell></row><row><cell>BFGS-LDL (KL, Exp)</cell><cell>3.85±0.05</cell><cell>6.79</cell><cell>0.53</cell></row><row><cell>C-ConvNet (softmax, Max)</cell><cell>3.02±0.05</cell><cell>9.48</cell><cell>0.63</cell></row><row><cell>C-ConvNet (softmax, Exp)</cell><cell>2.86±0.05</cell><cell>7.95</cell><cell>0.58</cell></row><row><cell>R-ConvNet ( 2)</cell><cell>3.17±0.04</cell><cell>5.94</cell><cell>0.50</cell></row><row><cell>R-ConvNet ( 1)</cell><cell>2.88±0.03</cell><cell>5.62</cell><cell>0.47</cell></row><row><cell>R-ConvNet ( -ins)</cell><cell>2.89±0.04</cell><cell>5.71</cell><cell>0.48</cell></row><row><cell>ConvNet+LS (KL, Max)</cell><cell>2.96±0.13</cell><cell>8.64</cell><cell>0.59</cell></row><row><cell>ConvNet+LS (KL, Exp)</cell><cell cols="2">5.02±0.13 11.58</cell><cell>0.77</cell></row><row><cell>ConvNet+LD (α-div, Max)</cell><cell>2.57±0.04</cell><cell>5.95</cell><cell>0.47</cell></row><row><cell>ConvNet+LD (α-div, Exp)</cell><cell>2.57±0.04</cell><cell>5.69</cell><cell>0.46</cell></row><row><cell>DLDL (KL, Max)</cell><cell>2.51±0.03</cell><cell>5.49</cell><cell>0.44</cell></row><row><cell>DLDL (KL, Exp)</cell><cell>2.52±0.03</cell><cell>5.34</cell><cell>0.44</cell></row><row><cell cols="2">DLDL+VGG-Face (KL, Max) 3 2.42±0.01</cell><cell>3.62</cell><cell>0.32</cell></row><row><cell cols="2">DLDL+VGG-Face (KL, Exp) 3 2.43±0.01</cell><cell>3.51</cell><cell>0.31</cell></row></table><note>1 Used 80% of Morph images for training and 20% for evaluation; 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table Ifor those methods involving Max (Eq. 8) and Exp (Eq. 9) (higher is better, best viewed in color).</figDesc><table><row><cell>40</cell><cell>19</cell><cell>62</cell><cell>23</cell><cell>38</cell><cell>24</cell><cell>26</cell><cell>66</cell><cell>52</cell><cell>22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II COMPARISONS</head><label>II</label><figDesc>OF DIFFERENT METHODS FOR HEAD POSE ESTIMATION ON THE Pointing'04 DATASET. 69±0.15 4.24±0.17 6.45±0.29 86.24±0.97 73.30±1.36 64.27±1.82 Baselines BFGS-LDL (KL) 1.99±0.19 4.00±0.20 5.68±0.13 88.78±0.11 74.37±0.13 66.42±0.11 C-ConvNet (softmax) 5.28±0.65 6.02±0.44 10.56±0.74 73.15±2.74 62.90±1.81 42.Table III COMPARISONS OF DIFFERENT METHODS FOR HEAD POSE ESTIMATION ON THE BJUT-3D DATASET.</figDesc><table><row><cell>Methods</cell><cell>Description</cell><cell>Pitch</cell><cell cols="2">MAE (lower is better) Yaw Pitch+Yaw</cell><cell>Pitch</cell><cell cols="2">Acc (higher is better) Yaw Pitch+Yaw</cell></row><row><cell></cell><cell>LDL-wJ [7]</cell><cell cols="6">2.97±1.67</cell></row><row><cell></cell><cell>R-ConvNet ( 2 )</cell><cell cols="3">6.11±0.33 6.61±0.17 10.13±0.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>R-ConvNet ( 1 )</cell><cell cols="2">5.94±0.71 5.90±0.39</cell><cell>9.43±0.79</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>R-ConvNet ( -ins)</cell><cell cols="2">5.77±0.45 6.66±0.19</cell><cell>9.04±0.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ConvNet+LS (KL)</cell><cell cols="3">5.23±0.39 5.87±0.53 10.42±0.66</cell><cell cols="3">72.62±1.01 62.90±2.76 41.83±2.20</cell></row><row><cell></cell><cell>ConvNet+LD (α-div)</cell><cell cols="2">1.94±0.20 3.68±0.16</cell><cell>5.34±0.17</cell><cell cols="3">90.00±0.77 76.27±0.82 69.00±0.89</cell></row><row><cell>Ours</cell><cell>DLDL (KL)</cell><cell cols="2">1.69±0.32 3.16±0.07</cell><cell>4.64±0.24</cell><cell cols="3">91.65±1.13 79.57±0.57 73.15±0.72</cell></row><row><cell>Methods</cell><cell>Description</cell><cell>Pitch</cell><cell cols="2">MAE (lower is better) Yaw Pitch+Yaw</cell><cell>Pitch</cell><cell cols="2">Acc (higher is better) Yaw Pitch+Yaw</cell></row><row><cell></cell><cell>BFGS-LDL (KL)</cell><cell cols="2">0.19±0.02 0.33±0.04</cell><cell>0.51±0.05</cell><cell cols="3">98.15±0.19 96.69±0.38 94.95±0.54</cell></row><row><cell></cell><cell>C-ConvNet (Softmax)</cell><cell cols="2">0.06±0.01 0.09±0.02</cell><cell>0.14±0.03</cell><cell cols="3">99.45±0.09 99.16±0.16 98.64±0.23</cell></row><row><cell></cell><cell>R-ConvNet ( 2 )</cell><cell cols="2">1.83±0.01 2.17±0.03</cell><cell>3.15±0.03</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baselines</cell><cell>R-ConvNet ( 1 )</cell><cell cols="2">1.25±0.06 1.37±0.09</cell><cell>2.11±0.09</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>R-ConvNet ( -ins)</cell><cell cols="2">1.21±0.07 1.42±0.07</cell><cell>2.09±0.10</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ConvNet+LS (KL)</cell><cell cols="2">0.05±0.01 0.08±0.01</cell><cell>0.12±0.01</cell><cell cols="3">99.55±0.06 99.28±0.08 98.86±0.10</cell></row><row><cell></cell><cell>ConvNet+LD (α-div)</cell><cell cols="2">0.07±0.01 0.12±0.02</cell><cell>0.19±0.02</cell><cell cols="3">99.31±0.04 98.82±0.20 98.15±0.21</cell></row><row><cell>Ours</cell><cell>DLDL (KL)</cell><cell cols="2">0.02±0.01 0.07±0.01</cell><cell>0.09±0.01</cell><cell cols="3">99.81±0.04 99.27±0.08 99.09±0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV MAE</head><label>IV</label><figDesc>AND ACC (% OF IMAGES WITH ±15 • ERROR) FOR DIFFERENT METHODS ON THE AFLW DATASET. For AFLW, 15,561 face images are randomly chosen for training, and the remaining 7,848 for evaluation. The setup is similar to the recent literature [36] (14,000 images for training and the rest 7,041 images for testing).</figDesc><table><row><cell>Description</cell><cell cols="4">MAE (lower is better) Acc (higher is better) Pitch Yaw Pitch+Yaw Pitch Yaw Pitch+Yaw</cell></row><row><cell>AVM [36]</cell><cell>-16.75</cell><cell>-</cell><cell>-60.75</cell><cell>-</cell></row><row><cell>BFGS-LDL (KL)</cell><cell>7.21 8.72</cell><cell>12.69</cell><cell>90.62 86.81</cell><cell>79.80</cell></row><row><cell cols="2">C-ConvNet (softmax) 7.87 9.34</cell><cell>13.65</cell><cell>87.75 83.79</cell><cell>75.04</cell></row><row><cell>R-ConvNet ( 2 )</cell><cell>6.57 8.44</cell><cell>11.88</cell><cell>92.84 84.76</cell><cell>79.56</cell></row><row><cell>R-ConvNet ( 1 )</cell><cell>6.01 7.07</cell><cell>10.34</cell><cell>94.60 89.62</cell><cell>85.45</cell></row><row><cell>R-ConvNet ( -ins)</cell><cell>5.96 7.13</cell><cell>10.35</cell><cell>94.94 90.00</cell><cell>86.21</cell></row><row><cell>ConvNet+LS (KL)</cell><cell>7.69 9.10</cell><cell>13.33</cell><cell>88.34 85.00</cell><cell>76.47</cell></row><row><cell cols="2">ConvNet+LD (α-div) 6.55 7.02</cell><cell>10.77</cell><cell>92.80 91.88</cell><cell>86.14</cell></row><row><cell>DLDL (KL)</cell><cell>5.75 6.60</cell><cell>9.78</cell><cell>95.41 92.89</cell><cell>89.27</cell></row><row><cell>following [7].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V SINGLE</head><label>V</label><figDesc>MODEL CLASSIFICATION MAP (IN %) ON VOC2007 (TRAINVAL/TEST). THE * SIGN INDICATES GROUND-TRUTH BOUNDING BOX INFORMATION WAS USED DURING TRAINING.</figDesc><table><row><cell cols="2">Methods Description</cell><cell cols="4">Net-D Net-D Net-E Net-E Max Avg Max Avg</cell></row><row><cell></cell><cell cols="2">Fev+Lv-20-VD* [40] 90.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>HCP-VGG [42]</cell><cell>90.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>VGG+SVM [16]</cell><cell>89.3</cell><cell>-</cell><cell>89.3</cell><cell>-</cell></row><row><cell>Baselines</cell><cell>IF-VGG-2</cell><cell cols="4">89.8 89.5 89.7 89.8</cell></row><row><cell></cell><cell>IF-VGG-KL</cell><cell cols="4">90.0 90.3 90.3 90.2</cell></row><row><cell>Ours</cell><cell>IF-DLDL PF-DLDL</cell><cell cols="4">90.1 90.5 90.6 90.7 92.3 92.1 92.5 92.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI COMPARISONS</head><label>VI</label><figDesc>OF THE CLASSIFICATION RESULTS (IN %) OF STATE-OF-THE-ART APPROACHES ON VOC2007 (TRAINVAL/TEST). * INDICATES METHODS USING GROUND-TRUTH BOUNDING BOX INFORMATION FOR TRAINING. Methods Description aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP AGS* [43] 82.2 83.0 58.4 76.1 56.4 77.5 88.8 69.1 62.2 61.8 64.2 51.3 85.4 80.2 91.1 48.1 61.7 67.7 86.3 70.9 71.1 AMM* [44] 84.5 81.5 65.0 71.4 52.2 76.2 87.2 68.5 63.8 55.8 65.8 55.6 84.8 77.0 91.1 55.2 60.0 69.7 83.6 77.0 71.3 HCP-2000C [37] 96.0 92.1 93.7 93.4 58.7 84.0 93.4 92.0 62.8 89.1 76.3 91.4 95.0 87.8 93.1 69.9 90.3 68.0 96.8 80.6 85.2 Fev+Lv-20-VD* [40] 97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7 95.9 98.6 77.6 88.7 78.0 98.3 89.Table VII COMPARISONS OF THE CLASSIFICATION RESULTS (IN %) OF STATE-OF-THE-ART APPROACHES ON VOC2012 (TRAINVAL/TEST). * INDICATES METHODS USING GROUND-TRUTH BOUNDING BOX INFORMATION FOR TRAINING.</figDesc><table><row><cell></cell><cell></cell><cell>0 90.6</cell></row><row><cell></cell><cell>HCP-VGG [42]</cell><cell>98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1 94.9 96.3 78.3 94.7 76.2 97.9 91.5 90.9</cell></row><row><cell></cell><cell>VGG+SVM [16]</cell><cell>98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3 93.1 97.2 70.0 92.1 80.3 98.1 87.0 89.7</cell></row><row><cell>Baselines</cell><cell>IF-VGG-2</cell><cell>98.9 95.7 97.3 95.5 65.0 92.8 93.7 97.1 74.2 90.8 87.0 97.1 97.1 93.8 97.0 70.8 94.3 77.8 98.0 86.4 90.0</cell></row><row><cell></cell><cell>IF-VGG-KL</cell><cell>99.1 95.5 97.4 94.9 68.1 92.7 94.3 97.0 75.7 90.3 89.0 97.0 97.6 94.6 97.2 76.3 93.8 80.1 98.2 87.9 90.8</cell></row><row><cell>Ours</cell><cell>IF-DLDL PF-DLDL</cell><cell>99.1 95.8 97.4 95.3 69.2 93.3 94.5 96.6 76.1 90.4 89.0 97.1 97.7 94.5 97.7 76.1 93.6 81.9 98.2 89.1 91.1 99.3 97.6 98.3 97.0 79.0 95.7 97.0 97.9 81.8 93.3 88.2 98.1 96.9 96.5 98.4 84.8 94.9 82.7 98.5 92.8 93.4</cell></row><row><cell cols="2">Methods Description</cell><cell>aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP</cell></row><row><cell></cell><cell>NUS-PSL*[43]</cell><cell>97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5 90.1 95.0 57.8 79.2 73.4 94.5 80.7 82.2</cell></row><row><cell></cell><cell>PRE-1512*[45]</cell><cell>94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4 88.6 96.1 64.3 86.6 62.3 91.1 79.8 82.8</cell></row><row><cell></cell><cell>HCP-2000C [37]</cell><cell>97.5 84.3 93.0 89.4 62.5 90.2 84.6 94.8 69.7 90.2 74.1 93.4 93.7 88.8 93.3 59.7 90.3 61.8 94.4 78.0 84.2</cell></row><row><cell></cell><cell cols="2">Fev+Lv-20-VD* [40] 98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3 97.5 73.1 91.2 75.4 97.0 88.2 89.4</cell></row><row><cell></cell><cell>HCP-VGG [42]</cell><cell>99.1 92.8 97.4 94.4 79.9 93.6 89.8 98.2 78.2 94.9 79.8 97.8 97.0 93.8 96.4 74.3 94.7 71.9 96.7 88.6 90.5</cell></row><row><cell></cell><cell>VGG+SVM [16]</cell><cell>99.0 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7 97.1 63.7 93.6 75.2 97.4 87.8 89.3</cell></row><row><cell>Baselines</cell><cell>IF-VGG-2</cell><cell>98.9 88.4 96.7 93.4 70.7 92.3 85.8 97.7 77.3 94.2 81.2 97.4 96.8 93.7 96.7 62.2 94.1 70.7 96.9 85.8 88.6</cell></row><row><cell></cell><cell>IF-VGG-KL</cell><cell>99.0 89.9 96.6 93.7 74.0 93.2 87.3 97.5 78.5 94.7 83.1 97.1 96.9 94.0 96.6 66.9 94.5 75.9 97.4 87.7 89.7</cell></row><row><cell>Ours</cell><cell>IF-DLDL PF-DLDL</cell><cell>99.0 89.7 96.6 94.1 74.8 93.1 87.8 97.6 79.3 94.3 83.4 97.2 96.9 94.0 97.3 67.8 94.2 76.5 97.4 87.8 89.9 99.5 94.1 97.9 95.9 81.0 94.8 93.1 98.2 82.4 96.1 84.0 98.0 97.8 95.7 97.7 78.9 95.5 78.0 97.8 92.2 92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VIII COMPARISONS</head><label>VIII</label><figDesc>OF DLDL AND FCN ON THE PASCAL VOC2011 AND VOC2012 TEST SETS.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">mean IU VOC2011 test VOC2012 test mean IU</cell></row><row><cell cols="2">FCN-8s [3]</cell><cell>62.7</cell><cell>62.2</cell></row><row><cell cols="2">DLDL-8s</cell><cell>64.9</cell><cell>64.5</cell></row><row><cell cols="2">DLDL-8s+CRF</cell><cell>67.6</cell><cell>67.1</cell></row><row><cell>Image</cell><cell>FCN-8s [3]</cell><cell cols="2">DLDL-8s DLDL-8s+CRF Ground-truth</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.vlfeat.org/matconvnet/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Facial age estimation by learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2401" to="2412" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Head pose estimation from a 2D face image using 3D face morphing with depth parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Mbouna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1801" to="1808" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Head pose estimation based on multivariate label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1837" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Logistic boosting regression for label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data-dependent label distribution learning for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2655445</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust optimization for deep regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2830" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real time head pose estimation with random regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cost-sensitive local binary feature learning for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5356" to="5368" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time head orientation from a monocular camera using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chalearn looking at people 2015: Apparent age and cultural event recognition datasets and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<idno>MSR-TR-2005-173</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A learning framework for age rank estimation based on face images with scattering transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="785" to="798" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep analysis on age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Segura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DEX: Deep EXpectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0940-36</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating face orientation from robust detection of salient facial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gourier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG Net Workshop on Visual Observation of Deictic Gestures</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BJUT-3D large scale 3D face database and information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Research and Development</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1009" to="1018" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model-based object pose in 25 lines of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Dementhon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="123" to="141" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Head pose estimation in the wild using approximate view manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CNN: single-label to multi-label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs:1406.5726</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BING: binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno>abs:1312.4894</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HCP: A flexible CNN framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Subcategoryaware object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contextualizing object detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Instance-Aware hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2469" to="2479" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Regional gating neural networks for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin-Bin Gao received the B.S. and M.S. degrees in applied mathematics in 2010 and 2013</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Technology, Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. He is currently pursuing the Ph.D. degree in the. His research interests include computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">He is currently a postgraduate student in the School of Computer Science and Engineering at Southeast University, China. His research interests include pattern recognition, machine learning, and data mining</title>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Chao Xing received the B.S. degree in software engineering from Southeast University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">He is currently a postgraduate student in the Department of Computer Science and Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen-Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>China; Nanjing University, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xie received his B.S. degree from Southeast University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer vision and machine learning</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">He is currently a Professor with the Department of Computer Science and Technology, Nanjing University, China, and is associated with the National Key Laboratory for Novel Software Technology, China. His current research interests include computer vision and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has served as an Area Chair for CVPR 2017 and ICCV 2015, a Senior PC Member for AAAI 2017 and AAAI 2016</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>M&apos;09) received the B.S. and M.S. degrees in computer science from Nanjing University, and the Ph. and an Associate Editor of Pattern Recognition Journal</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">and is currently a professor and vice dean of the school. He has authored over 50 refereed papers, and he holds five patents in these areas</title>
	</analytic>
	<monogr>
		<title level="m">Xin Geng (M&apos;13) received the B.S. and M.S. degrees in computer science from Nanjing University</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering at Southeast University, China</orgName>
		</respStmt>
	</monogr>
	<note>, respectively, and the Ph.D degree from Deakin University. His research interests include pattern recognition, machine learning, and computer vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
