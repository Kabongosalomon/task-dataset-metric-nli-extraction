<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Read-Write Memory Network for Movie Story Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seil</forename><surname>Na</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
							<email>sangho.lee@vision.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisung</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SK Telecom</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
							<email>gunhee@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Read-Write Memory Network for Movie Story Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel memory network model named Read-Write Memory Network (RWMN) to perform question and answering tasks for large-scale, multimodal movie story understanding. The key focus of our RWMN model is to design the read network and the write network that consist of multiple convolutional layers, which enable memory read and write operations to have high capacity and flexibility. While existing memory-augmented network models treat each memory slot as an independent block, our use of multi-layered CNNs allows the model to read and write sequential memory cells as chunks, which is more reasonable to represent a sequential story because adjacent memory blocks often have strong correlations. For evaluation, we apply our model to all the six tasks of the MovieQA benchmark <ref type="bibr" target="#b25">[26]</ref>, and achieve the best accuracies on several tasks, especially on the visual QA task. Our model shows a potential to better understand not only the content in the story, but also more abstract information, such as relationships between characters and the reasons for their actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For many problems of video understanding, including video classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, video captioning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and MovieQA <ref type="bibr" target="#b25">[26]</ref>, it is key to success for models to correctly process, represent, and store long sequential information. In the era of deep learning, one prevailing approach to model sequential input is to use recurrent neural networks (RNNs) <ref type="bibr" target="#b16">[17]</ref> which store the given information into a hidden memory and update it over time. However, RNNs accumulate information in a single fixed-length memory regardless of the length of an input sequence, thus tend to fail to utilize far-distant information due to a vanishing gradient problem, which is still not fully solved even with advanced models such as LSTM <ref type="bibr" target="#b11">[12]</ref> and GRU <ref type="bibr" target="#b2">[3]</ref>.</p><p>As another recent alternative to resolve this issue, many studies attempt to leverage an external memory structure for neural networks, often referred to as neural memory Higher level information Q. What sports they play in Hogwarts? A1. They box A2. They play golf A3. They fight with brooms A4. They play chess A5. Quidditch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Write Network</head><p>Read Network Movie Question Answering <ref type="figure">Figure 1</ref>. The intuition of the RWMN (Read-Write Memory Network) model for movie question and answering tasks. Using read/write networks of multi-layered CNNs, it abstracts a given series of frames stepwise to capture higher-level sequential information and stores it into memory slots. It eventually helps answer complex questions of movie QAs.</p><p>networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. One key benefit of external memory is to enable a neural model to cache sequential inputs in memory slots, and explicitly utilize even far early information. Such ability is particularly powerful to solve question and answering (QA) problems, which often require models to memorize a large amount of information, and correctly access the most relevant information to a given question. For this reason, memory networks have been popularly applied as state-of-the-art approaches to many QA tasks, such as bAbI task <ref type="bibr" target="#b27">[28]</ref>, SQuAD <ref type="bibr" target="#b22">[23]</ref>, and LSMDC <ref type="bibr" target="#b23">[24]</ref>.</p><p>MovieQA <ref type="bibr" target="#b25">[26]</ref> is another challenging visual QA dataset, in which models need to understand movies over two hours long, and solve QA problems related to movie content and plots. The MovieQA benchmark consists of six tasks according to which sources of information is usable to solve the QA problems, including videos, subtitles, DVS, scripts, plot synopses, and open-end information. Understanding a movie is a highly challenging task; it is necessary not only to understand the content of individual video frames such as a characters' actions, places of events, but also to infer more abstract and high-level knowledge such as reasons of a characters' behaviors, and relationships between them. For instance, in the Harry Potter movie, to answer a question (Q. What does Harry trick Lucius into doing? A. Freeing Dobby), models need to realize that Dobby was a Lucius's house elf, wanted to escape from him, had a positive relationship with Harry, and Harry helped him. Some of such information is visually or textually observable in the movie, but much information like relationships between characters and correlations between events should be deduced.</p><p>Our objective is to propose a novel memory network model to perform QA tasks for large-scale, multimodal movie story understanding. That is, the input to the model can be very long (e.g. videos more than two hours long), or be multimodal (e.g. text-only or video-text pairs). The key focus of our novel memory network named Read-Write Memory Networks (RWMN) is on defining the memory read/write operations to have high capacity and flexibility, for which we propose the read and write networks that consist of multiple convolutional layers. Existing neural memory network models treat each memory slot as an independent block. However, adjacent memory blocks often have strong correlations, which are the case to represent a sequential story. That is, when human understands a story, the entire story is often recognized as a sequence of closelyinterconnected abstract events. Hence, preferably memory networks need to read and write sequential memory cells as chunks, which are implemented by multiple convolutional layers of the read and write network.</p><p>To conclude introduction, we summarize the contributions of this work as follows.</p><p>1. We propose a novel memory network named RWMN that enables the model to flexibly read and write more complex and abstract information into memory slots through read/write networks. To the best of our knowledge, it is the first attempt to leverage multi-layer CNNs for read/write operations of a memory network.</p><p>2. The RWMN shows the best accuracies on several tasks of MovieQA benchmark <ref type="bibr" target="#b25">[26]</ref>; as of the ICCV2017 submission deadline (March 27, 2017 23:59 GMT), our RWMN achieves the best performance for four out of five tasks in the validation set, and four out of six tasks in the test set. Our quantitative and qualitative evaluation also assures that the read/write networks effectively utilize higher-level information in the external memory, especially on the visual QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Memory Networks. Recently, much research has been done to model sequential data using explicit memory architecture. The memory access of existing memory network models can be classified into content-based ad-dressing and location-based addressing <ref type="bibr" target="#b7">[8]</ref>. The contentbased addressing (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref>) lets the controller to generate a key vector and measure its similarity with each memory cell to find out which cells are to be attended as the relevant cells to the key vector. Location-based addressing (e.g. <ref type="bibr" target="#b7">[8]</ref>), on the other hand, enables simple arithmetic operations that find out the addresses to store or retrieve information, regardless of the content of the key vector.</p><p>Neural Turing Machine (NTM) <ref type="bibr" target="#b7">[8]</ref> and its extensions of DNC <ref type="bibr" target="#b8">[9]</ref>, D-NTM <ref type="bibr" target="#b9">[10]</ref>, focus on learning the entire process of memory interaction (read/write operations), and thus the degree of freedom (or capability) of the model is high in solving a given problem. They have been successfully applied to complex tasks such as sorting, sequence copying, and graph traversal. The memory networks of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref> address the QA problems using continuous memory representation similar to the NTM. However, while the NTM leverages both content-based and location-based addressing, they use only the former (content-based) memory interaction. They apply the concept of multi-hops to recurrently read the memory, which results in performance improvement in solving QA problems that require causal reasoning. The work of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref> proposes a key-value memory network that stores information in the form of (key, value) pairs into the external knowledge base. These methods are good at solving QA problems that focus on the content or facts in a context such as WikiMovies <ref type="bibr" target="#b18">[19]</ref> and bAbI dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>The work of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> deals with how to make the read/write operations scalable with extremely large amount of memory. Chandar et al. <ref type="bibr" target="#b1">[2]</ref> propose to organize memory hierarchically, and Rae et al. <ref type="bibr" target="#b21">[22]</ref> make read and write operations sparse, thereby increasing scalability and reducing the cost of operations. Cesc et al. <ref type="bibr" target="#b20">[21]</ref> adopt convolutional read from memory to jointly represent nearby ordered memory slots.</p><p>Compared to all the previous models, our RWMN model is explicitly equipped with learnable read/write networks of CNNs, which are specialized in storing and utilizing more abstract information, such as relationships between characters, reasons for characters' specific behaviors, as well as understanding of facts in a given story.</p><p>Models for MovieQA. Among the models applied to the MovieQA benchmark <ref type="bibr" target="#b25">[26]</ref>, the end-to-end memory network <ref type="bibr" target="#b24">[25]</ref> is the state-of-the-art approach. It splits each movie into shot subshots, and constructs memory slots with video and subtitle features. It then uses contentbased addressing to attend on the information relevant to a given question. Recently, Wang and Jiang <ref type="bibr" target="#b26">[27]</ref> present the compare-aggregate framework for word-level matching to measure the similarity of sentences. However, it is applied to only a single task (plot synopses) of MovieQA.</p><p>There have been also several studies to solve Video QA tasks in other datasets, such as LSMDC <ref type="bibr" target="#b23">[24]</ref>, MSR-VTT <ref type="bibr" target="#b29">[30]</ref>, and TGIF-QA <ref type="bibr" target="#b12">[13]</ref>, which mainly focus on understanding short video clips, and answering about factual elements in the clips. Yu et al. <ref type="bibr" target="#b30">[31]</ref> achieve compelling performance in video captioning, video QA, and video retrieval by constructing an end-to-end trainable conceptword-detector along with vision-to-language models. <ref type="figure" target="#fig_2">Figure 2</ref> shows the overall structure of our RWMN. The RWMN is trained to store the movie content with proper representation in the memory, extract relevant information from memory cells in response to a given query, and select correct answer from five choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Read-Write Memory Network (RWMN)</head><p>Based on the QA format of MovieQA dataset <ref type="bibr" target="#b25">[26]</ref>, the input of the model is (i) a sequence of video segment and subtitle pairs S movie = {(v 1 , s 1 ), ..., (v n , s n )} for the whole movie, which takes about 2 hours (n ∼ 1, 558 on average), (ii) a question q for the movie, and (iii) five answer candidates a = {a 1 , ..., a 5 }. In the video+subtitle task of MovieQA, for example, each s i is a dialog sentence of a character, and v i = {v i1 , ..., v im } is a video subshot (i.e. a set of frames) sampled at 6 fps that are temporally aligned with s i . The output is a confidence score vector over the five answer candidates.</p><p>In the following, we explain the architecture according to information flow, from movie embedding to answer selection via write/read networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Movie Embedding</head><p>We convert each subshot v i and text sentence s i into feature representation as follows. For each frame v ij ∈ v i , we first obtain its feature v ij by applying the ResNet-152 <ref type="bibr" target="#b10">[11]</ref> pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. We then mean-pool over all frames as v i = j v ij ∈ R 7×7×2,048 , as a representation of the subshot v i . For each sentence s i , we first divide the sentence into words, apply the pretrained Word2Vec <ref type="bibr" target="#b17">[18]</ref>, and then mean-pool with the position encoding (PE) <ref type="bibr" target="#b24">[25]</ref> as</p><formula xml:id="formula_0">s i = j PE(s ij ) ∈ R 300 .</formula><p>Finally, to obtain a multimodal space embedding of v i and s i , we use the Compact Bilinear Pooling (CBP) <ref type="bibr" target="#b5">[6]</ref> as</p><formula xml:id="formula_1">E[i] = CBP(v i , s i ) ∈ R 4,096 .<label>(1)</label></formula><p>We perform this procedure for all n pairs of subshots and text, resulting in a 2D movie embedding matrix E ∈ R n×4,096 , which is the input of our write network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Write Network</head><p>The write network takes a movie embedding matrix E as an input and generates a memory tensor M as output. The write network is motivated by that when human understands a movie, she does not remember it as a simple sequence of speech and visual content, but rather ties together several adjacent utterances and scenes in a form of events or episodes. That is, each memory cell needs to associate neighboring movie embeddings, instead of storing each of n movie embedding separately. To implement this idea of jointly storing adjacent embeddings into every slot, we exploit a convolutional neural network (CNN) as the write network. We experimentally confirm the following CNN design after thorough tests, by varying the dimensions, depths, strides of convolution layers.</p><p>To the movie embedding E ∈ R n×4,096 , we first apply a fully connected (FC) layer with parameter</p><formula xml:id="formula_2">W c ∈ R 4,096×d , b c ∈ R d to project each E[i] into a d-dimensional vector.</formula><p>The FC layer reduces the dimension of E in order to equalize the dimensions of query embedding and answer embedding, which is also beneficial to reduce the number of required convolution operations later. We then use a convolution layer consisting of a filter w w </p><formula xml:id="formula_3">conv ∈ R f w v ×f w h ×1×f w c , whose vertical and horizontal filter size is f w v = 40, f w h = d,</formula><formula xml:id="formula_4">M = ReLU(conv((EW c + b c ), w w conv , b w )) (2)</formula><p>where conv (input, filter, bias) indicates the convolution layer, b w ∈ R f w c is a bias, and ReLU indicates the elementwise ReLU activation <ref type="bibr" target="#b19">[20]</ref>. Finally, the generated memory</p><formula xml:id="formula_5">is M ∈ R m×d×3 , where m = ((n − 1)/s w v + 1)</formula><p>. Note that the write network can employ multiple convolutional layers. If the number of layers is ν w , then we obtain M by recursively applying</p><formula xml:id="formula_6">M (l+1) = ReLU(conv(M (l) , w w(l) conv , b (l) w ))<label>(3)</label></formula><p>from l = 1 . . . , ν w − 1. In section 4, we will report the result of ablation study to find out the best-performing ν w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Read Network</head><p>The read network takes a question q and then generate answer from a compatibility between q and M.</p><p>Question embedding. We embed the question sentence q as follows. We first obtain the Word2Vec vector <ref type="bibr" target="#b17">[18]</ref> q as done in section 3.1, and then project it as follows.</p><formula xml:id="formula_7">u = W q q + b q<label>(4)</label></formula><p>where parameters are W q ∈ R d×300 and b q ∈ R d . Next the read network takes the memory M and the query embedding u as input, and generates the confidence score vector o ∈ R d as follows.</p><p>Query-dependent memory embedding. We first transform the memory M to be query-dependent. Its intuition is that, according to the query, different types of information must be retrieved from the memory slots. For example, for the Harry Potter movie, suppose that one memory slot contains the information about a particular scene where Harry   is chanting magic spells. This memory slot should be read differently according to two different questions Q 1 : What color is Harry wearing? and Q 2 : Why is Harry chanting magic spells? In section 4, we will empirically show the effectiveness of this question-dependent memory update.</p><p>To transform the memory M into a query-dependent memory M q ∈ R m×d×3 , we apply the CBP <ref type="bibr" target="#b5">[6]</ref> between each memory cell of M and the query embedding u as</p><formula xml:id="formula_8">M q [i, :, j] = CBP(M[i, :, j], u)<label>(5)</label></formula><p>for all i = 1, · · · , m, and j = 1, 2, 3.</p><p>Convolutional memory read. As done in the write network, we also leverage a CNN to implement the read network. Our intuition is that, for correctly answering the question of movie understanding, it is important to connect and relate a series of scenes as a whole. Therefore, we use the CNN architecture to access chunks of sequential memory slots. We obtain the reconstructed memory M r by applying convolutional layers with a filter w r conv ∈ R f r v ×f r h ×3×f r c whose vertical and horizontal filter size is f r v = 3, f r h = d, the number of filter channel is f r c = 3 and strides are s r v = 1, s r h = 1, respectively. Finally, the reconstructed memory is M r ∈ R c×d×3 with c = (m − 1)/s r v + 1 :</p><formula xml:id="formula_9">M r = ReLU(conv(M q , w r conv , b r ))<label>(6)</label></formula><p>where b r ∈ R 3 is a bias term. As in the write network, the read network can also have a ν r number of stacks of convolutional layers; the formulation is the same with Eq.(3) only except replacing M, w w conv , b w with M r , w r conv , b r , respectively. We will also report the results of ablation study about different ν r in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Answer Selection</head><p>Next we compute the attention matrix p ∈ R c×3 through applying the softmax to the dot product between the query embedding u and each cell of memory M r :</p><formula xml:id="formula_10">p[i, j] = softmax(M r [i, :, j] · u)<label>(7)</label></formula><p>where · indicates the dot product. Finally, the output vector o ∈ R d is obtained through a weighted sum between each memory cell of M r and the attention vector p:</p><formula xml:id="formula_11">o[i] = c j=1 3 k=1 M r [j, i, k]p[j, k].<label>(8)</label></formula><p>Next we obtain the embedding of five answer candidate sentences {a} as done for the question in Eq.(4) with sharing the parameters W q and b q . As a result, we compute the embedding of answer candidates g ∈ R 5×d . We compute the confidence vector z ∈ R 5 by finding the similarity between g and the weighted sum of o and u. where α ∈ [0, 1] is a trainable parameter. Finally, we predict the answer y with the highest confidence score: y = argmax i∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> (z i ).</p><formula xml:id="formula_12">z = softmax((αo + (1 − α)u) T g),<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>For training of our model, we minimize the softmax cross-entropy between the prediction z and the groundtruth one-hot vector z gt . All training parameters are initialized with the Xavier method <ref type="bibr" target="#b6">[7]</ref>. Experimentally, we select the Adagrad <ref type="bibr" target="#b4">[5]</ref> optimizer with a mini-batch size of 32, a learning rate of 0.001, and an initial accumulator value of 0.1. We train our model up to 200 epochs, although we actively use the early stopping to avoid overfitting due to the small size of the MovieQA dataset. We repeat training each model with 12 different random initializations, and select the one with the lowest cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed RWMN model for all the tasks of MovieQA benchmark <ref type="bibr" target="#b25">[26]</ref>. We defer more experimental results and implementation details to the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MovieQA Tasks and Experimental Setting</head><p>As summarized in <ref type="table" target="#tab_1">Table 1</ref>, MovieQA dataset <ref type="bibr" target="#b25">[26]</ref> contains 408 movies and 14,944 multiple choice QA pairs, each of which consists of five answer choices with only one correct answer. The dataset provides with five types of story sources associated with the movies: videos, subtitles, DVS, scripts, and plot synopses, based on which the MovieQA challenge hosts 6 subtasks, according to which sources of information are differently used: (i) video+subtitle, (ii) subtitles only, (iii) DVS only, (iv) scripts only, (v) plot synopses only, and (vi) open-ended. That is, there are one video-text QA task, and four text-only QA tasks, and one open-end QA task with no restriction on additional story sources. We strictly follow the test protocols of the challenge, including training/validation/test split and evaluation metrics. More details of the dataset and rules are available in <ref type="bibr" target="#b25">[26]</ref> and its homepage <ref type="bibr" target="#b0">1</ref> .</p><p>Among six tasks, we discuss our results with more focus on the video+subtitle task, because it is the only VQA task that requires both video and text understanding, whereas the other tasks are text-only. We weight less on the plot synopses only task, since plot synopses are given with a question, and all the QA pairs are generated from plot synopses, this task can be tackled using simple word/sentence matching algorithms (with little movie understanding), achieving a very high accuracy of 77.63%. We solve the video+subtitle task using the proposed RWMN model in <ref type="figure" target="#fig_2">Figure 2</ref>. For the four text-only QA tasks, no visual sources {v 1 , ..., v n } are given, thus we use {s 1 , ..., s n } only to construct the movie embedding E of Eq.(1) without the CBP. Except this, we use the same RWMN model to solve four text-only QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare the performance of our approach with those of all the methods proposed in the original MovieQA paper <ref type="bibr" target="#b25">[26]</ref> or in the official MovieQA leaderboard 2 . We describe the baseline names in the caption of each result table.</p><p>In order to measure the effects of key components of the RWMN, we experiment with five variants: (i) (RWMN-noRW) model without read/write networks, (ii) (RWMN-noR) model with only the write network, (iii) (RWMN-noQ) model without query-dependant memory embedding, (iv) (RWMN-noVid) model trained without using videos to quantify the importance of visual input, and (v) (RWMN) model with both write/read networks.</p><p>We also test two ensemble versions of our model. Since the MovieQA dataset size is relatively small compared to task difficulty (e.g. <ref type="bibr" target="#b3">4</ref> overfitting, which the ensemble methods can mitigate. The first (RWMN-bag) is a bagged version of our approach, in which we independently learn RWMN models on 30 bootstrapped datasets, and obtain the averaged prediction. The second (RWMN-ensemble) is a simple ensemble, in which we independently train 20 models with different random initializations, and compute the average prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>We below report the results of each method on the validation and test sets, both of which are not used for training at all. While the original MovieQA paper <ref type="bibr" target="#b25">[26]</ref> reports the results on the validation set only, the official leaderboard shows the performance on the test set only, for which groundtruth answers are not observable and the evaluation is performed through the evaluation server. The test submission to the server is limited to once every 72 hours.</p><p>As of the ICCV2017 submission deadline, our RWMN achieves the best performance for four out of five tasks in the validation set, and four out of six tasks in the test set.</p><p>Results of VQA task. <ref type="table" target="#tab_2">Table 2</ref> compares the performance of our RWMN model with those of baselines for the video+subtitle task. We observe that RWMN achieves the best performance on both validation and test sets. For example, in the test set, RWMN attains 36.25%, which is significantly better than the runner-up DEMN of 29.97%.</p><p>As expected, the RWMN with both read/write networks is the best among our variants on both validation and test sets. It implicates that read/write networks play a key role in improving movie understanding. For example, the RWMN-noR with only write network attains higher performance than the RWMN-noRW, which has similar or lower performance than other existing models. The RWMN-noQ without question-dependent memory embedding also underperforms the normal RWMN, which shows that the memory update according to the question is indeed helpful to select a more relevant answer to the question. Finally, the RWMN-noVid is not as good as the RWMN, meaning that our RWMN successfully exploits both full videos and subtitles for training. Interestingly, the ensemble methods of our model, RWMN-bag and RWMN-ensemble, slightly underperform the single model RWMN.</p><p>Results of text-only tasks. <ref type="table" target="#tab_3">Table 3</ref> shows the results on the validation and test sets for text-only categories (i.e. subtitle only, DVS only, script only, plot synopses only). For the open-end task, we simply use the plot synopses version of our method, which outperforms the only trivial baseline for the test set (i.e. selecting the longest answer choice).</p><p>Our RWMN achieves the best performance in all tasks except for DVS-test set and plot synopses task. We also observe that the ensemble methods hardly improve the performance of our method noticeably. As discussed before, the memory network approaches including our RWMN and MEMN2N are not outstanding in the plot synopses only category. It is mainly due to that the queries and answer choices are made directly from the plot sentences, and thus, this task can be tackled better by word/sentence matching methods with little story comprehension. In addition, each plot synopsis consists of about 35 sentences on average as a summary of a movie, which is much shorter than other data types, for examples, about 1,558 sentences of subtitles per movie. Therefore, the memory abstraction by our method becomes less critical to solve the problems in this category.</p><p>One important difference between the four text-only tasks is that each story source has a different n (i.e. the number of sentences), and thus the density of information contained in each sentence is also different. For example, the average n of the scripts is about 2,877 per movie, while the average n of DVS is about 636; thus, each sentence in the script contains low-level details, while each sentence in the DVS contain high-level and abstract content. Given that the performance improvement by our RWMN is more significant in the DVS only task (e.g. RWMN: 40.0 and MEMN2N: 33.0), it can be seen that our proposal to read/write networks may be more beneficial to understand and answer high-level and abstract content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Results</head><p>We experiment the performance variation according to the structure of CNNs in the write/read networks. Among hyperparameters of the RWMN, the following three combinations have significant effects on the performance of the model; i) conv-filter/stride sizes of the write network (f w v , s w v , f w c ), ii) conv-filter/stride sizes of the read network (f r v , s r v , f r c ), and iii) number of read/write CNN layers ν r , ν w . Regarding the convolutions, the larger the convolution filter sizes, the more memories are read/written as a chunk. Also, as the stride size decreases or the number of output channels increases, the total number of memory blocks increases. <ref type="table" target="#tab_4">Table 4</ref> summarizes the performance variation on the video+subtitle task according to different combinations of these three hyperparameters. We make several observations from the results. First, as the number of CNN layers in read/write network increases, the capacity of memory interaction may increase as well; yet the performance becomes worsen. Presumably, the main reason may be overfitting due to a relative small dataset size of MovieQA as discussed. It is hinted by our results that the two-layer CNN is the best for training performance, while the one-layer CNN is the best for validation. Second, we observe that there is no absolute magic number of how many memory slots should be read/written as a single chunk and how many strides the memory controller moves. If the stride height is too small or too large compared to the height of a convolution filter, the performance decreases. It means that the performance can be degraded when too much information is read/written as a single abstracted slot, when too much information is overlapped in adjacent reads/writes (due to a small stride), or when the information overlap is too coarse (due to a high stride). We present more ablation results to the supplementary file. <ref type="figure">Figure 3</ref> compares between the MEMN2N <ref type="bibr" target="#b25">[26]</ref> and our RWMN model according to question types in the video+subtitle task. We examine the results of six question types, according to what starting word is used in the question: Who, Where, When, What, Why, and How. Usually, Why questions require abstraction and high-level reasoning to answer correctly (e.g. Why did Harry end his relationship with Helen?, Why does Michael depart for Sicily?). On the other hand, Who and When questions primarily deal with factual elements (e.g. Who is Harry's girlfriend?, When does Grissom plan to set up Napier to be murdered?). Compared to the MEMN2N <ref type="bibr" target="#b25">[26]</ref>, our RWMN shows higher performance enhancement in the questions starting with Why, which may implicate the superiority of the RWMN to deals with high-level reasoning questions.  answer choices in which groundtruth is in bold and our model's selection is red checked. We also show on which parts our RWMN attends over entire movies, along with the groundtruth (GT) attention maps indicating the temporal locations of the clips where the question is actually generated, provided by the dataset. As examples show, movie question answering is highly challenging, and sometimes is not easy even for human. Our predicted attention often agrees well with the GT; the RWMN can implicitly learn where to place its attention in a very long movie for answering, although such information is not available for training. However, sometimes the RWMN can find correct answers even with the attention mismatch with the GT. It is due to that the MovieQA dataset also includes many questions that are hardly solvable with only attending on the GT parts. That is, some questions re-Q. Why does Amy's disappearance receive heavy press coverage?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>[0] Because her parents are popular [1] Because Amy was the inspiration for the popular "Amazing Amy" children books <ref type="bibr" target="#b1">[2]</ref> Because Amy is a popular actress <ref type="bibr" target="#b2">[3]</ref> Because it happened on the day of her wedding anniversary <ref type="bibr" target="#b3">[4]</ref>  [0] His mother agrees to pay more money [1] His mother agrees to a one night stand with the shool principal <ref type="bibr" target="#b1">[2]</ref> He gets a football scholarship because he runs very fast <ref type="bibr" target="#b2">[3]</ref> His mother begs the principal and he takes mercy on her <ref type="bibr" target="#b3">[4]</ref> Forrest is very good in football so the school accepts him on this account Q. What does Gandalf learn from Pippin's visions? A1. Sauron will attack Minas Tirith A2. Sauron will hide in Minas Tirith A3. Sauron will attack Erebor A4. Sauron will attack The Shire A5. Sauron will flee from Minas Tirith  <ref type="figure" target="#fig_3">Figure 4</ref>. Qualitative examples of MovieQA video+subtitle problems solved by our methods (success cases in the top two rows, and failure cases in the last row). Bold sentences are groundtruth answers and red check symbols indicate our model's selection. In each example, we also show on which parts our RWMN model attend over entire movie. The attention by the RWMN often matches well with the groundtruth (GT) where the question is actually generated. quire understanding the relationship between characters or progress of event development, for which attending beyond GT parts is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a new memory network model named Read-Write Memory Network (RWMN), whose key idea is to propose the CNN-based read/write network that enable the model to have highly-capable and flexible read/write operations. We empirically validated that the proposed read/write networks indeed improve the performance of visual question answering tasks for large-scale, multimodal movie story understanding. Specifically, our approach achieved the best accuracies in multiple tasks of MovieQA benchmark, with a significant improvement on visual QA task. We believe that there are several future research directions that go beyond this work. First, we can apply our approach to other QA tasks that require complicated story understanding. Second, we can explore better video and text representation methods beyond ResNet and Word2Vec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the number of filter channel is f w c = 3 and strides are s w v = 30 and s w h = 1, respectively:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed Read-Write Network. (a) The multimodal movie embedding E is obtained using the ResNet feature and the Word2Vec representation from movie subshots and subscripts (section 3.1). (b) The write memory M abstracts higher-level sequential information through multiple convolution layers (section 3.2). (c) The query-dependent memory Mq is obtained via the Compact Bilinear Pooling (CBP) between the query and each slot of M, and then the read memory Mr is constructed through convolution layers (section 3.3). (d) Finally, the answer with the highest confidence score is chosen out of five candidates (section 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>illustrates selected qualitative examples of video+subtitle problems solved by our methods, including four success and two near-miss cases. In each example, we present a sampled query video, a question, and five</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Q. How does Travis think Miley knows Hannah Montana? [0] He thinks that Miley and Hannah are friends from school [1] He thinks that Hannah saved Miley's life in a surfing accident [2] He thinks that Miley and Hannah are cousins[3] He thinks that Miley saved Hannah's life in a surfing accident [4] He thinks that Miley saved Hannah's life in a car accident Q. Why did Lillian run away from her wedding? A1. Because she spilled something on her dress right before the ceremony and was too embarrassed of everyone seeing A2. Because of Annie's extravagant planning and out of fear of leaving her life in Milwaukee A3. Because it didn't feel right without Annie there A4. No reason in particular A5. Because of Helen's extravagant planning and out of fear of leaving her life in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The number of movies and QA pairs according to data sources in the MovieQA dataset<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>Story sources</cell><cell cols="2"># movie # QA pairs</cell></row><row><cell>Videos and subtitles</cell><cell>140</cell><cell>6,462</cell></row><row><cell>Subtitles</cell><cell>408</cell><cell>14,944</cell></row><row><cell>DVS</cell><cell>60</cell><cell>2,446</cell></row><row><cell>Scripts</cell><cell>199</cell><cell>7,810</cell></row><row><cell>Plot synopses</cell><cell>408</cell><cell>14,944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="2">Video+Subtitle val test</cell></row><row><cell>OVQAP</cell><cell>-</cell><cell>23.61</cell></row><row><cell>Simple MLP</cell><cell>-</cell><cell>24.09</cell></row><row><cell>LSTM + CNN</cell><cell>-</cell><cell>23.45</cell></row><row><cell>LSTM + Discriminative CNN</cell><cell>-</cell><cell>24.32</cell></row><row><cell>VCFSM</cell><cell>-</cell><cell>24.09</cell></row><row><cell>DEMN [15]</cell><cell>-</cell><cell>29.97</cell></row><row><cell>MEMN2N [26]</cell><cell>34.20</cell><cell>-</cell></row><row><cell>RWMN-noRW</cell><cell>34.20</cell><cell>-</cell></row><row><cell>RWMN-noR</cell><cell>36.50</cell><cell>-</cell></row><row><cell>RWMN-noQ</cell><cell>38.17</cell><cell>-</cell></row><row><cell>RWMN-noVid</cell><cell>37.20</cell><cell>-</cell></row><row><cell>RWMN</cell><cell cols="2">38.67 36.25</cell></row><row><cell>RWMN-bag</cell><cell cols="2">38.37 35.69</cell></row><row><cell>RWMN-ensemble</cell><cell>38.30</cell><cell>-</cell></row></table><note>1 http://movieqa.cs.toronto.edu/.Performance comparison for the video+subtitle task on MovieQA public validation/test dataset. (-) means that the method does not participate on the task. Baselines include DEMM (Deep embedded memory network), OVQAP (Only video question an- swer pairs) and VCFSM (Video clip features with simple MLP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison for all the tasks on MovieQA public validation/test dataset. (-) indicates that the method does not participate on the task. The description of baselines with no reference can be found in the MovieQA leaderboard.</figDesc><table><row><cell>,318 training QA examples in</cell></row><row><cell>video+subtitle category), models often suffer from severe</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance of the RWMN on the video+subtitle task, according to the structure parameters of write/read networks. ν w/r : the number of layers for write/read networks, (f The RWMN leads higher improvement for Why questions that often require abstract and high-level understanding.</figDesc><table><row><cell cols="2"># Layers</cell><cell>Write network</cell><cell cols="3">Read network Acc.</cell></row><row><cell cols="2">νw νr</cell><cell>(f w vi , s w vi , f w ci )</cell><cell cols="3">(f r vi , s r vi , f w ri )</cell></row><row><cell>0</cell><cell>0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>34.2</cell></row><row><cell>1</cell><cell>0</cell><cell>(40,7,1)</cell><cell></cell><cell>-</cell><cell>33.9</cell></row><row><cell>1</cell><cell>0</cell><cell>(40,30,3)</cell><cell></cell><cell>-</cell><cell>36.5</cell></row><row><cell>1</cell><cell>1</cell><cell>(40,30,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>38.6</cell></row><row><cell>1</cell><cell>1</cell><cell>(40,60,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>33.6</cell></row><row><cell>2</cell><cell>1</cell><cell>(40,10,3), (10,5,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>37.2</cell></row><row><cell>2</cell><cell>1</cell><cell>(5,3,1), (5,3,1)</cell><cell></cell><cell>(3,1,1)</cell><cell>37.3</cell></row><row><cell>2</cell><cell>2</cell><cell>(4,2,1), (4,2,1)</cell><cell cols="3">(3,1,1), (3,1,1) 36.9</cell></row><row><cell>2</cell><cell>2</cell><cell>(4,2,1), (4,2,1)</cell><cell cols="3">(4,2,1), (4,2,1) 37.3</cell></row><row><cell>3</cell><cell cols="2">1 (10,3,3), (40,3,3), (100,3,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>35.1</cell></row><row><cell>3</cell><cell>1</cell><cell>(40,3,3), (10,3,3), (10,3,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>37.9</cell></row><row><cell>3</cell><cell>1</cell><cell>(40,3,3), (40,3,3), (40,3,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>35.7</cell></row><row><cell>3</cell><cell cols="2">1 (100,3,3), (40,3,3), (10,3,3)</cell><cell></cell><cell>(3,1,1)</cell><cell>35.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">w/r vi , s</cell><cell>w/r vi , f</cell><cell>w/r ci ):</cell></row><row><cell cols="6">the height and the stride of convolution filters, and the number of</cell></row><row><cell cols="3">output channels.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">MEMN2N</cell><cell>RWMN</cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.3 0.35</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">who where when what</cell><cell>why</cell><cell>how</cell></row><row><cell cols="6">Figure 3. Accuracy comparison between RWMN and the</cell></row><row><cell cols="6">MEMN2N [26] baseline on the video+subtitle task according to</cell></row><row><cell cols="3">question types.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>How does Forrest get admitted to public school despite his low IQ?</figDesc><table><row><cell>Q. Where does the Joker set a trap for Vicki?</cell></row><row><cell>[0] At the Gotham Museum of Art</cell></row><row><cell>[1] At her house</cell></row><row><cell>[2] At Gotham Police Station</cell></row><row><cell>[3] At the Gotham Museum of History</cell></row><row><cell>[4] At Bruce's mansion</cell></row><row><cell>Because her husband is popular</cell></row><row><cell>Q.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://movieqa.cs.toronto.edu/leaderboard/ as of the ICCV2017 submission deadline (March 27, 2017 23:59 GMT).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is partially supported by SK Telecom and Basic Science Research Program through National Research Foundation of Korea (2015R1C1A1A02036562). Gunhee Kim is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8M: A Large-scale Video Classification Benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07427</idno>
		<title level="m">Hierarchical Memory Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural Turing Machines</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid Computing Using a Neural Network with Dynamic External Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepstory: video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Key-value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attend to You: Personalized Image Captioning with Context Sequence Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Movie Description. IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Compare-Aggregate Model for Matching Text Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards AIcomplete Question Answering: A Set of Prerequisite Toy Tasks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory Networks. ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MSR-VTT: A Large Video Description Dataset for Bridging Video and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jongwook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic Key-Value Memory Network for Knowledge Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
