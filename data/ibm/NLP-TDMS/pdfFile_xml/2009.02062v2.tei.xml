<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Looking for change? Roll the Dice and demand Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foivos</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICRAR</orgName>
								<orgName type="institution" key="instit2">University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<region>Floreat, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Waldner</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CSIRO Agriculture &amp; Food</orgName>
								<address>
									<settlement>St Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Caccetta</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<region>Floreat, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Looking for change? Roll the Dice and demand Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>change detection</term>
					<term>Attention</term>
					<term>Dice similarity</term>
					<term>Tanimoto</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection, i.e. identification per pixel of changes for some classes of interest from a set of bi-temporal co-registered images, is a fundamental task in the field of remote sensing. It remains challenging due to unrelated forms of change that appear at different times in input images. These are changes due to to different environmental conditions or simply changes of objects that are not of interest. Here, we propose a reliable deep learning framework for the task of semantic change detection in very high-resolution aerial images. Our framework consists of a new loss function, new attention modules, new feature extraction building blocks, and a new backbone architecture that is tailored for the task of semantic change detection. Specifically, we define a new form of set similarity, that is based on an iterative evaluation of a variant of the Dice coefficient. We use this similarity metric to define a new loss function as well as a new spatial and channel convolution Attention layer (the FracTAL ). The new attention layer, designed specifically for vision tasks, is memory efficient, thus suitable for use in all levels of deep convolutional networks. Based on these, we introduce two new efficient self-contained feature extraction convolution units. We term these units CEECNet and FracTAL ResNet units. We validate the performance of these feature extraction building blocks on the CIFAR10 reference data and compare the results with standard ResNet modules. We also compare the proposed FracTAL attention layer against the Convolution Block Attention Module (CBAM), showing 1% performance increase between two otherwise identical networks, that use different attention modules. Further, we introduce a new encoder/decoder scheme, a network macro-topology, that is tailored for the task of change detection. The key insight in our approach is to facilitate the use of relative attention between two convolution layers in order to compare them. Our network moves away from any notion of subtraction of feature layers for identifying change. We validate our approach by showing excellent performance and achieving state of the art score (F1 and Intersection over Union -hereafter IoU) on two building change detection datasets, namely, the LEVIRCD (F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882) datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Change detection is one of the core applications of remote sensing. The goal of change detection is to assign binary labels <ref type="bibr">("change" or no "change")</ref> to every pixel in a study area based on at least two co-registered images taken at different times. The definition of "change" varies across applications and includes, for instance, urban expansion , flood mapping <ref type="bibr" target="#b15">(Giustarini et al., 2012)</ref>, deforestation <ref type="bibr" target="#b39">(Morton et al., 2005)</ref>, and cropland abandonment <ref type="bibr" target="#b35">(Löw et al., 2018)</ref>. Changes of multiple land-cover classes, i.e. semantic change detection, can also be addressed simultaneously . It remains a challenging task due to various forms of change owed to varying environmental conditions that do not constitute a change for the objects of interest <ref type="bibr" target="#b50">(Varghese et al., 2018)</ref>.</p><p>A plethora of change-detection algorithms has been devised and summarised in several reviews <ref type="bibr" target="#b36">(Lu et al., 2004;</ref><ref type="bibr" target="#b11">Coppin et al., 2004;</ref><ref type="bibr" target="#b22">Hussain et al., 2013a;</ref><ref type="bibr" target="#b47">Tewkesbury et al., 2015)</ref>. <ref type="bibr">1</ref>  <ref type="bibr">foivos.diakogiannis@uwa.edu.au</ref> In recent years, computer vision has further pushed the state of the art, especially in applications where the spatial context is paramount. The rise of computer vision, especially deep learning, is related to advances and democratisation of powerful computing systems, increasing amounts of available data, and the development of innovative ways to exploit data .</p><p>Our starting point is the hypothesis that human intelligence identifies differences in images by looking for change in objects of interest at a higher cognitive level <ref type="bibr" target="#b50">(Varghese et al., 2018)</ref>. We understand this because the time required for identifying objects that changed between two images, increases with time when the number of changed objects increases <ref type="bibr" target="#b48">(Treisman and Gelade, 1980)</ref>. That is, there is strong correlation between processing time and number of individual objects that changed. In other words, the higher the complexity of the changes the more time is required to accomplish it. Therefore, simply subtracting extracted features from images (which is a constant time operation) cannot account for the complexities of human perception. As a result, the deep convolutional neural networks proposed in this paper address change detection without using bespoke tn tp fn fp <ref type="figure">Figure 1</ref>: Example of the proposed framework (architecture: mantis CEECNetV1) change detection performance on the LEVIRCD test set . From left to right: input image at date 1, input image at date 2, ground truth buildings change mask, and color coded the true negative (tn), true positive (tp), false positive (fp) and false negative (fn) predictions. features subtraction.</p><p>In this work, we developed neural networks using attention mechanisms that emphasize areas of interest in two bi-temporal coregistered aerial images. It is the network that learns what to emphasize, and how to extract features that describe change at a higher level. To this end, we propose a dual encoder -single decoder scheme, that fuses information of corresponding layers with relative attention and extracts as a final layer a segmentation mask. This mask designates change for classes of interest, and can also be used for the dual problem of class attribution of change. As in previous work, we facilitate the use of conditioned multi-tasking 2  that proves crucial for stabilizing the training process and improving performance. In summary, the main contributions of this work are:</p><p>1. We introduce a new set similarity metric that is a variant of the Dice coefficient, the Fractal Tanimoto similarity measure (section 3). This similarity measure has the advantage that it can be made steeper than the standard Tanimoto metric towards optimality, thus providing a finer-grained similarity metric between layers. The level of steepness is controlled from a depth recursion hyper-parameter. It can be used both as a "sharp" loss function when fine-tuning a model at the latest stages of training, as well as a set similarity metric between feature layers in the attention mechanism. 2. Using the above set similarity as a loss function, we propose an evolving loss strategy for fine-tuning training of neural networks (section 4). This strategy helps to avoid overfitting and improves performance. 3. We introduce the Fractal Tanimoto Attention Layer (hereafter FracTAL ), tailored for vision tasks (section 5). This layer uses the fractal Tanimoto similarity to compare queries with keys inside the Attention module. It is a form of spatial and channel attention combined.</p><p>4. We introduce a feature extraction building block that is based on the Residual neural network and fractal Tanimoto Attention (section 5.2.1). The new FracTAL ResNet converges faster to optimality than standard residual networks and enhances performance. 5. We introduce two variants of a new feature extraction building block, the Compress-Expand / Expand-Compress unit (hereafter CEECNet unit -section 6.1). This unit exhibits enhanced performance in comparison with standard residual units, and the FracTAL ResNet unit. 6. Capitalizing on these findings, we introduce a new backbone encoder/decoder scheme, a macro-topology -the mantis -that is tailored for the task of change detection (section 6.2). The encoder part is a Siamese dual encoder, where the corresponding extracted features at each depth are fused together with FracTAL relative attention. In this way, information exchange between features extracted from bi-temporal images is enforced. There is no need for manual feature subtraction. 7. Given the relative fusion operation between the encoder features at different levels, our algorithm achieves state of the art performance on the LEVIRCD and WHU datasets without requiring the use of contrastive loss learning during training (section 9). Therefore, it is easier to implement with standard deep learning libraries and tools.</p><p>Networks integrating the above-mentioned contributions yielded state of the art performance for the task of building change detection in two benchmark data sets for change detection: the WHU <ref type="bibr" target="#b26">(Ji et al., 2019b)</ref> and LEVIRCD  datasets.</p><p>In addition to the previously mentioned sections, the following complete the works. In Section 2 we present related work on Attention mechanism and change detection, specialised for the case of very high resolution (hereafter VHR) aerial images. In Section 7 we describe the setup of our experiments. In Section 8 we perform an ablation study of the proposed schemes. Finally, in Section Appendix C we present in mxnet/gluon style pseudocode various key elements of our architecture 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">On attention</head><p>The attention mechanism was first introduced by <ref type="bibr" target="#b2">Bahdanau et al. (2014)</ref> for the task of neural machine translation 4 (hereafter NMT). This mechanism addressed the problem of translating very long sentences in encoder/decoder architectures. An encoder is a neural network that encodes a phrase to a fixedlength vector. Then the decoder operates on this output and produces a translated phrase (of variable length). It was observed that these types of architectures were not performing well when the input sentences were very long . The attention mechanism provided a solution to this problem: instead of using all the elements of the encoder vector on equal footing for the decoder, the attention provided a weighted view of them. That is, it emphasized the locations of encoder features that were more important than others for the translation, or stated another way, it emphasized some input words that were more important for the meaning of the phrase. However, in NMT, the location of the translated words is not in direct correspondence with the input phrase, because of the syntax changes. Therefore, <ref type="bibr" target="#b2">Bahdanau et al. (2014)</ref> introduced a relative alignment vector, e i j , that was responsible for encoding the location dependences: in language, it is not only the meaning (value) of a word that is important but also its relative location in a particular syntax. Hence, the attention mechanism that was devised was comparing the emphasis of inputs at location i with respect to output words at locations j. Later, <ref type="bibr" target="#b51">Vaswani et al. (2017)</ref> developed further this mechanism and introduced the scaled dot product self-attention mechanism as a fundamental constituent of their Transformer architecture. This allowed the dot product to be used as a similarity measure between feature layers, including feature vectors having large dimensionality.</p><p>The idea of using attention for vision tasks soon passed to the community. <ref type="bibr" target="#b21">Hu et al. (2017)</ref> introduced channel based attention, in their squeeze and excitation architecture. <ref type="bibr" target="#b53">Wang et al. (2017)</ref> used spatial attention to facilitate non-local relationships across sequences of images. <ref type="bibr" target="#b8">Chen et al. (2016)</ref> combined both approaches by introducing joint spatial and channel wise attention in convolutional neural networks, demonstrating improved performance on image captioning datasets. <ref type="bibr" target="#b55">Woo et al. (2018)</ref> introduced the Convolution Block Attention Module (CBAM) which is also a form of spatial and channel attention, and showed improved performance on image classification and object detection tasks. To the best of our knowledge, the most faithful implementation of multi-head attention <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> for convolution layers, is <ref type="bibr" target="#b3">Bello et al. (2019)</ref> (spatial attention). <ref type="bibr" target="#b44">Sakurada and Okatani (2015)</ref> and <ref type="bibr" target="#b0">Alcantarilla et al. (2016)</ref> (see also <ref type="bibr" target="#b16">Guo et al. 2018)</ref> were some of the first to introduce found on https://github.com/feevos/ceecnet. <ref type="bibr">4</ref> That is, language to language translation of sentences, e.g. English to French. fully convolutional networks for the task of scene change detection in computer vision, and they both introduced street view change detection datasets. <ref type="bibr" target="#b44">Sakurada and Okatani (2015)</ref> extracted features from a convolutional neural networks and combined them with super pixel segmentation to recover change labels in the original resolution. <ref type="bibr" target="#b0">Alcantarilla et al. (2016)</ref> proposed an approach that chains multi-sensor fusion simultaneous localization and mapping (SLAM) with a fast 3D reconstruction pipeline that provides coarsely registered image pairs to an encoder/decoder convolutional network. The output of their algorithm is a pixel-wise change detection binary mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">On change detection</head><p>Researchers in the field of remote sensing picked up and evolved this knowledge and started using it for the task of land cover change detection. In the remote sensing community, the dual Siamese encoder and a single decoder is frequently adopted. The majority of different approaches then modifies how the different features extracted from the dual encoder are consumed (or compared) in order to produce a change detection prediction layer. In the following we focus on approaches that follow this paradigm and are most relevant to our work. For a general overview of land cover change detection in the field of remote sensing interested readers can consult <ref type="bibr" target="#b23">Hussain et al. (2013b)</ref> and <ref type="bibr" target="#b1">Asokan and Anitha (2019)</ref>. For a general review on AI applications of change detection to the field of remote sensing <ref type="bibr" target="#b46">Shi et al. (2020)</ref>.</p><p>Caye <ref type="bibr" target="#b12">Daudt et al. (2019)</ref> presented and evaluated various strategies for land cover change detection, establishing that their best algorithm was a joint multitasking segmentation and change detection approach. That is, their algorithm predicted simultaneously the semantic classes on each input image, as well as the binary mask of change between the two.</p><p>For the task of buildings change detection, <ref type="bibr" target="#b25">Ji et al. (2019a)</ref> presented a methodology that is a two-stage process, wherein the first part they use a building extraction algorithm from single date input images. In the second part, the binary masks that are extracted are concatenated together and inserted into a different network that is responsible for identifying changes between the two binary layers. In order to evaluate the impact of the quality of the building extraction networks, the authors use two different architectures. The first, one of the most successful networks to date for instance segmentation, the Mask-RCNN , and the second the MS-FCN (multi scale fully convolutional network) that is based on the original UNet architecture <ref type="bibr" target="#b43">(Ronneberger et al., 2015)</ref>. The advantage of this approach, according to the authors is the fact that they could use unlimited synthetic data for training the second stage of the algorithm. <ref type="bibr" target="#b7">Chen et al. (2021)</ref> used a dual attentive convolutional neural network, i.e. the feature extractor was a siamese VGG16 pretrained network. The attention module they used for vision, was both spatial and channel attention, and it was the one introduced in <ref type="bibr" target="#b51">Vaswani et al. (2017)</ref>, however with a single head. Training was performed with a contrastive loss function. <ref type="bibr" target="#b6">Chen and Shi (2020)</ref> presented the STANet, which consists of a feature extractor based on ResNet18 <ref type="bibr" target="#b19">(He et al., 2015)</ref>, and two versions of spatio-temporal attention modules, the Basic spatial-temporal attention module (BAM) and the pyramid spatial-temporal attention module (PAM). The authors introduced the LEVIRCD change detection dataset and demonstrated excellent performance. Their training process facilitates a contrastive loss applied at the feature pixel level. Their algorithm predicts binary change labels. <ref type="bibr" target="#b27">Jiang et al. (2020)</ref> introduced the PGA-SiamNet that uses a dual Siamese encoder that extracts features from the two input networks. They used VGG16 for feature extraction. A key ingredient to their algorithm is the co-attention module <ref type="bibr" target="#b37">(Lu et al., 2019)</ref> that was initially developed for video object segmentation. The authors use it for fusing the extracted features of each input image from the dual VGG16 encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fractal Tanimoto similarity coefficient</head><p>In  we analyzed the performance of the various flavours of the Dice coefficient and introduced the Tanimoto with complement coefficient. Here, we expand further our analysis, and we present a new functional form for this similarity metric. We use it both as a self-similarity measure between convolution layers in a new attention module, as well as a loss function for finetuning semantic segmentation models.</p><p>For two (fuzzy) binary vectors of equal dimension, p, l, whose elements lie in the range [0, 1] the Tanimoto similarity coefficient is defined:</p><formula xml:id="formula_0">T (p, l) = p · l p 2 + l 2 − p · l<label>(1)</label></formula><p>Interestingly, the dot product between two fuzzy binary vectors is another similarity measure of their agreement. This inspired us to introduce an iterative functional form of the Tanimoto:</p><formula xml:id="formula_1">T 0 ≡ T (p, l) = p · l p 2 + l 2 − p · l (2) T d = T d−1 (p, l) T d−1 (p, p) + T d−1 (l, l) − T d−1 (p, l)<label>(3)</label></formula><p>For example, expanding Eq. (2) for d = 2, yields:</p><formula xml:id="formula_2">T 2 (p, l) = p · l (l 2 − p · l + p 2 ) 2 − p·l l 2 −p·l+p 2        2 − p·l (l 2 −p.l+p 2 ) 2− p·l l 2 −p·l+p 2        (4)</formula><p>We can expand this for an arbitrary depth d and then we get the following simplified version 5 of the fractal Tanimoto similarity measure:</p><formula xml:id="formula_3">T d (p, l) = p · l 2 d (p 2 + l 2 ) − (2 d+1 − 1)p · l<label>(5)</label></formula><p>This function takes values in the range [0, 1] and it becomes steeper as d increases. At the limit d → ∞ it behaves like the integral of the Dirac δ function around point l, δ(p − l)dp. That is, the parameter d is a form of annealing "temperature". Interestingly, although the iterative scheme was defined with d being an integer, for continuous values d ≥ 0, T d remains bounded in the interval [0, 1]. That is:</p><formula xml:id="formula_4">T d : n × n → U ⊆ [0, 1]<label>(6)</label></formula><p>where n = dim(p) is the dimensionality of the fuzzy binary vectors p, l. In the following we will use the functional form of the fractal Tanimoto with complement , i.e.:</p><formula xml:id="formula_5">F T d (p, l) ≡ T d (p, l) + T d (1 − p, 1 − l) 2<label>(7)</label></formula><p>In <ref type="figure" target="#fig_0">Fig. 2</ref> we provide a simple example for a ground truth vector l = {0.4, 0.6} and a continuous vector of probabilities p = {p x , p y }. On the top panel, we construct density plots of the Fractal Tanimoto function with complement, F T d . Oveplotted are the gradient field lines that point to the ground truth. In the bottom pannels, we plot the corresponding 3D representations. From left to right, the first column corresponds to d = 0, the second to d = 3 and the third to d = 5. It is apparent that the effect of the d hyperparameter is to make the similarity metric steeper towards the ground truth. For all practical purposes (network architecture, evolving loss function) we use the average fractal Tanimoto loss (last column), due to having steeper gradients away from optimality 6 :</p><formula xml:id="formula_6">F T d (p, l) ≡ 1 d d−1 i=0 F T i (p, l)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evolving loss strategy</head><p>In this section, we describe a training strategy that modifies the depth of the fractal Tanimoto similarity coefficient, when used as a loss function, on each learning rate reduction. For minimization problems, the fractal Tanimoto loss is defined through: L = 1 − F T d . In the following, when we refer to the fractal Tanimoto loss function, it should be understood that this is defined trough the similarity coefficient, as described above.</p><p>During training, and until the first learning rate reduction we use the standard Tanimoto with complement F T 0 (p, l). The reason for this is that for a random initialization of the weights (i.e. for an initial prediction point in the space of probabilities away from optimality), the gradients are steeper towards the best values for this particular loss function (in fact, for cross entropy are even steeper). This can be seen in <ref type="figure" target="#fig_0">Fig. 2</ref> in the bottom row: clearly for an initial probability vector p = {p x , p y } away from the ground truth l = {0.4, 0.6} the gradients are steeper for d = 0. As training evolves, and the value of the weights approaches optimality, the predictions approach the ground truth and the loss function flattens out. With batch gradient descent (and variants), we are not really calculating the true (global) loss function, but a noisy approximate version of it. This is because in each batch loss evaluation, we are not using all of the data for the gradients evaluation. In <ref type="figure" target="#fig_1">Fig. 3</ref> we represent a graphical representation of the true landscape and a noisy version of it for a toy 2D problem. In the top row, we plot the value of the F T 0 similarity as well as the average value of the loss functions for d = 0, . . . , 9 for the ground truth vector l = {0.4, 0.6}. In 6 The formula is valid for d &gt;= 1, for d = 0 it reverts to the standard fractal Tanimoto loss, F T d=0 (p, l). the corresponding bottom rows, we have the same plot were we also added random Gaussian noise. In the initial phases of training, the average gradients are greater than the local values dues to noise. As the network reaches optimality the average gradient towards optimality becomes smaller and smaller, and the gradients due to noise dominate the training. Once we reduce the learning rate, the step the optimizer takes is even smaller, therefore it cannot easily escape local optima (due to noise). What we propose is to "shift gears": once training stagnates, we change the loss function to a similar but steeper one towards optimality that can provide gradients (on average) that can dominate the noise. Our choice during training is the following set of learning rates and depths of the fractal Tanimoto loss: {(lr : 10 −3 , d = 0), (lr : 10 −4 , d = 10), (lr : 10 −5 , d = 20)}. In all evaluations of loss functions for d &gt; 0, we use the average value for all d values (Eq. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fractal Tanimoto Attention</head><p>Here, we present a novel convolutional attention layer based on the new similarity metric and a methodology of fusing information from the output of the attention layer to features extracted from convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Fractal Tanimoto Attention layer</head><p>In the pioneering work of <ref type="bibr" target="#b51">Vaswani et al. (2017)</ref> the attention operator is defined through a scaled dot product operation. For images in particular, i.e. two dimensional features, assuming that q ∈ R C q ×H×W is the query, k ∈ R C×H×W the key and v ∈ R C×H×W it's corresponding value, the (spatial) attention is defined as (see also <ref type="bibr" target="#b57">Zhang et al. 2020)</ref>:</p><formula xml:id="formula_7">o = softmax q • 1 k √ d , ∈ R C q ×C (9) Att(q, k, v) = o • 2 v, ∈ R C q ×H×W<label>(10)</label></formula><p>Here d is the dimension of the keys and the softmax operation is with respect to the first (channel) dimension. The term √ d is a scaling factor that ensures the Attention layer scales well even with a large number of dimensions <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref>. The operator • 1 corresponds to inner product with respect to the spatial dimensions height, H, and width, W, while • 2 is a dot product with respect to channel dimensions 7 . In this formalism each channel of the query features is compared with each of the channels of the key values. In addition there is a 1 − 1 correspondence between keys and values, meaning that for each key corresponds a unique value. The point of the dot product is to emphasize the key-value pairs that are more relevant for the particular query. That is the dot product selects the keys that are most similar to the particular query. It represents the projection of queries on the keys space. The softmax operator provides a weighted "view" of all the values for a particular set of queries, keys and values-or else a "soft" attention mechanism. In the multi-head attention paradigm, multiple attention heads that follow the principles described above are concatenated together. One of the key disadvantages of this formulation when used in vision tasks (i.e. two dimensional features) is the very large memory footprint that this layer exhibits. For 1D problems, such as Natural Language Processing, this is not -in general -an issue.</p><p>Here we follow a different approach. We develop our formalism for the case where the number of query channels, C q is identical to the number of key channels, C. However, if desired, our formalism can work for the general case where C q C.</p><p>Let q ∈ R C×H×W be the query features, k ∈ R C×H×W the keys and v ∈ R C×H×W the values. In our formalism, it is a requirement for these operators to have values in [0, 1] 8 . Our approach is a joint spatial and channel attention mechanism. With the use of the Fractal Tanimoto similarity coefficient, we define the spatial, , and channel, , similarity between the query, q, and key, k, features according to:</p><formula xml:id="formula_8">T d (q, k) = q k 2 d (q q + k k) − (2 d+1 − 1)q k ∈ R C (12) T d (q, k) = q k 2 d (q q + k k) − (2 d+1 − 1)q k ∈ R H×W<label>(13)</label></formula><p>7 This is more apparent in index notation:</p><formula xml:id="formula_9">q • 1 k ≡ jk q i jk k l jk ∈ R Cq×C (11) o ≡ softmax (q • 1 k) ≡ o il Att(q, k, v) ≡ Att ik j ≡ o • 2 v ≡ l o il v lk j ∈ R Cq×H×W 8</formula><p>This can be easily achieved by applying the sigmoid operator.</p><p>where the spatial and channel products are defined as:</p><formula xml:id="formula_10">q k = jk q i jk k i jk ∈ R C q k = i q i jk k i jk ∈ R H×W</formula><p>It is important to note that the output of these operators lies numerically within the range [0,1], where 1 indicates identical similarity and 0 indicates no correlation between the query and key. That is, there is no need for normalization or scaling as is the case for the traditional dot product similarity. In our approach the spatial and channel attention layers are defined with element-wise multiplication 9 (denoted by the symbol ):</p><formula xml:id="formula_11">Att (q, k, v) = T d (q, k) v Att (q, k, v) = T d (q, k) v</formula><p>It should be stressed that these operations do not consider that there is a 1 − 1 mapping between keys and values. Instead, we consider a map of one-to-many, that is a single key can correspond to a set of values. Therefore, there is no need to use a softmax activation (see also <ref type="bibr" target="#b28">Kim et al. 2017)</ref>. The overall attention is defined as the average of the sum of these two operators:</p><formula xml:id="formula_12">Att(q, k, v) = 0.5(Att + Att )<label>(14)</label></formula><p>In practice we use the averaged fractal Tanimoto similarity coefficient with complement, F T d / , both for spatial and channel wise attention.</p><p>As stated previously, it is possible to extend the definitions of spatial and channel products in a way where we compare each of the channels (respectively, spatial pixels) of the query with each of the channels (respectively, spatial pixels) of the key. However, this imposes a heavy memory footprint, and makes deeper models, even for modern-day GPUs, prohibitive 10 . In addition, we found that this approach did not improve performance for the case of change detection and classification. Indeed, one needs to question this for vision tasks: the initial definition of attention  introduced a relative alignment vector, e i j , that was necessary because, for the task of NMT, the syntax of phrases changes from one language to the other. That is, the relative emphasis with respect to location between two vectors is meaningful. When we compare two images (features) at the same depth of a network (created by two different inputs, as is the case for change detection), we anticipate that the channels (or spatial pixels) will be in correspondence. For example, the RGB (or hyperspectral) order of inputs, does not change. That is, in vision, the situation can be different than NLP because we do not have a relative location change as it happens with words in phrases.</p><p>We propose the use of the Fractal Tanimoto Attention Layer (hereafter FracTAL ) for vision tasks as an improvement over the scaled dot product attention mechanism <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> for the following reasons:</p><p>1. The F T similarity is automatically scaled in the region [0, 1], therefore it does not require normalization, or activation to be applied. This simplifies the design and implementation of Attention layers and enables training without ad-hoc normalization operations. 2. The dot product does not have an upper or lower bound, therefore a positive value cannot be a quantified measure of similarity. In contrast F T has a bounded range of values in [0, 1]. The lowest value indicates no correlation, and the maximum value perfect similarity. It is thus easier to interpret. 3. Iteration d is a form of hyperparameter, like "temperature" in annealing. Therefore, the F T can become as steep as we desire (by modification of the temperature parameter d), steeper than the dot product similarity. This can translate to finer query and key similarity. 4. Finally, it is efficient in terms of GPU memory footprint (when one considers that it does both channel and spatial attention), thus allowing the design of more complex convolution building blocks.</p><p>The implementation of the FracTAL is given in Listing 2. The multihead attention is achieved using group convolutions for the evaluation of queries, keys and values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Attention fusion</head><p>A critical part in the design of convolution building blocks enhanced with attention is the way the information from attention is passed to convolution layers. To this aim we propose fusion methodologies of feature layers with the FracTAL for two cases: self attention fusion, and a relative attention fusion where information from two layers are combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Self attention fusion</head><p>We propose the following fusion methodology between a feature layer, L, and its corresponding FracTAL self-attention layer, A:</p><formula xml:id="formula_13">F = L + γL A = L (1 + γA)<label>(15)</label></formula><p>Here F is the output layer produced from the fusion of L and the Attention layer, A, describes element wise multiplication, 1 is a layer of ones like L, and γ a trainable parameter initiated at zero. We next describe the reasons why we propose this type of fusion. The Attention output is maximal (i.e. close to 1) in areas on the features where it must "attend" and minimal otherwise (i.e. close to zero). Multiplying element-wise directly the FracTAL attention layer A with the features, L, effectively lowers the values of features in areas that are not "interesting". It does not alter the value of areas that "are interesting". This can produce loss of information in areas where A "does not attend" (i.e. it does not emphasize), that would be otherwise valuable at a later stage. Indeed, areas of the image that the algorithm "does not attend" should not be perceived as empty space <ref type="bibr" target="#b48">(Treisman and Gelade, 1980)</ref>. For this reason the "emphasized" features, L A are added to the original input L. That is L + L A is identical to L in spatial areas where A tends to zero, and is emphasized in areas where A is maximal.</p><p>In the initial stages of training, the attention layer, A, does not contribute to L, due to the initial value of the trainable parameter γ 0 = 0. Therefore it does not add complexity during the initial phase of training and it allows for an annealing process of Attention contribution <ref type="bibr" target="#b58">(Zhang et al., 2018</ref>, see also <ref type="bibr" target="#b7">Chen et al. 2021</ref>). This property is particularly important when L is produced from a known performant recipe (e.g. residual building blocks).</p><p>In <ref type="figure" target="#fig_2">Fig 4</ref> we present this fusion mechanism for the case of a Residual unit <ref type="bibr" target="#b20">(He et al., 2016</ref><ref type="bibr" target="#b19">(He et al., , 2015</ref>. Here the input layer, X in , is subject to the residual block sequence of Batch normalization, convolutions, and ReLU activations, and produces the X out layer. A separate branch uses the X in input to produce the self attention layer A (see Listing 2). Then we multiply element wise the standard output of the residual unit, X in + X out , with the 1 + γA layer. In this way, at the beginning of training, this layer behaves as a residual layer, which has the excellent convergent properties of resnet at initial stages, and at later stages represents concatenation of features along the channel dimension (for V1). For version V2, we replace the concatenation, followed by the normalized convolution layer with a relative fusion attention, as described in Section 5.2.2 of training the Attention becomes gradually more active and allows for greater performance. A software routine of this fusion for the residual unit, in particular, can be seen in Listing 4 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Relative attention fusion</head><p>Assuming we have two input layers, L 1 , L 2 , we can calculate the relative attention of each with respect to the other. This is achieved by using as query the layer we want to "attend to" and as a key and value the layer we want to use as information for attention. In practical implementations, the query, the key, and the value layers result in after the application of a convolution layer to some input.</p><formula xml:id="formula_14">F 1 = L 1 1 + γ 1 A 122 (q(L 1 ), k (L 2 ) , v(L 2 )) (16) F 2 = L 2 [1 + γ 2 A 211 (q(L 2 ), k (L 1 ) , v(L 1 ))] (17) F = Conv2DN (concat([F 1 , F 2 ]))<label>(18)</label></formula><p>Here, the γ 1,2 parameters are initialized at zero, and the concatenation operations are performed along the channel dimension. Conv2DN is a two dimensional convolution operation followed by a normalization layer, e.g. BatchNorm <ref type="bibr" target="#b24">(Ioffe and Szegedy, 2015)</ref>). An implementation of this process in mxnet/gluon pseudocode style can be found in Listing 3. The relative attention fusion presented here can be used as a direct replacement of concatenation followed by a convolution layer in any network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Architecture</head><p>We break down the network architecture into three component parts: the micro-topology of the building blocks, which represents the fundamental constituents of the architecture; the macro-topology of the network, which describes how building blocks are connected to one another to maximize performance; and the multitasking head, which is responsible for transforming the features produced by the micro and macro-topologies into the final prediction layers where change is identified. Each of the choices of micro and macro topology has a different impact on the GPU memory footprint. Usually, selecting very deep macro-topology improves performance, but then this increases the overall memory footprint and does not leave enough space for using an adequate number of filters (channels) in each micro-topology. There is obviously a trade off between the micro-topology feature extraction capacity and overall network depth. Guided by this, we seek to maximize the feature expression capacity of the micro-topology for a given number of filters, perhaps at the expense of consuming computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Micro-topology: the CEECNet unit</head><p>The basic intuition behind the construction of the CEEC building block, is that it provides two different, yet complementary, views for the same input. The first view (the CE block -see <ref type="figure" target="#fig_3">Fig. 5</ref>) is a "summary understanding" operation (performed in lower resolution than the input -see also <ref type="bibr" target="#b40">Newell et al. 2016;</ref><ref type="bibr" target="#b33">Liu et al. 2020 and</ref><ref type="bibr" target="#b42">Qin et al. 2020)</ref>. The second view (the EC block) is an "analysis of detail" operation (performed in higher spatial resolution than the input). It then exchanges information between these two views using relative attention, and it finally fuses them together, by emphasizing the most important parts using the FracTAL .</p><p>Our hypothesis and motivation for this approach is quite similar to the scale-space analysis in computer vision (Lindeberg, 1994): viewing input features at different scales, allows the algorithm to focus on different aspects of the inputs, and thus perform more efficiently. The fact that by merely increasing the resolution of an image does not increase its content information is not relevant here: guided by the loss function the algorithm can learn to represent at higher resolution features that otherwise would not be possible in lower resolutions. We know this from the successful application of convolutional networks in super-resolution problems  as well as (variational) autoencoders <ref type="bibr" target="#b49">(Tschannen et al., 2018;</ref><ref type="bibr" target="#b30">Kingma and Welling, 2019)</ref>: in both of these paradigms deep learning approaches manage to increase meaningfully the resolution of features that exists in lower spatial dimension layers.</p><p>In the following we define the volume V of features of dimension (C, H, W) 11 , as the product of the number of their channels (or filters), C (or n f ), with their spatial dimensions, height, HEAD (segm, bound, dist) <ref type="figure">Figure 6</ref>: The mantis CEECNetV1 architecture for the task of change detection. The Fusion operation (FUSE) is described with mxnet/gluon style pseudocode in detail on Listing 3.</p><p>H, and width, W, i.e. V = n f ·H ·W 12 . The two branches consist of: a "mini ∪-Net" operation (CE block), that is responsible for summarizing information from the input features by first compressing the total volume of features into half its original size and then restoring it. The second branch, a "mini ∩-Net" operation (EC block), is responsible for analyzing in higher detail the input features: it initially doubles the volume of the input features, by halving the number of features and doubling each spatial dimension. It subsequently compresses this expanded volume to its original size. The input to both layers is concatenated with the output, and then a normed convolution restores the number of channels to their original input value. Note that the mini ∩-Net is nothing more than the symmetric (or dual) operation of the mini ∪-Net.</p><p>The outputs of the EC and CE blocks are fused together with relative attention fusion (section 5.2.2). In this way, exchange of information between the layers is encouraged. The final emphasized outputs are concatened together, thus restoring the initial number of filters, and the produced layer is passed through a normed convolution in order to bind the relative channels. The operation is concluded with a FracTAL residual operation and fusion (similar to <ref type="figure" target="#fig_2">Fig. 4)</ref>, where the input is added to the final output and emphasized by the self attention on the original input. The CEECNet building block is described schemati-cally in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>The compression operation, C, is achieved by applying a normed convolution layer of stride equal to 2 (k=3, p=1, s=2) followed by another convolution layer that is identical in every aspect except the stride that is now s=1. The purpose of the first convolution is to both resize the layer and extract features. The purpose of the second convolution layer is to extract features. The expansion operation, E, is achieved by first resizing the spatial dimensions of the input layer using Bilinear interpolation, and then the number of channels is brought to the desired size by the application of a convolution layer (k=3,p=1,s=1). Another identical convolution layer is applied to extract further features. The full details of the convolution operations used in the EC and CE blocks can be found on Listing 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Macro-topology: dual encoder, symmetric decoder</head><p>In this section we present the macro-topology (i.e. backbone) of the architecture that uses as building blocks either the CEECNet or the FracTAL ResNet units. We start by stating the intuition behind our choices and continue with a detailed description of the macro-topology. Our architecture is heavily influenced from the ResUNet-a model . We will refer to this macro-topology as the mantis topology 13 .</p><p>In designing this backbone, a key question we tried to address is how can we facilitate exchange of information between features extracted from images at different dates. The following two observations guided us:</p><p>1. We make the hypothesis that the process of change detection between two images requires a mechanism similar to human attention. We base this hypothesis on the fact that the time required for identifying objects that changed in an image correlates directly with the number of changed objects. That is, the more objects a human needs to identify between two pictures, the more time is required. This is in accordance with the feature-integration theory of Attention <ref type="bibr" target="#b48">(Treisman and Gelade, 1980)</ref>. In contrast, subtracting features extracted from two different input images is a process that is constant in time, independent of the complexity of the changed features. Therefore, we avoid using adhoc feature subtraction in all parts of the network. 2. In order to identify change, a human needs to look and compare two images multiple times, back and forth. We need things to emphasize on image at date 1, based on information on image at date 2 (Eq. 16), and, vice versa (Eq. 17). And then combine both of these information together (Eq. 18). That is, exchange information, with relative attention (section 5.2.2) between the two, at multiple levels. A different way of stating this as a question is: what is important on input image 1 based on information that exists on image 2, and vice versa?</p><p>Given the above, we now proceed in detailing the mantis macrotopology (with CEECNetV1 building blocks, see <ref type="figure">Fig. 6</ref>). The encoder part is a series of building blocks, where the size of the features is downscaled between the application of each subsequent building block. Downscaling is achieved with a normed convolution with stride, s=2 without using activations. There exist two encoder branches that share identical parameters in their convolution layers. The input to each branch is an image from a different date and the role of the encoder is to extract features at different levels from each input image. During the feature extraction by each branch, each of the two inputs is treated as an independent entity. At successive depths, the outputs of the corresponding building block are fused together with the relative attention methodology as described in section 5.2.2, but they are not used until later, in the decoder part. Crucially, this fusion operation, suggests to the network that the important parts of the first layer, will be defined by what exists on the second layer (and vice versa), but it does not dictate how exactly the network should compare the extracted features (e.g. by demanding the features to be similar for unchanged areas, and maximally different for changed areas 14 ). This is something that the network will have to discover in order to match its predictions with the ground truth. Finally, the last encoder layers are concatenated and inserted to the pyramid scene pooling layer (PSPPooling - <ref type="bibr" target="#b13">Diakogiannis et al. 2020;</ref><ref type="bibr" target="#b59">Zhao et al. 2017</ref>).</p><p>In the (single) decoder part is where the network extracts features based on the relative information that exist in the two inputs. Starting from the output of the PSPPooling layer (middle of network), we upscale lower resolution features with bilinear interpolation and combine them with the fused outputs of the decoder with a concatenation operation followed by a normed convolution layer, in a way similar to the ResUNet-a  model. The mantis CEECNetV2 model replaces all concatenation operations followed by a normed convolution, with a Fusion operation as described in Listing 3.</p><p>The final features extracted from this macro-topology architecture is the final layer from the CEECNet unit that has the same spatial dimensions as the first input layers, as well as the Fused layers from the first CEECNet unit operation. Both of these layers are inserted in the segmentation HEAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Segmentation HEAD</head><p>The features extracted from the features extractor ( <ref type="figure">Fig. 6</ref>) are inserted to a conditioned multitasking segmentation head <ref type="figure" target="#fig_4">(Fig. 7)</ref> that produces three layers: a segmentation mask, a boundary mask and a distance transform mask. This is identical with the ResUNet-a "causal" segmentation head, that has shown great performance in a variety of segmentation tasks <ref type="bibr" target="#b52">Waldner and Diakogiannis, 2020)</ref>, with two modifications.</p><p>The first modification relates to the evaluation of boundaries: instead of using a standard sigmoid activation for the <ref type="bibr">14</ref> We tried this approach and it was not successful. Here, features 1 and 2 are the outputs of the mantis CEECNet features extractor. The symbol represents concatenation along the channels dimension. The algorithm first predicts the distance transform of the classes (regression), then re-uses this information to estimate the boundaries and finally both of these predictions are re-used for the change prediction layer. Here, Chng Segm stands for change segmentation layer, and mtsk for multitasking predictions.</p><p>boundaries layer, we are inserting a scaling parameter, γ, that controls how sharp the transition from 0 to 1 takes place, i.e.</p><formula xml:id="formula_15">sigmoid crisp (x) = sigmoid(x/γ), γ ∈ [ , 1]<label>(19)</label></formula><p>Here = 10 −2 is a smoothing parameter. The γ coefficient is learned during training. We inserted this scaling after noticing in initial experiments that the algorithm needed improvement close to the boundaries of objects. In other words, the algorithm was having difficulty separating nearby pixels. Numerically, we anticipate that the distance between the values of activations of neighbouring pixels is small, due to the patch-wise nature of convolutions. Therefore, a remedy to this problem is making the transition boundary sharper. We initialize training with γ = 1.</p><p>The second modification to the segmentation HEAD relates to balancing the number of channels of the boundaries and distance transform predictions before re-using them in the final prediction of segmentation change detection. This is achieved by passing them through a convolution layer that brings the number of channels to the desired number. Balancing the number of channels treats the input features and the intermediate predictions as equal contributions to the final output. In <ref type="figure" target="#fig_4">Fig.  7</ref> we present schematically the conditioned multitasking head, and the various dependencies between layers. Interested users can refer to <ref type="bibr" target="#b13">Diakogiannis et al. (2020)</ref> for details of the conditioned multitasking head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Design</head><p>In this section, we describe the setup of our experiments for the evaluation of the proposed algorithms on the task of change detection. We start by describing the two datasets we used (LEVIRCD Chen and Shi 2020 and WHU <ref type="bibr" target="#b26">Ji et al. 2019b)</ref> as well as the data augmentation methodology we followed. Then we proceed in describing the metrics used for performance evaluation and the inference methodology. All models mantis CEECNetV1, V2 and mantis FracTAL ResNet have an initial number of filters equal to nf=32, and the depth of the encoder branches was equal to 6. We designate these models with D6nf32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">LEVIRCD Dataset</head><p>The LEVIR-CD change detection dataset  consists of 637 pairs of VHR aerial images of resolution 0.5m per pixel. It covers various types of buildings, such as villa residences, small garages, apartments, and warehouses. It contains 31,333 individual building changes. The authors provide a train/validation/test split, which standardizes the performance process. We used a different split for training and validation, however, we used the test set the authors provide for reporting performance. For each tile from the training and validation set, we used ∼47% of the area for training and the remaining ∼53% for validation. For a rectangle area with sides of length a and b, this is achieved by using as training area the rectangle with sides a = 0.6838 a and b = 0.6838 b, i.e. training area = 0.6838 2 ab ≈ 0.47 ab. Then val area = 1. − train area ≈ 0.53total area. From each of these areas, we extracted chips of size 256×256. These are overlapping in each dimension with stride equal to 256/2 = 128 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">WHU Building Change Detection</head><p>The WHU building change dataset <ref type="bibr" target="#b26">(Ji et al., 2019b)</ref> consists of two aerial images (2011 and 2016) that cover an area of ∼ 20km 2 , which was changed from 2011 (earthquake) to 2016. The images resolution is 0.3m spatial resolution. The dataset contains 12796 buildings. We split the triplets of images and ground truth change labels, in three areas with ratio 70% for training and validation and 30% for testing. We further split the 70% part in ∼47% area for training and ∼53% area for validation, in a way similar to the split we followed for each tile of the LEVIRCD dataset. The splitting can be seen in <ref type="figure" target="#fig_5">Fig. 8</ref>. Note that the training area is spatially separated from the test area (the validation area is in between the two). The reason for the rather large train/validation ratio is for us to ensure there is adequate spatial separation between training and test areas, thus minimize spatial correlation effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Data preprocessing and augmentation</head><p>We split the original tiles in training chips of size F 2 = 256 2 by using a sliding window methodology with stride s = F/2 = 128 pixels (the chips are overlapping in half the size of the sliding window). This is the maximum size we can fit to our architecture due to GPU memory limitations that we had at our Finally, the cyan rectangle (dashed) is the test data. The reasoning for our split is to include in the validation data both industrial and residential areas and isolate (spatially) the training area from the test area in order to avoid spurious spatial correlation between training/test sites. The train/validation/test ratio split is train:val:test ≈ 33 : 37 : 30. disposal (NVIDIA P100 16GB). With this batch size we managed to fit a batch size of 3 per GPU for each of the architectures we trained. Due to the small batch size, we used GroupNorm <ref type="bibr" target="#b56">(Wu and He, 2018)</ref> for all normalisation layers.</p><p>The data augmentation methodology we used during training our network was the one used for semantic segmentation tasks as described in <ref type="bibr" target="#b13">Diakogiannis et al. (2020)</ref>. That is, random rotations with respect to a random center with a (random) zoom in/out operation. We also implemented random brightness and random polygon shadows. In order to help the algorithm explicitly on the task of the change detection, we implemented time reversal (reversing the order of the input images should not affect the binary change mask) and random identity (we randomly gave as input one of the two images, i.e. null change mask). These latter transformations were implemented at a rate of 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Metrics</head><p>In this section, we present the metrics we used for quantifying the performance of our algorithms. With the exception of the Intersection over Union (IoU) metric, for the evaluation of all other metrics we used the Python library pycm as described in <ref type="bibr" target="#b17">Haghighi et al. (2018)</ref>. The statistical measures we used in order to evaluate the performance of our modelling approach are pixel-wise precision, recall, F1 score, Matthews Correlation Coefficient (MCC) <ref type="bibr" target="#b38">(Matthews, 1975)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Inference</head><p>In this section, we provide a brief description of the model selection after training (i.e. which epochs will perform best on the test set) as well as the inference methodology we followed for large raster images that exceed the memory capacity of modern-day GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1.">Inference on large rasters</head><p>Our approach is identical to the one used in <ref type="bibr" target="#b13">Diakogiannis et al. (2020)</ref>, with the difference that now we are processing two input images. Interested readers that want to know the full details can refer to Section 3.4 of <ref type="bibr" target="#b13">Diakogiannis et al. (2020)</ref>.</p><p>During inference on test images, we extract multiple overlapping windows of size 256 × 256 with a step (stride) size of 256/4 = 64 pixels. The final prediction "probability", per pixel, is evaluated as the average "probability" over all inference windows that overlap on the given pixel. In this definition, we refer  to "probability" as the output of the softmax final classification layer, which is a continuous value in the range [0, 1]. It is not a true probability, in the statistical sense, however, it does express the confidence of the algorithm in obtaining the inference result.</p><p>With this overlapping approach, we make sure that the pixels that are closer to the edges and correspond to boundary areas for some inference windows, appear closer to the center area of subsequent inference windows. For the boundary pixels of the large raster, we apply reflect padding before performing inference <ref type="bibr" target="#b43">(Ronneberger et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2.">Model selection using Pareto efficiency</head><p>For monitoring the performance of our modelling approach, we usually rely on the MCC metric on the validation dataset. We observed, however, that when we perform simultaneously learning rate reduction and F T d depth increase, initially the MCC decreases (indicating performance drop), while the F T d similarity is (initially) strictly increasing. After training starts to stabilize around some optimality region (with the standard noise oscillations), there are various cases where the MCC metric and F T d similarity coefficient do not agree on which is the best model. To account for this effect and avoid losing good candidate solutions, we evaluate the average of the inference output of a set of best candidate models. These best candidate models are selected according to the models that belong to the Pareto front of the most evolved solutions. We use all the Pareto front <ref type="bibr" target="#b14">(Emmerich and Deutz, 2018)</ref> model weights as acceptable solutions for inference. A similar approach was followed for the selection of hyper parameters for optimal solutions in <ref type="bibr" target="#b52">Waldner and Diakogiannis (2020)</ref>.</p><p>In <ref type="figure" target="#fig_7">Fig. 9</ref> we plot on the top panel the evolution of the MCC, and F T d for d = 30. Clearly, these two performance metrics do not always agree. For example, the F T 30 is close to optimality in approximate epoch ∼250, while the MCC is clearly suboptimal. We highlight with filled circles (cyan dots) the two solutions that belong to the pareto front. In the bottom panel we plot the correspondence of the MCC values with the F T 30 similarity metric. The two circles show the corresponding nondominated Pareto solutions (i.e. best candidates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">FracTAL units and evolving loss ablation study</head><p>In this section we present the performance of the FracTAL ResNet <ref type="bibr" target="#b19">(He et al., 2015</ref><ref type="bibr" target="#b20">(He et al., , 2016</ref> and CEECNet units we introduced against ResNet and CBAM <ref type="bibr" target="#b55">(Woo et al., 2018)</ref> baselines as well as the effect of the evolving F T d loss function on training a neural network. We also present a qualitative and quantitative analysis on the effect of the depth parameter in the FracTAL based on the mantis FracTAL ResNet network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">FracTAL building blocks performance</head><p>We construct three identical networks in macro-topological graph (backbone), but different in micro-topology (building blocks). The first two networks are equipped with two different versions of CEECNet: the first is identical with the one presented in <ref type="figure" target="#fig_3">Fig. 5</ref>. The second is similar to the one in <ref type="figure" target="#fig_3">Fig. 5</ref> with all concatenation operations that are followed by normed convolutions being replaced with Fusion operations, as described in Listing 3. The third network uses as building blocks the FracTAL ResNet building blocks <ref type="figure" target="#fig_2">(Fig. 4)</ref>. Finally, the fourth network uses as building blocks standard residual units as described in <ref type="bibr" target="#b19">He et al. (2015</ref><ref type="bibr" target="#b20">He et al. ( , 2016</ref>) (ResNet V2). All building blocks have the same dimensionality of input and output features. However, each type of building block has a different number of parameters. By keeping the dimensionality of input and output layers identical to all layers, we believe, the performance differences of the networks will reflect the feature expression capabilities of the building blocks we compare.</p><p>In <ref type="figure">Fig. 10</ref> we plot the validation loss for 300 epochs of training on CIFAR10 dataset <ref type="bibr" target="#b31">(Krizhevsky, 2009</ref>) without learning rate reduction, We use cross entropy loss and Adam optimizer <ref type="bibr" target="#b29">(Kingma and Ba, 2014)</ref>. The backbone of each of the networks is described in <ref type="table">Table A</ref>.3. It can be seen that the convergence and performance of all building blocks equipped with the FracTAL outperform standard Residual units. In particular we find that the performance and convergence properties of the networks follow: ResNet FracTAL ResNet CEECNetV1</p><p>CEECNetV2. The performance difference between FracTAL ResNet and CEECNetV1 will become more clearly apparent in the change detection datasets. The V2 version of CEECNet that uses Fusion with relative attention (cyan solid line) instead of concatenation (V1 -magenta dashed line), for combining layers in the Compress-Expand and Expand-Compress branches,  has superiority over V1. However, it is a computationally more intensive unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Comparing FracTAL with CBAM</head><p>Having shown the performance improvement over the residual unit, we proceed in comparing the FracTAL proposed attention with a modern attention module, and in particular the Convolution Block Attention Module (CBAM) <ref type="bibr" target="#b55">(Woo et al., 2018)</ref>. We construct two networks that are identical in all aspects except the implementation of the attention used. We base our implementation on a publicly available repository that reproduces the results of <ref type="bibr" target="#b55">Woo et al. (2018)</ref> -written in Pytorch 15 -that we translated into the mxnet framework. From this implementation we use the CBAM-resnet34 model and we compare it with a FracTAL-resnet34 model, i.e. a model which is identical to the previous one, with the exception that we replaced the CBAM attention with the FracTAL (attention). Our results can be seen on <ref type="figure">Fig. 11</ref>, where a clear performance improvement is evident merely by changing the attention layer used. The improvement is of the order of 1%, from 83.37% (CBAM) to 84.20% (FracTAL), suggesting that the FracTAL has better feature extraction capacity than the CBAM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Evolving loss</head><p>We continue by presenting experimental results on the performance of the evolving loss strategy on CIFAR10 using two networks, one with standard ResNet building blocks and one with CEECNetV1 units. The macro topology of the networks is identical to the one in <ref type="table">Table A.</ref>3. In addition, we also demonstrate performance differences on the change detection task, by training the mantis CEECNetV1 model on the LEVIRCD dataset, with static and evolving loss strategies for FracTAL depth, d = 5.</p><p>In <ref type="figure" target="#fig_0">Fig. 12</ref> we demonstrate the effect of this approach: we train the network on CIFAR10 with standard residual blocks (top panel <ref type="bibr" target="#b20">He et al., 2016</ref><ref type="bibr" target="#b19">He et al., , 2015</ref> under the two different loss strategies. In both strategies, we reduce the initial learning rate by a factor of 10 at epochs 250 and 350. In the first strategy, we train the networks with F T 0 . In the second strategy, we evolve the depth of the fractal Tanimoto loss function: we start training with F T 0 and on the two subsequent learning rate reductions we use F T 15 and F T 30 . In the top panel, we plot the validation accuracy for the two strategies. The performance gain following the evolving depth loss is ∼ 0.25% in validation accuracy. In the bottom panel we plot the validation accuracy for the CEECNetV1 based models. Here, the evolution strategy is same as above with the difference that we use different depths for the F T loss (to observe potential differences). These are d ∈ {0, 10, 20}. Again, the difference in the validation accuracy is ∼ +0.22% for the evolving loss strategy.</p><p>We should note that we observed performance degradation by using for training (from random weights) the F T d loss for d &gt; 1. This is evident in <ref type="figure" target="#fig_3">Fig. 15</ref> where we train from scratch on CIFAR10 three identical models with different depth for the F T d <ref type="figure" target="#fig_1">function: d = [0, 3, 6]</ref>. It is seen that as the hyperparameter d increases, the performance of the validation accuracy degrades. We consider that this happens due to the low value of the gradients away from optimality, as it requires the network to train longer to reach the same level of validation accuracy. In contrast, the greatest benefit we observed by using this training strategy is that the network can avoid overfitting after learning rate reduction (provided that the slope created by the choice of depth d is significant) and has the potential to reach higher performance.</p><p>Next we perform a test on evolving vs static loss strategy on the LEVIR CD change detection dataset, using the CEECNetV1 units, as it can be seen in <ref type="table">Table 1</ref>. The CEECNetV1 unit, trained with the evolving loss strategy, demonstrates +0.856% performance increase on the Interesection over Union (IoU) and +0.484% increase in MCC. Note that, for the same FracTAL depth, d = 5, the FracTAL ResNet network, trained with the evolving loss strategy performs better than the CEECNetV1 that is trained with the static loss strategy, while it falls behind the CEECNetV1 trained with the evolving loss strategy. We should also note that performance increment is larger, in comparison with the classification task on CIFAR10, reaching almost ∼1% for the IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Performance dependence on FracTAL depth</head><p>In order to understand how the FracTAL layer behaves with respect to different depths, we train three identical networks, the mantis FracTAL ResNet (D6nf32), using FracTAL depths in the range d ∈ {0, 5, 10}. The performance results on the LEVIRCD dataset can be seen on <ref type="table">Table 1</ref>. It seems the three networks perform similarly (they all achieve SOTA performance on the LEVIRCD dataset), with the d = 10 having top performance (+0.724% IoU), followed by the d = 0 (+0.332%IoU) and, lastly the d = 5 network (baseline). We conclude that the depth d is a hyper parameter dependent on the problem at task that users of our method can choose to optimize against. Given that all models have competitive performance, it seems also that the proposed depth d = 5 is a sensible choice.</p><p>In <ref type="figure" target="#fig_1">Fig. 13</ref> we visualize the features of the last convolution, before the multitasking segmentation head for FracTAL depth d = 0 (left panel) and d = 10 (right panel). The features at different depths appear similar, all identifying the regions of interest clearly. To the human eye, according to our opinion, the features for depth d = 10 appear slightly more refined in comparison with the features corresponding to depth d = 0 (e.g. by comparing the images in the corresponding bottom rows). The entropy of the features for d = 0 (entropy: 15.9982) is negligibly higher (+0.00625 %) than for the case d = 10 (entropy: 15.9972), suggesting both features have the same information content for these two models. We note that, from the perspective of information compression (assuming no loss of informa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Results</head><p>In this section, we report the quantitative and qualitative performance of the models we developed for the task of change detection on the LEVIRCD  and WHU <ref type="bibr" target="#b26">(Ji et al., 2019b)</ref> datasets. All of the inference visualizations are performed with models having the proposed FracTAL depth d = 5, although this is not always the best performant network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Performance on LEVIRCD</head><p>For this particular dataset, a fixed test set is provided and a comparison with methods that other authors followed is possible. Both FracTAL ResNet and CEECNet (V1, V2) outperform the baseline  with respect to the F1 score by ∼5%.</p><p>In <ref type="figure" target="#fig_2">Fig. 14</ref> we present the inference of the CEECNet V1 algorithm for various images from the test set. For each row, from left to right we have input image at date 1, input image at date 2, ground truth mask, inference (threshold = 0.5), and algorithm's confidence 16 heat map. It is interesting to note that the algorithm has zero doubt in areas where buildings exist in both input images. That is, it is clear our algorithm identifies change in areas covered by buildings, and not building footprints. In <ref type="table">Table  1</ref> we present numerical performance results of both FracTAL ResNet as well as CEECNet V1&amp; V2. All metrics, precision, recall, F1, MCC and IoU are excellent. The mantis CEECNet for FracTAL depth d = 5, outperforms the mantis FracTAL ResNet by a small numerical margin, however the difference is clear. This difference can also be seen in the bottom panel of <ref type="figure" target="#fig_4">Fig. 17</ref>. We should also note that the numerical difference <ref type="bibr">16</ref> This should not be confused with statistical confidence. on, say, F1 score, does not translate to equal portions of quality difference in images. That is, a 1% difference in F1 score, may have a significant impact on the quality of inference. We further discuss this on Section 9.4. Overall the best model is mantis CEECNet V2 with FracTAL depth d = 5. Second best is the mantis FracTAL ResNet with FracTAL depth d = 10. Among the same set of models (mantis FracTAL ResNet), it seems that depth d = 10 performs best, however we do not know if this generalizes to all models and datasets. We consider that FracTAL depth d is a hyperparameter that needs to be finetuned for optimal performance, and, as we've shown, the choice d = 5 is a sensible one as in this particular dataset it provided us with state of the art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Performance on WHU</head><p>In <ref type="table" target="#tab_3">Table 2</ref> we present the results of training the mantis network with FracTAL ResNet and CEECNetV1 building blocks. Both of our proposed architectures outperform all other modeling frameworks, although we need to stress that each of the other authors followed a different splitting strategy of the data. However, with our splitting strategy, we used only the 32.9% of the total area for training. This is significantly less than the majority of all other methods we report here, and we should anticipate a significant performance degradation in comparison with other methods. In contrast, despite the relatively smaller training set, our method outperforms other approaches. In particular, <ref type="bibr" target="#b25">Ji et al. (2019a)</ref>, used 50% of the raster for training, and the other half for testing ( <ref type="figure">Fig. 10 in their manuscript)</ref>. In addition, there is no spatial separation between training and test sites, as it exists in our case, and this should work in their advantage. Also, the usage of a larger window for training (their extracted chips are of spatial dimension 512 × 512) increases in principle the performance because it includes more context information. There is a tradeoff here though, in that using a larger window size reduces the number of available training chips, therefore the model sees a smaller number of chips during training. Chen . For each row, from left to right input image date 1, input image date 2, ground truth, change prediction (threshold 0.5) and confidence heat map. <ref type="table">Table 1</ref>: Model comparison on the LEVIR building change detection dataset. We designate with bold font the best values, with underline the second best, and with square brackets, [ ] the third best model. All of our frameworks (D6nf32) use the mantis macro-topology and achieve state of the art performance. Here evo represents evolving loss strategy, sta, static loss strategy and the depth d refers to the F T similarity metric of the FracTAL (attention) layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Precision Recall F1 MCC IoU <ref type="bibr" target="#b6">Chen and Shi (2020)</ref> 83.80 91.00 87.30 --   This should improve performance, because there is a tight spatial correlation for two extracted chips that are in geospatial proximity. <ref type="bibr" target="#b4">Cao et al. (2020)</ref> used as a test set ∼ 20% of the total area of the WHU dataset, however, they do not specify the splitting strategy they followed for the training and validation sets. Finally, <ref type="bibr" target="#b34">Liu et al. (2019)</ref> used approximately ∼ 10% of the total area for reporting test score performance. They also do not mention their splitting strategy.</p><formula xml:id="formula_16">CEECNetV1 (d = 5,</formula><p>In this table we could not include <ref type="bibr">(Jiang et al., 2020, PGA-SiamNet)</ref>   <ref type="figure">Fig. B</ref>.23 we plot from left to right, the test area on date 1, the test area on date 2, the ground truth mask, and the confidence heat map of these predictions. In <ref type="figure" target="#fig_5">Fig. 18</ref> we plot a set of examples of inference on the WHU dataset. The correspondence of the images in each row is identical to <ref type="figure" target="#fig_2">Fig. 14,</ref> with the addition that we denote with blue rectangles the locations of changed buildings (true positive predictions), and with red squares missed changes from our model (false negative). It can be seen that the most difficult areas are the ones that are heavily populated/heavily built up, and the changes are small area buildings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">The effect of scaled sigmoid on the segmentation HEAD</head><p>Starting from an initial value γ = 1 of the scaled sigmoid boundary layer, the fully trained model mantis CEECNetV1 learns the following parameters that control how "crisp" the boundaries should be, or else, how sharp the decision boundary should be:</p><formula xml:id="formula_17">γ LVR sigmoid = 0.610 γ WHU sigmoid = 0.625</formula><p>The deviation of these coefficients from their initial values, demonstrates that indeed the network finds useful to modify the decision boundary. In <ref type="figure">Fig. 16</ref> we plot the standard sigmoid function (γ = 1) and the sigmoid functions recovered after training on the LEVIRCD and WHU datasets. The algorithm in both cases learns to modify the decision boundary, by making it sharper. This means that for two nearby pixels, one belonging to a boundary, the other to a background class, the numerical distance between them needs to be smaller to achieve class separation, in comparison with standard sigmoid. Or else, a small δx change is sufficient to transition between boundary and no-boundary class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">Qualitative CEECNet and FracTAL performance</head><p>In this section we base our comparison on CEECNet V1 and FracTAL ResNet models with FracTAL depth d = 5. Although both CEECNet V1 and FracTAL ResNet achieve a very high MCC <ref type="figure" target="#fig_4">(Fig. 17)</ref>, the superiority of CEECNet, for the same FracTAL depth d = 5, is evident in the inference maps in both the LEVIRCD <ref type="figure" target="#fig_0">(Fig. 20)</ref> and WHU <ref type="figure" target="#fig_0">(Fig. 21)</ref> datasets. This confirms their relative scores (Tables 1 and 2) and the faster convergence of CEECNet V1 <ref type="figure">(Fig 10)</ref>. Interestingly, CEECNet V1 predicts change with more confidence than FracTAL ResNet <ref type="figure" target="#fig_0">(Figures 20 and 21)</ref>, even when it errs, as can be seen from the corresponding confidence heat maps. The decision on which of the models one should use is a decision to be made with respect to the relative "cost" of training each model, available hardware resources and performance target goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">Qualitative assesment of the mantis macro-topology</head><p>A key ingredient of our approach on the task of change detection is that we emphasize on the importance of avoiding using the difference of features to identify change. Instead, we propose the exchange of information between features extracted from images at different dates with the concept of relative attention (section 6.2) and fusion (Listing 3). In this section our aim is to get insight on the behaviour of the relative attention and fusion layers, and compare them with the features obtained by the difference of the outputs of convolution layers of images at different dates. We use the outputs of layers of a trained mantis FracTAL ResNet model, trained on LEVIRCD with FracTAL depth d = 10.</p><p>In <ref type="figure" target="#fig_7">Fig. 19</ref> we visualize the features of the first relative attention layers (channels=32, spatial size 256 × 256, ratt12 (left pannel) and ratt21 (right pannel) for a set of image patches belonging to the test set (size: 3 × 256 × 256). Here, the notation ratt12 indicates that the query features come from the input image at date t 1 , while the key/value features are extracted from the input image at date t 2 . Similar notation is applied for the relative attention, ratt21. Starting from the top left corner we provide the input image at date t 1 , the input image at date t 2 and the ground truth mask of change and after that we visualize the features as single channel images. Each feature (i.e. image per channel) is normalized in the range [−1, 1] for visualization purposes. It can be seen that the algorithm emphasizes from the early stages (i.e. first layers) to structures containing buildings and boundaries of these. In particular the ratt12 (left pannel) has emphasis on boundaries of buildings that exist on both images. It also seems to represent all buildings that exist in both images. The ratt21 layer (right pannel) seems to emphasize more the buildings that exist on date 1, but not on date 2. In addition, in both relative attention layers, emphasis is given on roads and pavements.</p><p>In <ref type="figure" target="#fig_0">Fig. 22</ref> we visualize the difference of features of the first convolution layers (channels=32, spatial size 256 × 256 -left  <ref type="figure" target="#fig_7">Figure 19</ref>: Visualization of the relative attention units, ratt12 (left pannel) and ratt21 (right pannel), for the mantis FracTAL ResNet with FracTAL depth, d = 10. These come from the first feature extractors (channels=32, filter spatial size 256 × 256). Here, ratt12 is the relative attention where for query we use input at date t 1 , and the key/value filters are created from input at date t 2 . In the top left rows for each pannel we have input image at date t 1 , input image at date t 2 , and ground truth building change labels, followed by the visualization of each of the 32 channels of the features. pannel) and the fused features (right pannel) obtained using the relative attention and fusion methodology (Listing 3). Some key differences between the two is that we observe that there is less variability within channels in the output of the fusion layer, in comparison with the difference of features. In order to quantify the information content of the features, we calculated the Shanon entropy of the features for each case and we found that the fusion features have half the entropy (11.027) in comparison with the entropy of the difference features (20.97). Similar entropy ratio was found for all images belonging to the test set. This means that the fusion features are less "surprising", than the difference features. This may suggest that the fusion provides a better compression of information in comparison with the difference of layers, assuming both layers have the same information content. It may also mean that the fusion layers have less information content than the difference features, i.e. they are harmful for the change detection process. However, if this was the case, our approach would fail to achieve state of the art performance on the change detection datasets. Therefore, we conclude that the lower entropy value translates to better encoding of information, in comparison with the difference of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusions</head><p>In this work, we propose a new deep learning framework for the task of semantic change detection on very high resolution aerial images, presented here for the case of changes in buildings. This framework is built on top of several novel contributions that can be used independently in computer vision tasks. Our contributions are: 1. A novel set similarity coefficient, the fractal Tanimoto coefficient, that is derived from a variant of the Dice coefficient. This coefficient can provide finer detail of similarity, at a desired level (up to a delta function), and this is regulated by a temperature-like hyper-parameter, d <ref type="figure" target="#fig_0">(Fig.  2)</ref>. 2. A novel training loss scheme, where we use an evolving loss function, that changes according to learning rate reductions. This helps avoid overfitting and allows for a small increase in performance <ref type="figure" target="#fig_0">(Figures 12 &amp; 15</ref>). In particular, this scheme provided ∼0.25% performance increase in validation accuracy on CIFAR10 tests, and performance increase of ∼0.9% on IoU and ∼0.5% on MCC on the LEVIRCD dataset. 3. A novel spatial and channel attention layer, the fractal Tanimoto Attention Layer (FracTAL -see Listing 2), that uses the fractal Tanimoto similarity coefficient as a means of quantifying the similarity between query and key entries. This layer is memory efficient and scales well with the size of input features. 4. A novel building block, the FracTAL ResNet <ref type="figure" target="#fig_2">(Fig 4)</ref>, that has a small memory footprint and excellent convergent and performance properties that outperform standard ResNet building blocks. 5. A novel building block, the Compress/Expand -Expand/-Compress (CEECNet) unit <ref type="figure" target="#fig_3">(Fig. 5</ref>), that has better performance than the FracTAL ResNet <ref type="figure" target="#fig_4">(Fig. 17)</ref>, that comes, however, at a higher computational cost. 6. A corollary that follows from the introduced building blocks, is a novel fusion methodology of layers and their corresponding attentions, both for self and relative attention, that improves performance <ref type="figure" target="#fig_4">(Fig. 17</ref>). This methodology can be used as a direct replacement of concatenation in convolution neural networks. 7. A novel macro-topology (backbone) architecture, the mantis topology <ref type="figure">(Fig. 6)</ref>, that combines the building blocks we developed and is able to consume images from two different dates and produce a single change detection layer. It should be noted that the same topology can be used in   <ref type="figure" target="#fig_0">Figure 22</ref>: For the same model as in <ref type="figure" target="#fig_7">Fig. 19</ref> we plot the difference of the first feature extractor blocks (left pannel) vs the first Fusion feature extraction block. The entropy of the fusion features is half that of the difference channels. This means there is less "surprise" in the fusion filters, in comparison with the difference of filters, for the same trained network.</p><p>general segmentation problems, where we have two input images to a network that are somehow correlated and produce a semantic map. That is, it can be used for fusion of features coming from different inputs (e.g. Digital Surface Maps and RGB images).</p><p>Putting all things together, all of the proposed networks that presented in this contribution, mantis FracTAL ResNet and mantis CEECNetV1&amp;V2, outperform other proposed networks and achieve state of the art results on the LEVIRCD  and the WHU 17 <ref type="bibr" target="#b26">(Ji et al., 2019b)</ref> building change detection datasets <ref type="table" target="#tab_3">(Tables 1 &amp; 2</ref>). In comparison with state of the art architectures that use atrous dilated convolutions, the proposed architectures do not require fine tuning of the dilation rates. Therefore, they are simpler, and easier to set up and train. In this work we did not experiment with deeper architectures, that would surely improve performance (e.g. D7nf32 models usually perform better), or with hyper parameter tuning. <ref type="table">Table A</ref>.3: CEECNetV1 vs CEECNetV2 vs FracTAL ResNet vs ResNet building blocks comparison. All Building Blocks use kernel size k=3 and padding p=1 (SAME) and stride s=1. The transition convolutions that half the size of the features use the same kernel size and padding, however the stride is s=2. In the following we indicate with nf the number of output channels of the convolution layers, and with nh the number of heads in the multihead FracTAL module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>Proposed  return v cspat #dim :(B,C,H,W)</p><p>Listing 3: mxnet/gluon style pseudocode for the Relative Attention Fusion module import mxnet as mx from mxnet import nd as F cl ass Fusion (nn. Block ): def i n i t (self , nchannels , nheads , ** kwards ): super (). i n i t ( ** kwards ) self.fuse = Conv2DN (nchannels , kernel =3, padding =1, groups = nheads ) self. att12 = FTAttention2D (nchannels , nheads ) self. att21 = FTAttention2D (nchannels , nheads ) self. gamma1 = self. params .get('gamma1 ', shape =(1 ,) , init=mx.init.Zero ()) self. gamma2 = self. params .get('gamma2 ', shape =(1 ,) , init=mx.init.Zero ()) def forward (self , input1 , input2 ): ones = nd. ones like ( input1 ) # Attention on 1, for k,v from 2 qin = input1 kin = input2 vin = input2 att12 = self. att12 (qin ,kin ,vin) out12 = input1 * (ones+self. gamma1 * att12 ) # Attention on 2, for k,v from 1 qin = input2 kin = input1 vin = input1 att21 = self. att21 (qin ,kin ,vin) out21 = input2 * (ones+self. gamma2 * att21 ) out = nd. concat (out12 ,out21 ,dim =1) out = self.fuse(out) return out</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C.2. FracTAL ResNet</head><p>In this Listing, the ResBlock consists of the sequence of BatchNorm, ReLU, Conv2D, BatchNorm, ReLU, Conv2D. The normalization can change to GroupNorm for a small batch size. In this section we provide with pseudo-code the implementation of the CEECNetV1 unit.</p><p>Listing 5: mxnet/gluon style pseudocode for the CEECNetV1 unit. import mxnet as mx from mxnet import nd as F cl ass CEECNet unit V1 (nn. Block ): def i n i t (self , nchannels , nheads , ** kwards ): super (). i n i t ( ** kwards ) # Compress−Expand self. conv1 = Conv2DN ( nchannels /2) self. compr11 = Conv2DN (nchannels ,k=3,p=1,s=2) self. compr12 = Conv2DN (nchannels ,k=3,p=1,s=1) self. expand1 = ExpandNComb ( nchannels /2) # Expand Compress self. conv2 = Conv2DN ( nchannels /2) self. expand2 = Expand ( nchannels /4) self. compr21 = Conv2DN ( nchannels /2,k=3,p=1,s=2) self. compr22 = Conv2DN ( nchannels /2,k=3,p=1,s=1) self. collect = Conv2DN (nchannels ,k=3,p=1,s=1) self.att= FTAttention2D (nchannels , nheads ) self. ratt12 = RelFTAttention2D (nchannels , nheads ) self. ratt21 = RelFTAttention2D (nchannels , nheads ) self. gamma1 = self. params .get('gamma1 ', shape =(1 ,) , init=mx.init.Zero ()) self. gamma2 = self. params .get('gamma2 ', shape =(1 ,) , init=mx.init.Zero ()) self. gamma3 = self. params .get('gamma3 ', shape =(1 ,) , init=mx.init.Zero ()) def forward (self , input ): # Compress−Expand out10 = self. conv1 ( input ) out1 = self. compr11 ( out10 ) out1 = F.relu(out1) out1 = self. compr12 (out1) out1 = F.relu(out1) out1 = self. expand1 (out1 , out10 ) out1 = F.relu(out1) # Expand−Compress out20 = self. conv2 ( input ) out2 = self. expand2 ( out20 ) out2 = F.relu(out2) out2 = self. compr21 (out2) out2 = F.relu(out2) out2 = F. concat ([ out2 , out20 ],axis =1) out2 = self. compr22 (out2) out2 = F.relu <ref type="formula">(</ref> The networks mantis CEECNet and FracTAL ResNet were built and trained using the mxnet deep learning library <ref type="bibr" target="#b9">(Chen et al., 2015)</ref>, under the GLUON API. Each of the models was trained with a batch size of ∼256 on 16 nodes containing 4 NVIDIA Tesla P100 GPUs each in CSIRO HPC facilities. Due to the complexity of the network, the batch size in a single GPU iteration cannot be made larger than ∼4 (per GPU). The models were trained in a distributed scheme, using the ring allreduce algorithm, and in particular it's implementation on Horovod <ref type="bibr" target="#b45">(Sergeev and Balso, 2018)</ref> for the mxnet <ref type="bibr" target="#b9">(Chen et al., 2015)</ref> deep learning library. For all models , we used the Adam <ref type="bibr" target="#b29">(Kingma and Ba, 2014)</ref> optimizer, with momentum parameters (β 1 , β 2 ) = (0.9, 0.999). The learning rate was reduced by an order of magnitude whenever the validation loss stopped decreasing. Overall we reduced the learning rate 3 times. The depth of the evolving loss function was increased every time the learning rate was reduced. The depths of the F T d that we used were d ∈ {0, 10, 20, 30}.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Fractal Tanimoto similarity measure. In the top row we plot the two dimensional density maps for the F T similarity coefficient. From left to right the depths are d ∈ {0, 3, 5}. The last column corresponds to the average of values up to depth d = 5, i.e. F T 5 = (1/5) d F T d . In the bottom figure we represent in 3D the same values. The horizontal contour plot at z = 1 corresponds to the Laplacian of the F T . It is observed that as the depth, d, of the iteration increases, the function becomes steeper towards optimality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Fractal Tanimoto similarity measure with noise. On the top row, from left to right is the F T 0 (p, l) and (1/10) 9 d=0 (F T d (p, l)). The bottom row is the same corresponding F T d (p, l) similarity measures, with Gaussian random noise added. When the algorithmic training approaches optimality with the standard Tanimoto, local noise gradients tend to dominate over the background average gradient. Increasing the slope of the background gradient at later stages of training is a remedy to this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The FracTAL Residual unit. This building block demonstrates the fusion of the residual block with self FracTAL evaluated from the input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Compress Expand Expand Compress unit (CEECNet). The symbol</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Conditioned multitasking segmentation HEAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Train -validation -test split of the WHU dataset. The yellow (dashdot line) rectangle represents the training data. The area between the magenta rectangle (solid line) and the yellow (dash-dot) represents the validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>× TN − FP × FN √ (TP + FP)(TP + FN)(TN + FP)(TN + FN) IoU = TP TP + FN + FP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Pareto front selection after the last reduction of learning rate. The bottom panel designates with open cyan circles the two points that are equivalent in terms of quality prediction when both MCC and F T are taken into account. The top two panels show the corresponding evolutions of these measures during training. There, the Pareto optimal points are designated with full circle dots (cyan).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Comparison of the V1 and V2 versions of CEECNet building blocks with a FracTAL ResNet implementation and a standard ResNet V2 building blocks. The models were trained for 300 epochs on CIFAR10 with standard cross entropy loss. Performance improvement of the FracTAL-resnet34 over CBAM-resnet34: replacing the CBAM attention layers, with FracTAL ones, for two otherwise identical networks, results in 1% performance improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Training on CIFAR10 of two classification networks with static and evolving loss strategies. The two networks have identical macro-topologies, but different micro-topologies. The first network (top) uses standard Residual units for its building blocks, while the second (bottom) CEECNetV1 units. The networks are trained with a static F T (d = 0) loss stragety, and an evolving one. We increase the depth d of the F T d (p, l) loss function with each learning rate reduction. The vertical dashed lines designate epochs where the learning rate was scaled to 1/10th of its original value. The validation accuracy is mildly increased, although there is a clear difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Visualization of the last features (before the multitasking head) for the mantisFracTAL ResNet models of FracTAL depth d = 0 (left pannel) and d = 10 (right pannel). The features appear similar. For each panel the top left first three images are the input image at date t 1 , the input image at date t 2 and the ground truth mask. tion), lower entropy values are favoured over higher values, as they indicate a better compression level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Examples of inferred change detection on some test tiles from the LEVIRCD dataset of the mantis CEECNetV1 model (evolving loss strategy, FracTAL depth d = 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Training on CIFAR10 of a network with standard ResNet building blocks and fixed depth, d, of the F T d loss. The vertical dashed lines designate epochs where the learning rate was scaled to 1/10th of its original value. As the depth of iteration, d, increases (d remains constant for each training) the convergence speed of the validation accuracy degrades. Trainable scaling parameters, γ, for the sigmoid activation, i.e. sigmoid(x/γ), that are used in the prediction of change mask boundary layers. et al. (2021) split randomly their training and validation chips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>Sample of change detection on windows of size 1024 × 1024 from the WHU dataset. Inference is with the mantis CEECNetV1 model. The ordering of the inputs, for each row, is as inFig. 14.We indicate with blue boxes successful findings and with red boxes missed changes on buildings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 21 :</head><label>21</label><figDesc>As in Fig. 20 for sample windows of size 2048 × 2048 from the WHU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure</head><label></label><figDesc>B.23: Inference across the whole test area over NZBLDG CD Dataset using the mantisCEECNetV1 D6nf32 model. From left to right: 2011 input image, 2016 input image, ground truth, prediction (threshold 0.5) and confidence heat map. q = F. sigmoid (self.q(qin ))#dim :(B,C,H,W) k = F. sigmoid (self.k(vin ))#dim :(B,C,H,W) v = F. sigmoid (self.v(kin ))#dim :(B,C,H,W) att spat = self. ChannelSim (q,k)#dim :(B,1,H,W) v spat = att spat * v #dim :(B,C,H,W) att chan = self. SpatialSim (q,k)#dim :(B,C ,1 ,1) v chan = att chan * v #dim :(B,C,H,W) v cspat = 0.5 * ( v chan + v spat ) v cspat = self.norm( v cspat )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Listing 4 :</head><label>4</label><figDesc>mxnet/gluon style pseudocode for the Residual Attention Fusion module import mxnet as mx from mxnet import nd as F class FTAttResUnit (nn. Block ): def i n i t (self , nchannels , nheads , ** kwards ): super (). i n i t ( ** kwards ) # Residual Block : sequence of # (BN ,ReLU ,Conv ,BN ,ReLU ,Conv) self. ResBlock = ResBlock (nchannels , kernel =3, padding =1) self.att = FTAttention2D (nchannels , nheads ) self. gamma = self. params .get('gamma ', shape =(1 ,) , init=mx.init.Zero ()) def forward (self , input ): out = self. ResBlock ( input )#dim :(B,C,H,W) qin = input vin = input kin = input att = self. attention (qin ,vin ,kin)#dim :(B,C,H,W) att = self. gamma * att out = ( input + out) * (F. ones like (out )+ att) return out Appendix C.3. CEECNet building blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>. gamma3 * self. ratt21 (qin ,kin ,vin) ones1 = F. ones like ( out10 )# nchannels /2 out122 = out1 * (ones1 + ratt12 ) out211 = out2 * (ones1 + ratt21 )out12 = F. concat ([ out122 , out211 ],dim =1) out12 = self. collect ( out12 ) out12 = F.relu( out12 ) # Final fusion ones2 = F. ones like (input) out = ( input + out12) * (ones2 +att) return outThe layers Expand and ExpandNCombine are defined through Listings 6 and 7.Listing 6: mxnet/gluon style pseudocode for the Expand layer used in the CEECNetV1 unit.import mxnet as mx from mxnet import nd as F cl ass Expand (nn. Block ): def i n i t (self , nchannels , nheads , ** kwards ): super (). i n i t ( ** kwards ) self. conv1 = Conv2DN (nchannels ,k=3, p=1, groups = nheads ) self. conv2 = Conv2DN (nchannels ,k=3, p=1, groups = nheads ) def forward (self , input ): out = F. BilinearResize2D (input , scale height =2, scale width =2) out = self. conv1 (out) out = F.relu(out) out = self. conv2 (out) out = F.relu(out) return out Listing 7: mxnet/gluon style pseudocode for the ExpandNCombine layer used in the CEECNetV1 unit. import mxnet as mx from mxnet import nd as F cl ass ExpandNCombine (nn. Block ): def i n i t (self , nchannels , nheads , ** kwards ): super (). i n i t ( ** kwards ) self. conv1 = Conv2DN (nchannels ,k=3, p=1, groups = nheads ) self. conv2 = Conv2DN (nchannels ,k=3, p=1, groups = nheads ) def forward (self , input1 , input2 ): # input1 has lower spatial dimensions out1 = F. BilinearResize2D (input1 , scale height =2, scale width =2) out1 = self. conv1 (out1) out1 = F.relu(out1) out2= F. concat ([ out1 , input2 ],dim =1) out2 = self. conv2 (out2) out2 = F.relu(out2) return out2 Appendix D. Software implementation and training characteristics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model comparison on the WHU building change detection dataset. We designate with bold font the best values, with underline the second best, and with square brackets, [ ] the third best model. Ji et al. (2019a) presented two models for extracting buildings prior estimating the change mask. These where the Mask-RCNN (in table: M1) and MS-FCN (in table: M2). Our models consume input images of size of 256 × 256 pixels. With the exception of Liu et al. (2019) that uses the same size, all other results consume inputs of size of 512 × 512 pixels.</figDesc><table><row><cell>Model</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>MCC</cell><cell>IoU</cell></row><row><cell>Ji et al. (2019a) M1</cell><cell>93.100</cell><cell>89.200</cell><cell>[91.108]</cell><cell>-</cell><cell>[83.70]</cell></row><row><cell>Ji et al. (2019a) M2</cell><cell>93.800</cell><cell>87.800</cell><cell>90.700</cell><cell>-</cell><cell>83.00</cell></row><row><cell>Chen et al. (2021)</cell><cell>89.2</cell><cell>[90.5]</cell><cell>89.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Cao et al. (2020)</cell><cell>[94.00]</cell><cell>79.37</cell><cell>86.07</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al. (2019)</cell><cell>90.15</cell><cell>89.35</cell><cell>89.75</cell><cell>-</cell><cell>81.40</cell></row><row><cell>FracTAL ResNet (d = 5, evo)</cell><cell>95.350</cell><cell>90.873</cell><cell>93.058</cell><cell>92.892</cell><cell>87.02</cell></row><row><cell>CEECNetV1 (d = 5, evo)</cell><cell>95.571</cell><cell>92.043</cell><cell>93.774</cell><cell>93.616</cell><cell>88.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>that report performance results evaluated only on the changed pixels, and not the complete test images. Thus, they are missing out all false positive predictions that can have a</figDesc><table><row><cell></cell><cell>0.985</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WHU</cell></row><row><cell>MCC</cell><cell>0.965 0.945</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CEECNet V1</cell></row><row><cell></cell><cell>0.925</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FracTAL ResNet</cell></row><row><cell>MCC</cell><cell>0.980 0.985</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LEVIR-CD</cell></row><row><cell></cell><cell>0.975</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CEECNet V1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FracTAL ResNet</cell></row><row><cell></cell><cell>0.970</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 17: mantis CEECNetV1 vs mantis FracTAL ResNet (FracTAL depth,</cell></row><row><cell cols="10">d = 5) evolution performance on change detection validation datasets. The top</cell></row><row><cell cols="10">panel corresponds to the LEVIRCD dataset. The bottom panel to the WHU</cell></row><row><cell cols="10">dataset. For each network we followed the evolving loss strategy: there are</cell></row><row><cell cols="10">two learning rate reductions followed by two scaling ups of the F T d loss</cell></row><row><cell cols="10">function. All four training histories avoid overfitting, thanks to making the loss</cell></row><row><cell cols="6">function sharper towards optimality.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">dire impact on the performance metrics. They report precision:</cell></row><row><cell cols="9">97.840, recall: 97.01, F1: 97.29 and IoU: 97.38.</cell></row><row><cell></cell><cell>In</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 20: Samples of relative quality change detection on test tiles of size 1024 × 1024 from the LEVIRCD dataset. For each row from left to right: input image date 1, input image date 2, ground truth, confidence heat maps of mantis CEECNetV1 and mantis FracTAL ResNet respectively.</figDesc><table><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>0.4 1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>0.4 1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>0.4 1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>0.4 0.0 1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2 0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0 0.6</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row><row><cell>date 1</cell><cell>date 2</cell><cell>Ground Truth</cell><cell>mantis-CEECNet</cell><cell>mantis-ResNet</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The algorithm predicts first the distance transform of the change mask, then it reuses this information and identifies the boundaries of the change mask, and, finally, re-uses both distance transform and boundaries to estimate the change segmentation mask.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A software implementation of the models that relate to this work can be</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The simplified formula was obtained with Mathematica 11 Software.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We use python numpy<ref type="bibr" target="#b41">(Oliphant, 2006)</ref> semantics of broadcasting since the dimensions do not match. 10 This will be made clear with an example of FracTAL spatial similarity vs the dot product similarity that appears in SAGAN<ref type="bibr" target="#b58">(Zhang et al., 2018)</ref> selfattention. Let us assume that we have an input feature layer of size (B × C × H × W) = 32 × 1024 × 16 × 16 (e.g. this appears in the layer at depth 6 of UNetlike architectures, starting from 32 initial features). From this, three layers are produced of the same dimensionalty, the query, the key and value. With the Fractal Tanimoto spatial similarity, T , the output of the similarity of query and keys is B×C ×1×1 = 32×1024×1×1 (Equation 12). The corresponding output of the dot similarity of spatial compoments in the self-attention is BxCxC → 32x1024x1024 (Equation 11), having C-times higher memory footprint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Here, C is the number of channels, H and W the spatial dimensions, height and width respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">For example, for each batch dimension, the output volume of a layer of size (C, H, W) = (32, 256, 256) is V = 32 · 256 2 = 2097152.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">For no particular reason, other than that the mantis shrimp is an amazing sea creature that resides in the waters of Australia.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://github.com/luuuyi/CBAM.PyTorch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">Note that there is not standardized test set for the WHU dataset, therefore relative performance is indicative but not absolute: it depends on the train/test split that other researchers have performed. However, we only used 32.9% of the area of the provided data for training (which is much less than what other methods we compared against have used) and this demonstrates the robustness and generalisation abilities of our algorithm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was supported by resources and expertise provided by CSIRO IMT Scientific Computing. The authors acknowledge the support of the mxnet community. The authors would like to thank Pan Chen for careful reading of the manuscript and feedback. The authors acknowledge the contribution of the anonymous referees, whos questions helped to improve the quality of the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. CIFAR10 comparison network characteristics</head><p>In <ref type="table">Table A</ref>.3 we present in detail the characteristics of the layers that we used to compare on the CIFAR10 dataset. All building blocks use kernel size = 3 and padding = 1 (SAME).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Inference across WHU test set</head><p>The inference for the best performing model, the mantis CEECNetV1 D6nf32 model can be seen on <ref type="figure">Fig. B</ref>.23. The predictions match very closely the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Algorithms</head><p>Here we present with mxnet style pseudocode the implementation of the FracTAL associated modules. In all the listings presented, Conv2DN is a sequential combination of a 2D convolution followed by a normalization layer. When the batch size is very small, due to GPU memory normalization (e.g. smaller than 4 datums per GPU), the normalization used was Group Normalization <ref type="bibr" target="#b56">Wu and He (2018)</ref>. Practically, in all mantis CEECNet realizations for change detection, we used GroupNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C.1. Fractal Tanimoto Attention 2D module</head><p>Listing 1: mxnet/gluon style pseudo code for the fractal Tanimoto coefficient, predefined for spatial similarity.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Streetview change detection with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
		<idno type="DOI">10.15607/RSS.2016.XII.044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>AnnArbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Change detection techniques for remote sensing applications: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asokan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anitha</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12145-019-00380-5</idno>
		<idno>doi:10.1007/s12145-019-00380-5</idno>
		<ptr target="https://doi.org/10.1007/s12145-019-00380-5" />
	</analytic>
	<monogr>
		<title level="j">Earth Science Informatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1409. 0473. cite arxiv:1409</idno>
		<ptr target="http://arxiv.org/abs/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>0473Comment: Accepted at ICLR 2015 as oral presentation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09925</idno>
		<ptr target="http://arxiv.org/abs/1904.09925" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of small changed regions in remote sensing imagery using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1088/1755-1315/502/1/012017</idno>
		<ptr target="https://doi.org/10.1088%2F1755-1315%2F502%2F1%2F012017" />
	</analytic>
	<monogr>
		<title level="j">IOP Conference Series: Earth and Environmental Science</title>
		<imprint>
			<biblScope unit="volume">502</biblScope>
			<biblScope unit="page">12017</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding 187</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caye</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2019.07.003</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2019.07.003" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs12101662</idno>
		<idno>doi:10. 3390/rs12101662</idno>
		<ptr target="https://www.mdpi.com/2072-4292/12/10/1662" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dasnet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2020.3037893</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">SCA-CNN: spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05594</idno>
		<ptr target="http://arxiv.org/abs/1611.05594" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Review articledigital change detection methods in ecosystem monitoring: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jonckheere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nackaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lambin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1565" to="1596" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitask learning for large-scale semantic change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page">102783</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Resuneta: A deep learning framework for semantic segmentation of remotely sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Caccetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2020.01.013</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2020.01.013" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="94" to="114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tutorial on multiobjective optimization: Fundamentals and evolutionary methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Deutz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11047-018-9685-y</idno>
		<idno>doi:10.1007/s11047-018-9685-y</idno>
		<ptr target="https://doi.org/10.1007/s11047-018-9685-y" />
	</analytic>
	<monogr>
		<title level="j">Natural Computing: An International Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="585" to="609" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A change detection approach to flood mapping in urban areas using terrasar-x</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Giustarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hostache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J P</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2417" to="2430" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to measure change: Fully convolutional siamese metric networks for scene change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09111</idno>
		<ptr target="http://arxiv.org/abs/1810.09111" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PyCM: Multiclass confusion matrix library in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jasemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hessabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zolanvari</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00729</idno>
		<idno>doi:10.21105/ joss.00729</idno>
		<ptr target="https://doi.org/10.21105/joss.00729" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<ptr target="http://arxiv.org/abs/1703.06870" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. CoRR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to objectbased approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.03.006</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2013.03.006" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples. Remote Sensing 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11111343</idno>
		<ptr target="https://www.mdpi.com/2072-4292/11/11/1343" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pga-siamnet: Pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs12030484</idno>
		<ptr target="https://www.mdpi.com/2072-4292/12/3/484" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<ptr target="http://arxiv.org/abs/1702.00887" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02691</idno>
		<ptr target="http://arxiv.org/abs/1906.02691" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deep residual learning serial segmentation network for extracting buildings from remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/https:/doi.org/10.1080/01431161.2020.1734251</idno>
		<idno>doi:10.1080/01431161.2020.1734251</idno>
		<ptr target="https://doi.org/10.1080/01431161.2020.1734251" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="5573" to="5587" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Building change detection for remote sensing images using a dual task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07726</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mapping cropland abandonment in the aral sea basin with modis time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Löw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Prishchepov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dubovyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akramkhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biradar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lamers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">159</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 10</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Change detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mausel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brondizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of remote sensing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2365" to="2401" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of t4 phage lysozyme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-2795(75)90109-9</idno>
		<ptr target="https://doi.org/10.1016/0005-2795(75" />
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA) -Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="90109" to="90118" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rapid assessment of annual deforestation in the brazilian amazon using modis data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Defries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Shimabukuro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Del Bon Espírito-Santo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Earth Interactions</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06937</idno>
		<ptr target="http://arxiv.org/abs/1603.06937" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A guide to NumPy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Trelgol Publishing USA</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U2-net: Going deeper with nested u-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107404</idno>
		<idno>doi:10.1016/j.patcog. 2020.107404</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2020.107404" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<ptr target="http://arxiv.org/abs/1505.04597" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Change detection from a street image pair using cnn features and superpixel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Change detection based on artificial intelligence: State-of-the-art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs12101688</idno>
		<idno>doi:10.3390/rs12101688</idno>
		<ptr target="http://dx.doi.org/10.3390/rs12101688" />
		<imprint>
			<date type="published" when="1688" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A critical synthesis of remotely sensed optical image change detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Tewkesbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Comber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0285(80)90005-5</idno>
		<ptr target="https://doi.org/10.1016/0010-0285(80" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="90005" to="90010" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Recent advances in autoencoderbased representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05069</idno>
		<ptr target="http://arxiv.org/abs/1812.05069" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Changenet: A deep learning architecture for visual change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balamuralidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep learning on edge: Extracting field boundaries from satellite images with a convolutional neural network. Remote Sensing of Environment 245</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2020.111741</idno>
		<ptr target="https://doi.org/10.1016/j.rse.2020.111741" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<ptr target="http://arxiv.org/abs/1711.07971" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep learning for image superresolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06068</idno>
		<ptr target="http://arxiv.org/abs/1902.06068" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<editor>Weiss, Y.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08494</idno>
		<ptr target="http://arxiv.org/abs/1803.08494" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Dive into Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="https://d2l.ai" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
