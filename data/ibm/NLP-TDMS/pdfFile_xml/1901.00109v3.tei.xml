<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Network: How Far Can We Go with Morphological Neurons?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-03">December 3, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjan</forename><surname>Mondal</surname></persName>
							<email>ranjan15r@isical.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumendu</forename><forename type="middle">Sundar</forename><surname>Mukherjee</surname></persName>
							<email>soumendu041@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhabatosh</forename><surname>Chanda</surname></persName>
							<email>chanda@isical.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Communication Sciences Unit</orgName>
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Interdisciplinary Statistical Research Unit Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Sanchayan Santra Electronics and Communication Sciences Unit Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Electronics and Communication Sciences Unit Indian Statistical Institute</orgName>
								<address>
									<settlement>Kolkata</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphological Network: How Far Can We Go with Morphological Neurons?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-03">December 3, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the idea of using morphological operations as networks has received much attention. Mathematical morphology provides very efficient and useful image processing and image analysis tools based on basic operators like dilation and erosion, defined in terms of kernels. Many other morphological operations are built up using the dilation and erosion operations. Although the learning of structuring elements such as dilation or erosion using the backpropagation algorithm is not new, the order and the way these morphological operations are used is not standard. In this paper, we have theoretically analyzed the use of morphological operations for processing 1D feature vectors and shown that this gets extended to the 2D case in a simple manner. Our theoretical results show that a morphological block represents a sum of hinge functions. Hinge functions are used in many places for classification and regression tasks (Breiman (1993) [6]). We have also proved a universal approximation theorem-a stack of two morphological blocks can approximate any continuous function over arbitrary compact sets. To experimentally validate the efficacy of this network in real-life applications, we have evaluated its performance on satellite image classification datasets since morphological operations are very sensitive to geometrical shapes and structures. We have also shown results on a few tasks like segmentation of blood vessels from fundus images, segmentation of lungs from chest x-ray and image dehazing. The results are encouraging and further establishes the potential of morphological networks. originated from the study of the geometry of porous materials, and later on formalized for binary images. Afterwards, it has been extended to grayscale [41] and color images [3] as well. Currently, mathematical morphology is employed in the analysis of graphs, meshes and many other spatial structures, although digital images remain its common application domain. It serves as a non-linear tool for processing digital images based on shapes present in them. Morphological operators decompose objects or shapes into meaningful parts which helps in understanding them in terms of the elements. Since identification of objects and their features are directly correlated with their shapes and arrangement, morphological methods are quite suited for visual tasks <ref type="bibr" target="#b12">[13]</ref>. Current approaches to image analysis mainly utilize tools of linear systems, which may not always be suitable or efficient for the task at hand. Being a powerful nonlinear methodology, mathematical morphology can potentially be a viable alternative to the existing image analysis tools.</p><p>The most basic morphological operation dilation and its dual, erosion, have quite a bit of similarity with the convolution operation. Similar to convolution, these morphological operators work by operating a structuring element (kernel in case of convolution) locally over the whole input. But instead of sum of products, they compute max (or, min) over sums (resp. differences). More complex morphological operators are constructed by the composition of these two basic operators. It is known in the community that, for a given problem, a single image transformation seldom gives the desired output. In fact, most real-world problems are complex in nature and potentially require a combination of elementary transformations. So, a sequence of operations are often employed with unique structuring elements. But the choice of the operations and the associated structuring elements requires expert knowledge along with trial-and-error. If we draw an analogy with convolutional neural networks (CNN), there also a sequence of convolution operators are employed to extract hierarchy of features. The network starts from simple ones, like edges and corners, and progressively composes them to extract complex features. Learning the hierarchy of features automatically from data is one of the reasons of the success of CNNs. Similar things may be achieved if we arrange the dilation and erosion operations in a network form and try to learn the structuring elements automatically from data. However, to achieve acceptable performance, this network should be generic enough. We have shown that the networks with morphological operations, when built in a certain way, are also generic like neural networks. Our contributions can be summarized as follows. We start with the most basic version of a 1D morphological network, where the morphological operators work over the whole input, not locally, to facilitate theoretical analysis. This single layer version of the proposed morphological network, called a Morphological Block, contains a layer of both dilation and erosion operations, followed by a layer computing linear combination of the outputs of these operators. We show that this Morphological block is a sum of hinge functions, and it can produce an exponential number of hyperplanes in comparison with a neural network with a single hidden layer. Then we turn our attention to the deeper architectures and provide the required analysis during the journey. We prove that two morphological blocks, when used together, can approximate any continuous function over arbitrary compact sets. This simple version of the network is then extended to the 2D version where the dilation and erosion are performed locally withholding all the properties of the 1D network.</p><p>The rest of the paper is organized as follows. Existing works that tried to combine morphological operations and neural networks are briefly outlined in Section 2. Some other related works are also mentioned in this section. In Section 3, we describe our morphological network along with a theoretical analysis of its capabilities. Section 4 provides a few applications of the proposed networks. Finally, concluding remarks are presented in Section 5. two axis-parallel hyperplanes as the decision boundary. This single-layer architecture has been extended to twolayer architecture by Sussner [45]. This two-layer architecture can learn multiple axis-parallel hyperplanes, and therefore is able to solve arbitrary binary classification tasks. But, in general, the decision boundaries may not be axis-parallel, and so, large number of hyperplanes need to be learned by the network for such decision boundaries. So, Barmpoutis and Ritter [4]  proposed to learn an additional rotational matrix that rotates the input before trying to classify data using axis-parallel hyperplanes. In a separate work by Ritter et al. [38]  the use of L 1 and L ∞ norm has been proposed as a replacement of the max/min operation of dilation and erosion in order to smooth the decision boundaries. Ritter and Urcid [37] first introduced the dendritic structure of biological neurons to the morphological networks. This new structure creates hyperbox based decision boundaries instead of hyperplanes. The authors have proved that hyperboxes can estimate any compact region and, thus, any two-class classification problem can be solved. A generalization of this structure to the multiclass case has also been done by Ritter et al. <ref type="bibr" target="#b38">[39]</ref>. Sussner and Esmi [46]  proposed a new type of structure called morphological perceptrons with competitive neurons, where the output followed winner-take-all strategy. This was modelled using the argmax operator that allows the network to learn more complex decision boundaries. For morphological neurons with dendritic structure Zamora and Sossa [51]  replaced the argmax operator with a softmax function. This overcomes the problem of gradient computation and, therefore, gradient descent optimizer was employed.</p><p>Due to the non-differentiability of the max/min operations, researchers had to use optimizer other than gradient descent based methods to train their networks. A separate line of research has been attempted to overcome this. Araújo <ref type="bibr" target="#b10">[11]</ref> utilized network architecture similar to morphological perceptrons with competitive learning to forecast stock markets. The argmax operator was replaced with a linear activation function so that the network is able to regress forecasts and enables the use of gradient descent for training. Recently, a few works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref> has appeared in the field. Franchi et al. <ref type="bibr" target="#b11">[12]</ref> replace standard max pooling in CNN with a learned morphological pooling. They build a network which is mixed with CNNs for image image denoising. Nogueira et al. <ref type="bibr" target="#b30">[31]</ref> introduces morphological networks with multiple morphological operations for image classification tasks. Islam et al.</p><p>[1] use morphological hit-or-miss transform to build the network. Mondal et al. <ref type="bibr" target="#b27">[28]</ref> introduce opening closing network for image de-raining and de-hazing with morphological opening and closing operation. The authors of <ref type="bibr" target="#b24">[25]</ref> use morphological operations to build Bipolar morphological neural networks. Although morphological operations has been utilized in a network form in various ways for specific applications, its generic theoretical justification is scarce till date. It is still an open question how morphological networks should be designed so that they become a generic tool that can solve any learning problem. In the following subsections, we have tried to answer these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Morphological networks</head><p>In this section we introduce the morphological network, or 'Morph-Net' in short. It is a simple feed forward network that consists of dilation and erosion neurons. We begin with defining dilation and erosion neurons, and then describe the simplest version of the network and its capabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been a renewed interest in learning structuring elements of morphological operations. Mathematical morphology is a set and lattice theoretic technique for the analysis of geometrical structures. It</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>The use of morphological operations in the form of a network was first introduced by Davidson and Hummer <ref type="bibr" target="#b9">[10]</ref> in their effort to learn the structuring element of dilation operation on images. Similar effort has been made to learn the structuring elements in a more recent work by Masci et al. <ref type="bibr" target="#b26">[27]</ref>. The use of morphological neurons in a more general setting was first proposed by Ritter and Sussner <ref type="bibr" target="#b35">[36]</ref>. They restricted the network to a single-layer architecture and focused only on the binary classification task. To classify the data, these networks used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dilation and erosion neurons</head><p>Dilation and Erosion neurons are the building blocks of our proposed network. Given an input x ∈ R d and a structuring element s ∈ R d , the operation of dilation (⊕) and erosion ( ) neurons are defined, respectively, as follows.</p><p>x ⊕ s = max</p><formula xml:id="formula_0">k {x k + s k },<label>(1)</label></formula><p>x s = min</p><formula xml:id="formula_1">k {x k − s k },<label>(2)</label></formula><p>x k denotes k th element of augmented input vector x. Note that the erosion operation may also be written as x s = − max k {s k − x k }. In these neurons the structuring element (s) is learned in the training phase. The max and min operators used in the dilation and erosion neurons are only piece-wise differentiable. As a result, only a single element of structuring element is updated at each iteration of backpropagation algorithm. To overcome this problem we propose to use the soft version of max and min <ref type="bibr" target="#b8">[9]</ref> to define soft dilation and erosion operation as follows.</p><formula xml:id="formula_2">x⊕s = 1 β log k e (x k +s k )β ,<label>(3)</label></formula><formula xml:id="formula_3">xˆ s = − 1 β log k e (s k −x k )β ,<label>(4)</label></formula><p>where⊕ andˆ denote the soft dilation and soft erosion, respectively, and β is the "hardness" of the soft operations.The soft version can be made close to their "hard" counterpart by making β large enough <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The morphological block</head><p>The simplest form of Morph-Net is the morphological block. It consists of a layer with dilation and erosion neurons followed by a linear combination ( <ref type="figure" target="#fig_0">Figure 1</ref>) of their outputs. We call the layer of dilation and erosion neurons as the dilation-erosion layer and the following layer as the linear combination layer. Let us consider a network with n dilation neurons and m erosion neurons in the dilation-erosion layer followed by c neurons in the linear combination layer. Let x ∈ R d be the input to the network and z + i and z − j be the output of the i th dilation neuron and the j th erosion neuron respectively:</p><formula xml:id="formula_4">z + i = x ⊕ s + i , (5) z − j = x s − j ,<label>(6)</label></formula><p>where s + i and s − j are the structuring elements of the respective neurons. Note that i ∈ {1, 2, . . . , n} and j ∈ {1, 2, . . . , m}. The final output of a node in the linear combination layer is computed as</p><formula xml:id="formula_5">M(x) = n i=1 z + i ω + i + m j=1 z − j ω − j ,<label>(7)</label></formula><p>where ω + i and ω − j are the weights of the combination layer. When the network is trained, it learns</p><formula xml:id="formula_6">s + i , s − j , ω + i and ω − j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Morphological block as sum of hinge functions</head><p>Hinging hyperplanes <ref type="bibr" target="#b5">[6]</ref> have many applications in regression and classification tasks. In this subsection, we show that the simple morphological block can be represented as a sum of hinge functions. Further, we try to develop the intuition behind a single morphological block with a toy example.</p><p>Definition 1 (k-order Hinge Function [47]). A k-order hinge function h (k) (x) consists of (k + 1) hyperplanes continuously joined together. It may be defined as</p><formula xml:id="formula_7">h (k) (x) = ± max{w T 1 x + b 1 , w T 2 x + b 2 , . . . , w T k+1 x + b k+1 }.<label>(8)</label></formula><p>Proposition 1. The function computed by a single layer Morph-Net (denoted by M(x)) with n dilation and m erosion neurons followed by their linear combination, is a sum of multi-order hinge functions.</p><p>In fact, we can show that</p><formula xml:id="formula_8">M(x) = l i=1 α i h (d) i (x),<label>(9)</label></formula><p>where l = m + n, α i ∈ {1, −1} and h</p><formula xml:id="formula_9">(d) i (x), 1 ≤ i ≤ l, are d-order hinge functions.</formula><p>The proof is given in the appendix.</p><p>The function M(x) learned by a single layer Morphological network may also be expressed in the following form:</p><formula xml:id="formula_10">M(x) = l i=1 α i max k {θ k x k + ρ ik },<label>(10)</label></formula><p>where α i , θ k , ρ ik ∈ R. The derivation can be found in the appendix. We see that M(x) is sum of l functions, each of which computes max over the linearly transformed elements of x. Since the max is computed over the (transformed) elements of x, each max operation selects only one element of x. So, the computed M(x) may not contain all the elements of x and the index (k) of selected element varies depending on the input and the structuring element. However, if l &gt; d, M(x) may contain all the elements of x. So equation 10 can be rewritten as</p><formula xml:id="formula_11">M(x) = α 1 (θ 1 x k 1 + ρ 1k 1 ) + α 2 (θ 2 x k 2 + ρ 2k 2 ) + · · · + α l (θ l x k l + ρ lk l ).<label>(11)</label></formula><p>where x k i represents any one of the d elements of x selected by i-th neuron by max operation depending on structuring element s i . So each x k i is chosen from d elements. Therefore, depending on which element of x gets selected by each neuron, M(x) forms one of the (d) l − 1 hyperplanes. In case of added bias to the neurons i.e appended 0 to input, M(x) forms one of the (d + 1) l − 1 hyperplanes. The −1 occurs in number of hyperpalnes because in one occasion all neurons select the augmented 0.</p><p>Note that some of these hyperplanes must be parallel to some axes. For M(x) to form a hyperplane that is not parallel to any of the axes, all elements of x must get selected by some max functions or other. This occurs in d! × l d ways. The remaining l − d number of elements x k i 's are repeat selection by some functions. So, there can be almost d! × l d × (d) l−d hinging hyperplanes that are not parallel to any of the axes. If we consider bias in morphological neurons the number will be d! × l d × (d + 1) l−d . Therefore, number of hinging hyperplanes increases exponentially with the number of neurons in the dilation-erosion layer. These hyperplanes can act as decision boundaries if the network is employed as a classifier. This is demonstrated experimentally using a toy dataset representing two-class problem.</p><p>The toy dataset contains samples that are distributed along two concentric circles, one circle for each class. Suppose, the circles are centered at the origin. We compare the results of various networks with two neurons in the hidden layer. It is observed that baseline neural network fails to classify this data as with two hidden neurons it learns only two hyperplanes, one for each neuron. The boundaries learned by the network with ReLU activation function (NN-ReLU) is shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. The result of maxout network is better, because, in this case, the network learns 2k = 4 hyperplanes as shown in <ref type="figure" target="#fig_1">Figure 2b</ref>. Note that with two morphological neurons in dilationerosion layer, our Morph-Net has learned 6 hyperplanes to form the decision boundary ( <ref type="figure" target="#fig_1">Figure 2c</ref>). equation 10 suggests that we should get at most 8 distinct lines. However, out of these only two decision boundaries are placed in any arbitrary orientation in the 2D space, while others are parallel to either of the axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multilayer Morph-Net: universal approximation</head><p>In this subsection, we discuss universal approximation properties of multilayer morphological networks. We show that two layer morphological networks can approximate any continuous function. We begin with a lemma that shows that two layer Morph-nets can represent any linear combination of hinge functions. Its proof is given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Any linear combination of hinge functions</head><formula xml:id="formula_12">m i=1 α i h (k i ) (x)</formula><p>can be represented over an arbitrary compact set K as a two-layer Morph-Net consisting of dilation neurons only.</p><p>Theorem 1 (Universal approximation). Two-layer Morph-nets can approximate continuous functions over arbitrary compact sets.</p><p>Proof. Continuous functions can be approximated over compact sets by sums of hinge functions (see Theorem 3.1 of <ref type="bibr" target="#b5">[6]</ref>). Therefore, by Lemma 1, it follows that any continuous function can be approximated over arbitrary compact sets by two-layer Morph-Nets.</p><p>Universal approximation properties of single layer Morph-Nets are not known. However, we have a few suggestive empirical results regarding this in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">2D morphological network</head><p>Let gray-scale image X is of size M × N . The Dilation (⊕) and Erosion ( ) operations on X are defined as [44]</p><formula xml:id="formula_13">(X ⊕ S)(i, j) = max (l,m)∈D (X(i − l, j − m) + S(l, m))<label>(12)</label></formula><formula xml:id="formula_14">(X S)(i, j) = min (l,m)∈D (X(i + l, j + m) − S(l, m))<label>(13)</label></formula><p>where S(i, j) ∈ IR is the 2D structuring element of size a × b defined on domain D = { (l, m) | l ∈ {1, 2, 3, .., a}; m ∈ {1, 2, 3, .., b}}. Since a &lt;&lt; M and b &lt;&lt; N , at each location (i, j) of image X we consider a box of pixels of size a × b which is operated on by S to produce the output at (i, j) independent of output at any other location. We can flatten the box of pixels to generate x ∈ R d (assuming d = ab). Similarly we can flatten S and append an element to obtain s ∈ R d+1 . So we can employ equation 1 and equation 2 to get similar result as obtained by equation 12 and equation 13 respectively. As a result, all the properties stated above for 1D are also true for 2D. Thus same morphological network is used for 2D operations too. Similarly, it can be defined for colour images. <ref type="figure" target="#fig_10">Figure 3</ref> shows a morphological block containing a layer of 6 dilation and 6 erosion , followed by 4 different linear combinations of feature maps. The structuring element size is 3 × 3. This entire block is denoted by DE 3×3 6 − L 4 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The applicability of the proposed Morphological Network in real-world problems is shown with the help of a few experiments. The Morphological operators are known to be effective at extracting geometrical features. This has been illustrated with the help of two satellite image dataset. Morphological Network is not only good at extracting geometrical features, it also adept at tasks involving image to image transformation. We have shown the results on a few tasks like, segmentation of medical images and image dehazing. They are given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Satellite image classification</head><p>We have evaluated morphological network for image classification task on two publicly available dataset: UCMerced Land-use [50] and WHU-RS19 Dataset <ref type="bibr">[49]</ref>. For training and testing, we have utilized the protocol mentioned in <ref type="bibr" target="#b30">[31]</ref>. Instead of convolution we have employed 2D morphological network to build up the network. We have employed Alex-net like architecture using for this purpose. The network is trained in each fold for upto 300 epochs for each dataset. All the images are resized to 256 × 256 before they are fed into the network. From the results reported in the <ref type="table" target="#tab_0">Table 1</ref>, it can be seen that our network works as good as the method of <ref type="bibr" target="#b30">[31]</ref> while requiring only a fraction of the parameters. It is also seen not just ours, all morphological networks are performing better than CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Morphological network for image transformation</head><p>In order to demonstrate the applicability of morphological networks in generic image transformation tasks, we have compared its performance in three different tasks: segmentation of retina from fundus images, segmentation of lung from from X-Ray images and image dehazing. We have shown the performance on two different type of segmentation, because segmenting fine structures from images is different from segmenting large structures. The problem of image dehazing is utilized to show how this network may perform on natural images. We have also shown image de-hazing with morphological network. Please refer to the appendix for a more detailed comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Retinal segmentation</head><p>For performance study, we have used two publicly available retinal image datasets: DRIVE 1 and STARE 2 <ref type="bibr" target="#b15">[16]</ref>. For this task, we have employed a 2D Morph-Net containing seven sequential morphological block to produce output. A sigmoid activation function is utilized only in the last layer to restrict the output to lie between 0 and 1.</p><p>Other layers do not make use of any activation function. A schematic block diagram of the network architecture is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In retinal images very small blood vessels small in size and randomly distributed. In <ref type="figure" target="#fig_4">Figure 5</ref> we have shown few qualitative results and compared with ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Lungs segmentation</head><p>Lung segmentation is an important biomedical image segmentation problem. Unlike the previous task of blood vessel segmentation, here we have to segment a large blob from the image. In this experiment we have used the publicly available Shenzhen chest X-Ray imageset <ref type="bibr" target="#b16">[17]</ref>. This problem requires both global and local information, so that a large blob with intricate detail of boundary region (region of interest) can be extracted. This suggests us to employ a U-net [40] type of network architecture. Batch normalization taken to increase the stability of the network. In <ref type="figure" target="#fig_5">Figure 6</ref>, we have shown a few results before thresholdig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Image dehazing</head><p>Image De-hazing is one of the trending problems of image to image transformation. Haze occurs due to scattering of the light by the particles present in the atmosphere. Here, the hazy image formation process is modelled as follows <ref type="bibr" target="#b28">[29]</ref> I</p><formula xml:id="formula_15">(x) = t(x)J(x) + K(x).<label>(14)</label></formula><p>where J(x) and I(x) are the haze-free and the observed hazy image, respectively. t(x) ∈ [0, 1] is the scene transmittance that denotes the amount of haze in pixel x, and K(x) is called air light map. To dehaze an image, we propose a 2D Morph-Net that takes hazy image (I(x)) as input and produces estimated the airlight map K(x) and the transmittance mapt(x) as output. We minimize a loss function inspired from the bi-directional consistency loss <ref type="bibr" target="#b28">[29]</ref> based on Structural Similarity Index (SSIM) between two images <ref type="bibr">[48]</ref>. The network is trained on the training images of O-HAZE dataset <ref type="bibr" target="#b1">[2]</ref> until convergence of the training error. Some results are shown in <ref type="figure" target="#fig_0">Figure 18</ref> for qualitative evaluation and comparison. It can seen that Morphological network is able to extract consistent transmittance map and airlight from image hazy images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel trainable morphological network (Morph-Net) using dilation and erosion operators as neurons. It is shown that these operators, in conjunction with linear combinations, represent a sum of hinge functions. As for multi-layer morphological networks, we have proved that taking two sequential morphological blocks can approximate any continuous function. The 1D Morph-Net is readily extended to 2D, forming a CNN like network with all the characteristics of the 1D morphological network. Efficacy of the 2D Morph-Net is evaluated by applying it on various computer vision problems. The proposed network, both in 1D and 2D, has great potential to be applied to many more problems, especially in image-processing. However, there is also scope for future work regarding architecture design and optimization. A Soft maximum Lemma 2. The soft dilation operation converges to the dilation operation as β → ∞. This is true for the soft erosion operation also.</p><p>Proof. The proof if standard. We include it here for completeness. Let a k = x k + s k . We have Proof. As defined in the main paper the computed M(x) has the following form.</p><formula xml:id="formula_16">M(x) = n i=1 ω + i z + i + m j=1 ω − j z − j ,<label>(15)</label></formula><p>where z + i and z − j are the output of i th dilation neuron and j th erosion neuron, respectively and ω + i and ω − j are the weights of the the linear combination layer. Replacing the z + i and z − j with their expression, the equation becomes the following.</p><formula xml:id="formula_17">M(x) = n i=1 ω + i max k {x k + s + ik } + m i=1 −ω − i max k {s − ik − x k },<label>(16)</label></formula><p>where s + ik and s − ik denote the k th component of the i th structuring element of dilation and erosion neurons, respectively. The above equation can be further expressed in the following form:</p><formula xml:id="formula_18">M(x) = n i=1 α + i max k {θi + x k + ρ + ik } + m i=1 α − i max k {θ − i x k + ρ − ik },<label>(17)</label></formula><p>where θ + i , θ − i , ρ + ik and ρ − ik are defined in the following way:</p><formula xml:id="formula_19">θ + i = ω + i if ω + i ≥ 0, −ω + i if ω + i &lt; 0, θ − i = −ω − i if ω − i ≥ 0, ω − i if ω − i &lt; 0, ρ + ik = s + ik ω + i if ω + i ≥ 0, −s + ik ω + i if ω + i &lt; 0, ρ − ik = s − ik ω − i if ω − i ≥ 0, −s − ik ω − i if ω − i &lt; 0, α + i = 1 if ω + i ≥ 0, −1 if ω + i &lt; 0, α − i = −1 if ω − i ≥ 0, 1 if ω − i &lt; 0.</formula><p>Now, without any loss of generality, we can write equation 17 as follows:</p><formula xml:id="formula_20">M(x) = m+n i=1 α i max k (θ i x k + ρ ik ),<label>(18)</label></formula><p>where</p><formula xml:id="formula_21">θ i = θi + if i ≤ n , θ − i−n if n &lt; i ≤ m + n , ρ ik = ρ + ik if i ≤ n , ρ − (i−n)k if n &lt; i ≤ m + n , α i = α + i if i ≤ n , α − (i−n) if n &lt; i ≤ m + n .</formula><p>Clearly, Equation <ref type="bibr" target="#b17">18</ref> can be rewritten as</p><formula xml:id="formula_22">M(x) = l i=1 α i φ i (x),<label>(19)</label></formula><p>where l = m + n, α i ∈ {1, −1} and</p><formula xml:id="formula_23">φ i (x) = max k (v T ik x + ρ ik ),<label>(20)</label></formula><p>with</p><formula xml:id="formula_24">v ikt = β i if t = k, 0 if t = k.<label>(21)</label></formula><p>In equation 20, v T ik x + ρ ik is affine and α i φ i (x) is a d-order hinge function. Hence l i=1 α i φ i (x), i.e. M(x), represents a sum of multi-order hinge functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 A single morphological block and universal approximation</head><p>A single morphological block represents a sum of hinge functions. However, it is not clear if all hinge functions can be represented by a single morphological block. In a numerical study, we have tried to approximate the hinge function max(x + y, 0) using a single morphological block by varying the number of dilation/erosion neurons. We have generated values of the function in the square [−5, 5] × [−5, 5], and trained the network with mean Loss-without bias Loss-with bias squared error (MSE) loss. In <ref type="figure" target="#fig_9">Figure 8</ref>, we have plotted the MSE loss (after convergence) against the number of morphological neurons used. It is seen that a single morphological block is unable to reduce the error unless we use additional bias terms in the morphological neurons. However, we do not know theoretically if having additional bias terms in morphological operations help in universal approximation. We have also tried to perform classification task in CIFAR-10 and SVHN using a single morphological block with bias in morphological neurons. The results are shown in <ref type="figure" target="#fig_11">Figures 9a and 9b</ref>. It can be seen that a single layer morphological block (with bias) works better than neural networks of similar architecture in CIFAR-10. Proof. Since we are in a compact set, there exists C &gt; 0 such that |x | ≤ C for any 1 ≤ ≤ d. Take</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss-without bias and Loss-with bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Two morphological blocks and universal approximation</head><formula xml:id="formula_25">s = −3C1 d + 3Ce ,d , 1 ≤ ≤ d,</formula><p>where 1 d is the vector of all ones and e ,d is the -th unit vector in R d . Then all but the -th coordinate of s are −3C, while the -th coordinate is 0. Then note that, for any x ∈ K, and 1 ≤ ≤ d,  for any j = . It follows that for any x ∈ K, and 1 ≤ ≤ d,</p><formula xml:id="formula_26">x + s , = x ≥ −C &gt; −2C = C − 3C ≥ x j − 3C = x j − s ,</formula><formula xml:id="formula_27">x ⊕ s = x .</formula><p>Now given any hyperplane w x + b, we can express it exactly as a linear combination of dilation neurons over K:</p><formula xml:id="formula_28">w x + b = d =1 w x + b = d =1 w (x ⊕ s ) + b.</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4 (Lemma 1 of the main paper). Any linear combination of hinge functions</head><formula xml:id="formula_29">m i=1 α i h (k i ) (x)</formula><p>can be represented over any compact set K as a two-layer Morph-Net consisting of dilation neurons only.</p><p>Proof. Let B = max 1≤i≤m sup x∈K |h (k i ) (x)|. We now give the architecture of the desired Morph-Net.</p><p>1. The first dilation-erosion layer has exactly d dilation neurons given by x ⊕ s , 1 ≤ ≤ d.</p><p>2. The first linear combination layer has k = m i=1 (k i + 1) neurons, with the i-th block of (k i + 1) neurons outputting the constituent hyperplanes of h (k i ) (x). This can be done by Lemma 3.</p><p>3. The second dilation-erosion layer just has m dilation neurons, each outputting a hinge function. The -th neuron is constructed as follows: Write any y ∈ R k as (y 1 , . . . , y m ) where y j = (y j,1 , . . . , y j,k j +1 ) . We want the output of the -th neuron to be max 1≤v≤k +1 y ,v . So we take t = (t ,1 , . . . , t ,m ) , where t ,j = −3B1 k j +1 for j = , and t , = 0 k +1 . Then, for any j = , 1 ≤ u ≤ k j + 1, and 1 ≤ v ≤ k + 1, we have</p><formula xml:id="formula_30">y j,u + t ,j,u = y j,u − 3B ≤ B − 3B = −2B &lt; −B ≤ y ,v = y ,v + t , ,v .</formula><p>It follows that y ⊕ t = max 1≤v≤k +1 y ,v . With this construction, the outputs of the second dilation-erosion layer are the m numbers h (k i ) (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The second linear combination layer just has a single neuron that combines the outputs of the previous layer in the desired way:</p><formula xml:id="formula_31">z → m i=1 α i z i .</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Properties of multilayer morphological networks</head><p>In this section, we record a few properties of multilayer morphological networks. Proof. The "only if" part is trivial. As for the "if" part, note that we have</p><formula xml:id="formula_32">f −1 {r} = n≥1 f −1 (r − 1/n, r] = n≥1 (f −1 (−∞, r] \ f −1 (−∞, r − 1/n]).</formula><p>Same goes for g, and so, by our hypothesis,</p><formula xml:id="formula_33">f −1 {r} = g −1 {r} for all r ∈ R.</formula><p>Therefore, for any x ∈ R d , we have x ∈ g −1 {g(x)} = f −1 {g(x)}, or, in other words, f (x) = g(x).</p><p>Theorem 2. The following are true for morphological network architectures. Which indicates taking multiple sequential dilation or erosion layer is equivalent of taking a single dilation or erosion layer.</p><formula xml:id="formula_34">(i) The architecture D m 1 E 0 → D m 2 E 0 → · · · → D m E</formula><formula xml:id="formula_35">(ii) The architecture D 1 E 1 → D 1 is not equivalent to D 1 E 0 .</formula><p>Similarly, it is not equivalent to D 0 E 1 , and, consequently, the architectures</p><formula xml:id="formula_36">D 1 E 1 → D 1 E 1 and D 1 E 1 are inequivalent. (iii) The architecture D 1 E 1 → D 1 → L is not equivalent to D 1 E 0 → L. (iv) The architecture D 2 E 0 → D 0 E 2 → D 1 is not equivalent to D 2 E 0 → D 1 .</formula><p>Proof. (i) Let x ∈ R d be the input to the network . Let there be two networks N 1 and N 2 . Let there be m 1 and m 2 dilated neurons in, respectively, the first and the second layers of Network N 1 . Let the parameters of the network N 1 in the first layer and 2nd layer are w 1 ∈ R d×m 1 and w 2 ∈ R l 1 ×l 2 respectively. Whereas let there are only single layer with m 1 number of dilated neurons in network N 2 and the parameters are denoted as u ∈ R d×m 2 . Let f (x) ∈ R m 2 and g(x) ∈ R m 2 are the output from the last layer of network N 1 and N 2 respectively. For Network N 1</p><formula xml:id="formula_37">y j = max i (x i + w 1 i,j ) ∀j ∈ {1, 2, ..m 1 } (22) f k (x) = max j (y j + w 2 j,k ) ∀j, k<label>(23)</label></formula><p>For network N 2</p><formula xml:id="formula_38">g k (x) = max j (x j + u 2 j,k ) ∀k, j<label>(24)</label></formula><p>Let</p><formula xml:id="formula_39">S k f = {x | f k (x) ≤ e k ; e k ∈ R}<label>(25)</label></formula><formula xml:id="formula_40">S k g = {x | f k (x) ≤ e k ; e k ∈ R}<label>(26)</label></formula><p>For Network N 1</p><formula xml:id="formula_41">f k (x) ≤ e k ; ∀k (27) y i + w 2 i,j ≤ e k ∀k, j<label>(28)</label></formula><formula xml:id="formula_42">y i ≤ e k − w 2 i,j ∀k, j<label>(29)</label></formula><p>From Equation <ref type="bibr" target="#b21">22</ref> and Equation <ref type="bibr" target="#b28">29</ref> we get</p><formula xml:id="formula_43">max i (x i + w 1 i,j ) ≤ e k − w 2 i,j ∀k, j<label>(30)</label></formula><formula xml:id="formula_44">x i + w 1 i,j ≤ e k − w 2 i,j ∀k, j, i<label>(31)</label></formula><formula xml:id="formula_45">x i ≤ e k − w 2 i,j − w 1 i,j ∀k, j, i<label>(32)</label></formula><p>Which means</p><formula xml:id="formula_46">x i ≤ min j (e k − w 2 i,j − w 1 i,j ) ∀k, i<label>(33)</label></formula><formula xml:id="formula_47">x i ≤ e k − max j (w 2 i,j + w 1 i,j ) ∀k, i<label>(34)</label></formula><p>For network N 2</p><formula xml:id="formula_48">g k (x) = max j (x j + u 2 j,k )<label>(35)</label></formula><formula xml:id="formula_49">x i ≤ (e k − u i,k )∀k, i<label>(36)</label></formula><p>To hold the set S k g is equal to S k f to ∀k  <ref type="figure" target="#fig_0">Figure 10</ref>: A network of architecture</p><formula xml:id="formula_50">D 1 E 1 → D 1 u i,k = max j (w 2 i,j + w 1 i,j )∀i, k<label>(37)</label></formula><p>Hence, from Lemma 5, given a parameter w 1 and w 2 of and 2 layer network N 1 , there exist a equivalent single layer network N 2 with dilated neurons u which can represent the same function. From the Equation <ref type="bibr" target="#b36">37</ref> we can see the parameters of the single layer network can be constructed considering the longest path from input to output. Recursively we can say it holds for multiple layers. Similar argument can be given in case of erosion layers.</p><p>(ii) For simplicity, we will assume 2-dimensional input. Suppose that the outputs from the first layer are f 1 (x, y) and g 1 (x, y) where f 1 is the output of a dilation neurone and g 1 is the output of an erosion neurone (see <ref type="figure" target="#fig_0">Figure 10</ref>). We write f 1 (x, y) = max{x + a, y + b}, g 1 (x, y) = min{x + c, y + d}.</p><p>After the second layer consisting of a single dilation neurone, we get the output</p><formula xml:id="formula_51">f 2 (x, y) = max{f 1 + a 1 , g 1 + b 1 }.</formula><p>Note that</p><formula xml:id="formula_52">f 2 (x, y) ≤ e ⇐⇒ f 1 + a 1 ≤ e and g 1 + b 1 ≤ e ⇐⇒ f 1 ≤ e − a 1 and g 1 ≤ e − b 1 ⇐⇒ (x + a ≤ e − a 1 and y + b ≤ e − a 1 ) and (x + c ≤ e − b 1 or y + d ≤ e − b 1 ) ⇐⇒ (x, y) ∈ (−∞, γ 1 ] × (−∞, γ 2 ] ∩ ((−∞, γ 3 ] × R ∪ R × (−∞, γ 4 ]) ⇐⇒ (x, y) ∈ (−∞, γ 1 ∧ γ 3 ] × (−∞, γ 2 ] ∪ (−∞, γ 1 ] × (−∞, γ 2 ∧ γ 4 ].</formula><p>[ Input ]</p><p>[ Output ] <ref type="figure" target="#fig_0">Figure 11</ref>: Results on the STARE dataset.</p><formula xml:id="formula_53">[ Ground Truth ] (a) (b) (c) (d)</formula><formula xml:id="formula_54">Note that γ 1 ≤ γ 3 ⇐⇒ a 1 + a ≥ b 1 + c and γ 2 ≤ γ 4 ⇐⇒ a 1 + b ≥ b 1 + d. Therefore, if a 1 + a ≥ b 1 + c and a 1 + b ≥ b 1 + d, then f −1 2 (−∞, e] = (−∞, γ 1 ] × (−∞, γ 2 ]</formula><p>. Thus in this case f 2 can be realized in the architecture D 1 E 0 .</p><p>If, however, a 1 + a &lt; b 1 + c and a 1 + b &lt; b 1 + d, then</p><formula xml:id="formula_55">f −1 2 (−∞, e] = (−∞, γ 3 ] × (−∞, γ 2 ] ∪ (−∞, γ 1 ] × (−∞, γ 4 ],</formula><p>which is not realizable as the sublevel set of a function of D 1 E 0 architecture. and</p><formula xml:id="formula_56">(x + c ≤ e α − b 1 or y + d ≤ e α − b 1 ) ⇐⇒ (x, y) ∈ (−∞, γ 1 ] × (−∞, γ 2 ] ∩ ((−∞, γ 3 ] × R ∪ R × (−∞, γ 4 ]) ⇐⇒ (x, y) ∈ (−∞, γ 1 ∧ γ 3 ] × (−∞, γ 2 ] ∪ (−∞, γ 1 ] × (−∞, γ 2 ∧ γ 4 ]. Note that γ 1 ≤ γ 3 ⇐⇒ a 1 + a ≥ b 1 + c and γ 2 ≤ γ 4 ⇐⇒ a 1 + b ≥ b 1 + d. Therefore, if a 1 + a ≥ b 1 + c and a 1 + b ≥ b 1 + d, then (αf 2 ) −1 (−∞, e] = (−∞, γ 1 ] × (−∞, γ 2 ].</formula><p>Thus, in this case, f 2 can be realized in the architecture D 1 E 0 → L. More explicitly, for β &gt; −1,</p><formula xml:id="formula_57">β max{x + u, y + v} ≤ e ⇐⇒ x ≤ e β − u and y ≤ e β − v.<label>(38)</label></formula><p>Equating e β − u = e α − a 1 − a, e β − v = e α − a 1 − b, we can see that one can take β = α, u = a + a 1 , v = b + a 1 to realize the function αf 2 in the D 1 E 0 → L architecture.</p><p>If, however, a 1 + a &lt; b 1 + c and a 1 + b &lt; b 1 + d, then</p><formula xml:id="formula_58">(αf 2 ) −1 (−∞, e] = (−∞, γ 3 ] × (−∞, γ 2 ] ∪ (−∞, γ 1 ] × (−∞, γ 4 ],</formula><p>which is not realizable as the sublevel set of a function of D 1 E 0 → L architecture. [ Input ]</p><p>[ Output ] − L 4 blocks. A sigmoid activation function is utilized only in the last layer to restrict the output to lie between 0 and 1. The network is designed to take an input of size 512 × 512 × 3 and produces a output of size 512 × 512. So, database images are resized to 512 × 512 using bilinear interpolation, before they are fed to the network. The network is trained using the binary cross-entropy loss and Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with batch size 6. The images obtained from the network is thresholded at 0.5 to get the final result. Since, unlike DRIVE dataset, STARE dataset is not divided in training and testing sets, we decided to train the network using the training set of the DRIVE dataset till convergence, and test the network using test set of the DRIVE data as well as all the images of STARE dataset. <ref type="table" target="#tab_3">Table 2</ref> presents the results in terms of accuracy and area under the curve (AUC). We measure the accuracy as Accuracy =</p><formula xml:id="formula_59">[ Ground Truth ] (a) (b) (c) (d)</formula><p>No. of correct classifications Total no. of inputs</p><p>The <ref type="table" target="#tab_3">Table 2</ref> shows that the network performs well also on the STARE dataset even being trained on DRIVE dataset. For both the datasets, Morph-Net performs comparably, if not better. The ROC curves for both the datasets are given in <ref type="figure" target="#fig_0">Figure 13</ref>. For qualitative comparison, we have presented samples from DRIVE dataset in <ref type="figure" target="#fig_0">Figure 12</ref>. Note that extracted blood vessels are little thicker in a few places in the images of DRIVE <ref type="figure" target="#fig_0">(Figure 12</ref>). This occurs due to use of fixed threshold 0.5 in the final step. We have shown sensitivity (Se), specificity (Sp), positive predictive value (Ppv), negative predictive value (Npv) in <ref type="table" target="#tab_5">Tables 3, 4 for the DRIVE and STARE datasets,</ref> where    the images are resized to 512 × 512 before before into the network. This problem requires both global and local information, so that a large blob with intricate detail of boundary region (region of interest) can be extracted. This suggests us to employ a U-net[40] type of network architecture <ref type="figure" target="#fig_0">(Figure 15</ref>). We have used max-pool to down sample the feature maps and 2D Up Sampling to get back to same size as input. 2D Up Sampling scales up the image by using nearest neighbour. Batch normalization taken to increase the stability of the network. The network is trained using binary cross entropy loss and Adam optimizer. The results have been generated with a network trained for 400 epochs with a batch size 2. The output of the network is binarized with a threshold of 0.5 to get the final output. For quantitative evaluation, we have reported the DICE coefficient scores obtained on the test set in <ref type="table" target="#tab_7">Table 5</ref>. DICE coefficient computes the overlap between the ground-truth and the predicted segmentation mask. So, higher the value, better is the result.</p><formula xml:id="formula_61">Se = T P T P + F N ,<label>(40)</label></formula><formula xml:id="formula_62">Sp = T N T N + F P ,<label>(41)</label></formula><formula xml:id="formula_63">P pv = T P T P + F P ,<label>(42)</label></formula><formula xml:id="formula_64">N pv = T N T N + F N .<label>(43)</label></formula><p>(a) (b) (c) (d) <ref type="figure" target="#fig_0">Figure 16</ref>: Results on the Shenzhen dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Image dehazing</head><p>The hazy image formation process is modelled as <ref type="bibr" target="#b19">[20]</ref> I(x) = t(x)J(x) + (1 − t(x))A,</p><p>where J(x) and I(x) are the haze-free and the observed hazy image, respectively. t(x) ∈ [0, 1] is the scene transmittance that controls the haze in a pixel x, and A denotes the uniform environmental illumination. We modify the model by introducing space-variant environmental illumination A(x) to accommodate more general situations as I(x) = t(x)J(x) + (1 − t(x))A(x),</p><p>Note that A(x) varies much slowly than t(x). Image dehazing methods try to estimate the J(x) having only the I(x) and this usually requires the estimation of t(x) and A(x). This makes dehazing an ill-posed problem. Before presenting our method let us re-write equation 45 as I(x) = t(x)J(x) + K(x).</p><p>Here (1 − t(x))A(x), called airlight map, is expressed as K(x). To dehaze an image, we propose a 2D Morph-Net ( <ref type="figure" target="#fig_0">Figure 17</ref>) that takes hazy image (I(x)) as input and produces estimated the airlight mapK(x) and the transmittance mapt(x) as output. Since, 0 ≤ t(x) ≤ 1 and 0 ≤ K(x) ≤ 1, we have used sigmoid activation function in the last layer. From the estimatedt(x) andK(x) the haze-free image may be obtained as follows.</p><p>J(x) = min I(x) −K(x) t(x) , 1 .</p><p>To train the network, given the hazy image I(x) and corresponding haze-free ground-truth image J(x), we minimize a loss function inspired from the bi-directional consistency loss <ref type="bibr" target="#b28">[29]</ref> based on Structural Similarity Index (SSIM) between two images <ref type="bibr">[48]</ref>. Specifically, the loss function is given by</p><formula xml:id="formula_69">L = L 1 + L 2 .<label>(48)</label></formula><p>L 1 and L 2 are obtained from I(x),Ĩ(x), J(x) andJ(x) as</p><formula xml:id="formula_70">L 1 = DSSIM(I,Ĩ),<label>(49)</label></formula><formula xml:id="formula_71">L 2 = DSSIM(J,J),<label>(50)</label></formula><p>whereĨ</p><formula xml:id="formula_72">(x) = J(x)t(x) +K(x)<label>(51)</label></formula><p>and DSSIM(X,X)</p><formula xml:id="formula_73">= 1 M M i=1 1 − SSIM(P i ,P i ) 2 .<label>(52)</label></formula><p>P i andP i are the i th patches of the X andX, respectively. M is the total number of patches. The network is trained on the training images of the O-HAZE dataset <ref type="bibr" target="#b1">[2]</ref> until convergence of the training error. In <ref type="table">Table 6</ref>, we have reported the PSNR and SSIM of the results obtained on the validation images of the dataset. Some results are shown in <ref type="figure" target="#fig_0">Figure 18</ref> for qualitative evaluation and comparison.</p><p>We have trained the Morph-Net for dehazing using the NYU portion of the D-Hazy dataset and tested them with the Middlebury portion of the D-Hazy dataset. We have reported the results of comparison in <ref type="table" target="#tab_8">Table 7</ref>. We have also shown on real dehazed images in <ref type="figure" target="#fig_0">Figure 19</ref> which is produced by Morph-Net trained on the NYU portion of the D-Hazy dataset.   <ref type="figure" target="#fig_0">Figure 19</ref>: Performance of Morph-Net on real outdoor images. Transmittance and airlight are also shown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of single layer morphological block. It contains an input layer, a dilation-erosion layer with n dilation and m erosion neuron and a linear combination layer with c neurons producing the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Decision boundaries learned by different networks with two hidden neurons. (a) Baseline neural network is able to learn only two planes (b) Maxout network is able to learn two more planes with the help of additional parameters. (c) Morph-Net is able to learn more planes with same number of parameters as NN-ReLU. (d) Using soft version of Morph-Net, smooths the learned decision boundary. This further enhances the discrimination capability of the network while retaining the same number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 6 − L 4 .</head><label>364</label><figDesc>A single 2D morphological block consisting of a Dilation-Erosion layer followed by linear combination of dilated and eroded featuremap. The block is denoted by DE 3×3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of the Morph-Net utilized for blood vessel segmentation in retinal images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results obtained by 2D Morph-Net on two samples of the STARE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Results obtained by 2D Morph-Net on the Shenzhen dataset[17] before thresholding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) Input image (b) Transmittance (c) Airlight (d) Our output (e) Ground truth Results of our 2D Morph-Net on three validation image of the O-HAZE dataset. Transmittance and airlight map is shown along with ground truth for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>a j e a j β k e a k β = lim β→∞ j a j 1 +lim β→∞ a j 1 +</head><label>11</label><figDesc>e a k β k e a k β (by L'Hospital's rule) k =j e (a k −a j )β = j k =j e (a k −a j )β = max j a j . This completes the proof. B Single morphological block as a sum of hinge functions Proposition 2 (Proposition 1 of the main paper). The function computed by a single layer Morph-Net with n dilation and m erosion neurons followed by a linear combination layer computes M(x), which is a sum of multi-order hinge functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Graph of approximation loss with varying morphological neurons in a single morphological block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 3 .</head><label>3</label><figDesc>Let K be a compact subset of R d . Then, over K, any hyperplane w x + b can be represented as an affine combination of d dilation neurons which only depend on K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Test accuracy attained over epochs by different methods with the same number of neurons (400) as in the hidden layer of a single morphological block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Lemma 5 .</head><label>5</label><figDesc>Suppose f and g are real-valued functions on R d . Then f = g if and only if, for all r ∈ R, one has equality of the sub-level sets: f −1 (−∞, r] = g −1 (−∞, r].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>0 consisting only of dilation layers is equivalent to the architecture D m E 0 with a single dilation layer. A similar statement is true if one considers architectures with only purely erosion layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(Figure 12 :Figure 13 :</head><label>1213</label><figDesc>iv) It can be proved in the same way as (ii) D 2D morphological network experiments D.1 Retinal segmentation The DRIVE dataset has 40 eye-fundus color images captured with Canon CR5 non-mydriatic 3CCD camera with a 45 • field-of-view (FOV). Each image is of size 768 × 584. The dataset is divided into two groups: 20 training (a) input (b) Morph-Net (c) Ground Truth Results obtained by 2D Morph-Net on two samples of the DRIVE dataset. ROC curve on the STARE and DRIVE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>U-Net like architecture of the 2D Morph-Net utilized for lungs segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Results of our 2D Morph-Net on the Middlebury part of the D-Hazy dataset. Transmittance and airlight maps are shown along with the ground truth for comparison. The network is trained with the NYU part of the D-Hazy dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy achieved on the UC Merced and WHU-RS19 datasets</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Parameters (Millions)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">UCMerced</cell><cell></cell><cell cols="4">WHU-RS19</cell></row><row><cell cols="2">LeNet[22]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.42</cell><cell></cell><cell></cell><cell></cell><cell cols="4">53.29 ± 0.86</cell><cell></cell><cell cols="4">48.26 ± 2.83</cell></row><row><cell cols="3">Depth-LeNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.04</cell><cell></cell><cell></cell><cell></cell><cell cols="4">54.81 ± 1.25</cell><cell></cell><cell cols="4">47.19 ± 2.43</cell></row><row><cell cols="4">DeepMorphLeNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.04</cell><cell></cell><cell></cell><cell></cell><cell cols="4">56.52 ± 1.74</cell><cell></cell><cell cols="4">52.91 ± 2.60</cell></row><row><cell cols="4">AlexNet-based[21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.5</cell><cell></cell><cell></cell><cell></cell><cell cols="4">72.62 ± 1.05</cell><cell></cell><cell cols="4">64.38 ± 2.93</cell></row><row><cell cols="5">Depth-AlexNet-based</cell><cell></cell><cell></cell><cell></cell><cell>7.47</cell><cell></cell><cell></cell><cell></cell><cell cols="4">73.14 ± 1.43</cell><cell></cell><cell cols="4">63.27 ± 2.14</cell></row><row><cell cols="6">DeepMorphAlexNet[31]</cell><cell></cell><cell></cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell cols="9">76.86 ± 1.97 68.20 ± 2.75</cell></row><row><cell cols="4">Morph-Net (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.23</cell><cell></cell><cell></cell><cell></cell><cell cols="9">74.75 ± 1.16 71.23 ± 2.91</cell></row><row><cell>4 × 4 4</cell><cell>→</cell><cell>8</cell><cell>4 × 4 8</cell><cell>→</cell><cell>8</cell><cell>4 × 4 8</cell><cell>→</cell><cell>8</cell><cell>4 × 4 8</cell><cell>→</cell><cell>8</cell><cell>4 × 4 8</cell><cell>→</cell><cell>8</cell><cell>4 × 4 8</cell><cell>→</cell><cell>8</cell><cell>3 × 3 8</cell><cell>→</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>[ 40 ]</head><label>40</label><figDesc>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015. 9, 29 [41] Jean Serra. Image Analysis and Mathematical Morphology. Academic Press, Inc., Orlando, FL, USA, 1983. 2 [42] João VB Soares, Jorge JG Leandro, Roberto M Cesar, Herbert F Jelinek, and Michael J Cree. Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification. IEEE Transactions on medical Imaging, 25(9):1214-1222, 2006. 23 [43] Joes Staal, Michael D Abràmoff, Meindert Niemeijer, Max A Viergever, and Bram Van Ginneken. Ridge-based vessel segmentation in color images of the retina. IEEE transactions on medical imaging, 23(4):501-509, 2004. 23</figDesc><table /><note>[44] Stanley R Sternberg. Grayscale morphology. Computer vision, graphics, and image processing, 35(3):333-355, 1986.7 [45] P. Sussner. Morphological perceptron learning. In Proceedings of the 1998 IEEE International Symposium on Intel- ligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell, pages 477-482, September 1998. 3 [46] Peter Sussner and Estevão Laureano Esmi. Morphological perceptrons with competitive learning: Lattice-theoretical framework and constructive learning algorithm. Information Sciences, 181(10):1929-1950, May 2011. 3 [47] Shuning Wang and Xusheng Sun. Generalization of hinging hyperplanes. IEEE Transactions on Information Theory, 51(12):4425-4431, 2005. 5 [48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600-612, 2004. 10, 32 [49] Gui-Song Xia, Wen Yang, Julie Delon, Yann Gousseau, Hong Sun, and Henri Maître. Structural High-resolution Satellite Image Indexing. In Wagner W., Székely, and B., editors, ISPRS TC VII Symposium -100 Years ISPRS, volume XXXVIII, pages 298-303, Vienna, Austria, July 2010. 8 [50] Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270-279, 2010.8 [51] Erik Zamora and Humberto Sossa. Dendrite morphological neurons trained by stochastic gradient descent. Neurocom- puting, 260:420-431, October 2017. 3 [52] He Zhang and Vishal M Patel. Densely connected pyramid dehazing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3194-3203, 2018. 31 [53] He Zhang, Vishwanath Sindagi, and Vishal M Patel. Multi-scale single image dehazing using perceptual pyramid deep network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 902-911, 2018. 31 [54] Qingsong Zhu, Jiaming Mai, and Ling Shao. A fast single image haze removal algorithm using color attenuation prior. IEEE transactions on image processing, 24(11):3522-3533, 2015. 31</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy and AUC achieved on the test set by different networks for the DRIVE and STARE datasets.</figDesc><table><row><cell>Methods</cell><cell cols="2">DRIVE</cell><cell cols="2">STARE</cell></row><row><cell></cell><cell cols="5">Accuracy AUC Accuracy AUC</cell></row><row><cell>Marin et al. [26]</cell><cell>94.50</cell><cell>0.958</cell><cell>95.20</cell><cell cols="2">0.976</cell></row><row><cell>Staal et al. [43]</cell><cell>94.40</cell><cell>0.952</cell><cell>-</cell><cell>-</cell></row><row><cell>Niemeijer et al. [30]</cell><cell>94.10</cell><cell>0.929</cell><cell>-</cell><cell>-</cell></row><row><cell>Soares et al. [42]</cell><cell>94.60</cell><cell>0.961</cell><cell>94.80</cell><cell cols="2">0.967</cell></row><row><cell>Ricci et al. [35]</cell><cell>95.90</cell><cell>0.963</cell><cell>96.40</cell><cell cols="2">0.968</cell></row><row><cell>2D Morph-Net</cell><cell>96.50</cell><cell>0.977</cell><cell>93.40</cell><cell cols="2">0.937</cell></row><row><cell cols="4">(iii) The proof is a simple modification of the proof of (ii). For α &gt; 0,</cell><cell></cell></row><row><cell cols="5">αf 2 (x, y) ≤ e ⇐⇒ f 1 + a 1 ≤ ⇐⇒ f 1 ≤ e α − a 1 and g 1 ≤ e α and g 1 + b 1 ≤ e − b 1 e α α ⇐⇒ (x + a ≤ e α − a 1 and y + b ≤</cell><cell>e α</cell><cell>− a 1 )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Figure 14 :</head><label>14</label><figDesc>Results on the DRIVE dataset. and 20 test images. The STARE dataset consists of 20 eye-fundus color images captured with a TopCon TRV-50 fundus camera at 35 • FOV. Each image of STARE datasets has a size of 700 × 605.</figDesc><table><row><cell>For this task we have employed a 2D Morph-Net containing 7 DE 3×3 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Individual results on the DRIVE dataset.</figDesc><table><row><cell>Image</cell><cell>Se</cell><cell>Sp</cell><cell>Ppv</cell><cell>Npv Acc</cell></row><row><cell>1</cell><cell>0.64</cell><cell>0.99</cell><cell>0.90</cell><cell>0.97 0.96</cell></row><row><cell>2</cell><cell>0.61</cell><cell>0.99</cell><cell>0.91</cell><cell>0.97 0.96</cell></row><row><cell>3</cell><cell>0.47</cell><cell>1.00</cell><cell>0.95</cell><cell>0.94 0.94</cell></row><row><cell>4</cell><cell>0.79</cell><cell>0.99</cell><cell>0.89</cell><cell>0.98 0.97</cell></row><row><cell>5</cell><cell>0.63</cell><cell>0.99</cell><cell>0.92</cell><cell>0.96 0.96</cell></row><row><cell>6</cell><cell>0.69</cell><cell>0.99</cell><cell>0.88</cell><cell>0.97 0.96</cell></row><row><cell>7</cell><cell>0.80</cell><cell>0.99</cell><cell>0.82</cell><cell>0.98 0.97</cell></row><row><cell>8</cell><cell>0.87</cell><cell>0.98</cell><cell>0.82</cell><cell>0.99 0.97</cell></row><row><cell>9</cell><cell>0.69</cell><cell>0.99</cell><cell>0.91</cell><cell>0.97 0.96</cell></row><row><cell>10</cell><cell>0.80</cell><cell>0.98</cell><cell>0.83</cell><cell>0.98 0.97</cell></row><row><cell>11</cell><cell>0.73</cell><cell>0.99</cell><cell>0.87</cell><cell>0.97 0.97</cell></row><row><cell>12</cell><cell>0.74</cell><cell>0.99</cell><cell>0.89</cell><cell>0.98 0.97</cell></row><row><cell>13</cell><cell>0.75</cell><cell>0.99</cell><cell>0.85</cell><cell>0.98 0.97</cell></row><row><cell>14</cell><cell>0.77</cell><cell>0.99</cell><cell>0.82</cell><cell>0.98 0.97</cell></row><row><cell>15</cell><cell>0.77</cell><cell>0.99</cell><cell>0.83</cell><cell>0.98 0.97</cell></row><row><cell>16</cell><cell>0.82</cell><cell>0.98</cell><cell>0.75</cell><cell>0.99 0.97</cell></row><row><cell>17</cell><cell>0.75</cell><cell>0.99</cell><cell>0.88</cell><cell>0.98 0.97</cell></row><row><cell>18</cell><cell>0.69</cell><cell>0.99</cell><cell>0.90</cell><cell>0.97 0.96</cell></row><row><cell>19</cell><cell>0.75</cell><cell>0.99</cell><cell>0.84</cell><cell>0.98 0.97</cell></row><row><cell>20</cell><cell>0.61</cell><cell>0.99</cell><cell>0.88</cell><cell>0.96 0.96</cell></row><row><cell cols="5">Average 0.718 0.989 0.867 0.973 0.96</cell></row><row><cell>D.2 Lungs segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>In this experiment, we have used the publicly available Shenzhen chest X-Ray imageset [17]. This dataset is collected by Shenzhen No.3 Hospital in Shenzhen, Guangdong Province, China. The dataset contains 326 normal and 336 abnormal X-Ray images showing various manifestations of tuberculosis. Each image is of size approxi- mately 3000 × 3000. We have randomly selected 90% of the data for training and remaining 10% as test set. All</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on the STARE dataset</figDesc><table><row><cell>Image</cell><cell>Se</cell><cell>Sp</cell><cell>Ppv Npv Acc</cell></row><row><cell>1</cell><cell cols="3">0.25 0.99 0.72 0.96 0.95</cell></row><row><cell>2</cell><cell cols="3">0.69 0.97 0.74 0.96 0.94</cell></row><row><cell>3</cell><cell cols="3">0.23 1.00 0.87 0.93 0.93</cell></row><row><cell>4</cell><cell cols="3">0.40 0.99 0.91 0.93 0.93</cell></row><row><cell>5</cell><cell cols="3">0.15 1.00 0.94 0.95 0.95</cell></row><row><cell>6</cell><cell cols="3">0.57 1.00 0.97 0.94 0.94</cell></row><row><cell>7</cell><cell cols="3">0.54 1.00 0.98 0.93 0.93</cell></row><row><cell>8</cell><cell cols="3">0.10 1.00 0.94 0.91 0.91</cell></row><row><cell>9</cell><cell cols="3">0.69 0.99 0.94 0.95 0.95</cell></row><row><cell>10</cell><cell cols="3">0.12 1.00 0.93 0.94 0.94</cell></row><row><cell>11</cell><cell cols="3">0.30 1.00 0.97 0.92 0.92</cell></row><row><cell>12</cell><cell cols="3">0.40 0.99 0.82 0.97 0.96</cell></row><row><cell>13</cell><cell cols="3">0.46 1.00 0.97 0.92 0.92</cell></row><row><cell>14</cell><cell cols="3">0.35 1.00 0.98 0.92 0.92</cell></row><row><cell>15</cell><cell cols="3">0.62 1.00 0.95 0.95 0.95</cell></row><row><cell>16</cell><cell cols="3">0.36 1.00 0.97 0.92 0.93</cell></row><row><cell>17</cell><cell cols="3">0.42 0.99 0.93 0.90 0.91</cell></row><row><cell>18</cell><cell cols="3">0.54 1.00 0.98 0.93 0.94</cell></row><row><cell>19</cell><cell cols="3">0.55 1.00 0.94 0.97 0.97</cell></row><row><cell>20</cell><cell cols="3">0.47 1.00 0.96 0.91 0.92</cell></row><row><cell cols="4">Average 0.41 0.995 0.92 0.94 0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Attained DICE coefficient on the Shenzhen dataset by different networks.</figDesc><table><row><cell>Method</cell><cell>Candemir al.[8]</cell><cell>et</cell><cell>ED-CNN[18]</cell><cell>FCN[32]</cell><cell>2D Morph-Net</cell></row><row><cell>Dice Coefficient</cell><cell>94.1</cell><cell></cell><cell>97.4</cell><cell>97.7</cell><cell>95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results obtained on the Middlebury portion of the D-Hazy dataset.</figDesc><table><row><cell>Image</cell><cell>He et al. [15] Ren et al. [34] Berman [5] AOD-Net [24] Morph-Net</cell></row><row><cell></cell><cell>PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://drive.grand-challenge.org/ 2 http://cecas.clemson.edu/ ahoover/stare/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 17</ref><p>: Architecture of the 2D Morph-Net utilized for image dehazing. The network outputs both transmittance and airlight maps which are later utilized to obtain the dehazed image.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Muhammad Aminul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keller</surname></persName>
		</author>
		<title level="m">Deep morphological hit-or-miss transform neural network. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1912</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">O-haze: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Morphological coding of color images by vector connected filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Symposium on Signal Processing and Its Applications</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Orthonormal Basis Lattice Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barmpoutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Conference on Fuzzy Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-local Image Dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hinging hyperplanes for regression, classification, and function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="999" to="1013" />
			<date type="published" when="1993-05-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sema</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannappan</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Musco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Basic properties of the soft maximum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics Working Paper Series</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>UT MD Anderson Cancer Center Department of</orgName>
		</respStmt>
	</monogr>
	<note>Working Paper 70</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Morphology neural networks: An introduction with applications. Circuits, Systems and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-06" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="177" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A morphological perceptron with gradient-based learning for Brazilian stock market forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep morphological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Fehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">107246</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computer and Robot Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">G</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single Image Haze Removal Using Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sema</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yì-Xiáng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu-Xuan</forename><surname>Wáng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lung image ssgmentation using deep learning methods and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980. 26</idno>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Theorie der horizontalen sichtweite. Beitrage zur Physik der freien Atmosphare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Koschmieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1924" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AOD-Net: All-In-One Dehazing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bipolar morphological neural networks: convolution without multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Limonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Matveev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">V</forename><surname>Arlazarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth International Conference on Machine Vision</title>
		<imprint>
			<biblScope unit="volume">11433</biblScope>
			<biblScope unit="page">114333</biblScope>
		</imprint>
	</monogr>
	<note>ICMV 2019</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">Emilio</forename><surname>Gegúndez-Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Manuel</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A learning framework for morphological operators using counter-harmonic mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mathematical Morphology and Its Applications to Signal and Image Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image restoration by learning morphological openingclosing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjan</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moni</forename><surname>Shankar Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhabatosh</forename><surname>Chanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Morphology-Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image dehazing by joint estimation of transmittance and airlight using bi-directional consistency loss minimized fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjan</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchayan</forename><surname>Santra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhabatosh</forename><surname>Chanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparative study of retinal vessel segmentation methods on a new publicly available database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meindert</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joes</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bram Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical imaging 2004: image processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5370</biblScope>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An introduction to deep morphological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiller</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><forename type="middle">Dalla</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Robson</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefersson A Dos</forename><surname>Santos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01751</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional neural network for lungs segmentation from chest x-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabia</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taimur</forename><surname>Muhammad Usman Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single Image Dehazing via Multiscale Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wenqi Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzo</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An introduction to morphological neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sussner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th International Conference on Pattern Recognition</title>
		<meeting>13th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="709" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lattice algebra approach to single-neuron computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urcid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="282" to="295" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two lattice metrics dendritic computing for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urcid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Juan-Carlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning in Lattice Neural Networks that Employ Dendritic Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><forename type="middle">X</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Urcid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence Based on Lattice Theory, Studies in Computational Intelligence</title>
		<editor>Vassilis G. Kaburlasos and Gerhard X. Ritter</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="44" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
