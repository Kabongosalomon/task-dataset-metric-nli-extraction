<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
							<email>rikvannoord@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">CLCG University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CLCG University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
							<email>johan.bos@rug.nl</email>
							<affiliation key="aff2">
								<orgName type="institution">CLCG University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Character-level models have obtained impressive performance on a number of NLP tasks, ranging from the classic POS-tagging <ref type="bibr" target="#b61">(Santos and Zadrozny, 2014)</ref> to complex tasks such as Discourse Representation Structure (DRS) parsing <ref type="bibr" target="#b50">(van Noord et al., 2018b)</ref>. However, this was before the large pretrained language models <ref type="bibr" target="#b56">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref> took over the field, with the consequence that for most NLP tasks, state-ofthe-art performance is now obtained by fine-tuning on one of these models (e.g., <ref type="bibr" target="#b4">Conneau et al., 2020)</ref>.</p><p>Does this mean that, despite a long tradition of being used in language-related tasks (see Section 2.1), character-level representations are no longer useful? We try to answer this question by looking at semantic parsing, specifically DRS parsing <ref type="bibr">(Abzianidze et al., 2017;</ref><ref type="bibr" target="#b49">van Noord et al., 2018a)</ref>. We aim to answer the following research questions:</p><p>1. Do pretrained language models (LMs) outperform character-level models for DRS parsing?</p><p>2. Can character and LM representations be combined to improve performance, and if so, what is the best method of combining them? 3. How do these improvements compare to adding linguistic features? 4. Are the improvements robust across different pretrained language models, languages, and data sets? 5. On what type of sentences do character-level representations specifically help?</p><p>Why semantic parsing? Semantic parsing is the task of automatically mapping natural language utterances to interpretable meaning representations. The produced meaning representations can then potentially be used to improve downstream NLP applications (e.g., <ref type="bibr" target="#b25">Issa et al., 2018;</ref><ref type="bibr" target="#b64">Song et al., 2019;</ref><ref type="bibr" target="#b47">Mihaylov and Frank, 2019)</ref>, though the introduction of large pretrained language models has shown that explicit formal meaning representations might not be a necessary component to achieve high accuracy. However, it is now known that these models lack reasoning capabilities, often simply exploiting statistical artifacts in the data sets, instead of actually understanding language <ref type="bibr" target="#b48">(Niven and Kao, 2019;</ref><ref type="bibr" target="#b46">McCoy et al., 2019)</ref>. Moreover, <ref type="bibr" target="#b12">Ettinger (2020)</ref> found that the popular BERT model <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> completely failed to acquire a general understanding of negation. <ref type="bibr">Related, Bender and Koller (2020)</ref> contend that meaning cannot be learned from form alone, and argue for approaches that focus on grounding the language (communication) in the real world. We believe formal meaning representations therefore have an important role to play in future semantic applications, as semantic parsers produce an explicit model of a real-world interpretation.</p><p>Why Discourse Representation Structures? DRS parsing is a task that combines logical, pragmatic and lexical components of semantics in a arXiv:2011.04308v1 [cs.CL] 9 Nov 2020</p><p>Sent: I haven't been to Boston since 2013.</p><formula xml:id="formula_0">b1 NEGATION b2 b3 REF x1 b1 REF t1</formula><p>b3 Name x1 "boston" b1 TPR t1 "now" b3 PRESUPPOSITION b2 b1 time "n.08" t1 b3 city "n.01" x1 b2 REF e1</p><p>b2 Start e1 t2 b2 Theme e1 "speaker" b2 REF t2 b2 Time e1 t1 b2 time "n.08" t2 b2 be "v.03" e1 b2 Location e1 x1 b2 YearOfCentury t2 "2013" city.n.01 (x1) Name (x1, "boston") x1 be.v.03 (e1) Theme (e1, "speaker") Time (e1, t1) Location (e1, x1) Start (e1, t2) time.n.08 (t2) <ref type="bibr">YearOfCentury (t2, "2013")</ref>  single meaning representation. The task is complex and comprises other NLP tasks, such as semantic role labeling, word sense disambiguation, co-reference resolution and named entity tagging. Also, DRSs show explicit scope for certain operators, which allows for a more principled and linguistically motivated treatment of negation, modals and quantification, as has been advocated in formal semantics. Moreover, DRSs can be translated to formal logic, which allows for automatic forms of inference by third parties. Lastly, annotated DRSs are available in four languages (Abzianidze et al., 2017, see Section 3.3), allowing us to evaluate our models on multiple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background 2.1 Character-level models</head><p>The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech <ref type="bibr" target="#b62">(Sejnowski and Rosenberg, 1987)</ref>, parallel text alignment <ref type="bibr" target="#b2">(Church, 1993)</ref>, grapheme to phoneme conversion <ref type="bibr" target="#b30">(Kaplan and Kay, 1994)</ref>, language identification <ref type="bibr" target="#b10">(Dunning, 1994)</ref>, topical similarity prediction <ref type="bibr">(Cavnar, 1994)</ref>, named entity recognition <ref type="bibr" target="#b32">(Klein et al., 2003)</ref>, authorship attribution <ref type="bibr" target="#b54">(Peng et al., 2003)</ref> and statistical machine translation <ref type="bibr" target="#b71">(Vilar et al., 2007)</ref>. More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling <ref type="bibr" target="#b67">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b31">Kim et al., 2016;</ref><ref type="bibr">Bojanowski et al., 2017)</ref>, but also for a range of other tasks, including tokenization <ref type="bibr" target="#b14">(Evang et al., 2013)</ref>, <ref type="bibr">POS-tagging (Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b57">Plank et al., 2016)</ref>, dependency parsing <ref type="bibr">(Ballesteros et al., 2015;</ref><ref type="bibr" target="#b68">Vania et al., 2018)</ref> and neural machine translation <ref type="bibr" target="#b1">(Chung et al., 2016;</ref><ref type="bibr" target="#b5">Costa-jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b44">Luong and Manning, 2016;</ref><ref type="bibr">Cherry et al., 2018)</ref>.</p><p>In semantic parsing, if character-level represen-tations are employed, they are commonly used in combination with non-contextual word-level representations <ref type="bibr" target="#b37">(Lewis et al., 2016;</ref><ref type="bibr">Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b23">Groschwitz et al., 2018;</ref><ref type="bibr">Cai and Lam, 2019)</ref>. There are a few recent studies that did use character-level representations in combination with BERT <ref type="bibr">(Zhang et al., 2019a,b;</ref><ref type="bibr">Cai and Lam, 2020)</ref>, though only <ref type="bibr" target="#b77">Zhang et al. (2019a)</ref> provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and <ref type="bibr" target="#b50">van Noord et al. (2018b)</ref>, on the other hand, used solely character-level representations in an end-to-end fashion, using a bi-LSTM sequence-to-sequence model, which outperformed word-based models that employed non-contextual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discourse Representation Structures</head><p>DRSs are formal meaning representations introduced by Discourse Representation Theory <ref type="bibr" target="#b29">(Kamp and Reyle, 1993)</ref> with the aim to capture the meaning of texts ( <ref type="figure" target="#fig_0">Figure 1</ref>).  <ref type="bibr">(Fellbaum, 1998)</ref> or <ref type="bibr">VerbNet (Bonial et al., 2011)</ref>. Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts <ref type="bibr" target="#b27">(Johnson and Klein, 1986;</ref><ref type="bibr" target="#b72">Wada and Asher, 1986;</ref><ref type="bibr">Bos, 2001)</ref>. The first open domain DRS parser is Boxer <ref type="bibr">(Bos, 2008</ref><ref type="bibr">(Bos, , 2015</ref>, which is a combination of rule-based and statistical models. <ref type="bibr" target="#b35">Le and Zuidema (2012)</ref> used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, <ref type="bibr" target="#b40">Liu et al. (2018)</ref> proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work <ref type="bibr" target="#b41">(Liu et al., 2019a)</ref> they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. <ref type="bibr" target="#b19">Fu et al. (2020)</ref> in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work <ref type="bibr" target="#b50">(van Noord et al., 2018b)</ref>, we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters as the preferred representation. In follow-up work, we improved on these scores by adding linguistic features <ref type="bibr" target="#b52">(van Noord et al., 2019</ref>). The first shared task on DRS parsing (Abzianidze et al., 2019) sparked more interested in the topic, with a system based on stack-LSTMs <ref type="bibr" target="#b13">(Evang, 2019</ref>) and a neural graph-based system <ref type="bibr" target="#b15">(Fancellu et al., 2019</ref>). The best system <ref type="bibr" target="#b42">(Liu et al., 2019b)</ref> used a similar approach as van Noord et al. (2018b), but swapped the bi-LSTM encoder for a Transformer. We will compare our approach to these models in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Architecture</head><p>As our baseline system, we start from a fairly standard sequence-to-sequence model with attention <ref type="bibr">(Bahdanau et al., 2015)</ref>, implemented in AllenNLP . <ref type="bibr">1</ref> We improve on this model 1 https://github.com/RikVN/allennlp in a number of ways, mainly based on Nematus <ref type="bibr" target="#b63">(Sennrich et al., 2017)</ref>: (i) we initialize the decoder hidden state with the mean of all encoder states, (ii) we add an extra linear layer between this mean encoder state and the initial decoder state and (iii) we add an extra linear layer after each decoder state.</p><p>Specifically, given a source sequence (s 1 , . . . , s l ) of length l, and a target sequence (t 1 , . . . , t k ) of length k, let e i be the embedding of source symbol i, let h i be the encoder hidden state at source position i and let d j be the decoder state at target position j. A single forward encoder state is obtained as follows:</p><formula xml:id="formula_1">− → h i = LSTM( − → h i−1 , e i ).</formula><p>The final state is obtained by concatenating the forward and backward hidden states,</p><formula xml:id="formula_2">h i = [ − → h i ; ← − h i ].</formula><p>The decoder is initialized with the average over all encoder states: c tok = l i=1 h i / l and d 0 = tanh (W init c tok ). Characters in one encoder We will experiment with adding character-level information in either one or two encoders. For one encoder, we use char-CNN <ref type="bibr" target="#b31">(Kim et al., 2016)</ref>, which runs a <ref type="bibr">Convolutional Neural Network (LeCun et al., 1990)</ref> over the characters for each token. It applies convolution layers for certain widths, which in essence select n-grams of characters. For each width, it does this a predefined number of times, referred to as the number of filters. The filter vectors form a matrix, which is then pooled to a vector by taking the max value of each initial filter vector. A detailed schematic overview of this procedure is shown in Appendix A. However, we usually do not look at only a single width, but at a range of widths, e.g., <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref>. In that case, we simply concatenate the resulting vectors to obtain our final char-CNN embedding: e char i = [e w1 ; e w2 ; e w3 ; e w4 ; e w5 ]. Each widthfilter combination has independent learnable parameters. Finally, the char-CNN embedding is concatenated to the token-level representation, which is fed to the encoder: e i = [e tok i ; e char i ]. Characters in two encoders In the two-encoder setup, we run separate (but structurally identical) bi-LSTM encoders over the tokens and characters, and concatenate the resulting context vector before we feed it to the decoder:</p><formula xml:id="formula_3">d 0 = tanh (W init [c tok ; c char ])</formula><p>In the decoder, we replace the LSTM with a doubly-attentive LSTM, based on the doubly- attentive <ref type="bibr">GRU (Calixto et al., 2017)</ref>. We apply soft-dual attention <ref type="bibr" target="#b28">(Junczys-Dowmunt and Grundkiewicz, 2017)</ref> to be able to attend over both encoders in the decoder (also see <ref type="figure" target="#fig_1">Figure 2</ref>):</p><formula xml:id="formula_4">d j = LSTM 1 d j−1 , e t j−1 a j = ATT C tok , d j ; ATT C char , d j d j = LSTM 2 d j , a j</formula><p>Here, e t j−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and d j the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4.</p><p>This type of multi-source model is commonly used to represent different languages, e.g., in machine translation <ref type="bibr" target="#b79">(Zoph and Knight, 2016;</ref><ref type="bibr" target="#b18">Firat et al., 2016)</ref> and semantic parsing <ref type="bibr" target="#b66">(Susanto and Lu, 2017;</ref><ref type="bibr" target="#b11">Duong et al., 2017)</ref>, though it has also been successfully applied in multi-modal translation <ref type="bibr" target="#b39">(Libovický and Helcl, 2017)</ref>, multi-framework semantic parsing <ref type="bibr" target="#b65">(Stanovsky and Dagan, 2018)</ref> and adding linguistic information <ref type="bibr" target="#b6">(Currey and Heafield, 2018;</ref><ref type="bibr" target="#b52">van Noord et al., 2019)</ref>. To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>We also experiment with the Transformer model <ref type="bibr" target="#b69">(Vaswani et al., 2017)</ref>, using the stacked self attention model as implemented in AllenNLP. A possible advantage of this model is that it might handle longer sentences and documents better. However, it might be harder to tune <ref type="bibr" target="#b58">(Popel and Bojar, 2018)</ref> 2 and its improved performance has mainly been shown for large data sets, as opposed to the generally smaller semantic parsing data sets (Section 3.3). Indeed, we cannot outperform the LSTM architecture (see Section 4), even when tuning more extensively. We therefore do not experiment with adding character-level representations to this architecture, though the char-CNN can be added similarly as for the LSTM model. Hyper-parameters To make a fair comparison, we conduct an independent hyper-parameter search on the development set for all nine input text representations (see Section 3.2) across the two neural architectures, starting from the settings of van <ref type="bibr" target="#b52">Noord et al. (2019)</ref>. We found that the best settings were very close for all systems, with the only notable difference that the learning rate of the Transformer models is considerably smaller than for the bi-LSTM models (0.0002 vs 0.001). <ref type="bibr">3</ref> For the char-CNN model, we use 100 filters, an embedding size of 75 and n-gram filter sizes of [1, 2, 3] for English and <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref> for German, Italian and Dutch. For experiments where we add characters or linguistic features, the only extra search we do is the size of the hidden vector of the RNN encoder (300 − 600), since this vector now has to contain more information, and could potentially benefit from a larger size. Note that (possible) improved performance is not simply due to larger model capacity, since during tuning of the baseline models a larger RNN hidden size did not result in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representations</head><p>We will experiment with five well-known pretrained language models: ELMO <ref type="bibr" target="#b56">(Peters et al., 2018)</ref>, BERT base/large <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and ROBERTA base/large <ref type="bibr" target="#b43">(Liu et al., 2019c)</ref>. <ref type="bibr">4</ref> The performance of these five large LMs is contrasted with results of a character-level model and three wordbased models. The word-based models either learn the embeddings from scratch or use non-contextual GLOVE <ref type="bibr" target="#b55">(Pennington et al., 2014)</ref> or FASTTEXT <ref type="bibr" target="#b22">(Grave et al., 2018)</ref> embeddings. Pre-and postprocessing of the DRSs is done using the method described in <ref type="bibr" target="#b50">van Noord et al. (2018b)</ref>. <ref type="bibr">5</ref> The DRSs are linearized, after which the variables are rewritten to a relative representation. The character-level model has character representations for the DRS concepts and constants, but not for variables, roles and operators. For all word-level models, the DRS concepts are initialized with GLOVE embeddings, while the other target tokens are learned from scratch. BERT specifics For the BERT models, we obtained the best performance by only keeping the vector of the first WordPiece per original token (e.g., only keep play out of play ##ing). For ROBERTA, it was best to use the WordPiece tokenization as is. Since linguistic features are added on token level, we duplicate the semantic tags for multi-piece tokens of ROBERTA in <ref type="table" target="#tab_7">Table 5</ref>. Interestingly, we found that for both BERT and ROBERTA, it was best to keep the pretrained weights frozen. This was not a small difference: models using finetuning always obtained low scores (45 to 60).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data and Evaluation</head><p>We use PMB releases 2.2.0 and 3.0.0 6 in our experiments <ref type="table">(Table 1)</ref>. The latter is a larger and more diverse extension of 2.2.0, which will be used for most of our experiments. We use 2.2.0 to compare to previous work and to verify that our results are robust across datasets. The PMB releases contain DRSs for four languages (English, German, Italian and Dutch) for three levels of annotation: gold (fully manually checked), silver (partially manually corrected) and bronze (no manual corrections). To make a fair comparison to previous work, we only employ the gold and silver data, by pretraining on gold + silver data and subsequently fine-tuning on only the gold data. If there is no gold train data available, we train on silver + bronze and fine-tune on silver.  Linguistic features We want to contrast our method of character-level information to adding sources of linguistic information. Based on van Noord et al. <ref type="formula">(2019)</ref>, we employ these five sources: part-of-speech tags (POS), dependency parses (DEP), lemmas (LEM), CCG supertags (CCG) and semantic tags (SEM). For the first three sources, we use Stanford CoreNLP  to parse the documents in our dataset. The CCG supertags are obtained by using easyCCG <ref type="bibr" target="#b38">(Lewis and Steedman, 2014)</ref>. For semantic tagging, we train our own trigram-based tagger using TnT <ref type="bibr">(Brants, 2000)</ref>. 7 <ref type="table" target="#tab_3">Table 2</ref> shows a tagged example sentence for all five sources of information. Moreover, we also include non-contextual GLOVE and FASTTEXT embeddings as an extra source of information.</p><p>We add these sources of linguistic information in the same way as we add the character-level information, in either one or two encoders (see Section 3.1). In two encoders, we can use the exact same architecture. For one encoder, we (obviously) do not use the char-CNN, but learn a separate embedding for the tags (of size 200), that is then concatenated to the token-level representation, i.e., e i = [e tok i ; e ling i ]. If we use two encoders with a LM, characters and linguistic information (e.g., <ref type="table" target="#tab_6">Table 4</ref>), the characters are added separately in the second encoder, while the LM and linguistic information representations are added in the first encoder. Evaluation We compare the produced DRSs to the gold standard using Counter <ref type="bibr" target="#b49">(van Noord et al., 2018a)</ref>, which calculates micro precision, recall and F1-score based on the number of matching clauses. <ref type="bibr">8</ref> We use Referee <ref type="bibr" target="#b50">(van Noord et al., 2018b)</ref> to ensure that the produced DRSs are syntactically and semantically well-formed (i.e., no free variables, no loops in subordinate relations) and form a connected graph. DRSs that are ill-formed get an F1-score of 0.0. All shown scores are averaged F1-scores over five training runs of the system, in which the same five random seeds are used. 9 For significance testing we use approximate randomization <ref type="bibr" target="#b53">(Noreen, 1989)</ref>, with α = 0.05 and R = 1000.</p><p>We also introduce and release DRS-JURY. This program provides a detailed overview of the performance of a DRS parser, but can also compare experiments, possibly over multiple runs. Features include significance testing, semantic tag analysis (Section 5.1), sentence length plotting (Section 5.2), new detailed Counter scores (Appendix D), and analysing (relative) best/worst produced DRSs (Appendix E). We hope this is a step in the direction of a more principled way of evaluating DRS parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>LMs vs char-level models DRS parsing is no exception to the general trend in NLP: it is indeed the case that the pretrained language models outperform the char-only model <ref type="table" target="#tab_5">(Table 3)</ref>. Interestingly, the Transformer model has worse performance for all representations. 10 Surprisingly, we find that BERT-BASE is the best model, though the differences are small. <ref type="bibr">11</ref> We use this model in further experiments (referred to as BERT). Adding characters to BERT We can see the impact of adding characters to BERT (first row of results in <ref type="table" target="#tab_6">Table 4</ref>). For both methods, it results in a clear and significant improvement over the BERT-only baseline, 87.6 versus 88.1. Adding linguistic features to BERT However, another common method of improving performance is adding linguistic features to the tokenlevel representations. We try a range of linguistic features (described in Section 3.3), that are added in either one or two encoders. We see in the first two columns of results of <ref type="table" target="#tab_6">Table 4</ref> that even though linguistic information sources indeed do improve performance (up to 0.4 absolute), there is no single source that can beat adding just the character-level representations (88.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining characters and linguistic features</head><p>An obvious follow-up question is whether we still see improvements for character-level models when 9 Standard deviations are omitted for brevity, though available for all experiments here: https://github.com/ RikVN/Neural_DRS/ 10 The Transformer models were even tuned longer, since they were more sensitive to small hyperparameter changes. 11 BERT-BASE significantly outperformed all the other models, except for BERT-LARGE.   also adding linguistic information. In a single encoder, adding characters (third column of results in <ref type="table" target="#tab_6">Table 4</ref>) is beneficial for 6 out of 7 linguistic sources (i.e., compared to the first column of results). The scores are, however, not higher than simply adding characters on their own, suggesting that linguistic features are not always beneficial if character-level features are also included. For two encoders, the pattern is less clear, but we do find our highest score thus far when we combine characters and semantic tags (88.4). <ref type="bibr">12</ref> Using three encoders did not yield clear improvements over two encoders. Therefore, we do not experiment with using more than three encoders. Robustness to different LMs We want to verify that the character improvements are robust to using different language models (  semantic tags also results in an improvement over just using characters for all the LMs considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness across languages</head><p>We train systems for German, Italian and Dutch for four models: char-only, BERT-ONLY, BERT + char in 1 encoder, and BERT + char in two encoders. <ref type="bibr">13</ref> The BERT model we use is bert-multilingual-uncased. The results for both PMB releases are shown in <ref type="figure">Figure 3</ref>. For all languages, adding characters leads to a clear improvement for both one and two encoders, though for Dutch the improvement is smaller than for German and Italian. Interestingly, the two-encoder setup seems to be preferable for these smaller, non-English data sets. For 2.2.0, we outperform the system of <ref type="bibr" target="#b15">Fancellu et al. (2019)</ref> for German and Italian and obtain competitive scores for Dutch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to previous work</head><p>To check whether the improvements hold on unseen data, we run our best models on the test set and compare the scores to previous work <ref type="table" target="#tab_10">(Table 6</ref>). 14 We see <ref type="bibr">13</ref> We do not train a model that uses semantic tags as features, since there is not enough gold semantic tag data available to train a good tagger for any of these languages. <ref type="bibr">14</ref>    <ref type="table">Table 7</ref>: F-scores on subsets of sentences that contain a certain phenomenon, based on semantic tags, for the combined dev and test set of PMB release 3.0.0. Full scores shown for BERT and absolute differences for the remaining systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic tag analysis</head><p>We are also interested in finding out why the character-level representations help improve performance. As a start, we investigate on what type of sentences and semantic phenomena the character representations are the most beneficial. We introduce a novel method of analysis: selecting subsets of sentences based on the occurrence of certain semantic tags. In the PMB release, each token is also annotated with a semantic tag, which indicates the semantic properties of the token in the given context (Abzianidze and Bos, 2017). This allows us to easily select all sentences that contain certain (semantic) phenomena and evaluate the performance of the different models on those sentences. <ref type="bibr">15</ref> The selected phenomena and corresponding Fscores for our four best models (see <ref type="table" target="#tab_10">Table 6</ref>) are shown in <ref type="table">Table 7</ref>. <ref type="bibr">16</ref> Our best model (+ch+sem) has the best performance on six of the seven phenomena selected, even though the differences are small. The character-level representations seem to help across the board; the +char models improve on the baseline (BERT) in almost all instances.</p><p>For Numerals and Named Entities we expected the characters to help specifically, since (i) BERT representations might not be as optimal for all individual numerals <ref type="bibr" target="#b73">(Wallace et al., 2019)</ref>, and (ii) the 15 Note that this method of analysis can easily be used for other NLP tasks as well, the only requirement being that a semantic tagger has to be used to get the semantic tags. <ref type="bibr">16</ref>   character representations might attend more to capital letters, which often indicate the presence of a named entity. Indeed, the character representations clearly help for Numerals, but less so for Named Entities. Of course, this analysis only scratched the surface as to why the character-level representations improve performance. We leave a more detailed investigation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentence length analysis</head><p>We are also interested in finding out which model performs well on longer documents. When the Transformer model was introduced, one of the advantages was less decrease in performance for longer sentences <ref type="bibr" target="#b69">(Vaswani et al., 2017)</ref>. Also, since Boxer is partly rule-based and not trained in an endto-end fashion, it might be able to handle longer sentences better. <ref type="figure" target="#fig_2">Figure 4</ref> shows the performance over sentence length for seven of our trained systems. We see a similar trend for all models: a decrease in performance for longer sentences. We also create a regression model that predicts F-score, with as predictors parser and document length in tokens, similar to <ref type="bibr" target="#b50">van Noord et al. (2018b)</ref>. We do not find a significant interaction of any model with sentence length, i.e., none of the models decreases significantly less or more than any other model. To get some idea how well our models would do on longer (possibly multi-sentence) documents, we create a new evaluation set. We select all silver documents with 15 or more and less than 51 tokens that have at least the semtagging or CCG layer marked as gold standard. This resulted in a  set of 128 DRSs, which should contain the higher quality silver documents. We retrain our models with those sentences removed and plot the performance over sentence length in <ref type="figure" target="#fig_3">Figure 5</ref>. We see that performance still decreases for longer sentences, though not as much after 30 tokens per document. The Transformer model does not seem to catch up with the bi-LSTM models, even for longer documents. The addition of characters is still beneficial for longer documents, though only in one encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>We found that adding character-level representations generally improved performance, though we did not find a clear preference for either the oneencoder or two-encoder model. We believe that, given the better performance of the two-encoder model on the fairly short documents of the non-English languages (see <ref type="figure">Figure 3</ref>), this model is likely the most useful in semantic parsing tasks with single sentences, such as SQL parsing <ref type="bibr" target="#b76">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b26">Iyer et al., 2017;</ref><ref type="bibr" target="#b17">Finegan-Dollak et al., 2018)</ref>, while the one encoder char-CNN model has more potential for tasks with longer sentences/documents, such as AMR (Banarescu et al., 2013), UCCA <ref type="bibr" target="#b0">(Abend and Rappoport, 2013)</ref> and <ref type="bibr">GMB-based DRS parsing (Bos et al., 2017;</ref><ref type="bibr" target="#b40">Liu et al., 2018</ref><ref type="bibr" target="#b41">Liu et al., , 2019a</ref>). The latter model also has more potential to be applicable for other (semantic parsing) systems as it can be applied to all systems that form token-level representations from a document. In this sense, we hope that our findings here are also applicable for other, more structured, encoder-decoder models devel-oped for semantic parsing (e.g., <ref type="bibr" target="#b75">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b9">Dong and Lapata, 2018;</ref><ref type="bibr" target="#b41">Liu et al., 2019a</ref>). An unexpected finding is that the BERT models outperformed the larger ROBERTA models. In addition, it was even preferable to use BERT only as initial token embedder, instead of fine-tuning using the full model. Perhaps this is an indication that certain NLP tasks cannot be solved by simply training ever larger language models. Moreover, the Transformer model did not improve performance for any of the input representations, while being harder to tune as well. We are a bit hesitant with drawing strong conclusions here, though, since we only experimented with a vanilla Transformer, while recent extensions (e.g., <ref type="bibr" target="#b7">Dehghani et al., 2019;</ref><ref type="bibr" target="#b24">Guo et al., 2019;</ref><ref type="bibr" target="#b59">Press et al., 2020)</ref> might be more promising for smaller data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We performed a range of experiments on Discourse Representation Structure Parsing using neural sequence-to-sequence models, in which we vary the neural representation of the input documents. We show that, not surprisingly, using pretrained contextual language models is better than simply using characters as input (RQ1). However, characters can still be used to improve performance, in both a single encoder and two encoders (RQ2). The improvements are larger than using individual sources of linguistic information, and performance still improves in combination with these sources (RQ3). The improvements are also robust across different languages models, languages and data sets (RQ4) and improve performance across a range of semantic phenomena (RQ5). These methods should be applicable to other semantic parsing and perhaps other natural language analysis tasks.  <ref type="figure">Figure 6</ref> shows a schematic overview of using the char-CNN <ref type="bibr" target="#b31">(Kim et al., 2016)</ref> to encode the word have with a width of 2. A width of 2 selects the bigrams ha, av and ve, returning a scalar for each bigram operation, which in turn form a vector f 1 for filter 1. We then take the max value of this vector to obtain the first value of our width 2 (w 2 ) char-CNN embedding e w2 1 . The final vector e w2 is thus of length n.  <ref type="figure">Figure 6</ref>: Overview of the char-CNN encoder, encoding the word have with bigrams (width = 2) for n filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental settings</head><p>Tuning <ref type="table" target="#tab_15">Table 8</ref> gives an overview of the hyperparameters we used and/or experimented with in the tuning stage. This table only gives an overview of the settings for the BERT-BASE model, though the settings for the other representations (described in Section 3.2) are usually very similar. We performed manual tuning, selecting the settings with the highest F1-score. The number of tuning runs was between 10 and 40 for each representation type and model combination (see <ref type="table" target="#tab_5">Table 3</ref>). Output, evaluation (containing F1-scores, standard deviation and confidence interval) and configuration files for our four best models (see <ref type="table" target="#tab_10">Table 6</ref>) are available here: https://github.com/RikVN/Neural_ DRS/.</p><p>Data filtering We filtered ill-formed DRSs from the PMB data sets, which only occurs for silver and bronze data (&lt; 0.1% of DRSs). For the bi-LSTM models, the filtering of source and target tokens (see <ref type="table" target="#tab_15">Table 8</ref>) only filters out three very large documents from training. This was done for efficiency and memory purposes, it did not make a difference in terms of F1-score. However, for the Transformer model this improved F1-score by around 0.   <ref type="table">Table 9</ref>: Semantic tags that were used to select sentences that contain a certain phenomenon. The example sentence in <ref type="table" target="#tab_3">Table 2</ref> is included in the categories Modality, Pronouns, Named Entities and Numerals .  <ref type="table" target="#tab_19">Table 11</ref> shows the sentences for which our best model (on 3.0.0 English dev) produced the lowest quality DRSs, with a possible explanation. In Table 12, we show the sentences for which our best model has the best performance (relative to the BERT-ONLY baseline model). It is harder to give an explanation in this case, though we indicate which clauses were (in)correctly predicted by the models.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Detailed scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Sentence analysis</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example DRS in both clause (left) and box (right) representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic overview of our neural architecture when using two encoders (BERT and characters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>F-scores over document length (tokens) on the combined English dev and test set of 3.0.0. X-axis shows document length (top) and the number of documents for that length (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>F-scores over document length (tokens) on the silver standard evaluation set of longer documents. X-axis shows the sentence length bins (top) and the number of documents for that length (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Unless otherwise indicated, our results are on the English development set of release 3.0.0.</figDesc><table><row><cell>Sent I</cell><cell>have</cell><cell>n't</cell><cell>been</cell><cell>to</cell><cell>Boston</cell><cell>since</cell><cell>2013</cell></row><row><cell>POS PRP</cell><cell>VBP</cell><cell>RB</cell><cell>VBN</cell><cell>TO</cell><cell>NNP</cell><cell>IN</cell><cell>CD</cell></row><row><cell>SEM PRO</cell><cell>NOW</cell><cell cols="2">NOT EXT</cell><cell>REL</cell><cell>GPE</cell><cell>REL</cell><cell>YOC</cell></row><row><cell>LEM I</cell><cell cols="3">have not be</cell><cell>to</cell><cell cols="2">Boston since</cell><cell>2013</cell></row><row><cell cols="2">DEP nsubj aux</cell><cell cols="2">neg cop</cell><cell cols="2">case ROOT</cell><cell>case</cell><cell>nmod</cell></row><row><cell>CCG NP</cell><cell cols="5">VP\VP VPVP VP/PP PP/NP N</cell><cell cols="2">(VP\VP)/NP N</cell></row></table><note>6 https://pmb.let.rug.nl/data.php</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Example representation for each source of linguistic information (PMB document p00/d1489).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Baseline model for the nine input representations considered, for the bi-LSTM and Transformer architectures. Best score in each column shown in bold.</figDesc><table><row><cell></cell><cell>No chars</cell><cell>+ characters</cell></row><row><cell></cell><cell cols="2">1-enc 2-enc 1-enc 2-enc 3-enc</cell></row><row><cell>BERT</cell><cell cols="2">87.6 NA 88.1 88.1 NA</cell></row><row><cell>BERT + word</cell><cell cols="2">87.7 87.4 87.8 87.6 86.9</cell></row><row><cell>BERT + GLOVE</cell><cell cols="2">87.9 87.2 88.1 88.0 86.9</cell></row><row><cell cols="3">BERT + FASTTEXT 87.8 87.7 87.9 87.9 87.0</cell></row><row><cell>BERT + pos</cell><cell cols="2">87.6 87.6 87.4 87.6 87.8</cell></row><row><cell>BERT + sem</cell><cell cols="2">87.9 88.0 88.0 88.4 88.1</cell></row><row><cell>BERT + lem</cell><cell cols="2">87.8 88.0 88.1 88.0 87.4</cell></row><row><cell>BERT + dep</cell><cell cols="2">87.9 87.5 88.0 87.8 87.8</cell></row><row><cell>BERT + ccg</cell><cell cols="2">87.8 87.3 87.9 87.8 87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Results for adding characters, linguistic infor- mation and a combination of the two to the bi-LSTM BERT-BASE model on 3.0.0 English dev.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Dev and test scores (F1) for the four models we trained for three languages (German, Italian and Dutch). For 2.2.0, we compare our results to<ref type="bibr" target="#b15">Fancellu et al. (2019)</ref>.</figDesc><table><row><cell>). We see that</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Results on 3.0.0 English dev of four LMs for adding characters and both characters and semtags.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Comparison of our four main models to previ-</cell></row><row><cell>ous work for PMB 2.2.0 and 3.0.0 (English only).</cell></row><row><cell>that adding the character-level information has sim-</cell></row><row><cell>ilar (significant) improvements for dev and test on</cell></row><row><cell>both data sets. The addition of semantic tags might</cell></row><row><cell>be questionable: for 2.2.0, both the BERT + char</cell></row><row><cell>models outperform this model, while for 3.0.0 the</cell></row><row><cell>0.1 improvement over BERT + char in one encoder</cell></row><row><cell>is not significant. Despite this, we reach state-of-</cell></row><row><cell>the-art performance on both data sets, significantly</cell></row><row><cell>outperforming the previous best scores by van No-</cell></row><row><cell>ord et al. (2019) and Liu et al. (2019b). We also</cell></row><row><cell>compare to the semantic parser Boxer, which needs</cell></row><row><cell>input for 6 different PMB layers (Abzianidze et al.,</cell></row><row><cell>2017). Amateur Boxer is trained with internal PMB</cell></row><row><cell>taggers, while Pro Boxer uses the output of a neural</cell></row><row><cell>multi-task learning system based on</cell></row></table><note>BERT (van der Goot et al., 2020). Even though this is an unfair comparison to our system, since the rule-based components of Boxer are (partly) optimized on the dev and test sets, our best model still improves slightly over Pro Boxer (significantly on test).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>See Appendix C for the list of semtags per category.</figDesc><table><row><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F-score</cell><cell>78 80 82 84 86</cell><cell></cell><cell>char only bert (transformer) bert (bi-LSTM) bert + char (1e) bert + char (2e) bert + char + sem (2e) pro boxer</cell><cell></cell></row><row><cell></cell><cell>3 56</cell><cell>4 233</cell><cell>5 331 Document length (tokens) 6 378 7 305 8 208 9 126 10 62</cell><cell>11 40</cell><cell>12 17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Wai Lam. 2020. AMR parsing via graphsequence iterative inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1290-1301, Online. Association for Computational Linguistics.</figDesc><table><row><cell>Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre Ludmann, Deng Cai and Iacer Calixto, Qun Liu, and Nick Campbell. 2017. Doubly-attentive decoder for multi-modal neural machine translation. In Proceedings of the 55th An-nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1913-Duc-Duy Nguyen, and Johan Bos. 2017. The par-allel meaning bank: Towards a multilingual corpus of translations annotated with compositional mean-1924, Vancouver, Canada. Association for Computa-tional Linguistics.</cell><cell>strations at the 13th Conference of the Euro-pean Chapter of the Association for Computational Linguistics (EACL 2012), pages 92-96, Avignon, France. Emily M. Bender and Alexander Koller. 2020. Climb-ing towards NLU: On meaning, form, and under-standing in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Compu-tational Linguistics, pages 5185-5198, Online. As-sociation for Computational Linguistics. Johannes Bjerva, Barbara Plank, and Johan Bos. 2016. Semantic tagging with deep residual networks. In</cell></row><row><cell>ing representations. In Proceedings of the 15th Con-ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-pers, pages 242-247, Valencia, Spain. Association for Computational Linguistics. William B. Cavnar. 1994. Using an n-gram-based doc-ument representation with a vector processing re-trieval model. In Proceedings of TREC, NIST spe-cial publication, 500-225, pages 269-277.</cell><cell>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Proceedings of COLING 2016, the 26th Interna-tional Conference on Computational Linguistics: Technical Papers, pages 3531-3541, Osaka, Japan. The COLING 2016 Organizing Committee.</cell></row><row><cell>Colin Cherry, George Foster, Ankur Bapna, Orhan Lasha Abzianidze and Johan Bos. 2017. Towards uni-Firat, and Wolfgang Macherey. 2018. Revisiting versal semantic tagging. In Proceedings of the 12th International Conference on Computational Seman-character-based neural machine translation with ca-</cell><cell>Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa-tion for Computational Linguistics, 5:135-146.</cell></row><row><cell>pacity and compression. In Proceedings of the 2018 tics (IWCS 2017) -Short Papers, pages 307-313, Conference on Empirical Methods in Natural Lan-Montpellier, France. Association for Computational guage Processing, pages 4295-4305, Brussels, Bel-Linguistics. and Johan Bos. 2019. The first shared task on dis-Lasha Abzianidze, Rik van Noord, Hessel Haagsma, gium. Association for Computational Linguistics.</cell><cell>Claire Bonial, William J. Corvey, Martha Palmer, Volha Petukhova, and Harry Bunt. 2011. A hierar-Conference on Semantic Computing (ICSC 2011), chical unification of LIRICS and VerbNet semantic roles. In Proceedings of the 5th IEEE International</cell></row><row><cell>course representation structure parsing. In Proceed-</cell><cell>pages 483-489, Palo Alto, CA, USA.</cell></row><row><cell>ings of the IWCS Shared Task on Semantic Pars-ing, Gothenburg, Sweden. Association for Compu-tational Linguistics.</cell><cell>Johan Bos. 2001. Doris 2001: Underspecification, res-olution and inference for discourse representation structures. In ICoS-3 Inference in Compuational Se-</cell></row><row><cell>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-</cell><cell>mantics. Workshop Proceedings, pages 117-124.</cell></row><row><cell>gio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd Inter-national Conference on Learning Representations, ICLR 2015.</cell><cell>Johan Bos. 2008. Wide-coverage semantic analy-sis with Boxer. In Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277-</cell></row><row><cell>Miguel Ballesteros and Yaser Al-Onaizan. 2017. AMR</cell><cell>286. College Publications, Venice, Italy.</cell></row><row><cell>parsing using stack-LSTMs. In Proceedings of the 2017 Conference on Empirical Methods in Natu-ral Language Processing, pages 1269-1275, Copen-hagen, Denmark. Association for Computational Linguistics.</cell><cell>Johan Bos. 2015. Open-domain semantic parsing with Boxer. In Proceedings of the 20th Nordic Con-ference of Computational Linguistics (NODALIDA 2015), pages 301-304, Vilnius, Lithuania.</cell></row><row><cell></cell><cell>Johan Bos, Valerio Basile, Kilian Evang, Noortje Ven-</cell></row><row><cell>Miguel Ballesteros, Chris Dyer, and Noah A. Smith.</cell><cell>huizen, and Johannes Bjerva. 2017. The Gronin-</cell></row><row><cell>2015. Improved transition-based parsing by mod-</cell><cell>gen Meaning Bank. In Nancy Ide and James Puste-</cell></row><row><cell>eling characters instead of words with LSTMs. In</cell><cell>jovsky, editors, Handbook of Linguistic Annotation.</cell></row><row><cell>Proceedings of the 2015 Conference on Empirical</cell><cell>Springer Netherlands.</cell></row><row><cell>Methods in Natural Language Processing, pages</cell><cell></cell></row><row><cell>349-359, Lisbon, Portugal. Association for Compu-</cell><cell>Thorsten Brants. 2000. Tnt: A statistical part-of-</cell></row><row><cell>tational Linguistics.</cell><cell>speech tagger. In Proceedings of the Sixth Con-</cell></row><row><cell></cell><cell>ference on Applied Natural Language Processing,</cell></row><row><cell>Laura Banarescu, Claire Bonial, Shu Cai, Madalina</cell><cell>ANLC '00, pages 224-231, Stroudsburg, PA, USA.</cell></row><row><cell>Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin</cell><cell>Association for Computational Linguistics.</cell></row><row><cell>Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguis-tic Annotation Workshop and Interoperability with</cell><cell>Deng Cai and Wai Lam. 2019. Core semantic first: A top-down approach for AMR parsing. In Proceed-ings of the 2019 Conference on Empirical Methods</cell></row><row><cell>Discourse, pages 178-186, Sofia, Bulgaria.</cell><cell>in Natural Language Processing and the 9th Inter-national Joint Conference on Natural Language Pro-</cell></row><row><cell>Valerio Basile, Johan Bos, Kilian Evang, and Noortje</cell><cell>cessing (EMNLP-IJCNLP), pages 3799-3809, Hong</cell></row><row><cell>Venhuizen. 2012. A platform for collaborative se-</cell><cell>Kong, China. Association for Computational Lin-</cell></row><row><cell>mantic annotation. In Proceedings of the Demon-</cell><cell>guistics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>this to around 8 hours, but with only 34 million trainable parameters. New evaluation set When training models that are evaluated on the silver-standard evaluation set of longer documents, we do not perform fine-tuning on the gold standard data. Also, we run Counter with the --default-sense setting (not punishing models that get the word sense wrong), since the word senses of the evaluation set are not gold standard. This has a similar increase of around 1.0 for all models.</figDesc><table><row><cell cols="4">with a runtime of around 6 hours. Using a two</cell></row><row><cell cols="4">encoder setup increases Parameter LSTM Transf. Range</cell></row><row><cell>Hidden RNN size</cell><cell>300</cell><cell>NA</cell><cell>200 -600</cell></row><row><cell cols="2">Decoder RNN size 300</cell><cell>NA</cell><cell>300</cell></row><row><cell>Num heads</cell><cell>NA</cell><cell>6</cell><cell>2, 4, 6, 10</cell></row><row><cell>hidden dim</cell><cell>NA</cell><cell>300</cell><cell>300 -600</cell></row><row><cell>ff hidden dim</cell><cell>NA</cell><cell>900</cell><cell>300 -1200</cell></row><row><cell>dropout: layer</cell><cell>NA</cell><cell>0.1</cell><cell>0.1, 0.2</cell></row><row><cell>residual</cell><cell>NA</cell><cell>0.2</cell><cell>0.1, 0.2</cell></row><row><cell cols="2">attention NA</cell><cell>0.1</cell><cell>0.1, 0.2</cell></row><row><cell>target emb dim</cell><cell>300</cell><cell>300</cell><cell>300 (GLOVE)</cell></row><row><cell>max src tokens</cell><cell>125</cell><cell>50</cell><cell>30 -no max</cell></row><row><cell>max tgt tokens</cell><cell>1160</cell><cell>560</cell><cell>300 -no max</cell></row><row><cell>layers</cell><cell>1</cell><cell>6</cell><cell>1-3 LSTM, 1-10 Trans</cell></row><row><cell>max norm</cell><cell>3</cell><cell>3</cell><cell>3, 4, 5</cell></row><row><cell cols="3">scale grad by freq False False</cell><cell>True/False</cell></row><row><cell>label smoothing</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0, 0.05, 0.1, 0.2</cell></row><row><cell>beam size</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell cols="2">max decoding steps 1000</cell><cell>500</cell><cell>500, 1000</cell></row><row><cell cols="2">scheduled sampling 0.2</cell><cell>0.0</cell><cell>0.0, 0,1, 0.2, 0.3, 0.4</cell></row><row><cell>batch size</cell><cell>48</cell><cell>32</cell><cell>12, 24, 32, 48, 64, 128</cell></row><row><cell>optimizer</cell><cell cols="2">adam adam</cell><cell>adam, sgd, BertAdam</cell></row><row><cell>learining rate</cell><cell cols="3">0.001 0.0002 0.0001 -0.01</cell></row><row><cell>grad norm</cell><cell>0.9</cell><cell>0.9</cell><cell>0.7 -0.95</cell></row><row><cell>min target occ</cell><cell>3</cell><cell>3</cell><cell>1, 3, 5, 10, 20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Training time and model size A single run of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the baseline BERT model takes about 5 hours to</cell></row><row><cell></cell><cell></cell><cell></cell><cell>train on a single NVIDIA V100 GPU, with around</cell></row><row><cell></cell><cell></cell><cell></cell><cell>17 million trainable parameters. Adding character-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>level representations in one encoder (using the char-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CNN) uses around 55 million trainable parameters,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>An overview of the hyperparameters used for the LSTM and Transformer architecture, that use the BERT-BASE representations. Parameters not specified are left at their default value.</figDesc><table><row><cell cols="2">C Semantic tag selection</cell></row><row><cell>Modality</cell><cell>NOT NEC POS</cell></row><row><cell>Logical</cell><cell>ALT XCL DIS AND IMP BUT</cell></row><row><cell>Pronouns</cell><cell>PRO HAS REF EMP</cell></row><row><cell>Attributes</cell><cell>QUC QUV COL IST SST</cell></row><row><cell></cell><cell>PRI DEG INT REL SCO</cell></row><row><cell>Comparatives</cell><cell>EQU APX MOR LES</cell></row><row><cell></cell><cell>TOP BOT ORD</cell></row><row><cell cols="2">Named entities PER GPE GPO GEO ORG ART</cell></row><row><cell></cell><cell>HAP UOM CTC LIT NTH</cell></row><row><cell>Numerals</cell><cell>QUC MOY SCO ORD DAT</cell></row><row><cell></cell><cell>DOM YOC DEC CLO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10</head><label>10</label><figDesc>shows the detailed F-scores for the English dev and test sets of release 2.2.0 and 3.0.0. Infreq. sense is the F-score on all concept clauses that were not the most frequent sense for that word in the training set (e.g., be.v.01, like.v.03). Perfect sense is the F-score when we ignore word senses during matching, i.e., be.v.01 can match with be.v.02. The last 9 rows are not in the original detailed Counter scores, but are produced by DRS-JURY. Character-level representations help to produce fewer ill-formed and more perfect DRSs, especially on 3.0.0.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Prec 87.3 87.8 87.4 87.6 89.8 89.9 89.9 89.5 88.8 88.9 89.3 89.5 90.0 90.6 90.3 90.5 Rec 83.6 84.4 83.6 83.5 86.2 86.7 86.4 86.0 86.4 87.3 86.9 87.2 87.1 87.9 87.6 88.0 F1 85.4 86.1 85.5 85.5 87.9 88.3 88.1 87.7 87.6 88.1 88.1 88.4 88.5 89.2 88.9 89.3 Operators 94.7 95.2 94.7 94.4 94.8 94.7 94.4 94.7 95.0 95.4 95.4 95.7 95.7 95.7 95.7 96.1 Roles 88.0 88.4 88.2 88.0 90.3 90.3 90.5 89.8 89.0 89.0 89.2 89.9 89.4 90.1 89.9 90.0 Concepts 83.9 84.5 84.0 84.8 87.4 87.9 87.6 87.4 84.7 84.9 85.6 85.4 87.3 87.9 87.4 87.7 Nouns 90.8 91.5 91.1 91.4 92.4 92.8 92.4 92.5 90.6 91.0 91.4 91.5 92.0 92.5 91.8 92.5 Verbs 65.6 65.4 64.8 67.6 75.7 76.4 76.3 75.5 69.1 68.9 70.4 69.2 75.3 76.0 76.4 75.3 Adjectives 70.4 74.0 72.7 71.5 70.9 72.3 70.8 71.5 76.1 75.3 76.6 75.5 75.8 77.5 76.2 76.0</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">PMB release 2.2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PMB release 3.0.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Development set</cell><cell></cell><cell></cell><cell cols="2">Test set</cell><cell></cell><cell cols="3">Development set</cell><cell></cell><cell></cell><cell cols="2">Test set</cell><cell></cell></row><row><cell></cell><cell cols="4">bert +ch +ch +ch</cell><cell cols="4">bert +ch +ch +ch</cell><cell cols="4">bert +ch +ch +ch</cell><cell cols="4">bert +ch +ch +ch</cell></row><row><cell></cell><cell></cell><cell cols="3">(1e) (2e) +sem</cell><cell></cell><cell cols="3">(1e) (2e) +sem</cell><cell></cell><cell cols="3">(1e) (2e) +sem</cell><cell></cell><cell cols="3">(1e) (2e) +sem</cell></row><row><cell>Adverbs</cell><cell cols="16">90.0 67.7 83.3 63.3 70.0 71.7 73.3 61.0 78.1 77.7 78.7 80.1 88.0 88.2 87.7 88.9</cell></row><row><cell>Events</cell><cell cols="16">66.7 67.3 66.5 68.4 74.8 75.7 75.4 74.7 70.8 70.5 71.9 70.7 75.4 76.3 76.4 75.4</cell></row><row><cell>Perfect sense</cell><cell cols="16">87.3 88.1 87.6 87.4 89.3 89.7 89.5 89.1 89.6 90.3 90.2 90.4 91.6 92.2 92.0 92.1</cell></row><row><cell>Infreq. sense</cell><cell cols="16">50.5 50.5 46.7 52.3 57.2 58.3 58.8 59.1 54.9 57.6 56.5 56.0 62.0 62.8 62.7 63.1</cell></row><row><cell>F1 std dev</cell><cell cols="16">0.30 0.30 0.17 0.05 0.22 0.22 0.16 0.19 0.19 0.25 0.30 0.34 0.26 0.24 0.29 0.22</cell></row><row><cell>F1 confidence</cell><cell cols="16">85.0 85.6 85.2 85.4 87.6 88.0 87.9 87.5 87.3 87.8 87.7 87.9 88.2 88.9 88.5 89.0</cell></row><row><cell>interval</cell><cell cols="16">85.8 86.5 85.7 85.5 88.2 88.6 88.3 88.0 87.9 88.5 88.5 88.8 88.9 89.5 89.4 89.6</cell></row><row><cell># illformed</cell><cell>0.4</cell><cell>0.0</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.0</cell><cell>0.2</cell><cell>0.0</cell><cell>3.2</cell><cell>0.8</cell><cell>2.8</cell><cell>2.0</cell><cell>4.6</cell><cell>3.0</cell><cell>2.8</cell><cell>2.0</cell></row><row><cell cols="17"># perfect (avg) 235.4 237.4 239.0 239.8 267.0 265.8 266.4 267.2 336.2 350.6 352.4 352.8 358.0 372.4 365.0 367.8</cell></row><row><cell># perfect (all 5)</cell><cell cols="4">180 187 183 188</cell><cell cols="4">206 213 212 205</cell><cell cols="4">212 238 229 226</cell><cell cols="4">242 255 239 241</cell></row><row><cell># zero (avg)</cell><cell>4.4</cell><cell>3.4</cell><cell>4.2</cell><cell>4.2</cell><cell>1.6</cell><cell>1.8</cell><cell>1.2</cell><cell>1.8</cell><cell>6.6</cell><cell>3.6</cell><cell>5.0</cell><cell>3.6</cell><cell>5.0</cell><cell>3.2</cell><cell>3.6</cell><cell>2.6</cell></row><row><cell># zero (all 5)</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell># same (all 5)</cell><cell cols="4">368 398 379 384</cell><cell cols="4">356 368 361 352</cell><cell cols="4">347 387 386 365</cell><cell cols="4">364 378 361 361</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Detailed Counter scores for our models on the English dev and test sets of release 2.2.0 and 3.0.0. All scores are averages of 5 runs. Scores are produced by using DRS-JURY. Necessity, infrequent sense of drive What a lot of books! Do they belong to the university library? 0.38 Multi-sentence Maybe he is Italian or Spanish. 0.40 Possibility and conjunction I always get up at 6 o'clock in the morning. 0.40 Necessity + clocktime</figDesc><table><row><cell>Document</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>Sentences of the English 3.0.0 dev set for which our best model (+char +sem) produced the worst DRSs. Correctly produced Goal Oil this bicycle. 0.482 Correctly produced oil as a verb I'm fed up with this winter, I want spring right now! 0.404 Correctly produced CONTINUATION and Pivot He's Argentinian. 0.386 BERT-ONLY failed to produce country and Name Alas! 0.364 Odd sentence, but correctly produced state.v.01 Fire burns. 0.300 Bad performance for both, BERT-ONLY got a score of 0.0 All journeys begin with a first step. 0.300 BERT-ONLY produced a lot of non-matching clauses How heavy you are! 0.299 BERT-ONLY produced a lot of non-matching clauses One plus two is equal to three. 0.252 Correctly produced summation.n.04 He's not like us.</figDesc><table><row><cell>Document</cell><cell>Diff Comment</cell></row><row><cell>Fish surface for air.</cell><cell>0.554</cell></row></table><note>0.246 Correctly produced Theme and Co-Theme</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Sentences of the English 3.0.0 dev set for which our best model (+char +sem) produced the best DRSs, relative to the BERT-ONLY baseline.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Also see: https://twitter.com/srush_nlp/ status/1245825437240102913</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See Appendix B for specific hyperparameter settings.4  We are aware that there exist several other large pretrained language models (e.g.,<ref type="bibr" target="#b74">Yang et al., 2019;</ref><ref type="bibr" target="#b60">Raffel et al., 2020;</ref><ref type="bibr" target="#b3">Clark et al., 2020)</ref>, but we believe that the models we used have had the largest impact on the field. 5 https://github.com/RikVN/Neural_DRS/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This tagger is also used in the PMB pipeline, seeAbzianidze and Bos (2017). It outperformed an ngram-based CRFtagger<ref type="bibr" target="#b34">(Lafferty et al., 2001)</ref> we also tried, obtaining an accuracy of 94.4% on the dev set. 8 https://github.com/RikVN/DRS_parsing/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">This improvement is significant. With gold semantic tags (ceiling performance) we score 88.6.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the NWO-VICI grant "Lost in Translation-Found in Meaning" (288-89-003). The Tesla K40 GPU used in this work was kindly donated to us by the NVIDIA Corporation. We thank the Center for Information Technology of the University of Groningen for providing access to the Peregrine high performance computing cluster. We also want to thank three anonymous reviewers and our colleagues Lasha Abzianidze, Gosse Minnema, Malvina Nissim, and Chunliu Wang for their comments on earlier versions of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (UCCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Char align: a program for aligning parallel texts at the character level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual meeting on Association for Computational Linguistics</title>
		<meeting>the 31st annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multisource syntactic neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2961" to="2966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical identification of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Las Cruces, NM, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computing Research Laboratory, New Mexico State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual semantic parsing and code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Estival</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00298</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transition-based DRS parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IWCS Shared Task on Semantic Parsing</title>
		<meeting>the IWCS Shared Task on Semantic Parsing<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Sweden</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Elephant: Sequence labeling for word and sentence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1422" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic graph parsing with recurrent neural network DAG grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Fancellu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2769" to="2778" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">WordNet. An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Ma., USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving text-to-SQL evaluation methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Finegan-Dollak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sesh</forename><surname>Sadasivam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DRTS parsing with structureaware encoding and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6818" to="6828" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Massive choice, ample tasks (MaChAmp):A toolkit for multi-task learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Van Der Goot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ahmetüstün</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14672</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AMR dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1831" to="1841" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Startransformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuad</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="442" to="452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1089</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="963" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discourse, anaphora and parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Computational Linguistics. Proceedings of Coling &apos;86</title>
		<meeting><address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="669" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An exploration of neural sequence-tosequence architectures for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regular models of phonological rule systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="378" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Named entity recognition with character-level models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Smarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="180" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA. Morgan</addrLine></address></meeting>
		<imprint>
			<publisher>Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning compositional semantics for open domain semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1535" to="1552" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LSTM CCG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A* CCG parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="990" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention strategies for multi-source sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discourse representation structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="429" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discourse representation parsing for sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1629</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6248" to="6262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discourse representation structure parsing with recurrent neural networks and the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IWCS Shared Task on Semantic Parsing</title>
		<meeting>the IWCS Shared Task on Semantic Parsing<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1054" to="1063" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discourseaware semantic self-attention for narrative reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2541" to="2552" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1459</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evaluating scoped meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1685" to="1693" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring neural methods for parsing discourse representation structures. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00241</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="619" to="633" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Linguistic information in neural semantic parsing with multiple encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-0504</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Computational Semantics -Short Papers</title>
		<meeting>the 13th International Conference on Computational Semantics -Short Papers<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Computer-intensive Methods for Testing Hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Language independent authorship attribution with character level n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlado</forename><surname>Keselj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2067</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Training tips for the transformer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st international conference on machine learning (ICML-14)</title>
		<meeting>the 31st international conference on machine learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Parallel networks that learn to pronounce english text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Terrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="168" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nȃdejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semantic neural machine translation using AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00252</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semantics as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2412" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural architectures for multilingual semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">What do character-level models learn about morphology? The case of dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Grivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2573" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Discourse semantics with information structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noortje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brouwer</surname></persName>
		</author>
		<idno type="DOI">10.1093/jos/ffx017</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Can we translate letters?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">BUILDRS: An implementation of DR theory and LFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Computational Linguistics. Proceedings of Coling &apos;86</title>
		<meeting><address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="540" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? Probing numeracy in embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1534</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on artificial intelligence</title>
		<meeting>the national conference on artificial intelligence<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">AMR parsing as sequence-tograph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Broad-coverage semantic parsing as transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3786" to="3798" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
