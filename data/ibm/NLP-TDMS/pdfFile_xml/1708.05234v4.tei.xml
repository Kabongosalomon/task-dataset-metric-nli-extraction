<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<email>shifeng.zhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
							<email>xiangyu.zhu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<email>zlei@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
							<email>hailin.shi@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
							<email>xiaobo.wang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although tremendous strides have been made in face detection, one of the remaining open challenges is to achieve real-time speed on the CPU as well as maintain high performance, since effective models for face detection tend to be computationally prohibitive. To address this challenge, we propose a novel face detector, named FaceBoxes, with superior performance on both speed and accuracy. Specifically, our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable Face-Boxes to achieve real-time speed on the CPU. The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales. Besides, we propose a new anchor densification strategy to make different types of anchors have the same density on the image, which significantly improves the recall rate of small faces. As a consequence, the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the number of faces. We comprehensively evaluate this method and present stateof-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB. Code is available at https://github.com/ sfzhang15/FaceBoxes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is one of the fundamental problems in computer vision and pattern recognition. It plays an important role in many subsequent face-related applications, such as face alignment <ref type="bibr" target="#b46">[47]</ref>, face recognition <ref type="bibr" target="#b47">[48]</ref> and face tracking <ref type="bibr" target="#b11">[12]</ref>. With the great progress over the past few decades, especially the breakthrough of convolutional neural network, face detection has been successfully applied in our daily life under various scenarios.</p><p>However, there are still some tough challenges in uncontrolled face detection problem, especially for the CPU devices. The challenges mainly come from two requirements for face detectors: 1) The large visual variation of faces in the cluttered backgrounds requires face detectors to accurately address a complicated face and non-face classification problem; 2) The large search space of possible face positions and face sizes further imposes a time efficiency requirement. These two requirements are conflicting, since high-accuracy face detectors tend to be computationally expensive. Therefore, it is one of the remaining open issues for practical face detectors on the CPU devices to achieve real-time speed as well as maintain high performance.</p><p>In order to meet these two conflicting requirements, face detection has been intensely studied mainly in two ways. The early way is based on hand-craft features. Following the pioneering work of Viola-Jones face detector <ref type="bibr" target="#b36">[37]</ref>, most of the early works focus on designing robust features and training effective classifiers. Besides the cascade structure, the deformable part model (DPM) is introduced into face detection tasks and achieves remarkable performance. However, these methods highly depend on nonrobust hand-craft features and optimize each component separately, making the face detection pipeline sub-optimal. In brief, they are efficient on the CPU but not accurate enough against the large visual variation of faces.</p><p>The other way is based on the convolutional neural network (CNN) which has achieved remarkable successes in recent years, ranging from image classification to object detection. Recently, CNN has been successfully introduced into the face detection task as feature extractor in the traditional face detection framewrok <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Moreover, some face detectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref> have inherited valid techniques from the generic object detection methods, such as Faster R-CNN <ref type="bibr" target="#b28">[29]</ref>. These CNN based face detection methods are robust to the large variation of facial appearances and demonstrate state-of-the-art performance. But they are too time-consuming to achieve real-time speed, especially on the CPU devices.</p><p>These two ways have their own advantages. The former has fast speed while the latter owns high accuracy.</p><p>To perform well on both speed and accuracy, one natural idea is to combine the advantages of these two types of methods. Therefore, cascaded CNN based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">45]</ref> are proposed to put features learned by CNN into cascade framework in order to boost the performance and keep efficient. However, there are three problems in cascaded CNN based methods: 1) Their speed is negatively related to the number of faces on the image. The speed would dramatically degrade as the number of faces increases; 2) The cascade based detectors optimize each component separately, making the training process extremely complicated and the final model sub-optimal; 3) For the VGA-resolution images, their runtime efficiency on the CPU is about 14 FPS, which is not fast enough to reach the real-time speed.</p><p>In this paper, inspired by the RPN in Faster R-CNN <ref type="bibr" target="#b28">[29]</ref> and the multi-scale mechanism in SSD <ref type="bibr" target="#b20">[21]</ref>, we develop a state-of-the-art face detector with real-time speed on the CPU. Specifically, we propose a novel face detector named FaceBoxes, which only contains a single fully convolutional neural network and can be trained end-to-end. The proposed method has a lightweight yet powerful network structure (as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) that consists of the Rapidly Digested Convolutional Layers (RDCL) and the Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable FaceBoxes to achieve real-time speed on the CPU, and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces. Besides, we propose a new anchor densification strategy to make different types of anchors have the same density on the input image, which significantly improves the recall rate of small faces. Consequently, for VGA-resolution images, our face detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU. More importantly, the speed of FaceBoxes is invariant to the number of faces on the image. We comprehensively evaluate this method and demonstrate state-of-the-art detection performance on several face detection benchmark datasets, including the AFW, PASCAL face, and FDDB.</p><p>For clarity, the main contributions of this work can be summarized as four-fold:</p><p>• We design the Rapidly Digested Convolutional Layers (RDCL) to enable face detection to achieve real-time speed on the CPU; • We introduce the Multiple Scale Convolutional Layers (MSCL) to handle various scales of face via enriching receptive fields and discretizing anchors over layers. • We present a new anchor densification strategy to improve the recall rate of small faces; • We further improve the state-of-the-art performance on the AFW, PASCAL face, and FDDB datasets.</p><p>The rest of the paper is organized as follows. Section 2 reviews the related work. Analysis of the FaceBoxes is presented in section 3. Section 4 shows the experimental results and section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Modern face detection approaches can be roughly divided into two different categories. One is based on handcraft features, and the other one is built on CNN. This section briefly reviews these two kinds of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hand-craft based methods</head><p>Previous face detection systems are mostly based on hand-craft features. Since the seminal Viola-Jones face detector <ref type="bibr" target="#b36">[37]</ref> that proposes to combine Haar feature, Adaboost learning and cascade inference for face detection, many subsequent works are proposed for real-time face detection, such as new local features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref>, new boosting algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> and new cascade structures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Besides the cascade framework, methods based on structural models progressively achieve better performance and become more and more efficient. Some researches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref> introduce the deformable part model (DPM) into face detection tasks. These works use supervised parts, more pose partition, better training or more efficient inference to achieve remarkable detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CNN based methods</head><p>The first use of CNN for face detection can be traced back to 1994. Vaillant et al. <ref type="bibr" target="#b35">[36]</ref> use a trained CNN in a sliding windows manner to detect faces. Rowley et al. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> introduce a retinally connected neural network for upright frontal face detection, and a "router" network designed to estimate the orientation for rotation invariant face detection. Garcia et al. <ref type="bibr" target="#b6">[7]</ref> develop a neural network to detect semi-frontal faces. Osadchy et al. <ref type="bibr" target="#b23">[24]</ref> train a CNN for simultaneous face detection and pose estimation. These earlier methods can get relatively good performance only on easy dataset.</p><p>Recent years have witnessed the advance of CNN based face detectors. CCF <ref type="bibr" target="#b40">[41]</ref> uses boosting on top of CNN features for face detection. Farfade et al. <ref type="bibr" target="#b5">[6]</ref> fine-tune CNN model trained on 1k ImageNet classification task for face and non-face classification task. Faceness <ref type="bibr" target="#b41">[42]</ref> trains a series of CNNs for facial attribute recognition to detect partially occluded faces. CascadeCNN <ref type="bibr" target="#b15">[16]</ref> develops a cascade architecture built on CNNs with powerful discriminative capability and high performance. Qin et al. <ref type="bibr" target="#b25">[26]</ref> propose to jointly train CascadeCNN to realize end-to-end optimization. Similar to <ref type="bibr" target="#b4">[5]</ref>, MTCNN <ref type="bibr" target="#b44">[45]</ref> proposes a multi-task cascaded CNNs based framework for joint face detection and alignment. UnitBox <ref type="bibr" target="#b43">[44]</ref> introduces a new intersectionover-union loss function. CMS-RCNN <ref type="bibr" target="#b45">[46]</ref> uses Faster R-CNN in face detection with body contextual information. Convnet <ref type="bibr" target="#b18">[19]</ref> integrates CNN with 3D face model in an endto-end multi-task learning framework. STN <ref type="bibr" target="#b3">[4]</ref> proposes a new supervised transformer network and a ROI convolution for face detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FaceBoxes</head><p>This section presents our three contributions that make the FaceBoxes accurate and efficient on the CPU devices: the Rapidly Digested Convolutional Layers (RDCL), the Multiple Scale Convolutional Layers (MSCL) and the anchor densification strategy. Finally, we introduce the associated training methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Rapidly Digested Convolutional Layers</head><p>Most of the CNN based face detection methods are usually limited by the heavy cost of time, especially on the CPU devices. More precisely, the convolution operation for CPU is extremely time-consuming when the size of input, kernel and output are large. Our RDCL is designed to fast shrink the input spatial size by suitable kernel size with reducing the number of output channels, enabling the FaceBoxes to reach real-time speed on the CPU devices as follows:</p><p>• Shrinking the spatial size of input: To rapidly shrink the spatial size of input, our RDCL sets a series of large stride sizes for its convolution and pooling layers. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the stride size of Conv1, Pool1, Conv2 and Pool2 are 4, 2, 2 and 2, respectively. The total stride size of RDCL is 32, which means the input spatial size is reduced by 32 times quickly. • Choosing suitable kernel size: The kernel size of the first few layers in one network should be small so as to speed up, while it is also supposed to be large enough to alleviate the information loss brought by the spatial size reducing. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, to keep efficient as well as effective, we choose 7×7, 5×5 and 3×3 kernel size for Conv1, Conv2 and all Pool layers, respectively. • Reducing the number of output channels: We utilize the C.ReLU activation function (illustrated in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) to reduce the number of output channels. C.ReLU <ref type="bibr" target="#b31">[32]</ref> is motivated from the observation in CNN that the filters in the lower layers form pairs (i.e., filters with opposite phase). From this observation, C.ReLU can double the number of output channels by simply concatenating negated outputs before applying ReLU. Using C.ReLU significantly increases speed with negligible decline in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple Scale Convolutional Layers</head><p>The proposed method is based on RPN which is developed as a class-agnostic proposer in the scenario of multicategory object detection. For the single-category detection task (e.g., face detection), RPN is naturally a detector for the only category concerned. However, as a stand-alone face detector, RPN is not able to obtain competitive performances. We argue that such unsatisfactory performance comes from two aspects. Firstly, the anchors in the RPN are only associated with the last convolutional layer whose feature and resolution are too weak to handle faces of various sizes. Secondly, an anchor-associated layer is responsible for detecting faces within a corresponding range of scales, but it only has a single receptive field that can not match different scales of faces. To solve the above two problems, our MSCL is designed along the following two dimensions:</p><p>• Multi-scale design along the dimension of network depth. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our designed MSCL consists of several layers. These layers decrease in size progressively and form the multi-scale feature maps. Similar to <ref type="bibr" target="#b20">[21]</ref>, our default anchors are associated with multi-scale feature maps (i.e., Inception3, Conv3 2 and Conv4 2). These layers, as a multi-scale design along the dimension of network depth, discretize anchors over multiple layers with different resolutions to naturally handle faces of various sizes. • Multi-scale design along the dimension of network width. To learn visual patterns for different scales of faces, output features of the anchor-associated layers should correspond to various sizes of receptive fields, which can be easily fulfilled via Inception modules <ref type="bibr" target="#b33">[34]</ref>. The Inception module consists of multiple convolution branches with different kernels. These branches, as a multi-scale design along the dimension of network width, is able to enrich the receptive fields. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the first three layers in MSCL are based on the Inception module. <ref type="figure" target="#fig_1">Fig. 2(b)</ref> illustrates our Inception implementation, which is a cost-effective module to capture different scales of faces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Anchor densification strategy</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we impose 1:1 aspect ratio for the default anchors (i.e., square anchor), because the face box is approximately square. The scale of anchor for the Incep-tion3 layer is 32, 64 and 128 pixels, for the Conv3 2 layer and Conv4 2 layer are 256 and 512 pixels, respectively.</p><p>The tiling interval of anchor on the image is equal to the stride size of the corresponding anchor-associated layer. For example, the stride size of Conv3 2 is 64 pixels and its anchor is 256 × 256, indicating that there is a 256 × 256 anchor for every 64 pixels on the input image. We define the tiling density of anchor (i.e., A density ) as follows:</p><formula xml:id="formula_0">A density = A scale /A interval<label>(1)</label></formula><p>Here, A scale is the scale of anchor and A interval is the tiling interval of anchor. The tiling intervals for our default anchors are 32, 32, 32, 64 and 128, respectively. According to Equ. (1), the corresponding densities are 1, 2, 4, 4 and 4, where it is obviously that there is a tiling density imbalance problem between anchors of different scales. Comparing with large anchors (i.e., 128×128, 256×256 and 512×512), small anchors (i.e., 32 × 32 and 64 × 64) are too sparse, which results in low recall rate of small faces. To eliminate this imbalance, we propose a new anchor densification strategy. Specifically, to densify one type of anchors n times, we uniformly tile A number = n 2 anchors around the center of one receptive field instead of only tiling one at the center of this receptive field to predict. Some examples are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. In our paper, to improve the tiling density of the small anchor, our strategy is used to densify the 32 × 32 anchor 4 times and the 64 × 64 anchor 2 times, which guarantees that different scales of anchor have the same density (i.e., 4) on the image, so that various scales of faces can match almost the same number of anchors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>This subsection introduces the training dataset, data augmentation, matching strategy, loss function, hard negative mining, and other implementation details.</p><p>Training dataset. Our model is trained on 12, 880 images of the WIDER FACE <ref type="bibr" target="#b42">[43]</ref> training subset.</p><p>Data augmentation. Each training image is sequentially processed by the following data augmentation strategies:</p><p>• Color distortion: Applying some photo-metric distortions similar to <ref type="bibr" target="#b8">[9]</ref>. • Random cropping: We randomly crop five square patches from the original image: one is the biggest square patch, and the size of the others range between [0.3, 1] of the short size of the original image. Then we arbitrarily select one patch for subsequent operations. • Scale transformation: After random cropping, the selected square patch is resized to 1024 × 1024. • Horizontal flipping: The resized image is horizontally flipped with probability of 0.5. • Face-box filter: We keep the overlapped part of the face box if its center is in the above processed image, then filter out these face boxes whose height or width is less than 20 pixels. Matching strategy. During training, we need to determine which anchors correspond to a face bounding box. We first match each face to the anchor with the best jaccard overlap, and then match anchors to any face with jaccard overlap higher than a threshold (i.e., 0.35).</p><p>Loss function. Our loss function is the same as RPN in Faster R-CNN <ref type="bibr" target="#b28">[29]</ref>. We adopt a 2-class softmax loss for classification and the smooth L1 loss for regression.</p><p>Hard negative mining. After the anchor matching step, most of the anchors are found to be negative, which introduces a significant imbalance between the positive and negative examples. For faster optimization and stable training, we sort them by the loss values and pick the top ones so that the ratio between the negatives and positives is at most 3:1.</p><p>Other implementation details. All the parameters are randomly initialized with the "xavier" method. We finetune the resulting model using SGD with 0.9 momentum, 0.0005 weight decay and batch size 32. The maximum number of iterations is 120k and we use 10 −3 learning rate for the first 80k iterations, then continue training for 20k iterations with 10 −4 and 10 −5 , respectively. Our method is implemented in the Caffe library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly introduce the runtime efficiency of FaceBoxes, then analyze our model in an ablative way, finally evaluate it on the common face detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Runtime efficiency</head><p>CNN based methods have always been accused of its runtime efficiency. Although the existing CNN face detectors can be accelerated via high-end GPUs, they are not fast enough in most practical applications, especially CPU based applications. As described below, our FaceBoxes is efficient enough to meet practical requirements.</p><p>During inference, our method outputs a large number of boxes (e.g., 8, 525 boxes for a VGA-resolution image). We first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS, then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes. We measure the speed using Titan X (Pascal) and cuDNN v5.1 with Intel Xeon E5-2660v3@2.60GHz. As listed in Tab. 1, comparing with recent CNN-based methods, our FaceBoxes can run at 20 FPS on the CPU with state-of-the-art accuracy. Besides, our method can run at 125 FPS using a single GPU and has only 4.1MB in size.  <ref type="bibr" target="#b3">[4]</ref>, its mAP is the true positive rate at 179 false positives and with ROI convolution, its FPS can be accelerated to 30 with 0.6% recall rate drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model analysis</head><p>We carried out extensive ablation experiments on the FDDB dataset to analyze our model. Comparing with AFW and PASCAL face, FDDB is much more difficult so that analyzing our model on FDDB is convincing. For all the experiments, we use the same settings, except for specified changes to the components.</p><p>Ablative Setting. To better understand FaceBoxes, we ablate each component one after another to examine how each proposed component affects the final performance. 1) Firstly, we ablate the anchor densification strategy. 2) Then, we replace MSCL with three convolutional layers, which all have 3 × 3 kernel size and whose output number is the same as the first three Inception modules of MSCL. Meantime, we only associate the anchors with the last convolutional layer. 3) Finally, we take the place of C.ReLU with ReLU in RDCL. The ablative results are listed in Tab. 2 and some promising conclusions can be summed up as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>FaceBoxes Anchor densification strategy is crucial. Our anchor densification strategy is used to increase the density of small anchors (i.e., 32 × 32 and 64 × 64) in order to improve the recall rate of small faces. From the results listed in Tab. 2, we can see that the mAP on FDDB is reduced from 96.0% to 94.9% after ablating the anchor densification strategy. The sharp decline (i.e., 1.1%) demonstrates the effectiveness of the proposed anchor densification strategy.</p><formula xml:id="formula_1">RDCL × MSCL × × Strategy × × ×</formula><p>MSCL is better. The comparison between the second and third columns in Tab. 2 indicates that MSCL effectively increases the mAP by 1.0%, owning to the diverse receptive fields and the multi-scale anchor tiling mechanism.</p><p>RDCL is efficient and accuracy-preserving. The design of RDCL enables our FaceBoxes to achieve real-time speed on the CPU. As reported in Tab. 2, RDCL leads to a negligible decline on accuracy but a significant improvement on speed. Specifically, the FDDB mAP decreases by 0.1% in return for the about 19.3ms speed improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on benchmark</head><p>We evaluate the FaceBoxes on the common face detection benchmark datasets, including Annotated Faces in the Wild (AFW), PASCAL Face, and Face Detection Data Set and Benchmark (FDDB).</p><p>AFW dataset <ref type="bibr" target="#b48">[49]</ref>. It has 205 images with 473 faces. We evaluate FaceBoxes against the well-known works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref> and commercial face detectors (e.g., Face.com, Face++ and Picasa). As illustrated in <ref type="figure" target="#fig_3">Fig.4</ref>, our FaceBoxes outperforms all others by a large margin. <ref type="figure" target="#fig_6">Fig.7(a)</ref> shows some qualitative results on the AFW dataset. PASCAL face dataset <ref type="bibr" target="#b38">[39]</ref>. It is collected from the test set of PASCAL person layout dataset, consisting of 1335 faces with large face appearance and pose variations from 851 images. <ref type="figure" target="#fig_4">Fig.5</ref> shows the precision-recall curves on this dataset. Our method significantly outperforms all other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref> and commercial face detectors (e.g., SkyBiometry, Face++ and Picasa). <ref type="figure" target="#fig_6">Fig.7(b)</ref> shows some qualitative results on the PASCAL face dataset. FDDB dataset <ref type="bibr" target="#b9">[10]</ref>. It has 5, 171 faces in 2, 845 images taken from news articles on Yahoo websites. FDDB adopts the bounding ellipse, while our FaceBoxes outputs rectangle bounding box. This inconsistency has a great impact to the continuous score. For a more fair comparison under the continuous score evaluation, we train an elliptical regressor to transform our predicted bounding boxes to bounding ellipses. We evaluate our face detector on FDDB against the other methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. The results are shown in <ref type="figure" target="#fig_5">Fig. 6(a)</ref> and <ref type="figure" target="#fig_5">Fig.6(b)</ref>. Our FaceBoxes achieves the state-of-the-art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves. These results indicate that our FaceBoxes can robustly detect unconstrained faces. <ref type="figure" target="#fig_6">Fig.7(c)</ref> shows some qualitative results on the FDDB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Since effective models for the face detection task tend to be computationally prohibitive, it is challenging for the CPU devices to achieve real-time speed as well as maintain high performance. In this work, we present a novel face detector with superior performance on both speed and accuracy. The proposed method has a lightweight yet powerful network structure, which consists of RDCL and MSCL. The former enables FaceBoxes to achieve real-time speed, and the latter aims at enriching receptive fields and discretizing anchors over different layers to handle faces of various scales. Besides, a new anchor densification strategy is proposed to improve the recall rate of small faces. The experiments demonstrate that our contributions lead Face-Boxes to the state-of-the-art performance on the common face detection benchmarks. The proposed detector is very fast, achieving 20 FPS for VGA-resolution images on CPU and can be accelerated to 125 FPS on GPU. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Architecture of the FaceBoxes and the detailed information table about our anchor designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) The C.ReLU modules where Negation simply multiplies −1 to the output of Convolution. (b) The Inception modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of anchor densification. For clarity, we only densify anchors at one receptive field centre (i.e., the central black cell), and only color the diagonal anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Precision-recall curves on AFW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Precision-recall curves on PASCAL face dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>(a) Discontinuous ROC curves (b) Continuous ROC curves Evaluation on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on face detection benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overall CPU inference time and mAP compared on different methods. The FPS is for VGA-resolution images on CPU and the mAP means the true positive rate at 1000 false positives on FDDB. Notably, for STN</figDesc><table><row><cell></cell><cell>CPU-model</cell><cell cols="2">mAP(%) FPS</cell></row><row><cell>ACF [40]</cell><cell>i7-3770@3.40</cell><cell>85.2</cell><cell>20</cell></row><row><cell>CasCNN [16]</cell><cell>E5-2620@2.00</cell><cell>85.7</cell><cell>14</cell></row><row><cell>FaceCraft [26]</cell><cell>N/A</cell><cell>90.8</cell><cell>10</cell></row><row><cell>STN [4]</cell><cell>i7-4770K@3.50</cell><cell>91.5</cell><cell>10</cell></row><row><cell>MTCNN [45]</cell><cell>N/A@2.60</cell><cell>94.4</cell><cell>16</cell></row><row><cell>Ours</cell><cell>E5-2660v3@2.60</cell><cell>96.0</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablative results of the FaceBoxes on FDDB dataset. Accuracy (mAP) means the true positive rate at 1000 false positives. Speed (ms) is for the VGA-resolution images on the CPU.</figDesc><table><row><cell cols="2">Accuracy (mAP) 96.0</cell><cell>94.9</cell><cell>93.9</cell><cell>94.0</cell></row><row><cell>Speed (ms)</cell><cell cols="4">50.98 48.27 48.23 67.48</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Plan (Grant No.2016YFC0801002), the Chinese National Natural Science Foundation Projects #61473291, #61502491, #61572501, #61572536, #61672521 and AuthenMetric R&amp;D Funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Face detection with a 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gramajo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3596</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural architecture for fast and robust face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<title level="m">Occlusion coherence: Detecting and localizing occluded faces</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learned-Miller. Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UMass Amherst Technical Report</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weighted sampling for large-scale boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face tracking and recognition with visual constraints in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual phrases for exemplar face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic elastic part model for unsupervised face detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient boosted exemplar-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face detection with endto-end integration of a convnet and a 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">To boost or not to boost? on the limits of boosted trees for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast training and selection of haar features using statistics in boosting-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint training of cascaded cnn for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural networkbased face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation invariant neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting and aligning faces by image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fast deep convolutional neural network for face detection in big visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Triantafyllidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INNS Conference on Big Data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings-Vision, Image and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robust real-time face detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Face detection by structural models. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05413</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
