<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">H3DNet: 3D Object Detection Using Hybrid Geometric Primitives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78710</postCode>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78710</postCode>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78710</postCode>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<postCode>78710</postCode>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">H3DNet: 3D Object Detection Using Hybrid Geometric Primitives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Deep Learning</term>
					<term>Geometric Deep Learning</term>
					<term>3D Point Clouds</term>
					<term>3D Bounding Boxes</term>
					<term>3D Object Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D. Our code is open-sourced at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is a fundamental problem in visual recognition. In this work, we aim to detect the 3D layout (i.e., oriented 3D bounding boxes (or BBs) and associated semantic labels) from a colorless 3D point cloud. This problem is fundamentally challenging because of the irregular input and a varying number of objects across different scenes. Choosing suitable intermediate representations to integrate low-level object cues into detected objects is key to the performance of the resulting system. While early works <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39]</ref> classify sliding windows for object detection, recent works <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b46">45]</ref> have shown the great promise of designing end-to-end neural networks to generate, classify, and refine object proposals. This paper introduces H3DNet, an end-to-end neural network that utilizes a novel intermediate representation for 3D object detection. Specifically, H3DNet  <ref type="figure">Fig. 1</ref>: Our approach leverages a hybrid and overcomplete set of geometric primitives to detect and refine 3D object bounding boxes (BBs). Note that red BBs are initial object proposals, green BBs are refined object proposals, and blue surfaces and lines are hybrid geometric primitives.</p><p>first predicts a hybrid and overcomplete set of geometric primitives (i.e., BB centers, BB face centers, and BB edge centers) and then detects objects to fit these primitives and their associated features. This regression methodology, which is motivated from the recent success of keypoint-based pose regression for 6D object pose estimation <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b37">36]</ref>, displays two appealing advantages for 3D object detection. First, each type of geometric primitives focuses on different regions of the input point cloud (e.g., points of an entire object for predicting the BB center and points of a planar boundary surface for predicting the corresponding BB face center). Combing diverse primitive types can add the strengths of their generalization behaviors. On new instances, they offer more useful constraints and features than merely using one type of primitives. Second, having an overcomplete set of primitive constraints can tolerate outliers in predicted primitives (e.g., using robust functions) and reduce the influence of individual prediction errors. The design of H3DNet fully practices these two advantages.</p><p>Specifically, H3DNet consists of three modules. The first module computes dense pointwise descriptors and uses them to predict geometric primitives and their latent features. The second module converts these geometric primitives into object proposals. A key innovation of H3DNet is to define a parametric distance function that evaluates the distance between an object BB and the predicted primitives. This distance function can easily incorporate diverse and overcomplete geometric primitives. Its local minimums naturally correspond to object proposals. This method allows us to optimize object BBs continuously and generate high-quality object proposals from imprecise initial proposals.</p><p>The last module of H3DNet classifies each object proposal as a detected object or not, and also predicts for each detected object an offset vector of its geometric parameters and a semantic label to fine-tune the detection result. The performance of this module depends on the input. As each object proposal is associated with diverse geometric primitives, H3DNet aggregates latent features associated with these primitives, which may contain complementary semantic and geometric information, as the input to this module. We also introduce a network design that can handle a varying number of geometric primitives.</p><p>We have evaluated H3DNet on two popular benchmark datasets ScanNet and SUN RGB-D. On ScanNet, H3DNet achieved 67.2% in mAP (0.25), which corresponded to a 8.5% relative improvement from state-of-the-art methods that merely take the 3D point positions as input. On SUN RGB-D, H3DNet achieved 60.1% in mAP (0.25), which corresponded to a 2.4% relative improvement from the same set of state-of-the-art methods. Moreover, on difficult categories of both datasets (i.e., those with low mAP scores), the performance gains of H3DNet are significant (e.g., from 38.1/47.3/57.1 to 51.9/61.0/75.3/ on window/door/shower-curtain, respectively). We have also performed an ablation study on H3DNet. Experimental results justify the importance of regressing a hybrid and overcomplete set of geometric primitives for generating object proposals and aggregating features associated with matching primitives for classifying and refining detected objects. In summary, the contributions of our work are:</p><p>-Formulation of object detection as regressing and aggregating an overcomplete set of geometric primitives -Predicting multiple types of geometric primitives that are suitable for different object types and scenes -State-of-the-art results on SUN RGB-D and ScanNet with only point clouds 2 Related Works 3D object detection. From the methodology perspective, there are strong connections between 3D object detection approaches and their 2D counterparts. Most existing works follow the approach of classifying candidate objects that are generated using a sliding window <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39]</ref> or more advanced techniques <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b46">45]</ref>. Objectness classification involves template-based approaches or deep neural networks. The key differences between 2D approaches and 3D approaches lie in feature representations. For example, <ref type="bibr" target="#b23">[23]</ref> leverages a pair-wise semantic context potential to guide the proposals' objectness score. <ref type="bibr" target="#b33">[32]</ref> uses clouds of oriented gradients (COG) for object detection. <ref type="bibr" target="#b8">[9]</ref> utilizes the power of 3D convolution neural networks to identify locations and keypoints of 3D objects. Due to the computational cost in the 3D domain, many methods utilize 2D-3D projection techniques to integrate 2D object detection and 3D data processing. For example, MV3D <ref type="bibr" target="#b3">[4]</ref> and VoxelNet <ref type="bibr" target="#b55">[54]</ref> represent the 3D input data in a birdseye view before proceeding to the rest of the pipeline. Similarly, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">29]</ref> first process 2D inputs to identify candidate 3D object proposals.</p><p>Point clouds have emerged as a powerful representation for 3D deep learning, particularly for extracting salient geometric features and spatial locations (c.f. <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31]</ref>). Prior usages of point-based neural networks include classification <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>, segmentation <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">45]</ref>, normal estimation <ref type="bibr" target="#b1">[2]</ref>, and 3D reconstruction <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">49]</ref>.</p><p>There are also growing interests in object detection from point clouds <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b46">45]</ref>. H3DNet is most relevant to <ref type="bibr" target="#b29">[28]</ref>, which leverages deep neural networks to predict object bounding boxes. The key innovation of H3DNet is that it utilizes an overcomplete set of geometric primitives and a distance function to integrate them for object detection. This strategy can tolerate inaccurate primitive predictions (e.g., due to partial inputs). Multi-task 3D understanding. Jointly predicting different types of geometric primitives is related to multi-task learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b54">53]</ref>, where incorporating multiple relevant tasks together boosts the performance of feature learning. In a recent work HybridPose <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr">Song et al.</ref> show that predicting keypoints, edges between keypoints, and symmetric correspondences jointly lift the prediction accuracies of each type of features. In this paper, we show that predicting BB centers, BB face centers, and BB edge centers together help to improve the generalization behavior of primitive predictions. Overcomplete constraints regression. The main idea of H3DNet is to incorporate an overcomplete set of constraints. This approach achieves considerable performance gains from <ref type="bibr" target="#b29">[28]</ref>, which uses a single type of geometric primitives. At a conceptual level, similar strategies have been used in tasks of object tracking <ref type="bibr" target="#b44">[43]</ref>, zero-shot fine-grained classification <ref type="bibr" target="#b0">[1]</ref>, 6D object pose estimation <ref type="bibr" target="#b37">[36]</ref> and relative pose estimation between scans <ref type="bibr" target="#b51">[50]</ref>, among others. Compared to these works, the novelties of H3DNet lie in designing hybrid constraints that are suitable for object detection, continuous optimization of object proposals, aggregating hybrid features for classifying and fine-tuning object proposals, and end-to-end training of the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>This section describes the technical details of H3DNet. Section 3.1 presents an approach overview. Section 3.2 to Section 3.5 elaborate on the network design and the training procedure of H3DNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach Overview</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the input of H3DNet is a dense set of 3D points (i.e., a point cloud) S ∈ R 3×n (n = 40000 in this paper). Such an input typically comes from depth sensors or the result of multi-view stereo matching. The output is given by a collection of (oriented) bounding boxes (or BB) O S ∈ O, where O denotes the space of all possible objects. Each object o ∈ O is given by its class label l o ∈ C, where C is pre-defined, its center c o = (c x o , c y o , c z o ) T ∈ R 3 in a world coordinate system, its scales s o = (s x o , s y o , s z o ) T ∈ R 3 , and its orientation n o = (n x o , n y o ) T ∈ R 2 in the xy-plane of the same world coordinate system (note that the upright direction of an object is always along the z axis).</p><p>H3DNet consists of three modules, starting from geometric primitive prediction, to proposal generation, to object refinement. The theme is to predict and integrate an overcomplete set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. The entire network is trained end-to-end. Geometric primitive module. The first module of H3DNet takes a point cloud S as input and outputs a set of geometric primitives P S that predicts locations of BB centers, BB face centers, and BB edge centers of the underlying objects. The network design extends that of <ref type="bibr" target="#b29">[28]</ref>. Specifically, it combines a submodule for extracting dense point-wise descriptors and sub-modules that take point-wise descriptors as input and output offset vectors between input points and the corresponding centers. The resulting primitives are obtained through clustering. In addition to locations, each predicted geometric primitive also possesses a latent feature that is passed through subsequent modules of H3DNet.</p><p>In contrast to <ref type="bibr" target="#b29">[28]</ref>, H3DNet exhibits two advantages. First, since only a subset of predicted geometric primitives is sufficient for object detection, the detected objects are insensitive to erroneous predictions. Second, different types of geometric primitives show complementary strength. For example, BB centers are accurate for complete and solid objects, while BB face centers are suitable for partial objects that possess rich planar structures. Proposal generation module. The second module takes predicted geometric primitives as input and outputs a set of object proposals. A critical innovation of H3DNet is to formulate object proposals as local minimums of a distance function. This methodology is quite flexible in several ways. First, it is easy to incorporate overcomplete geometric primitives, each of which corresponds to an objective term of the distance function. Second, it can handle outlier predictions and mispredictions using robust norms. Finally, it becomes possible to optimize object proposals continuously, and this property relaxes the burden of generating high-quality initial proposals. Classification and refinement module. The last module of H3DNet classifies each object proposal into a detected object or not. This module also computes offset vectors to refine the BB center, BB size, and BB orientation of each detected object, and a semantic label. The key idea of this module is to aggregate features of the geometric primitives that are close to the corresponding primitives of each object proposal. Such aggregated features carry rich semantic information that is unavailable in the feature associated with each geometric primitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Primitive Module</head><p>The first module of H3DNet predicts a set of geometric primitives from the input point cloud. Each geometric primitive provides some constraints on the detected objects. In contrast to most prior works that compute a minimum set of primitives, i.e., that is sufficient to determine the object bounding boxes, H3DNet leverages an overcomplete set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. In other words, these geometric primitives can provide up-to 19 positional constraints for one BB. As we will see later, they offer great flexibilities in generating, classifying, and refining object proposals.</p><p>Similar to <ref type="bibr" target="#b29">[28]</ref>, the design of this module combines a descriptor sub-module and a prediction sub-module. The descriptor sub-module computes dense pointwise descriptors. Its output is fed into the prediction sub-module, which consists of three prediction branches. Each branch predicts one type of geometric primitives. Below we provide the technical details of the network design.</p><p>Descriptor sub-module. The output of the descriptor sub-module provides semantic information to group points for predicting geometric primitives (e.g., points of the same object for BB centers and points of the same planar boundary faces for BB face centers). Instead of using a single descriptor computation tower <ref type="bibr" target="#b29">[28]</ref>, H3DNet integrates four separate descriptor computation towers. The resulting descriptors are concatenated together for primitive prediction and subsequent modules of H3DNet. Our experiments indicate that this network design can learn distinctive features for predicting each type of primitives. However, it does not lead to a significant increase in network complexity. BB center prediction. The same as <ref type="bibr" target="#b29">[28]</ref>, H3DNet leverages a network with three fully connected layers to predict the offset vector between each point and its corresponding object center. The resulting BB centers are obtained through clustering (c.f. <ref type="bibr" target="#b29">[28]</ref>). Note that in additional to offset vectors, H3DNet also computes an associated feature descriptor for each BB center. These feature descriptors serve as input feature representations for subsequent modules of H3DNet.</p><p>Predictions of BB centers are accurate on complete and rectangular shaped objects. However, there are shifting errors for partial and/or occluded objects, and thin objects, such as pictures or curtains, due to imbalanced weighting for offset prediction. This motivates us to consider centers of BB faces and BB edges. BB face center prediction. Planar surfaces are ubiquitous in man-made scenes and objects. Similar to BB center, H3DNet uses 3 fully connected layers to perform point-wise predictions. The predicted attributes include a flag that indicates whether a point is close to a BB face or not and if so, an offset vector between that point and its corresponding BB face center. For training, we generate the ground-truth labels by computing the closest BB face for each point. We say a point lies close to a BB face (i.e., a positive instance) if that distance is smaller than 0.2m. Similar to BB centers, each BB face center prediction also possesses a latent feature descriptor that is fed into the subsequent modules.</p><p>Since face center predictions are only affected by points that are close to that face, we found that they are particularly useful for objects with rich planar patches (e.g., refrigerator and shower-curtain) and incomplete objects. BB edge center prediction. Boundary line features form another type of geometric cues in all 3D scenes and objects. Similar to BB faces, H3DNet employs 3 fully connected layers to predict for each point a flag that indicates whether it is close to a BB edge or not and if so, an offset vector between that point and the corresponding BB edge center. The same as BB face centers, we generate ground-truth labels by computing the closest BB edge for each point. We say a point lies close to a BB edge if the closest distance is smaller than 0.2m. Again, each BB edge center prediction possesses a latent feature of the same dimension. Compared to BB centers and BB face centers, BB edge centers are useful for objects where point densities are irregular (e.g., with large holes) but BB edges appear to be complete (e.g., window and computer desk).</p><p>As analyzed in details in the supplemental material, error distributions of different primitives are largely uncorrelated with each other. Such uncorrelated prediction errors provide a foundation for performance boosting when integrating them together for detecting objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposal Module</head><p>After predicting geometric primitives, H3DNet proceeds to compute object proposals. Since the predicted geometric primitives are overcomplete, H3DNet converts them into a distance function and generates object proposals as local minimums of this distance function. This approach, which is the crucial contribution of H3DNet, exhibits several appealing properties. First, it automatically incorporates multiple geometric primitives to determine the parameters of each object proposal. Second, the distance function can optimize object proposals continuously. The resulting local minimums are insensitive to initial proposals, allowing us to use simple initial proposal generators. Finally, each local minimum is attached to different types of geometric primitives, which carry potentially complementary semantic information. As discussed in Section 3.4, the last module of H3DNet builds upon this property to classify and refine object proposals. Note that each object proposal o has 19 object primitives (i.e., one BB center, six BB face centers, and twelve BB edge centers). Let p i (o), 1 ≤ i ≤ 19 be the location of the i-th primitive of o. Denote t i ∈ T := {center, face, edge} as the type of the i-th primitive. Let P t,S ⊆ P S collect all predicted primitives with type t ∈ T . We define</p><formula xml:id="formula_0">F S (o) := t∈T β t c∈P t,S min min 1≤i≤19,ti=t c i − p i (o) 2 − δ, 0 .<label>(1)</label></formula><p>In other words, we employ the truncated L2-norm to match predicted primitives and closest object primitives. β t describes the trade-off parameter for type t.</p><p>Both β t and the truncation threshold δ are determined via cross-validation. Initial proposals. H3DNet detects object proposals by exploring the local minimums of the distance function from a set of initial proposals. From the perspective of optimization, we obtain the same local minimum from any initial solution that is sufficiently close to that local minimum. This means the initial proposals do not need to be exact. In our experiments, we found that a simple object proposal generation approach is sufficient. Specifically, H3DNet utilizes the method of <ref type="bibr" target="#b29">[28]</ref>, which initializes an object proposal from each detected BB center. Proposal refinement. By minimizing F S , we refine each initial proposal. Note that different initial proposals may share the same local minimum. The final object proposals only collect distinctive local minimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification and Refinement Module</head><p>The last module of H3DNet takes the output of the proposal module as input and outputs a collection of detected objects. This module combines a classification sub-module and a refinement sub-module. The classification sub-module determines whether each object proposal is an object or not. The refinement sub-module predicts for each detected object the offsets in BB center, BB size, and BB orientation and a semantic label. The main idea is to aggregate features associated the primitives (i.e., object centers, edge centers, and face centers) of each object proposal. Such features capture potentially complementary information, yet only at this stage (i.e., after we have detected groups of matching primitives) it becomes possible to fuse them together to determine and fine-tune the detected objects.</p><p>As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, we implement this sub-module by combing four fully connected layers. The input layer concatenates input features of 19 object primitives of an object proposal (i.e., one BB center, six BB face centers, and twelve BB edge centers). Each input feature integrates features associated with primitives that are in the neighborhood of the corresponding object primitive. To address the issue that there is a varying number of neighborhood primitives (e.g., none or multiple), we utilize a variant of the max-pooling layer in PointNet <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b32">31]</ref> to compute the input feature. Specifically, the input to each max-pooling layer consists of the feature associated with the input object proposal, which addresses the issue of no matching primitives, and 32 feature points that are randomly sampled in the neighborhood of each object primitive. In our implementation, we determine the neighboring primitives via range query, and the radius is 0.05m.</p><p>The output of this module combines the label that indicates objectiveness, offsets in BB center, BB size, and BB orientation, and a semantic label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Training</head><p>Training H3DNet employs a loss function with five objective terms: min θg,θp,θc,θo</p><formula xml:id="formula_1">λ g l g (θ g ) + λ p l p (θ g , θ p ) + λ f l f (θ g , θ p , θ o ) + λ c l c (θ g , θ p , θ c ) + λ o l o (θ g , θ p , θ o )<label>(2)</label></formula><p>where l g trains the geometric primitive module θ g , l p trains the proposal module θ p , l f trains the potential function and refinement sub-network θ o , l c trains the classification sub-network θ c , and l o trains the refinement sub-network. The trade-off parameters λ g , λ p , λ f , λ c , and λ o are determined through 10-fold crossvalidation. Intuitively, l c , l o and l f provide end-to-end training of H3DNet, while l g and l p offer intermediate supervisions.</p><p>Formulation. Formulations of l g , l p , l c , and l o follow common strategies in the literature. Specifically, both l g and l p utilize L2 regression losses and a crossentropy loss for geometric primitive location and existence flag prediction, and initial proposal generation; l c applies a cross-entropy loss to train the object classification sub-network; l o employs L2 regression losses for predicting the shape offset, and a cross-entropy loss for predicting the semantic label. Since these four loss terms are quite standard, we leave the details to the supplemental material. l f seeks to match the local minimums of the potential function and the underlying ground-truth objects. Specifically, consider a parametric potential function f Θ (x) parameterized by Θ. Consider a local minimum x Θ which is a function of Θ. Let x gt be the target location of x Θ . We define the following alignment potential to pull x Θ to close to x gt :</p><formula xml:id="formula_2">l m (x Θ , x gt ) := x Θ − x gt 2 .<label>(3)</label></formula><p>The following proposition describes how to compute the derivatives of l m with respect to Θ. The proof is deferred to the supp. material.</p><p>Proposition 1. The derivatives of l m with respect to Θ is given by</p><formula xml:id="formula_3">∂l m ∂Θ := 2(x Θ − x gt ) T · ∂x Θ ∂Θ , ∂x Θ ∂Θ := − ∂ 2 f Θ (x ) ∂ 2 x −1 · ∂ 2 f Θ (x ) ∂x∂Θ .<label>(4)</label></formula><p>We proceed to use l m to define l f . For each scene S, we denote the set of ground-truth objects and the set of local minimums of potential  Computing the derivatives of l f with respect to the network parameters is a straightforward application of Prop.1. Training. We train H3DNet end-to-end and from scratch with the Adam optimizer <ref type="bibr" target="#b14">[14]</ref>. Please defer to the supplemental material for hyper-parameters used in training, such as learning rate etc.</p><formula xml:id="formula_4">l f := S∈Strain (o ,o gt )∈C S l m (o , o gt ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we first describe the experiment setup in Section 4.2. Then, we compare our method with current state-of-the-art 3D object detection methods quantitatively, and analyze our results in Section 4.2, where we show the importance of using geometric primitives and discuss our advantages. Finally, we show ablation results in Section 4.3 and qualitative comparison in Figures <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_4">(5)</ref>. More results and discussions can be found in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We employ two popular datasets ScanNet V2 <ref type="bibr" target="#b4">[5]</ref> and SUN RGB-D V1 <ref type="bibr" target="#b38">[37]</ref>. ScanNet is a dataset of richly-annotated 3D reconstructions of indoor scenes. It contains 1513 indoor scenes annotated with per-point instance and semantic labels for 40 semantic classes. SUN RGB-D is a single-view RGB-D dataset for 3D scene understanding, which contains 10335 indoor RGB and depth images with per-point semantic labels and object bounding boxes. For both datasets, we use the same training/validation split and BB semantic classes (18 classes for ScanNet and 10 classes for SUN RGB-D) as in VoteNet <ref type="bibr" target="#b29">[28]</ref> and sub-sample 40000 points from every scene. Evaluation protocol. We use Average Precision(AP) and the mean of AP across all semantic classes (mAP) <ref type="bibr" target="#b38">[37]</ref> under different IoU values (the minimum IoU to consider a positive match). Average precision computes the average precision value for recall value over 0 to 1. IoU is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground truth bounding box. Specifically, we use AP/mAP@0.25 and AP/mAP@0.5. Baseline Methods We compare H3DNet with STAR approaches: VoteNet <ref type="bibr" target="#b29">[28]</ref> is a geometric-only detector that combines deep point set networks and a voting procedure. GSPN <ref type="bibr" target="#b52">[51]</ref> uses a generative model for instance segmentation. Both 3D-SIS <ref type="bibr" target="#b8">[9]</ref> and DSS <ref type="bibr" target="#b40">[39]</ref> extract features from 2D images and 3D shapes to generate object proposals. F-PointNet <ref type="bibr" target="#b30">[29]</ref> and 2D-Driven <ref type="bibr" target="#b17">[17]</ref> first propose 2D detection regions and project them to 3D frustum for 3D detection. Cloud of gradient(COG) <ref type="bibr" target="#b33">[32]</ref> integrates sliding windows with a 3D HoG-like feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Results</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, our approach leads to an average mAP score of 67.2%, with 3D IoU threshold 0.25 (mAP@0.25), on ScanNet V2, which is 8.5% better than the top-performing baseline approach <ref type="bibr" target="#b29">[28]</ref>. In addition, our approach is 14.6% better than the baseline approach <ref type="bibr" target="#b29">[28]</ref> with 3D IoU threshold 0.5 (mAP@0.5).</p><p>For SUN RGB-D, our approach gains 2.4% and 6.1% in terms of mAP, with 3D IoU threshold 0.25 and 0.5 respectively. On both datasets, the performance gains of our approach under mAP@0.5 are larger than those under mAP@0.25, meaning our approach offers more accurate predictions than baseline approaches. Such improvements are attributed to using an overcomplete set of geometric primitives and their associated features for generating and refining object proposals. We can also understand the relative less salient improvements on SUN RGB-D than ScanNet in a similar way, i.e., labels of the former are less accurate than the latter, and the strength of H3DNet is not fully utilized on SUN RGB-D. Except for the classification and refinement module, our approach shares similar computation pipeline and complexity with VoteNet. The computation on multiple descriptor towers and proposal modules can be paralleled, which should not increase computation overhead. In our implementation, our approach requires 0.058 seconds for the last module per scan. Conceptually, our approach requires 50% more time compared to <ref type="bibr" target="#b29">[28]</ref> but operates with a higher detection accuracy. Improvement on thin objects. One limitation of the current top-performing baseline <ref type="bibr" target="#b29">[28]</ref> is predicting thin objects in 3D scenes, such as doors, windows and pictures. In contrast, with face and edge primitives, H3DNet is able to extract better features for those thin objects. For example, the frames of window or picture provide dense edge feature, and physical texture of curtain or showercurtain provide dense face/surface feature. As shown in <ref type="table" target="#tab_1">Table 1</ref>, H3DNet leads to significant performance gains on thin objects, such as door (13.7%), window (13.8%), picture (10.8%), curtain (10.1%) and shower-curtain (18.2%). Improvement on objects with dense geometric primitives. Across the individual object classes in ScanNet in <ref type="table" target="#tab_1">Table 1</ref>, other than those thin objects, our approach also leads to significant performance gain on cabinet (13.1%), table (6.1%), bookshelf (10.3%), refrigerator (11.8%), sink (12.7%) and other-furniture (16.4%). One explanation is that the geometric shapes of these object classes possess rich planar structures and/or distinct edge structures, which contribute greatly on geometric primitive detection and object refinement. Effect of primitive matching and refinement. Using a distance function to refine object proposals and aggregating features of matching primitives are crucial for H3DNet. On ScanNet, merely classifying the initial proposals results in a 14.6% drop on mAP 0.5. <ref type="figure" target="#fig_4">Figure 4</ref> shows qualitative object detection results, which again justify the importance of optimizing and refining object proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Effects of using different geometric primitives. H3DNet can utilize different groups of geometric primitives for generating, classifying, and refining  object proposals. Such choices have profound influences on the detected objects. As illustrated in <ref type="figure">Figure 7</ref>, when only using BB edge primitives, we can see that objects with prominent edge features, i.e., window, possess accurate predictions.</p><p>In contrast, objects with dense face/surface features, such as shower curtain, exhibit relative low prediction accuracy. However, these objects can be easily detected by activating BB face primitives. H3DNet, which combines BB centers, BB edge centers, and BB face centers, adds the strength of their generalization behaviors together. The resulting performance gains are salient when compared to using a single set of geometric primitives.  <ref type="figure">Fig. 7</ref>: Quantitative comparisons between VoteNet, our approach, ours with only face primitive and ours with only edge primitive, across sampled categories for ScanNet.   Effects of proposal refinement. During object proposal refinement, object center, size, heading angle, semantic and existence are all optimized. As shown in <ref type="table" target="#tab_4">Table 3</ref>, without fine-tuning any of the geometric parameters of the detected objects, the performance drops, which shows the importance of this sub-module. Effect of different truncation threshold As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, with different truncation values of δ, results with mAP@0.25 and mAP@0.5 remain stable. It shows that our model is robust to different truncation threshold δ. Effect of multiple descriptor computation towers. One hyper-parameter of H3DNet is the number of descriptor computation towers. <ref type="table" target="#tab_5">Table 4</ref> shows that adding more descriptor computation towers leads to better results, yet the performance gain of adding more descriptor computation towers quickly drops. Moreover, the performance gain of H3DNet from VoteNet comes from the hybrid set of geometric primitives and object proposal matching and refinement. For example, replacing the descriptor computation tower of VoteNet by the four descriptor computation towers of H3DNet only results in modest and no performance gains on ScanNet and SUN RGB-D, respectively (See <ref type="table" target="#tab_5">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we have introduced a novel 3D object detection approach that takes a 3D scene as input and outputs a collection of labeled and oriented bounding boxes. The key idea of our approach is to predict a hybrid and overcomplete set of geometric primitives and then fit the detected objects to these primitives and their associated features. Experimental results demonstrate the advantages of this approach on ScanNet and SUN RGB-D. In the future, we would like to apply this approach to other 3D scene understanding tasks such as instance segmentation and CAD model reconstruction. Another future direction is to integrate more geometric primitives, like BB corners, for 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Introduction</head><p>This supplemental material provides the proof of Proposition 1 in Section B, additional details on network architecture and loss functions in Section C, more analysis and experiment results on geometric primitive prediction in Section D, and more analysis and experiment results on 3D object detection in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Proposition 1</head><p>We show the proof of Proposition 1 here. With chain rule and equation <ref type="formula" target="#formula_2">(3)</ref> in the main paper, we can directly get:</p><formula xml:id="formula_5">∂l m ∂Θ = 2(x Θ − x gt ) T · ∂x Θ ∂Θ .<label>(6)</label></formula><p>Since x is the local minimum of f Θ (x), we have:</p><formula xml:id="formula_6">∂f Θ (x ) ∂x = 0.<label>(7)</label></formula><p>Compute the derivatives of both sides w.r.t. Θ, i.e.</p><formula xml:id="formula_7">∂ 2 f Θ (x ) ∂ 2 x · ∂x ∂Θ + ∂ 2 f Θ (x ) ∂x∂Θ = 0,<label>(8)</label></formula><p>which leads to the equation (4) in the main paper.</p><p>C Details on network architecture and loss functions C.1 Network architecture details</p><p>In the main paper, we mentioned that there are three modules in H3DNet: geometric primitive module, proposal generation module, and classification and refinement module. We will discuss each module in detail. The geometric primitive module first uses a tower of multiple backbone networks to extract down-sampled per-point feature, as shown in <ref type="figure" target="#fig_9">Figure 9</ref>. The backbone network, which is based on PointNet++ <ref type="bibr" target="#b32">[31]</ref>, was borrowed from <ref type="bibr" target="#b29">[28]</ref>, and the same network configurations are used in our implementation. For all four backbone networks, we use the same index to sample 1024 points from input point clouds with 40000 points. We then concatenate the features for each point and then use two fully connected layers to reduce the feature dimension to 256. This hybrid feature then feeds into a cluster network, which contains three fully connected layers to predict an offset vector between each point and its corresponding center, i.e., object center, face center, and edge center. For face and edge primitive, we also predict a flag that indicates whether a point is close to a primitive or not. The cluster network also produces a residual feature vector, which will be added to the input feature vector. Finally, we use a set abstraction layer <ref type="bibr" target="#b32">[31]</ref>, followed by four layers of multilayer perceptron (MLP) after  the max-pooling in each local region to propagate features. For object centers, we sub-sampled 256 points for initial proposal generation, using furthest-pointsampling. For face and edge centers, we use the propagated features to predict a point-wise offset vector to refine the center prediction, and a point-wise semantic label to add semantic information in features of geometric primitives. As shown in <ref type="figure" target="#fig_9">Figure 9</ref>, we then use three layers of MLP to generate initial object proposals. We use the same configuration as in <ref type="bibr" target="#b29">[28]</ref>. As mentioned in the main paper, we then associate each initial object proposal with an overcomplete set of geometric primitives based on the local minimums of the distance function. However, the detected geometric primitives are firstly selected with the predicted flag, which indicates whether a point is close to a primitive or not. Again, we use a set abstraction layer <ref type="bibr" target="#b32">[31]</ref>, followed by four layers of multilayer perceptron (MLP) after the max-pooling in each local region (i.e., a query ball with radius 0.5m), to propagate features between the predicted geometric primitives and the corresponding primitives of an object proposal. The propagated features are then concatenated and fed into a two-layer MLP for the final object proposals.</p><p>The last module is the classification and refinement module. It contains three layers of MLP. We add the feature generated from the proposal module with the object center feature generated in the primitive module, and feed it to the last module to acquire the final object proposal, including an object indicator, offset vectors to refine the BB center, BB size, and BB orientation, and a semantic label. The object indicator is used to determine whether an object exists in the scene or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Loss function details</head><p>As mentioned in the main paper, the network is trained end-to-end with a multitask loss function with five major objective terms. We will discuss each objective term in detail.</p><formula xml:id="formula_8">l g = l vote + λ 1 l f lag + λ 2 l res + λ 3 l sem<label>(9)</label></formula><p>l g trains the geometric primitive module. Each primitive has its own objective. For object center offset, face center offset and edge center offset prediction, we adopt the same voting loss defined in <ref type="bibr" target="#b29">[28]</ref>. As shown in equation 9, for face and edge center, we add l f lag for flag prediction, l res for point-wise center offset prediction (i.e. used in center refinement), and l sem for point-wise center semantic label prediction. We use a L1 loss, defined in <ref type="bibr" target="#b29">[28]</ref>, for l res , and a standard cross-entropy loss for l f lag and l sem . For equation 9, we weight the losses so that they are in similar scales with λ 1 = 3, λ 2 = 0.1, and λ 3 = 0.1.</p><p>l p trains the proposal module θ p , which contains an objectness loss, a 3D bounding box estimation loss, and a semantic classification loss for initial object proposal generation. We adopt the same loss function defined in <ref type="bibr" target="#b29">[28]</ref>. l f is the distance function defined in the main paper. l c is a standard cross-entropy loss, which trains the classification sub-network, and l o contains a cross-entropy loss for semantic label prediction, and an L2 regression loss for BB center, BB size, and BB orientation offset vector prediction. In our experiments, we set the tradeoff parameters mentioned in the main paper, λ g , λ p , λ f , λ c , and λ o to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Training details</head><p>Our network is implemented in PyTorch and optimized using Adam. The batch size is 8 and the number of epochs is 360. For ScanNet, the learning rate is initialized with 1e-2 and decreases by 10 times after 80, 140, 200, 240 epochs respectively. The learning rate of SUN RGB-D starts with 1e-3 and decreases by 10 times after 160, 220, 260 epochs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Geometric primitive prediction results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Dataset Statistics</head><p>For an object with a 3D bounding box label, its maximum number of boundary faces is 6, and the maximum number of boundary edges is 12. In a real 3D scan, some faces or edges are not visible due to occlusion or irregular-shaped objects. As shown in <ref type="table" target="#tab_6">Table 5</ref> and 6, we can see that on both datasets, there are dense labels for faces and edges. However, we can see that the edge labels per object in SUN RGB-D is significantly fewer than in ScanNet. Based on our observation, labels of the 3D bounding boxes in SUN RGB-D are less accurate than in ScanNet, and without per point instance labels, it is much more difficult to generate accurate and dense labels in SUN RGB-D.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Qualitative Results</head><p>We show some qualitative examples for detected geometric primitives in <ref type="figure" target="#fig_10">Figure  10</ref>. For better visualization purposes, we highlight the detected points if the predicted flag is valid. Most of the examples show that our model performs reasonably well on geometric primitive detection. However, the predictions on edges in SUN RGB-D are sparse due to the lack of labels in training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Quantitative Analysis</head><p>In this section, we provide an empirical analysis of the benefits of different geometric primitives. The primary observations are:</p><p>different geometric primitives are suitable for various object categories; the bias of the predictions are generally smaller than the variance of the predictions; errors in different predictions are mostly uncorrelated.</p><p>When aggregating different predictions together, the truncated L2 loss function can prune outlier predictions. Therefore, we obtain a variance reduction and improved prediction accuracy. Prediction errors under different geometric primtiives. Prediction accuracy of geometric primitives, i.e. face and edge centers, is shown in <ref type="table" target="#tab_8">Table 7</ref> and   8. Since we are predicting an overcomplete set of geometric primitives, we only show the prediction accuracy of detected geometric primitives near the target ground-truth primitives. As shown in <ref type="table" target="#tab_8">Table 7</ref>, for most categories, the prediction accuracy of edge center primitives is higher. However, for some categories, like window and curtain, we observe higher accuracy with face center primitive. It shows the different error distributions of BB face centers and BB edge centers, which demonstrate the importance of utilizing multiple geometric primitives. The prediction accuracy of geometric primitives in SUN RGB-D is significantly lower than in ScanNet, especially for edge center labels. This is again caused by sparse and inaccurate labels in training data. <ref type="figure" target="#fig_1">Figure 12</ref> shows the prediction errors of different geometric primitives under four categories of the test set of ScanNet v2 dataset. We can see that the error patterns are different when varying the categories. This again shows the benefits of having different types of geometric primitives as intermediate supervision. <ref type="table">Table 9</ref>: 3D object detection results on SUN RGB-D val dataset. We show percategory results of average precision (AP) with 3D IoU threshold 0.25 as proposed by <ref type="bibr" target="#b38">[37]</ref>, and mean of AP across all semantic classes. Note that both COG <ref type="bibr" target="#b33">[32]</ref> and 2D-driven <ref type="bibr" target="#b17">[17]</ref> use room layout context to boost performance. For fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data. <ref type="table" target="#tab_1">Table 10</ref>: 3D object detection results on SUN RGB-D val dataset. We show percategory results of average precision (AP) with 3D IoU threshold 0.5 as proposed by <ref type="bibr" target="#b38">[37]</ref>, and mean of AP. The evaluation is on the SUN RGB-D V1 data. <ref type="bibr">RGB</ref>    Note that each prediction error is a 3D vector, and we report its norm as the error. Bias is smaller than the variance. Figure shows that bias and variance of each geometric primitive with respect to the test sets of ScanNet v2 and SUNRBG-D. Here we report the norm of the expecation and the spectrum norm of each 3x3 co-variance matrix. We can see that generally the bias is smaller than the variance. Moreover, this ratio is even smaller on face centers and edge centers than the box center. This shows the usefulness of hybrid geometric primitives.</p><p>The reason why bias is smaller than the variance can be understood from the perspective that during training, the training error is generally smaller than the testing error. Different predictions are mostly uncorrelated. In <ref type="figure" target="#fig_11">Figure 11</ref>, we visualize the covariance matrix of the error distribution for 19 geometric primitives. For each target geometric primitive, we measure the Euclidean distance to the nearest predicted point and concatenate the results across every object in every testing scene. As shown in <ref type="figure" target="#fig_11">Figure 11</ref>, the error distributions across all 19 geometric primitives are uncorrelated in ScanNet. Although the error distributions of 6 BB face centers in SUN RGB-D are slightly correlated, other geometric primitives are still uncorrelated.</p><p>One interpretation is that in the over-parameterized regime, the optimized network weights are close to the initial network weights. Therefore, if the initial weights are independent, then the optimized weights are also approximately independent. It follows that different predictions are not strongly correlated. We leave a detailed theoretical analysis for future work. </p><p>where y box denotes the box center prediction; y face,i denotes the prediction of the i-th face center; y edge,i denotes the prediction of the i-th edge center. Denote the norm of the bias vector of x pred as b pred . It is clear that</p><formula xml:id="formula_10">b pred ≤ b box + β face 6 i=1 b face,i + β edge 12 i=1 b edge,i 1 + 6β face + 12β edge</formula><p>where b face,i and b edge,i are the norms of the bias vectors of y face,i and y edge,i , respectively. It is clear that b pred is smaller than the largest bias of each individual prediction.</p><p>The variance of x pred is given by</p><formula xml:id="formula_11">V [x pred ] = V [y box ] + β face 6 i=1 V [y face,i ] + β edge 12 i=1 V [y edge,i ]</formula><p>(1 + 6β face + 12β edge ) 2 Therefore, with suitable chosen trade-off parameters, we obtain a reduction in variance. Combing the fact that the bias is smaller than the variance, x pred is expected to lead to improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Analysis Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 More quantitative results</head><p>We show the per-category results on ScanNet with 3D IoU threshold 0.5 in <ref type="table" target="#tab_1">Table  11</ref>, and the per-category results on SUN RGB-D with both 3D IoU threshold 0.25 and 0.5 in <ref type="table">Table 9</ref> and 10. For accurate object detection, our approach outperforms the baseline approaches significantly. For thin objects in ScanNet, our approach can gain 14.9%, 24.0%, 25.7%, and 27.0% increase on Window, Counter, Curtain, and Shower-curtain. Again, these improvements are achieved by using an overcomplete set of hybrid geometric primitives and their associated features for generating and refining object proposals. Such performance gain can also be observed for SUN RGB-D in <ref type="table" target="#tab_1">Table 10</ref>, where our approach performs significantly better on more accurate object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 More qualitative results</head><p>We show more qualitative examples of 3D object detection for both datasets in <ref type="figure" target="#fig_6">Figure 16</ref> and 17. In <ref type="figure" target="#fig_4">Figure 14</ref>, we show qualitative comparisons between our approach and the top-performing baseline approach on thin objects. Our method is more accurate and detects more positive thin objects than baseline approaches. We also show some failure cases with our approach in <ref type="figure" target="#fig_5">Figure 15</ref>, and we summarize the failure patterns in the caption.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>H3DNet consists of three modules. The first module computes a dense descriptor and predicts three geometric primitives, namely, BB centers, BB face centers, and BB edge centers. The second module converts geometric primitives into object proposals. The third module classifies object proposals and refines the detected objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Proposal distance function. The proposal distance function F S (o) measures a cumulative proximity score between the predicted geometric primitives P S and the corresponding object primitives of an object proposal o. Recall that l o ∈ C, c o ∈ R 3 , s o ∈ R 3 , and n o ∈ R 2 denote the label, center, scales, and orientation of o. With o = (c T o , s T o , n T o ) T we collect all the geometric parameters of o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the matching, feature aggregation and refinement process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Effect of geometric primitive matching and refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative baseline comparisons on ScanNet V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative baseline comparisons on SUN RGB-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Quantitative comparisons between different truncation threshold δ for ScanNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>The pipeline of H3DNet. N represents the number of points of input point clouds, and we use 40000 for both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative examples for detected geometric primitives (face, edge).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Left: covariance matrix of ScanNet. Right: covariance matrix of SUN RGB-D. c represents object center, f0-f6 represent 6 BB face centers, and l0-l11 represent 12 BB edge centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>Prediction errors of different geometric primitives under four different categories of the ScanNet v2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :</head><label>13</label><figDesc>(Left): magnitudes of bias and variance (square-root) of geometric primitive predictions on ScanNet. (Right): magnitudes of bias and variance (squareroot) of geometric primitive predictions on SUNRBGD Variance reduction and improved accuracy. For simplicity, we focus on analyzing the error of the predicted box center. The analysis of both box parameters are similar. For box center, the prediction is simply a weighted average of all predictions: x pred = y box + β face 6 i=1 y face,i + β edge 12 i=1 y edge,i 1 + 6β face + 12β edge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 :</head><label>14</label><figDesc>Qualitative evaluation on thin object detection. Red arrows are used to highlight the thin objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 :</head><label>15</label><figDesc>Samples of failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 :</head><label>16</label><figDesc>More qualitative results on ScanNet V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 :</head><label>17</label><figDesc>More qualitative results on SUN RGB-D V1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>function F S as O gt and O , respectively. Note that O depends on the network parameters and hyper-parameters. Let C S ⊂ O gt × O collect the nearest object in O for each object in O gt . Consider a training set of scenes S train , we define</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>3D object detection results on ScanNet V2 val dataset. We show percategory results of average precision (AP) with 3D IoU threshold 0.25 as proposed by<ref type="bibr" target="#b38">[37]</ref>, and mean of AP across all semantic classes with 3D IoU threshold 0.25.RGB cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP 3DSIS-5[9] 19.8 69.7 66.2 71.8 36.1 30.6 10.9 27.3 0.0 10.0 46.9 14.1 53.8 36.0 87.6 43.0 84.3 16.2 40.2 3DSIS[9] 12.8 63.1 66.0 46.3 26.9 8.0 2.8 2.3 0.0 6.9 33.3 2.5 10.4 12.2 74.5 22.9 58.7 7.1 25.4 Votenet[28] 36.3 87.9 88.7 89.6 58.8 47.3 38.1 44.6 7.8 56.1 71.7 47.2 45.4 57.1 94.9 54.7 92.1 37.2 58.7 Ours 49.4 88.6 91.8 90.2 64.9 61.0 51.9 54.9 18.6 62.0 75.9 57.3 57.2 75.3 97.9 67.4 92.5 53.6 67.2 w\o refine 37.2 89.3 88.4 88.5 64.4 53.0 44.2 42.2 11.1 51.2 59.8 47.0 54.3 74.3 93.1 57.0 85.6 43.5 60.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Left: 3D object detection results on ScanNetV2 val set. Right: results on SUN RGB-D V1 val set. We show mean of average precision (mAP) across all semantic classes with 3D IoU threshold 0.25 and 0.5.</figDesc><table><row><cell></cell><cell>Input</cell><cell cols="2">mAP@0.25 mAP@0.5</cell><cell></cell><cell>Input</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>DSS[39]</cell><cell>Geo + RGB</cell><cell>15.2</cell><cell>6.8</cell><cell>DSS[39]</cell><cell>Geo + RGB</cell><cell>42.1</cell><cell>-</cell></row><row><cell cols="2">F-PointNet[29] Geo + RGB</cell><cell>19.8</cell><cell>10.8</cell><cell>COG[32]</cell><cell>Geo + RGB</cell><cell>47.6</cell><cell>-</cell></row><row><cell>GSPN[51]</cell><cell>Geo + RGB</cell><cell>30.6</cell><cell>17.7</cell><cell cols="2">2D-driven[17] Geo + RGB</cell><cell>45.1</cell><cell>-</cell></row><row><cell>3D-SIS [9]</cell><cell>Geo + 5 views</cell><cell>40.2</cell><cell>22.5</cell><cell cols="2">F-PointNet[29] Geo + RGB</cell><cell>54.0</cell><cell>-</cell></row><row><cell>VoteNet [28]</cell><cell>Geo only</cell><cell>58.7</cell><cell>33.5</cell><cell>VoteNet [28]</cell><cell>Geo only</cell><cell>57.7</cell><cell>32.9</cell></row><row><cell>Ours</cell><cell>Geo only</cell><cell>67.2</cell><cell>48.1</cell><cell>Ours</cell><cell>Geo only</cell><cell>60.1</cell><cell>39.0</cell></row><row><cell>w\o refine</cell><cell>Geo only</cell><cell>60.2</cell><cell>37.3</cell><cell>w\o refine</cell><cell>Geo only</cell><cell>58.5</cell><cell>34.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results without refining predicted center, size, semantic or object existence score for Scan-Net, and without refining predicted angle for SUN RGB-D and differences compared with refining all.</figDesc><table><row><cell></cell><cell>mAP@0.25 mAP@0.5</cell></row><row><cell>w\o center</cell><cell>66.9 -0.3 46.3 -1.8</cell></row><row><cell>w\o size</cell><cell>65.4 -1.8 44.2 -3.9</cell></row><row><cell cols="2">w\o semantic 66.2 -1.0 47.3 -0.8</cell></row><row><cell cols="2">w\o existence 65.2 -1.8 45.1 -3.0</cell></row><row><cell>w\o angle</cell><cell>58.6 -1.5 36.6 -2.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparisons between different number of descriptor computation towers, among our approach and VoteNet, for ScanNet and SUN RGB-D.</figDesc><table><row><cell></cell><cell cols="3"># of Towers mAP@0.25 mAP@0.5</cell></row><row><cell></cell><cell>1</cell><cell>64.4</cell><cell>43.4</cell></row><row><cell>Ours</cell><cell>2 3</cell><cell>65.4 66.0</cell><cell>46.2 47.7</cell></row><row><cell></cell><cell>4</cell><cell>67.2</cell><cell>48.3</cell></row><row><cell>Vote</cell><cell>4 (Scan) 4 (SUN)</cell><cell>60.11 57.5</cell><cell>37.12 32.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Average number of edges and faces labelled per object in the ScanNet training dataset for different categories. type cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn Avg Face 1.96 3.99 1.74 3.69 2.61 1.71 1.84 3.02 0.75 2.84 2.85 1.97 2.23 2.04 2.27 1.24 4.30 1.63 2.37 Edge 5.60 7.10 6.33 7.48 7.64 3.76 5.19 6.89 2.80 6.90 7.62 5.40 6.17 4.62 7.95 6.23 9.72 5.03 6.25</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Average number of edges and faces labelled per object in the SUN RGB-D training dataset for different categories. type bathtub bed bkshf chair desk drser nigtstd sofa table toilet Avg Face 4.42 4.22 2.21 3.61 3.64 2.89 1.04 4.27 3.57 4.21 3.41 Edge 3.04 2.07 1.50 1.80 3.12 2.44 1.16 1.93 2.83 1.91 2.18</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Prediction accuracy of the location of geometric primitives, i.e. face and edge centers, across different categories for ScanNet. For each target primitive, if there is a prediction within 0.3m, we count it as a correct prediction.type cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn Face 0.89 0.86 0.97 0.91 0.94 0.93 0.83 0.86 0.53 0.93 0.88 0.91 0.99 0.98 0.98 0.98 0.93 0.84 Edge 0.92 0.88 1.00 0.98 1.00 0.91 0.79 0.88 0.81 0.95 0.96 0.81 0.96 0.97 1.00 1.00 0.99 0.76</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Prediction accuracy of the location of geometric primitives, i.e. face and edge centers, across different categories for SUN RGB-D. For each target primitive, if there is a prediction within 0.3m, we count it as a correct prediction.</figDesc><table><row><cell>type bathtub bed bkshf chair desk drser nigtstd sofa table toilet</cell></row><row><cell>Face 0.72 0.57 0.11 0.83 0.57 0.29 0.15 0.53 0.68 0.91</cell></row><row><cell>Edge 0.21 0.04 0.02 0.18 0.29 0.10 0.00 0.04 0.45 0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP.<ref type="bibr" target="#b26">25</ref> </figDesc><table><row><cell>VoteNet [28]</cell><cell>49.9 47.3 4.6 54.1 5.2 13.6 35.0 41.4 19.7 58.6 32.9</cell></row><row><cell>Ours</cell><cell>47.6 52.9 8.6 60.1 8.4 20.6 45.6 50.4 27.1 69.1 39.0</cell></row><row><cell>w\o refine</cell><cell>48.9 50.6 5.0 55.6 6.3 14.6 32.7 45.1 23.3 60.1 34.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>3D object detection results on ScanNet V2 val dataset. We show percategory results of average precision (AP) with 3D IoU threshold 0.5 as proposed by<ref type="bibr" target="#b38">[37]</ref>, and mean of AP across all semantic classes with 3D IoU threshold 0.5.RGB cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP 3DSIS-5[9] 5.73 50.28 52.59 55.43 21.96 10.88 0.00 13.18 0.00 0.00 23.62 2.61 24.54 0.82 71.79 8.94 56.40 6.87 22.53 3DSIS[9] 5.06 42.19 50.11 31.75 15.12 1.38 0.00 1.44 0.00 0.00 13.66 0.00 2.63 3.00 56.75 8.68 28.52 2.55 14.60 Votenet[28] 8.1 76.1 67.2 68.8 42.4 15.3 6.4 28.0 1.3 9.5 37.5 11.6 27.8 10.0 86.5 16.8 78.9 11.7 33.5 Ours 20.5 79.7 80.1 79.6 56.2 29.0 21.3 45.5 4.2 33.5 50.6 37.3 41.4 37.0 89.1 35.1 90.2 35.4 48.1 w\o refine 12.4 80.8 69.3 71.8 42.6 19.5 12.3 26.1 2.4 15.7 27.3 32.6 29.5 33.6 79.1 23.0 74.0 18.9 37.3</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to acknowledge the support from NSF DMS-1700234, a Gift from Snap Research, and a hardware donation from NVIDIA. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-cue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<title level="m">Point convolutional neural networks by extension operators</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="39" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">235</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgbd scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.694</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.694" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6555" to="6564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate localization of 3d objects from RGB-D data using segmentation hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3182" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2013.409</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.409" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.495</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.495" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="4632" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9256" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epnp: An accurate O(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-008-0152-6</idno>
		<ptr target="https://doi.org/10.1007/s11263-008-0152-6" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01231-1_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01231-1_42" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2013.179</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.179" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2017.7989233</idno>
		<ptr target="https://doi.org/10.1109/ICRA.2017.7989233" />
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<biblScope unit="page" from="2011" to="2018" />
			<date type="published" when="2017-05-29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00469</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00469" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jsis3d: Joint semanticinstance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09664</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>CoRR abs/1706.05098</idno>
		<ptr target="http://arxiv.org/abs/1706.05098" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="527" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2001.01869</idno>
		<ptr target="http://arxiv.org/abs/2001.01869" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-cue correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4844" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Extreme relative pose network under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1912.11695</idno>
		<ptr target="http://arxiv.org/abs/1912.11695" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1707.08114</idno>
		<ptr target="http://arxiv.org/abs/1707.08114" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Path-invariant map networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="11084" to="11094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">RGB bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
