<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
							<email>zhouyuyiner@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
							<email>efishman@jhmi.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">The Johns Hopkins Medical Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>alan.l.yuille@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">The Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach <ref type="bibr" target="#b45">[46]</ref>, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage.</p><p>This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper focuses on small organ (e.g., the pancreas) segmentation from abdominal CT scans, which is an important prerequisite for enabling computers to assist human doctors for clinical purposes. This problem falls into the coronal view ( -axis) sagittal view ( -axis) axial view ( -axis) NIH Case #001 <ref type="figure">Figure 1</ref>. A typical example from the NIH pancreas segmentation dataset <ref type="bibr" target="#b33">[34]</ref> (best viewed in color). We highlight the pancreas in red seen from three different viewpoints. It is a relatively small organ with irregular shape and boundary. research area named medical imaging analysis. Recently, great progress has been brought to this field by the fast development of deep learning, especially convolutional neural networks <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b26">[27]</ref>. Many conventional methods, such as the graph-based segmentation approaches <ref type="bibr" target="#b0">[1]</ref> or those based on handcrafted local features <ref type="bibr" target="#b40">[41]</ref>, have been replaced by deep segmentation networks, which typically produce higher segmentation accuracy <ref type="bibr" target="#b32">[33]</ref> <ref type="bibr" target="#b33">[34]</ref>.</p><p>Segmenting a small organ from CT scans is often challenging. As the target often occupies a small part of input data (e.g., less than 1.5% in a 2D image, see <ref type="figure">Figure 1</ref>), deep segmentation networks such as FCN <ref type="bibr" target="#b26">[27]</ref> and DeepLab <ref type="bibr" target="#b4">[5]</ref> can be easily confused by the background region, which may contain complicated and variable contents. This motivates researchers to propose a coarse-to-fine approach <ref type="bibr" target="#b45">[46]</ref> with two stages, in which the coarse stage provides a rough localization and the fine stage performs accurate segmentation. But, despite state-of-the-art performance achieved in pancreas segmentation, this method suffers from inconsistency between its training and testing flowcharts, which is to say, the training phase dealt with coarse and fine stages individually and did not minimize a global energy function, but the testing phase assumed that these two stages can cooperate with each other in an iterative process. From another perspective, this also makes it difficult for multistage visual cues to be incorporated in segmentation, e.g., the previous segmentation mask which carries rich information is discarded except for the bounding box. As a part of its consequences, the fine stage consisting of a sequence of iterations cannot converge very well, and sometimes the fine stage produced even lower segmentation accuracy than the coarse stage (see Section 3.1).</p><p>Motivated to alleviate these shortcomings, we propose a Recurrent Saliency Transformation Network. The chief innovation is to relate the coarse and fine stages with a saliency transformation module, which repeatedly transforms the segmentation probability map from previous iterations as spatial priors in the current iteration. This brings us two-fold advantages over <ref type="bibr" target="#b45">[46]</ref>. First, in the training phase, the coarse-scaled and fine-scaled networks are optimized jointly, so that the segmentation ability of each of them gets improved. Second, in the testing phase, the segmentation mask of each iteration is preserved and propagated throughout iterations, enabling multi-stage visual cues to be incorporated towards more accurate segmentation. To the best of our knowledge, this idea was not studied in the computer vision community, as it requires making use of some special properties of CT scans (see Section 3.4).</p><p>We perform experiments on two CT datasets for small organ segmentation. On the NIH pancreas segmentation dataset <ref type="bibr" target="#b33">[34]</ref>, our approach outperforms the state-of-theart by an average of over 2%, measured by the average Dice-Sørensen coefficient (DSC). On another multi-organ dataset collected by the radiologists in our team, we also show the superiority of our approach over the baseline on a variety of small organs. In the testing phase, our approach enjoys better convergence properties, which guarantees its efficiency and reliability in real clinical applications.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews related work, and Section 3 describes the proposed approach. After experiments are shown in Sections 4 and 5, we draw our conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Computer-aided diagnosis (CAD) is an important technique which can assist human doctors in many clinical scenarios. An important prerequisite of CAD is medical imaging analysis. As a popular and cheap way of medical imaging, contrast-enhanced computed tomography (CECT) produces detailed images of internal organs, bones, soft tissues and blood vessels. It is of great value to automatically segment organs and/or soft tissues from these CT volumes for further diagnosis <ref type="bibr">[</ref>  <ref type="bibr" target="#b40">[41]</ref>, etc. Small organs (e.g., the pancreas) are often more difficult to segment, partly due to their low contrast and large anatomical variability in size and (most often irregular) shape.</p><p>Compared to the papers cited above which used conventional approaches for segmentation, the progress of deep learning brought more powerful and efficient solutions. In particular, convolutional neural networks have been widely applied to a wide range of vision tasks, such as image classification <ref type="bibr" target="#b17">[18]</ref>[37] <ref type="bibr" target="#b13">[14]</ref>, object detection [10] <ref type="bibr" target="#b31">[32]</ref>, and semantic segmentation <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b4">[5]</ref>. Recurrent neural networks, as a related class of networks, were first designed to process sequential data <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b38">[39]</ref>, and later generalized to image classification <ref type="bibr" target="#b21">[22]</ref> and scene labeling <ref type="bibr" target="#b30">[31]</ref> tasks. In the area of medical imaging analysis, in particular organ segmentation, these techniques have been shown to significantly outperform conventional approaches, e.g., segmenting the liver <ref type="bibr" target="#b7">[8]</ref>, the lung <ref type="bibr" target="#b11">[12]</ref>, or the pancreas <ref type="bibr" target="#b34">[35]</ref>[3] <ref type="bibr" target="#b35">[36]</ref>. Note that medical images differ from natural images in that data appear in a volumetric form. To deal with these data, researchers either slice a 3D volume into 2D slices (as in this work), or train a 3D network directly <ref type="bibr" target="#b28">[29]</ref>[30] <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b42">[43]</ref>. In the latter case, limited GPU memory often leads to patchbased training and testing strategies. The tradeoff between 2D and 3D approaches is discussed in <ref type="bibr" target="#b19">[20]</ref>.</p><p>By comparison to the entire CT volume, the organs considered in this paper often occupy a relatively small area. As deep segmentation networks such as FCN <ref type="bibr" target="#b26">[27]</ref> are less accurate in depicting small targets, researchers proposed two types of ideas to improve detection and/or segmentation performance. The first type involved rescaling the image so that the target becomes comparable to the training samples <ref type="bibr" target="#b41">[42]</ref>, and the second one considered to focus on a subregion of the image for each target to obtain higher accuracy in detection <ref type="bibr" target="#b3">[4]</ref> or segmentation <ref type="bibr" target="#b45">[46]</ref>. The coarse-to-fine idea was also well studied in the computer vision area for saliency detection <ref type="bibr" target="#b18">[19]</ref> or semantic segmentation <ref type="bibr" target="#b20">[21]</ref> <ref type="bibr" target="#b23">[24]</ref>. This paper is based on a recent coarse-to-fine framework <ref type="bibr" target="#b45">[46]</ref>, but we go one step further by incorporating multi-stage visual cues in optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We investigate the problem of segmenting an organ from abdominal CT scans. Let a CT image be a 3D volume X of size W × H × L which is annotated with a binary groundtruth segmentation Y where y i = 1 indicates a foreground voxel. The goal of our work is to produce a binary output volume Z of the same dimension. Denote Y and Z as the set of foreground voxels in the ground-truth and prediction, i.e., Y = {i | y i = 1} and Z = {i | z i = 1}. The accuracy of segmentation is evaluated by the Dice-Sørensen coefficient</p><formula xml:id="formula_0">(DSC): DSC(Y, Z) = 2×|Y∩Z|</formula><p>|Y|+|Z| . This metric falls in the range of [0, 1] with 1 implying perfect segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse Segmentation</head><p>Fine Segmentation axial view ( -axis) DSC = 57.03% DSC = 45.42% <ref type="figure">Figure 2</ref>. A failure case of the stage-wise pancreas segmentation approach <ref type="bibr" target="#b45">[46]</ref> (in the axial view, best viewed in color). The red masks show ground-truth segmentations, and the green frames indicate the bounding box derived from the coarse stage. In either slice, unsatisfying segmentation is produced at the fine stage, because the cropped region does not contain enough contextual information, whereas the coarse-scaled probability map carrying such information is discarded. This is improved by the proposed Recurrent Saliency Transformation Network, see <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Coarse-to-Fine Segmentation and Drawbacks</head><p>We start with training 2D deep networks for 3D segmentation <ref type="bibr" target="#b0">1</ref> . Each 3D volume X is sliced along three axes, the coronal, sagittal and axial views, and these 2D slices are denoted by X C,w (w = 1, 2, . . . , W ), X S,h (h = 1, 2, . . . , H) and X A,l (l = 1, 2, . . . , L), where the subscripts C, S and A stand for coronal, sagittal and axial, respectively. On each axis, an individual 2D-FCN [27] on a 16-layer VG-GNet <ref type="bibr" target="#b36">[37]</ref> is trained 2 . Three FCN models are denoted by M C , M S and M A , respectively. We use the DSC loss <ref type="bibr" target="#b29">[30]</ref> in the training phase so as to prevent the models from being biased towards the background class. Both multislice segmentation (3 neighboring slices are combined as a basic unit in training and testing) and multi-axis fusion (majority voting over three axes) are performed to incorporate pseudo-3D information into segmentation.</p><p>The organs investigated in this paper (e.g., the pancreas) are relatively small. In each 2D slice, the fraction of the foreground pixels is often smaller than 1.5%. To prevent deep networks such as FCN <ref type="bibr" target="#b26">[27]</ref> from being confused by the complicated and variable background contents, <ref type="bibr" target="#b45">[46]</ref> 1 Please see Section 4.3 for the comparison to 3D networks. <ref type="bibr" target="#b1">2</ref> This is a simple segmentation baseline with a relatively shallow network. Deeper network structures such as ResNet <ref type="bibr" target="#b13">[14]</ref> and more complicated segmentation frameworks such as DeepLab <ref type="bibr" target="#b4">[5]</ref>, while requiring a larger memory and preventing us from training two stages jointly (see Section 3.2), often result in lower segmentation accuracy as these models seem to over-fit in these CT datasets.</p><p>proposed to focus on a smaller input region according to an estimated bounding box. On each viewpoint, two networks were trained for coarse-scaled segmentation and finescaled segmentation, respectively. In the testing process, the coarse-scaled network was first used to obtain the rough position of the pancreas, and the fine-scaled network was executed several times and the segmentation mask was updated iteratively until convergence.</p><p>Despite the significant accuracy gain brought by this approach, we notice a drawback originating from the inconsistency between its training and testing strategies. That is to say, the training stage dealt with two networks individually without enabling global optimization, but the testing phase assumed that they can cooperate with each other in a sequence of iterations. From another perspective, a pixel-wise segmentation probability map was predicted by the coarse stage, but the fine stage merely preserved the bounding box and discarded the remainder, which is a major information loss. Sometimes, the image region within the bounding box does not contain sufficient spatial contexts, and thus the fine stage can be confused and produce even lower segmentation accuracy than the coarse stage. A failure case is shown in <ref type="figure">Figure 2</ref>. This motivates us to connect these two stages with a saliency transformation module so as to jointly optimize their parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Recurrent Saliency Transformation Network</head><p>Following the baseline approach, we train an individual model for each of the three viewpoints. Without loss of generality, we consider a 2D slice along the axial view, denoted by X A,l . Our goal is to infer a binary segmentation mask Z A,l of the same dimensionality. In the context of deep neural networks <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b4">[5]</ref>, this is often achieved by first computing a probability map P A,l = f [X A,l ; θ], where f [·; θ] is a deep segmentation network (FCN throughout this paper) with θ being network parameters, and then binarizing P A,l into Z A,l using a fixed threshold of 0.5, i.e., Z A,l = I[P A,l 0.5].</p><p>In order to assist segmentation with the probability map, we introduce P A,l as a latent variable. We introduce a saliency transformation module, which takes the probability map to generate an updated input image, i.e., I A,l = X A,l g(P A,l ; η), and uses the updated input I A,l to replace X A,l . Here g[·; η] is the transformation function with parameters η, and denotes element-wise product, i.e., the transformation function adds spatial weights to the original input image. Thus, the segmentation process becomes:</p><formula xml:id="formula_1">P A,l = f [X A,l g(P A,l ; η) ; θ].<label>(1)</label></formula><p>This is a recurrent neural network. Note that the saliency transformation function g[·, η] needs to be differentiable so that the entire recurrent network can be optimized in an end-to-end manner. As X A,l and P A,l share the same <ref type="figure">Figure 3</ref>. We formulate our approach into a recurrent network, and unfold it for optimization and inference. spatial dimensionality, we set g[·, η] to be a size-preserved convolution, which allows the weight added to each pixel to be determined by the segmentation probabilities in a small neighborhood around it. As we will show in the experimental section (see <ref type="figure">Figure 5</ref>), the learned convolutional kernels are able to extract complementary information to help the next iteration.</p><formula xml:id="formula_2">A, A, A, A, A, −1 A, −1 A, A, A, A, A, +1 A, +1 unfolding A, A, −1 A, A, +1</formula><p>To optimize Eqn <ref type="formula" target="#formula_1">(1)</ref>, we unfold the recurrent network into a plain form (see <ref type="figure">Figure 3</ref>). Given an input image X A,l and an integer T which is the maximal number of iterations, we update I (t) A,l and P (t) A,l , t = 0, 1, . . . , T :</p><formula xml:id="formula_3">I (t) A,l = X A,l g P (t−1) A,l ; η ,<label>(2)</label></formula><formula xml:id="formula_4">P (t) A,l = f I (t) A,l ; θ .<label>(3)</label></formula><p>Note that the original input image X A,l does not change, and the parameters θ and η are shared by all iterations. At t = 0, we directly set I</p><p>A,l = X A,l . When segmentation masks P (t) A,l (t = 0, 1, . . . , T − 1) are available for reference, deep networks benefit considerably from a shrunk input region especially when the target organ is very small. Thus, we define a cropping function</p><formula xml:id="formula_6">Crop ·; P (t) A,l , which takes P (t) A,l as the reference map, binarizes it into Z (t) A,l = I P (t) A,l</formula><p>0.5 , finds the minimal rectangle covering all the activated pixels, and adds a Kpixel-wide margin (padding) around it. We fix K to be 20; our algorithm is not sensitive to this parameter.</p><p>Finally note that I</p><p>A,l , the original input (the entire 2D slice), is much larger than the cropped inputs I (t) A,l for t &gt; 0. We train two FCN's to deal with such a major difference in input data. The first one is named the coarse-scaled segmentation network, which is used only in the first iteration. The second one, the fine-scaled segmentation network, takes the charge of all the remaining iterations. We denote their parameters by θ C and θ F , respectively. These two FCN's are optimized jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: The Testing Phase</head><p>Input : input volume X, viewpoint V = {C, S, A}; parameters θ C v , θ F v and η v , v ∈ V; max number of iterations T , threshold thr; Output: segmentation volume Z;</p><formula xml:id="formula_8">1 t ← 0, I (0) v ← X, v ∈ V; 2 P (0) v,l ← f I (0) v,l ; θ C v , v ∈ V, ∀l; 3 P (0) = P (0) C +P (0) S +P (0) A 3 , Z (0) = I P (0) 0.5 ; 4 repeat 5 t ← t + 1; 6 I (t) v,l ← X v,l g P (t−1) v,l ; η , v ∈ V, ∀l; 7 P (t) v,l ← f Crop I (t) v,l ; P (t−1) v,l ; θ F v , v ∈ V, ∀l; 8 P (t) = P (t) C +P (t) S +P (t) A 3 , Z (t) = I P (t) 0.5 ; 9 until t = T or DSC Z (t−1) , Z (t) thr; Return: Z ← Z (t) .</formula><p>We compute a DSC loss term on each probability map P (t) A,l , t = 0, 1, . . . , T , and denote it by L Y A,l , P</p><formula xml:id="formula_9">(t) A,l .</formula><p>Here, Y A,l is the ground-truth segmentation mask, and L{Y, P} = 1 − 2× i YiPi i Yi+Pi is based on a soft version of DSC <ref type="bibr" target="#b29">[30]</ref>. Our goal is to minimize the overall loss:</p><formula xml:id="formula_10">L = T t=0 λ t · L Y (t) A,l , Z (t) A,l .<label>(4)</label></formula><p>This leads to joint optimization over all iterations, which involves network parameters θ C , θ F , and transformation parameters η. {λ t } T t=0 controls the tradeoff among all loss terms. We set 2λ 0 = λ 1 = . . . = λ T = 2/ (2T + 1) so as to encourage accurate fine-scaled segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Testing</head><p>The training phase is aimed at minimizing the loss function L, defined in Eqn <ref type="formula" target="#formula_10">(4)</ref>, which is differentiable with respect to all parameters. In the early training stages, the coarse-scaled network cannot generate reasonable probability maps. To prevent the fine-scaled network from being confused by inaccurate input regions, we use the groundtruth mask Y A,l as the reference map. After a sufficient number of training, we resume using P (t) A,l instead of Y A,l . In Section 4.2, we will see that this "fine-tuning" strategy improves segmentation accuracy considerably.</p><p>Due to the limitation in GPU memory, in each minibatch containing one training sample, we set T to be the maximal integer (not larger than 5) so that we can fit the entire framework into the GPU memory. The overall framework is illustrated in <ref type="figure">Figure 4</ref>. As a side note, we find that setting T ≡ 1 also produces high accuracy, suggesting that major improvement is brought by joint optimization.  <ref type="figure">Figure 4</ref>. Illustration of the training process (best viewed in color). We display an input image along the axial view which contains 3 neighboring slices. To save space, we only plot the coarse stage and the first iteration in the fine stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-scaled Network</head><p>The testing phase follows the flowchart described in Algorithm 1. There are two minor differences from the training phase. First, as the ground-truth segmentation mask Y A,l is not available, the probability map P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(t)</head><p>A,l is always taken as the reference map for image cropping. Second, the number of iterations is no longer limited by the GPU memory, as the intermediate outputs can be discarded on the way. In practice, we terminate our algorithm when the similarity of two consecutive predictions, measured by</p><formula xml:id="formula_11">DSC Z (t−1) , Z (t) = 2× i Z (t−1) i Z (t) i i Z (t−1) i +Z (t) i</formula><p>, reaches a threshold thr, or a fixed number (T ) of iterations are executed. We will discuss these parameters in Section 4.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussions</head><p>Coarse-to-fine recognition is an effective idea in medical imaging analysis. Examples include <ref type="bibr" target="#b45">[46]</ref>, our baseline, and <ref type="bibr" target="#b3">[4]</ref> for metosis detection. Our approach can be applied to most of them towards higher recognition performance.</p><p>Attention-based or recurrent models are also widely used for natural image segmentation <ref type="bibr" target="#b5">[6]</ref>[21][42] <ref type="bibr" target="#b23">[24]</ref>. Our approach differs from them in making full use of the special properties of CT scans, e.g., each organ appears at a roughly fixed position, and has a fixed number of components. Our approach can be applied to detecting the lesion areas of an organ <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b44">[45]</ref>, or a specific type of vision problems such as hair segmentation in a face <ref type="bibr" target="#b27">[28]</ref>, or detecting the targets which are consistently small in the input images <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pancreas Segmentation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation</head><p>We evaluate our approach on the NIH pancreas segmentation dataset <ref type="bibr" target="#b33">[34]</ref>, which contains 82 contrast-enhanced abdominal CT volumes. The resolution of each scan is 512 × 512 × L, where L ∈ [181, 466] is the number of slices along the long axis of the body. The distance between neighboring voxels ranges from 0.5mm to 1.0mm.</p><p>Following the standard cross-validation strategy, we split the dataset into 4 fixed folds, each of which contains approximately the same number of samples. We apply cross validation, i.e., training the models on 3 out of 4 subsets and testing them on the remaining one. We measure the segmentation accuracy by computing the Dice-Sørensen coefficient (DSC) for each sample, and report the average and standard deviation over all 82 cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Different Settings</head><p>We use the FCN-8s model <ref type="bibr" target="#b26">[27]</ref> pre-trained on Pas-calVOC <ref type="bibr" target="#b8">[9]</ref>. We initialize the up-sampling layers with random weights, set the learning rate to be 10 −4 and run 80,000 iterations. Different options are evaluated, including using different kernel sizes in saliency transformation, and whether to fine-tune the models using the predicted segmentations as reference maps (see the description in Section 3.3). Quantitative results are summarized in <ref type="table">Table 1</ref>.</p><p>As the saliency transformation module is implemented  <ref type="table">Table 1</ref>. Accuracy (DSC, %) comparison of different settings of our approach. Please see the texts in Section 4.2 for detailed descriptions of these variants. For each variant, the "gain" is obtained by comparing its accuracy with the basic model. by a size-preserved convolution (see Section 3.2), the size of convolutional kernels determines the range that a pixel can use to judge its saliency. In general, a larger kernel size improves segmentation accuracy (3 × 3 works significantly better than 1 × 1), but we observe the marginal effect: the improvement of 5 × 5 over 3 × 3 is limited. As we use 7 × 7 kernels, the segmentation accuracy is slightly lower than that of 5 × 5. This may be caused by the larger number of parameters introduced to this module. Another way of increasing the receptive field size is to use two convolutional layers with 3 × 3 kernels. This strategy, while containing a smaller number of parameters, works even better than using one 5 × 5 layer. But, we do not add more layers, as the performance saturates while computational costs increase. As described in Section 3.3, we fine-tune these models with images cropped from the coarse-scaled segmentation mask. This is to adjust the models to the testing phase, in which the ground-truth mask is unknown, so that the finescaled segmentation needs to start with, and be able to revise the coarse-scaled segmentation mask. We use a smaller learning rate (10 −6 ) and run another 40,000 iterations. This strategy not only reports 0.52% overall accuracy gain, but also alleviates over-fitting (see <ref type="bibr">Section 4.4.3)</ref>.</p><p>In summary, all these variants produce higher accuracy than the state-of-the-art (82.37% by <ref type="bibr" target="#b45">[46]</ref>), which verifies that the major contribution comes from our recurrent framework which enables joint optimization. In the later experiments, we inherit the best variant learned from this section, including in a large-scale multi-organ dataset (see <ref type="bibr">Section 5)</ref>. That is to say, we use two 3 × 3 convolutional layers for saliency transformation, and fine-tune the models with coarse-scaled segmentation. This setting produces an average accuracy of 84.50%, as shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to the State-of-the-Art</head><p>We show that our approach works better than the baseline, i.e., the coarse-to-fine approach with two stages trained individually <ref type="bibr" target="#b45">[46]</ref>. As shown in <ref type="table">Table 2</ref>, the average improvement over 82 cases is 2.13 ± 2.67%, which is impressive given such a high baseline accuracy (82.37% is already the state-of-the-art). The standard deviations (5.68% of <ref type="bibr" target="#b45">[46]</ref> and 4.97% of ours) are mainly caused by the difference in  <ref type="table">Table 2</ref>. Accuracy (DSC, %) comparison between our approach and the state-of-the-arts on the NIH pancreas segmentation dataset <ref type="bibr" target="#b33">[34]</ref>. <ref type="bibr" target="#b43">[44]</ref> was implemented in <ref type="bibr" target="#b45">[46]</ref>. scanning and labeling qualities. The student's t-test suggests statistical significance (p = 3.62 × 10 −8 ). A case-bycase study reveals that our approach reports higher accuracies on 67 out of 82 cases, with the largest advantage being +17.60% and the largest deficit being merely −3.85%. We analyze the sources of improvement in Section 4.4.</p><p>Another related work is <ref type="bibr" target="#b43">[44]</ref> which stacks two FCN's for segmentation. Our work differs from it by (i) our model is recurrent, which allows fine-scaled segmentation to be updated iteratively, and (ii) we crop the input image to focus on the salient region. Both strategies contribute significantly to segmentation accuracy. Quantitatively, <ref type="bibr" target="#b43">[44]</ref> reported an average accuracy of 77.89%. Our approach achieves 78.23% in the coarse stage, 82.73% after only one iteration, and an entire testing phase reports 84.50%.</p><p>We briefly discuss the advantages and disadvantages of using 3D networks. 3D networks capture richer contextual information, but also require training more parameters. Our 2D approach makes use of 3D contexts more efficiently. At the end of each iteration, predictions from three views are fused, and thus the saliency transformation module carries these information to the next iteration. We implement VNet <ref type="bibr" target="#b29">[30]</ref>, and obtain an average accuracy of 83.18% with a 3D ground-truth bounding box provided for each case. Without the ground-truth, a sliding-window process is required which is really slow -an average of 5 minutes on a Titan-X Pascal GPU. In comparison, our approach needs 1.3 minutes, slower than the baseline <ref type="bibr" target="#b45">[46]</ref> (0.9 minutes), but faster than other 2D approaches <ref type="bibr">[</ref> ⋯ ⋯ <ref type="figure">Figure 5</ref>. Visualization of how recurrent saliency transformation works in coarse-to-fine segmentation (best viewed in color). This is a failure case of the stage-wise approach <ref type="bibr" target="#b45">[46]</ref> (see <ref type="figure">Figure 2</ref>), but segmentation accuracy is largely improved by making use of the probability map from the previous iteration to help the current iteration. Note that three weight maps capture different visual cues, with two of them focused on the foreground region, and the remaining one focused on the background region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Diagnosis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Joint Optimization and Mutli-Stage Cues</head><p>Our approach enables joint training, which improves both the coarse and fine stages individually. We denote the two networks trained in <ref type="bibr" target="#b45">[46]</ref> by I C and I F , and similarly, those trained in our approach by J C and J F , respectively. In the coarse stage, I C reports 75.74% and J C reports 78.23%.</p><p>In the fine stage, applying J F on top of the output of I C gets 83.80%, which is considerably higher than 82.37% (I F on top of I C ) but lower than 84.50% (J F on top of J C ). Therefore, we conclude that both the coarse-scaled and finescaled networks benefit from joint optimization. A stronger coarse stage provides a better starting point, and a stronger fine stage improves the upper-bound. In <ref type="figure">Figure 5</ref>, We visualize show how the recurrent network assists segmentation by incorporating multi-stage visual cues. This is a failure case by the baseline approach <ref type="bibr" target="#b45">[46]</ref> (see <ref type="figure">Figure 2</ref>), in which fine-scaled segmentation worked even worse because the missing contextual information. It is interesting to see that in saliency transformation, different channels deliver complementary information, i.e., two of them focus on the target organ, and the remaining one adds most weights to the background region. Similar phenomena happen in the models trained in different viewpoints and different folds. This reveal that, except for foreground, background and boundary also contribute to visual recognition <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Convergence</head><p>We study convergence, which is a very important criterion to judge the reliability of our approach. We choose the best model reporting an average accuracy of 84.50%, and record the inter-iteration DSC throughout the testing pro-</p><formula xml:id="formula_12">cess: d (t) = DSC Z (t−1) , Z (t) = 2× i Z (t−1) i Z (t) i i Z (t−1) i +Z (t) i .</formula><p>After 1, 2, 3, 5 and 10 iterations, these numbers are 0.9037, 0.9677, 0.9814, 0.9908 and 0.9964 for our approach, and 0.8286, 0.9477, 0.9661, 0.9743 and 0.9774 for <ref type="bibr" target="#b45">[46]</ref>, respectively. Each number reported by our approach is considerably higher than that by the baseline. The better convergence property provides us with the opportunity to set a more strict terminating condition, e.g., using thr = 0.99 rather than thr = 0.95.</p><p>We note that <ref type="bibr" target="#b45">[46]</ref> also tried to increase the threshold from 0.95 to 0.99, but only 3 out of 82 cases converged after 10 iterations, and the average accuracy went down from 82.37% to 82.28%. In contrary, when the threshold is increased from 0.95 to 0.99 in our approach, 80 out of 82 cases converge (in an average of 5.22 iterations), and the average accuracy is improved from 83.93% to 84.50%. In addition, the average number of iterations needed to achieve thr = 0.95 is also reduced from 2.89 in <ref type="bibr" target="#b45">[46]</ref> to 2.02 in our approach. On a Titan-X Pascal GPU, one iteration takes 0.2 minutes, so using thr = 0.99 requires an average of 1.3 minutes in each testing case. In comparison, <ref type="bibr" target="#b45">[46]</ref> needs an average of 0.9 minutes and <ref type="bibr" target="#b34">[35]</ref>   <ref type="table">Table 3</ref>. Comparison of coarse-scaled (C) and fine-scaled (F) segmentation by <ref type="bibr" target="#b45">[46]</ref> and our approach on our own dataset. A fine-scaled accuracy is indicated by if it is lower than the coarsescaled one. The pancreas segmentation accuracies are higher than those in <ref type="table">Table 2</ref>, due to the increased number of training samples and the higher resolution in CT scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">The Over-Fitting Issue</head><p>Finally, we investigate the over-fitting issue by making use of oracle information in the testing process. We follow <ref type="bibr" target="#b45">[46]</ref> to use the ground-truth bounding box on each slice, which is used to crop the input region in every iteration. Note that annotating a bounding box in each slice is expensive and thus not applicable in real-world clinical applications. This experiment is aimed at exploring the upper-bound of our segmentation networks under perfect localization. With oracle information provided, our best model reports 86.37%, which is considerably higher than the number (84.50%) without using oracle information. If we do not fine-tune the networks using coarse-scaled segmentation (see <ref type="table">Table 1</ref>), the above numbers are 86.26% and 83.68%, respectively. This is to say, fine-tuning prevents our model from relying on the ground-truth mask. It not only improves the average accuracy, but also alleviates over-fitting (the disadvantage of our model against that with oracle information is decreased by 0.67%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Mutli-Organ Segmentation Experiments</head><p>To verify that out approach applies to other organs, we collect a large dataset which contains 200 CT scans, 11 abdominal organs and 5 blood vessels. This corpus took 4 full-time radiologists around 3 months to annotate. To the best of our knowledge, this dataset is larger and contains more organs than any public datasets. We choose 5 most challenging targets including the pancreas and a blood vessel, as well as two kidneys which are relatively easier. Other easy organs such as the liver are ignored. To the best of our knowledge, some of these organs were never investigated before, but they are important in diagnosing pancreatic diseases and detecting the pancreatic cancer at an early stage. We randomly partition the dataset into 4 folds for cross validation. Each organ is trained and tested individually. When a pixel is predicted as more than one organs, we choose the one with the largest confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Segmentation by <ref type="bibr" target="#b45">[46]</ref> adrenal gland gallbladder pancreas kidneys (left/right) duodenum inferior vena cava Segmentation by RSTN <ref type="figure">Figure 6</ref>. Mutli-organ segmentation in the axial view (best viewed in color). Organs are marked in different colors (input image is shown with the ground-truth annotation).</p><p>Results are summarized in <ref type="table">Table 3</ref>, We first note that <ref type="bibr" target="#b45">[46]</ref> sometimes produced a lower accuracy in the fine stage than in the coarse stage. Apparently this is caused by the unsatisfying convergence property in iterations, but essentially, it is the loss of contextual information and the lack of globally optimized energy function. Our approach solves this problem and reports a 4.29% average improvement over 5 challenging organs (the kidneys excluded). For some organs, e.g., the gallbladder, we do not observe significant accuracy gain by iterations. But we emphasize that in these scenarios, our coarse stage already provides much higher accuracy than the fine stage of <ref type="bibr" target="#b45">[46]</ref>, and the our fine stage preserves such high accuracy through iterations, demonstrating stability. An example is displayed in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This work is motivated by the difficulty of small organ segmentation. As the target is often small, it is required to focus on a local input region, but sometimes the network is confused due to the lack of contextual information. We present the Recurrent Saliency Transformation Network, which enjoys three advantages. (i) Benefited by a (recurrent) global energy function, it is easier to generalize our models from training data to testing data. (ii) With joint optimization over two networks, both of them get improved individually. (iii) By incorporating multi-stage visual cues, more accurate segmentation results are obtained. As the fine stage is less likely to be confused by the lack of contexts, we also observe better convergence during iterations.</p><p>Our approach is applied to two datasets for pancreas segmentation and multi-organ segmentation, and outperforms the baseline (the state-of-the-art) significantly. Confirmed by the radiologists in our team, these segmentation results are helpful to computer-assisted clinical diagnoses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2][40][13][45]. To capture specific properties of different organs, researchers often design individualized algorithms for each of them. Typical examples include the the liver [25][15], the spleen [26], the kidneys [23][1], the lungs [16], the pancreas [7]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>34][35] (2-3 minutes).</figDesc><table><row><cell>NIH Case #031</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>axial view ( -axis)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">fusion @ R1, DSC = 75.80%</cell><cell></cell><cell cols="2">fusion @ R8, DSC = 80.24%</cell><cell></cell><cell>Fine Segmentation</cell></row><row><cell>slice #1</cell><cell>slice #1</cell><cell>slice #1</cell><cell>slice #1</cell><cell>slice #1</cell><cell>slice #1</cell><cell>slice #1</cell><cell>DSC = 45.42% stage-wise @ R7</cell></row><row><cell>slice #2</cell><cell>slice #2</cell><cell>slice #2</cell><cell>slice #2</cell><cell>slice #2</cell><cell>slice #2</cell><cell>slice #2</cell><cell></cell></row><row><cell>slice #3</cell><cell>slice #3</cell><cell>slice #3</cell><cell>slice #3</cell><cell>slice #3</cell><cell>slice #3</cell><cell>slice #3</cell><cell></cell></row><row><cell>Coarse Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fine Segmentation</cell></row><row><cell>DSC = 57.03%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>joint-training @ R9</cell></row><row><cell>weights @ R1</cell><cell>input @ R1</cell><cell>P-map @ R1</cell><cell>P-map @ R8</cell><cell>weights @ R9</cell><cell cols="2">input @ R9 P-map @ R9</cell><cell>DSC = 80.39%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Cuts Framework for Kidney Segmentation with Prior Shape Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep 3D Convolutional Encoder Networks with Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Traboulsee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1239" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Improving Deep Pancreas Segmentation in CT and MRI Images via Recurrent Neural Contextual Learning and Direct Loss Function. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mitosis Detection in Breast Cancer Histology Images via Deep Cascaded Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention to Scale: Scale-aware Semantic Image Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<title level="m">Multiorgan Segmentation based on Spatially-Divided Probabilistic Atlas from 3D Abdominal CT Images. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Speech Recognition with Deep Recurrent Neural Networks. International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive and Multi-Path Holistically Nested Neural Networks for Pathological Lung Segmentation from CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brain Tumor Segmentation with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison and Evaluation of Methods for Liver Segmentation from CT Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Arzhaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aurich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bekes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1251" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Lung Segmentation for Accurate Quantitation of Volumetric Xray CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="490" to="498" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent Attentional Networks for Saliency Detection. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02000</idno>
		<title level="m">Deep Learning for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Instance-Level Salient Object Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Recurrent Convolutional Neural Network for Object Recognition. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computer-Aided Kidney Segmentation on Abdominal CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">RefineNet: Multi-Path Refinement Networks with Identity Mappings for High-Resolution Semantic Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning-based Automatic Liver Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suehling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated Segmentation and Quantification of Liver and Spleen from CT Images Using Normalized Probabilistic Atlases and Enhancement Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully Convolutional Networks for Semantic Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure-Aware Hair Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<title level="m">Vascular Boundary Detection. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<title level="m">V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation. International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<title level="m">Recurrent Convolutional Neural Networks for Scene Labeling. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">U-Net: Convolutional Networks for Biomedical Image Segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeporgan</surname></persName>
		</author>
		<title level="m">Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<title level="m">Spatial Aggregation of Holistically-Nested Networks for Automated Pancreas Segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00045</idno>
		<title level="m">Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to Localize Little Landmarks. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing Natural Scenes and Natural Language with Recursive Neural Networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gargeya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05718</idno>
		<title level="m">Deep Learning for Identifying Metastatic Breast Cancer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<title level="m">Geodesic Patch-based Segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Volumetric ConvNets with Mixed Residual Connections for Automated Prostate Segmentation from 3D MR Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coarseto-Fine Stacked Fully Convolutional Nets for Lymph Node Segmentation in Ultrasound Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object Recognition with and without Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
