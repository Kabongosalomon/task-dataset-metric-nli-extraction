<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
							<email>tianyug@princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/ RE-Context-or-Names.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An example for the information provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. engines <ref type="bibr" target="#b34">(Xiong et al., 2017)</ref>. With the recent advance of deep learning, neural relation extraction (NRE) models <ref type="bibr" target="#b31">(Socher et al., 2012;</ref><ref type="bibr" target="#b17">Liu et al., 2013;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref> have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks.</p><p>The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names).</p><p>From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in <ref type="figure">Figure 1</ref>, "... be founded ... by ..." is a pattern for the relation founded by. The early RE systems <ref type="bibr" target="#b13">(Huffman, 1995;</ref><ref type="bibr" target="#b4">Califf and Mooney, 1997)</ref> formalize patterns into string templates and determine relations by matching these templates. The later neural models <ref type="bibr" target="#b31">(Socher et al., 2012;</ref><ref type="bibr" target="#b17">Liu et al., 2013)</ref> prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better.</p><p>Besides, entity mentions also provide much information for relation classification. As shown in <ref type="figure">Figure 1</ref>, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE <ref type="bibr" target="#b39">(Zhang et al., 2019;</ref><ref type="bibr" target="#b25">Peters et al., 2019)</ref>. Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training <ref type="bibr" target="#b26">(Petroni et al., 2019)</ref>.</p><p>In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that:</p><p>(1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities.</p><p>(2) Existing RE benchmarks may leak shallow cues via entity mentions, which contribute to the high performance of existing models. Our experiments show that models still can achieve high performance only given entity mentions as input, suggesting that there exist biased statistical cues from entity mentions in these datasets.</p><p>The above observations demonstrate how existing models work on RE datasets, and suggest a way to further improve RE models: we should enhance them via better understanding context and utilizing entity types, while preventing them from simply memorizing entities or exploiting biased cues in mentions. From these points, we investigate an entity-masked contrastive pre-training framework for RE. We use Wikidata to gather sentences that may express the same relations, and let the model learn which sentences are close and which are not in relational semantics by a contrastive objective. In this process, we randomly mask entity mentions to avoid being biased by them. We show its effectiveness across several settings and benchmarks, and suggest that better pre-training technique is a reliable direction towards better RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pilot Experiment and Analysis</head><p>To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models and Dataset</head><p>There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses:</p><p>CNN We use the convolutional neural networks described in <ref type="bibr" target="#b24">Nguyen and Grishman (2015)</ref> and augment the inputs with part-of-speech, named entity recognition and position embeddings following <ref type="bibr" target="#b38">Zhang et al. (2017)</ref>.</p><p>BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini <ref type="bibr" target="#b1">Soares et al. (2019)</ref>. In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. <ref type="bibr">dini Soares et al., 2019)</ref> is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly released, we pre-train a BERT base version of MTB and give the details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching the blanks (MTB) MTB (Bal</head><p>There are also a number of public benchmarks for RE, and we select the largest supervised RE dataset TACRED <ref type="bibr" target="#b38">(Zhang et al., 2017)</ref> in our pilot experiments. TACRED is a supervised RE dataset with 106, 264 instances and 42 relations, which also provides type annotations for each entity.</p><p>Note that we use more models and datasets in our main experiments, of which we give detailed descriptions and analyses in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Settings</head><p>We use several input formats for RE, based on which we can observe the effects of context and entity mentions in controllable experiments. The following two formats are adopted by previous literature and are close to the real-world RE scenarios:</p><p>Context+Mention (C+M) This is the most widely-used RE setting, where the whole sentence  (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings <ref type="bibr" target="#b36">(Zeng et al., 2014)</ref> for the CNN model and special entity markers <ref type="bibr" target="#b39">(Zhang et al., 2019;</ref><ref type="bibr" target="#b1">Baldini Soares et al., 2019)</ref> for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from <ref type="bibr" target="#b38">Zhang et al. (2017)</ref>, we do not repeat the special tokens for entity-length times to avoid leaking entity length information.</p><p>Besides the above settings, we also adopt three synthetic settings to study how much information context or mentions contribute to RE respectively: Only Context (OnlyC) To analyze the contribution of textual context to RE, we replace all entity mentions with the special tokens [SUBJ] and <ref type="bibr">[OBJ]</ref>. In this case, the information source of entity mentions is totally blocked. Only Mention (OnlyM) In this setting, we only provide entity mentions and discard all the other textual context for the input. Only Type (OnlyT) This is similar to OnlyM, except we only provide entity types in this case. <ref type="table" target="#tab_1">Table 1</ref> shows a detailed comparison across different input formats and models on TACRED. From the results we can see that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Result Analysis</head><p>(1) Both textual context and entity mentions provide critical information to support relation classification, and the most useful information in entity mentions is type information. As shown in <ref type="table" target="#tab_1">Table 1</ref>  by entity mentions is their type information. We also provide several case studies in Section 2.4, which further verify this conclusion.</p><p>(2) There are superficial cues leaked by mentions in existing RE datasets, which may contribute to the high performance of RE models. We observe high performance on OnlyM with all three models on TACRED, and this phenomenon also exists in other datasets (see <ref type="table" target="#tab_9">Table 5</ref>). We also take a deep look into the performance drop of OnlyC compared to C+M in Section 2.4, and find out that in some cases that models cannot well understand the context, they turn to rely on shallow heuristics from mentions. It inspires us to further improve models in extracting relations from context while preventing them from rote memorization of entity mentions.</p><p>We notice that CNN results are a little inconsistent with BERT and MTB: CNN on OnlyC is almost the same as OnlyM, and C+M is 5% lower than C+T. We believe that it is mainly due to the limited encoding power of CNN, which cannot fully utilize context and is more easily to overfit the shallow cues of entity mentions in the datasets.  <ref type="table">Table 3</ref>: Case study on unique wrong predictions made by OnlyC (compared to C+M). We sample 10% of the wrong predictions, filter the wrong-labeled instances and manually annotate the wrong types to get the proportions. We use red and blue to highlight the subject and object entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Case Study on TACRED</head><p>To further understand how performance varies on different input formats, we carry out a thorough case study on TACRED. We choose to demonstrate the BERT examples here because BERT represents the state-of-the-art class of models and we have observed a similar result on MTB. First we compare C+M and C+T. We find out that C+M shares 95.7% correct predictions with C+T, and 68.1% wrong predictions of C+M are the same as C+T. It indicates that most information models take advantage of from entity mentions is their type information. We also list some of the unique errors of C+M and C+T in Table 2. C+M may be biased by the entity distributions in the training set. For the two examples in <ref type="table" target="#tab_3">Table 2</ref>, "Washington" is only involved in per:stateorprovince of residence and "Bank of America Corp." is only involved in no relation in the training set, and this bias may cause the error. On the other hand, C+T may have difficulty to correctly understand the text without specific entity mentions. As shown in the example, after replacing mentions with their types, the model is confused by "general assembly" and fails to detect the relation between "Ruben" and "Ruben van Assouw". It suggests that entity mentions provide information other than types to help models understand the text.</p><p>We also study why OnlyC suffers such a significant drop compared to C+M. In <ref type="table">Table 3</ref>, we cluster all the unique wrong predictions made by OnlyC (compared to C+M) into three classes. "Wrong" represents sentences with clear patterns but misun-derstood by the model. "No pattern" means that after masking the entity mentions, it is hard to tell what relation it is even for humans. "Confusing" indicates that after masking the entities, the sentence becomes ambiguous (e.g., confusing cities and countries). As shown in <ref type="table">Table 3</ref>, in almost half (42%) of the unique wrong predictions of On-lyC, the sentence has a clear relational pattern but the model fails to extract it, which suggests that in C+M, the model may rely on shallow heuristics from entity mentions to correctly predict the sentences. In the rest cases, entity mentions indeed provide critical information for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contrastive Pre-training for RE</head><p>From the observations in Section 2, we know that both context and entity type information is beneficial for RE models. However, in some cases RE models cannot well understand the relational patterns in context and rely on the shallow cues of entity mentions for classification. In order to enhance the ability to grasp entity types and extract relational facts from context, we propose the entity-masked contrastive pre-training framework for RE. We start with the motivation and process of relational contrastive example generation, and then go through the pre-training objective details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relational Contrastive Example Generation</head><p>We expect that by pre-training specifically towards RE, our model can be more effective at encoding relational representations from textual context and modeling entity types from mentions. To do so,  <ref type="figure">Figure 2</ref>: Our contrastive pre-training framework for RE. We assign relations to sentences by linking entity pairs in sentences to Wikidata and checking their relations in the KG. We assume that sentences with the same relation should have similar representations, and those with different relations should be pushed apart. Entity mentions are randomly masked (boxes with colored background) to avoid simple memorization. Compared to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names.</p><p>we adopt the idea of contrastive learning <ref type="bibr" target="#b8">(Hadsell et al., 2006)</ref>, which aims to learn representations by pulling "neighbors" together and pushing "nonneighbors" apart. After this, "neighbor" instances will have similar representations. So it is important to define "neighbors" in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision <ref type="bibr" target="#b23">(Mintz et al., 2009)</ref>, we assume that sentences with entity pairs sharing the same relation in KGs are "neighbors".</p><p>Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as X A and X B , which have entity mentions h A , t A and h B , t B respectively. We define X A and X B as "neighbors" if there is a relation r such that (h A , r, t A ) ∈ K and (h B , r, t B ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentence pair (X A , X B ) linked to r. To learn contrastively, we randomly sample N sentences X i B , 1 ≤ i ≤ N so they can form N negative pairs with X A . The model needs to classify which sentence among all the postive and negative samples has the same relation with X A .</p><p>To avoid memorizing entity mentions or extracting shallow features from them during pre-training, we randomly mask entity mentions with the special token <ref type="bibr">[BLANK]</ref>. We use P BLANK to denote the ratio of replaced entities and set P BLANK = 0.7 following Baldini Soares et al. <ref type="bibr">(2019)</ref>. Note that masking all mentions during pre-training is also not a good option since it will create a gap be-tween pre-training and fine-tuning and also block the pre-trained models from utilizing entity mention information (e.g., learning entity types).</p><p>Take an example to understand our data generation process: In <ref type="figure">Figure 2</ref>, there are two sentences "SpaceX was founded in 2002 by Elon Musk" and "As the co-founder of Microsoft, Bill Gates ..." where both (SpaceX, founded by, Elon Musk) and (Microsoft, founded by, Bill Gates) exist in the KG. We expect the two sentences to have similar representations reflecting the relation. On the other hand, for the other two sentences in the right part of the figure, since their entity pairs do not have the relation founded by, they are regarded as negative samples and are expected to have diverse representations from the left one. During pre-training, each entity mention has a probability of P BLANK to be masked.</p><p>The main problem of the generation process is that the sentence may express no relation between the entities at all, or express the relation different from what we expect. For example, a sentence mentioning "SpaceX" and "Elon Musk" may express the relation founded by, CEO or CTO, or simply does not express any relation between them. An example could be "Elon Musk answers reporters' questions on a SpaceX press conference", which expresses no clear relation between the two. However, we argue that the noise problem is not critical for our pre-training framework: Our goal is to get relatively better representations towards RE compared to raw pre-trained models like BERT, rather than to directly train an RE model for downstream tasks, so noise in the data is acceptable.  <ref type="table">Table 4</ref>: Statistics for RE datasets used in the paper, including numbers of relations, numbers of instances and proportions of N/A instances. "-" for the last column means that there is no N/A relation in the dataset.</p><p>With the help of the generated relational contrastive examples, our model can learn to better grasp type information from mentions and extract relational semantics from textual context: (1) The paired two sentences, which mention different entity pairs but share the same relation, prompt the model to discover the connections between these entity mentions for the relation. Besides, the entity masking strategy can effectively avoid simply memorizing entities. This eventually encourages the model to exploit entity type information.</p><p>(2) Our generation strategy provides a diverse set of textual context expressing the same relation to the model, which motivates the model to learn to extract the relational patterns from a variety of expressions. Compared with our model, MTB (Baldini Soares et al., 2019) takes a more strict rule which requires the two sampled sentences to share the same entity pair. While it reduces the noise, the model also samples data with less diversity and loses the chance to learn type information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objectives</head><p>In our contrastive pre-training, we use the same Transformer architecture <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> as BERT. Denote the Transformer encoder as ENC and the output at the position i as ENC i (·). For the input format, we use special markers to highlight the entity mentions following Baldini Soares et al. During the pre-training, we have two objectives: contrastive pre-training objective and masked language modeling objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive</head><p>Pre-training Objective As shown in <ref type="figure">Figure 2</ref>, given the positive sentence pair (x A , x B ), and negative sentence pairs (x A , x i B ), 1 ≤ i ≤ N , we first use the Transformer encoder to get relation-aware representation for</p><formula xml:id="formula_1">x in {x A , x B } ∪ {x i B } N i=1 : x = ENC h (x) ⊕ ENC t (x),<label>(1)</label></formula><p>where h and t are the positions of special tokens [E1] and [E2], and ⊕ stands for concatenation. With the sentence representation, we have the following training objective:</p><formula xml:id="formula_2">L CP = − log e x T A x B e x T A x B + i≤N i=1 e x T A x i B .<label>(2)</label></formula><p>By optimizing the model with respect to L CP , we expect representations for x A and x B to be closer and eventually sentences with similar relations will have similar representations. Masked Language Modeling Objective To maintain the ability of language understanding inherited from BERT and avoid catastrophic forgetting <ref type="bibr" target="#b21">(McCloskey and Cohen, 1989)</ref>, we also adopt the masked language modeling (MLM) objective from BERT. MLM randomly masks tokens in the inputs and by letting the model predict the masked tokens, MLM learns contextual representation that contains rich semantic and syntactic knowledge. Denote the MLM loss as L M LM .</p><p>Eventually, we have the following training loss:</p><formula xml:id="formula_3">L = L CP + L M LM .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we explore the effectiveness of our relational contrastive pre-training across two typical RE tasks and several RE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RE Tasks</head><p>For comprehensive experiments, we evaluate our models on various RE tasks and datasets. Supervised RE This is the most widely-adopted setting in RE, where there is a pre-defined relation set R and each sentence x in the dataset expresses one of the relations in R. In some benchmarks, there is a special relation named N/A or no relation, indicating that the sentence does not express any relation between the given entities, or their relation is not included in R.</p><p>For supervised RE datasets, we use TACRED <ref type="bibr" target="#b38">(Zhang et al., 2017)</ref>, <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>), Wiki80  and ChemProt <ref type="bibr" target="#b15">(Kringelum et al., 2016)</ref>. <ref type="table">Table 4</ref> shows the comparison between the datasets.  We also add 1% and 10% settings, meaning using only 1% / 10% data of the training sets. It is to simulate a low-resource scenario and observe how model performance changes across different datasets and settings. Note that ChemProt only has 4, 169 training instances, which leads to the abnormal results on 1% ChemProt in <ref type="table" target="#tab_9">Table 5</ref>. We give details about this problem in Appendix B.</p><p>Few-Shot RE Few-shot learning is a recently emerged topic to study how to train a model with only a handful of examples for new tasks. A typical setting for few-shot RE is N -way K-shot RE <ref type="bibr" target="#b11">(Han et al., 2018)</ref>, where for each evaluation episode, N relation types, K examples for each type and several query examples (all belonging to one of the N relations) are sampled, and models are required to classify the queries based on given N × K samples. We take FewRel <ref type="bibr" target="#b11">(Han et al., 2018;</ref> as the dataset and list its statistics in <ref type="table">Table 4</ref>.</p><p>We use Prototypical Networks as in <ref type="bibr" target="#b30">Snell et al. (2017)</ref>; <ref type="bibr" target="#b11">Han et al. (2018)</ref> and make a little change:</p><p>(1) We take the representation as described in Section 3.2 instead of using [CLS].</p><p>(2) We use dot production instead of Euclidean distance to measure the similarities between instances. We find out that this method outperforms original Prototypical Networks in <ref type="bibr" target="#b11">Han et al. (2018)</ref> by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RE Models</head><p>Besides BERT and MTB we have introduced in Section 2.1, we also evaluate our proposed contrastive pre-training framework for RE (CP). We write the detailed hyper-parameter settings of both the pre-training and fine-tuning process for all the models in Appendix A and B.</p><p>Note that since MTB and CP use Wikidata for pre-training, and Wiki80 and FewRel are constructed based on Wikidata, we exclude all entity pairs in test sets of Wiki80 and FewRel from pretraining data to avoid test set leakage. <ref type="table" target="#tab_9">Table 5</ref> and 6 show a detailed comparison between BERT, MTB and our proposed contrastive pre-trained models. Both MTB and CP improve model performance across various settings and datasets, demonstrating the power of RE-oriented pre-training. Compared to MTB, CP has achieved even higher results, proving the effectiveness of our proposed contrastive pre-training framework. To be more specific, we observe that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Strength of Contrastive Pre-training</head><p>(1) CP improves model performance on all C+M, OnlyC and OnlyM settings, indicating that our pretraining framework enhances models on both context understanding and type information extraction.</p><p>(2) The performance gain on C+M and OnlyC is universal, even for ChemProt and FewRel 2.0, which are from biomedical domain. Our models trained on Wikipedia perform well on biomedical datasets, suggesting that CP learns relational patterns that are effective across different domains.</p><p>(3) CP also shows a prominent improvement of OnlyM on TACRED, Wiki80 and FewRel 1.0, which are closely related to Wikipedia. It indicates that our model has a better ability to extract type information from mentions. Both promotions on context and mentions eventually lead to better RE results of CP (better C+M results).</p><p>(4) The performance gain made by our contrastive pre-training model is more significant on  low-resource and few-shot settings. For C+M, we observe a promotion of 7% on 10-way 1-shot FewRel 1.0, 18% improvement on 1% setting of TACRED, and 24% improvement on 1% setting of Wiki80. There is also a similar trend for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Development of RE RE of early days has gone through pattern-based methods <ref type="bibr" target="#b13">(Huffman, 1995;</ref><ref type="bibr" target="#b4">Califf and Mooney, 1997)</ref>, feature-based methods <ref type="bibr" target="#b14">(Kambhatla, 2004;</ref><ref type="bibr" target="#b40">Zhou et al., 2005)</ref>, kernel-based methods <ref type="bibr" target="#b5">(Culotta and Sorensen, 2004;</ref><ref type="bibr" target="#b3">Bunescu and Mooney, 2005)</ref>, graphical models <ref type="bibr">Yih, 2002, 2004)</ref>, etc. Since <ref type="bibr" target="#b31">Socher et al. (2012)</ref> propose to use recursive neural networks for RE, there have been extensive studies on neural RE <ref type="bibr" target="#b17">(Liu et al., 2013;</ref><ref type="bibr" target="#b36">Zeng et al., 2014;</ref><ref type="bibr" target="#b37">Zhang and Wang, 2015)</ref>. To solve the data deficiency problem, researchers have developed two paths: distant supervision <ref type="bibr" target="#b23">(Mintz et al., 2009;</ref><ref type="bibr" target="#b22">Min et al., 2013;</ref><ref type="bibr" target="#b27">Riedel et al., 2010;</ref><ref type="bibr" target="#b35">Zeng et al., 2015;</ref><ref type="bibr" target="#b16">Lin et al., 2016)</ref> to automatically collect data by aligning KGs and text, and few-shot learning <ref type="bibr" target="#b11">(Han et al., 2018;</ref> to learn to extract new relations by only a handful of samples.</p><p>Pre-training for RE With the recent advance of pre-trained language models <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, applying BERT-like models as the backbone of RE systems <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref> has become a standard procedure. Based on BERT, <ref type="bibr">Bal-dini Soares et al. (2019)</ref> propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT <ref type="bibr" target="#b39">(Zhang et al., 2019;</ref><ref type="bibr" target="#b25">Peters et al., 2019;</ref><ref type="bibr" target="#b9">Liu et al., 2020)</ref>. We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper.</p><p>Analysis of <ref type="bibr">RE Han et al. (2020)</ref> suggest to study how RE models learn from context and mentions. <ref type="bibr" target="#b0">Alt et al. (2020)</ref> also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models may not have the strong abilities to understand context as we expect. From these points, we propose an entity-masked contrastive pre-training framework for RE to better understand textual context and entity types, and experimental results prove the effectiveness of our method.</p><p>In the future, we will continue to explore better RE pre-training techniques, especially with a focus on open relation extraction and relation discovery. These problems require models to encode good relational representation with limited or even zero annotations, and we believe that our pre-trained RE models will make a good impact in the area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pre-training Details</head><p>Pre-training Dataset We construct a dataset for pre-training following the method in the paper. We use Wikipedia articles as corpus and Wikidata <ref type="bibr" target="#b33">(Vrandečić and Krötzsch, 2014)</ref> as the knowledge graph. Firstly, We use anchors to link entity mentions in Wikipedia corpus with entities in Wikidata. Then, in order to link more unanchored entity mentions, we adopt spaCy 1 to find all possible entity mentions, and link them to entities in Wikidata via name matching. Finally, we get a pre-training dataset containing 744 relations and 867, 278 sentences. We release this dataset together with our source code at our GitHub repository 2 .</p><p>We also use this dataset for MTB, which is slightly different from the original paper <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref>. The original MTB takes all entity pairs into consideration, even if they do not have a relationship in Wikidata. Using the above dataset means that we filter out these entity pairs. We do this out of training efficiency, for those entity pairs that do not have a relation are likely to express little relational information, and thus contribute little to the pre-training.</p><p>Data Sampling Strategy For MTB (Baldini Soares et al., 2019), we follow the same sampling strategy as in the original paper. For pre-training our contrastive model, we regard sentences labeled with the same relation as a "bag". Any sentence pair whose sentences are in the same bag is treated as a positive pair and as a negative pair otherwise. So there will be a large amount of possible positive samples and negative samples. We dynamically sample positive pairs of a relation with respect to the number of sentences in the bag.</p><p>Hyperparameters We use Huggingface's Transformers 3 to implement models for both pre-training and fine-tuning and use AdamW <ref type="bibr" target="#b19">(Loshchilov and Hutter, 2019)</ref>    N/2 negative pairs. For CP, batch size N means that a batch contains 2N sentences, which form N positive pairs. For negative samples, we pair the sentence in each pair with sentences in other pairs. We set hyperparameters according to results on supervised RE dataset TACRED (micro F 1 ). Table 7 shows hyperparameters for pre-training MTB and our contrastive model (CP). The batch size of our implemented MTB is different from that in Baldini <ref type="bibr" target="#b1">Soares et al. (2019)</ref>, because in our experiments, MTB with a batch size of 256 performs better on TACRED than the batch size of 2048.</p><p>Pre-training Efficiency MTB and our contrastive model have the same architecture as BERT BASE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, so they both hold 110M parameters approximately. We use four Nvidia 2080Ti GPUs to pre-train models. Pretraining MTB takes 30, 000 training steps and approximately 24 hours. Pre-training our model takes 3, 500 training steps and approximately 12 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RE Fine-tuning RE Datasets</head><p>We download TACRED from LDC 4 , Wiki80, SemEval from OpenNRE 5 , ChemProt from sciBert 6 , and FewRel from FewRel 7 . <ref type="table" target="#tab_14">Table 8</ref> shows detailed statistics for each dataset and   Hyperparameters <ref type="table" target="#tab_1">Table 10</ref> shows hyperparameters when finetuning on different RE tasks for BERT, MTB and CP. For CNN, we train the model by SGD with a learning rate of 0.5, a batch size of 160 and a hidden size of 230. For few-shot RE, we use the recommended hyperparameters in FewRel 8 .</p><p>Multiple Trial Settings For all the results on supervised RE, we run each experiment 5 times using 5 different seeds <ref type="bibr">(42,</ref><ref type="bibr">43,</ref><ref type="bibr">44,</ref><ref type="bibr">45,</ref><ref type="bibr">46)</ref> and select the median of 5 results as the final reported number. For few-shot RE, as the model varies little with different seeds and it is evaluated in a sampling manner, we just run one trial with 10000 evaluation episodes, which is large enough for the result to converge. We report accuracy (proportion of correct instances in all instances) for Wiki80 and FewRel, and micro F 1 9 for all the other datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>TACRED results (micro F 1 ) with CNN, BERT and MTB on different settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>per:state of birth Prediction: per:state of residenceper:alternate names Prediction: no relation</head><label></label><figDesc>Although her family was from Arkansas, she was born in Washington state, where ...Label:The boy, identified by the Dutch foreign ministry as Ruben but more fully by Dutch media as Ruben van Assouw, ... Label:</figDesc><table><row><cell>C+M</cell></row><row><cell>Dozens of lightly regulated subprime lenders, including</cell></row><row><cell>New Century Financial Corp., have failed and troubled</cell></row><row><cell>Countrywide Financial Corp. was acquired by Bank of</cell></row><row><cell>America Corp.</cell></row><row><cell>Label: org:parents</cell></row><row><cell>Prediction: no relation</cell></row><row><cell>C+T</cell></row><row><cell>First, Natalie Hagemo says, she fought the Church of</cell></row><row><cell>Scientology just to give birth to her daughter.</cell></row><row><cell>Label: no relation</cell></row><row><cell>Prediction: per:children</cell></row><row><cell>Earlier this week Jakarta hosted the general assembly of</cell></row><row><cell>the Organisation of Asia-Pacific News Agencies, ...</cell></row><row><cell>Label: no relation</cell></row><row><cell>Prediction: org:members</cell></row><row><cell>, OnlyC, OnlyM and OnlyT suffer a sig-</cell></row><row><cell>nificant performance drop compared to C+M and</cell></row><row><cell>C+T, indicating that relying on only one source is</cell></row><row><cell>not enough, and both context and entity mentions</cell></row><row><cell>are necessary for correct prediction. Besides, we</cell></row><row><cell>also observe that C+T achieves comparable results</cell></row><row><cell>on TACRED with C+M for BERT and MTB. This</cell></row><row><cell>demonstrates that most of the information provided</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Wrong predictions made only by C+M and only by C+T, where red and blue represent subject and object entities respectively. As the examples suggest, C+M is more easily biased by the entity distribution in the training set and C+T loses some information from mentions that helps to understand the text.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Type Example Wrong ..., Jacinto Suarez, Nicaraguan deputy to the Central American Parliament (PARLACEN) said Monday. 42% Label: org:top members/employees Prediction: no relation US life insurance giant MetLife said on Monday it will acquire American International Group unit American Life Insurance company (ALICO) in a deal worth 155 billion dollars.</figDesc><table /><note>Label: org:subsidiaries Prediction: no relation No pattern On Monday, the judge questioned the leader of the Baptist group, Laura Silsby, who ... 31% Label: per:religion Prediction: no relation Confusing About a year later, she was transferred to Camp Hope, Iraq. 27% Label: per:countries of residence Prediction: per:stateorprovinces of residence</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>SpaceX was founded in 2002 by Elon Musk . Gates , as the co-founder of Microsoft , … Sundar Pichai is the CEO of Alphabet Inc. Cook joined Apple in March 1998.</figDesc><table><row><cell></cell><cell></cell><cell>MTB:</cell><cell>Elon Musk founded SpaceX in 2002.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q5284</cell><cell>founded by</cell><cell>Q2283</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q3503829</cell><cell>CEO</cell><cell>Q20800404</cell></row><row><cell>Q193701</cell><cell>founded by</cell><cell>Q317521</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q265852</cell><cell>CEO</cell><cell>Q312</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. For example, for the sentence "SpaceX was founded by Elon Musk.", the input sequence is "[CLS][E1] SpaceX [/E1] was founded by [E2] Elon Musk [/E2] . [SEP]".</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Results on supervised RE datasets TACRED (micro F 1 ), SemEval (micro F 1 ), Wiki80 (accuracy) and</cell></row><row><cell>ChemProt (micro F 1 ). 1% / 10% indicate using 1% / 10% supervised training data respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on FewRel dataset. FewRel 1.0 is trained and tested on Wikipedia domain. FewRel 2.0 is trained on Wikipedia domain but tested on biomedical domain.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>for optimization. For most pre-training hyperparameters, we select the same values as Baldini Soares et al. (2019). We search hyperparameter batch size in {256, 2048} and P BLANK in {0.3, 0.7}. For MTB, batch size N means that a batch contains 2N sentences, which form N/2 positive pairs and</figDesc><table><row><cell>Parameter</cell><cell>MTB</cell><cell>CP</cell></row><row><cell>Learning Rate</cell><cell cols="2">3 × 10 −5 3 × 10 −5</cell></row><row><cell>Batch Size</cell><cell>256</cell><cell>2048</cell></row><row><cell>Sentence Length</cell><cell>64</cell><cell>64</cell></row><row><cell>P BLANK</cell><cell>0.7</cell><cell>0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for pre-training models. P BLANK corresponds to the probability of replacing entities with [BLANK].</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>TACRED</cell><cell cols="3">68,124 22,631 15,509</cell></row><row><cell>SemEval</cell><cell>6,507</cell><cell>1,493</cell><cell>2,717</cell></row><row><cell>Wiki80</cell><cell>39,200</cell><cell cols="2">5,600 11,200</cell></row><row><cell>ChemProt</cell><cell>4,169</cell><cell>2,427</cell><cell>3,469</cell></row><row><cell>FewRel</cell><cell cols="3">44,800 11,200 14,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Numbers of instances in train / dev / test splits for different RE datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc>demonstrates the sizes of training data for different supervised RE datasets 4 https://catalog.ldc.upenn.edu/ LDC2018T24 5 https://github.com/thunlp/OpenNRE 6 https://github.com/allenai/scibert 7 https://github.com/thunlp/fewrel</figDesc><table><row><cell>Dataset</cell><cell>1%</cell><cell>10%</cell><cell>100%</cell></row><row><cell>TACRED</cell><cell cols="3">703 6,833 68,124</cell></row><row><cell>SemEval</cell><cell>73</cell><cell>660</cell><cell>6,507</cell></row><row><cell>Wiki80</cell><cell cols="3">400 3,920 3,9200</cell></row><row><cell>ChemProt</cell><cell>49</cell><cell>423</cell><cell>4,169</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Numbers of training instances in supervised RE datasets under different proportion settings.</figDesc><table><row><cell>Parameter</cell><cell cols="2">Supervised RE Few-Shot RE</cell></row><row><cell>Learning Rate</cell><cell>3 × 10 −5</cell><cell>2 × 10 −5</cell></row><row><cell>Batch Size</cell><cell>64</cell><cell>4</cell></row><row><cell>Epoch</cell><cell>6</cell><cell>10</cell></row><row><cell>Sentence Length</cell><cell>100</cell><cell>128</cell></row><row><cell>Hidden Size</cell><cell>768</cell><cell>768</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters for fine-tuning on relation extraction tasks(BERT, MTB and CP).    in 1%, 10% and 100% settings. For 1% and 10% settings, we randomly sample 1% and 10% training data for each relation (so the total training instances for 1% / 10% settings are not exactly 1% / 10% of the total training instances in the original datasets). As shown in the table, the numbers of training instances in SemEval and ChemProt for 1% setting are extremely small, which explains the abnormal performance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://spacy.io/ 2 https://github.com/thunlp/ RE-Context-or-Names 3 https://github.com/huggingface/ transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/thunlp/FewRel 9 https://en.wikipedia.org/wiki/F1_ score</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relational learning of pattern-match rules for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">423</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FewRel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1649</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6251" to="6256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">More data, more relations, more context and more openness: A review and outlook for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OpenNRE: An open and extensible toolkit for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-3029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP: System Demonstrations</title>
		<meeting>EMNLP-IJCNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning information extraction patterns from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huffman</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-60925-3_51</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJ-CAI</title>
		<meeting>IJ-CAI</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ChemProt-3.0: A global chemical biology diseases mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kringelum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonny</forename><forename type="middle">Kim</forename><surname>Kjaerulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Taboureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolution neural network for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Haotang Deng, and Ping Wang. 2020. K-BERT: Enabling language representation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2019</title>
		<meeting>ICLR 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Vector Space Modeling for NLP</title>
		<meeting>the NAACL Workshop on Vector Space Modeling for NLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic reasoning for entity &amp; relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of CACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explicit semantic ranking for academic search via knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1271" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01006</idno>
		<title level="m">Relation classification via recurrent neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
