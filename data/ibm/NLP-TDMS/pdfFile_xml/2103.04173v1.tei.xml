<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Sachdeva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
						</author>
						<title level="a" type="main">LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural network models are robust to a limited amount of label noise, but their ability to memorise noisy labels in high noise rate problems is still an open issue. The most competitive noisy-label learning algorithms rely on a 2-stage process comprising an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning that minimises the empirical vicinal risk (EVR) using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. In this paper, we hypothesise that the generalisation of such 2-stage noisy-label learning methods depends on the precision of the unsupervised classifier and the size of the training set to minimise the EVR. We empirically validate these two hypotheses and propose the new 2stage noisy-label training algorithm LongReMix. We test LongReMix on the noisy-label benchmarks CIFAR-10, CIFAR-100, WebVision, Cloth-ing1M, and Food101-N. The results show that our LongReMix generalises better than competing approaches, particularly in high label noise problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code will be available upon paper acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training Deep Neural Networks (DNNs) often requires large data sets to perform well on challenging problems such as image classification <ref type="bibr" target="#b23">(Litjens et al., 2017)</ref>. However, the larger the data set, the greater the likelihood for it to be contaminated with noisy labels due to reasons such as low-quality data, human failure, or challenging labelling tasks <ref type="bibr" target="#b6">(Fr√©nay &amp; Verleysen, 2013)</ref>. The main issue is that DNNs can easily fit noisy labels, particularly for large rates of label noise, reducing their accuracy, as shown by <ref type="bibr" target="#b43">Zhang et al. (Zhang et al., 2016)</ref>.</p><p>In the literature, several methods have been proposed to deal with noisy labels <ref type="bibr" target="#b36">Wang et al., 2019b;</ref><ref type="bibr" target="#b29">Ren et al., 2018;</ref><ref type="bibr" target="#b36">Wang et al., 2019b;</ref><ref type="bibr" target="#b27">Nguyen et al., 2019;</ref><ref type="bibr" target="#b21">Li et al., 2020)</ref>, where the most successful methods explore a 2-stage process formed by an unsupervised learning method to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) to minimise the empirical vicinal risk (EVR) with a labelled set formed by the samples classified as clean, and an unlabelled set with the samples classified as noisy. The unsupervised learning stage is generally based on the small-loss strategy <ref type="bibr" target="#b41">(Yu et al., 2019)</ref>, where at every epoch, samples with small loss are classified as clean, and large loss as noisy. This strategy can lead to a low classification precision of clean samples, particularly in high noise rate scenarios, where the loss values can be unstable at different training epochs. The SSL stage <ref type="bibr" target="#b0">(Arazo Sanchez et al., 2019;</ref><ref type="bibr" target="#b27">Nguyen et al., 2019;</ref><ref type="bibr" target="#b21">Li et al., 2020)</ref> is usually based on MixMatch <ref type="bibr" target="#b1">(Berthelot et al., 2019)</ref> that minimises the empirical vicinal risk (EVR) <ref type="bibr" target="#b45">(Zhang et al., 2017)</ref>, where a robust estimation of the vicinal distribution is critical for an effective optimisation that generalises well. Such robust estimation depends on a large training set to minimise the EVR <ref type="bibr" target="#b1">(Berthelot et al., 2019;</ref>, but problems with high noise rate usually cause the unsupervised learning stage to build a small training set to be used by this optimisation, affecting the generalisation of the SSL stage.</p><p>In this paper, we hypothesise that the generalisation of 2stage noisy-label learning methods depends on the precision of the unsupervised learning stage to classify clean and noisy samples and a large training set to minimise the EVR at the SSL stage. We empirically validate these two hypotheses and propose a new 2-stage noisy-label training algorithm, called LongReMix. LongReMix is based on a theoretically sound unsupervised learning method to maximise the precision of the clean sample classification by considering the small-loss strategy over a range of epochs instead of a single one. Then, we artificially increase the training set size to improve the generalisation of MixMatch for the minimi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>Several methods have been proposed for the noisy-label problem, and they explore different strategies, such as robust loss functions <ref type="bibr" target="#b34">(Wang et al., 2019a;</ref>, label cleansing <ref type="bibr" target="#b12">(Jaehwan et al., 2019;</ref><ref type="bibr" target="#b42">Yuan et al., 2018)</ref>, sample weighting <ref type="bibr" target="#b29">(Ren et al., 2018)</ref>, meta-learning <ref type="bibr" target="#b7">(Han et al., 2018a)</ref>, ensemble learning <ref type="bibr" target="#b26">(Miao et al., 2015)</ref>, and others <ref type="bibr" target="#b40">(Yu et al., 2018;</ref><ref type="bibr" target="#b15">Kim et al., 2019;</ref><ref type="bibr" target="#b46">Zhang et al., 2019)</ref>. Below, we focus on the prior work that is close to our approach and that show competitive results on the main benchmarks. It is important to mention that we do not consider methods that need a clean validation set, such as <ref type="bibr" target="#b48">(Zhang et al., 2020)</ref>, because we believe this forms a less general experimental setup.</p><p>Several approaches explore the sample noise characterisation. <ref type="bibr" target="#b38">Xue et al. (2019)</ref> present a probabilistic Local Outlier Factor algorithm (pLOF) to estimate the probability that a sample is an outlier, which is assumed to have label noise. The idea explored by pLOF is that the density around a noisy sample is significantly different from the density around its (clean) neighbors. However, in high noise rate problems, the effectiveness of pLOF is reduced because it cannot find significant differences between the densities of noisy and clean samples. <ref type="bibr" target="#b35">Wang et al. (2018)</ref> also use pLOF combined with a Siamese network to increase the dissimilarities between clean and noisy samples. Nevertheless, the incorrect classification of clean samples by pLOF can induce the learning of wrong feature representations. Arazo <ref type="bibr" target="#b0">Sanchez et al. (2019)</ref> propose the use of a Beta Mixture Model (BMM) to separate the clean and noise samples during training, based on the loss value of each sample. Similarly, <ref type="bibr" target="#b21">Li et al. (2020)</ref> use Gaussian Mixture Model (GMM) for the same goal. Although the use of BMM and GMM applied on the loss values works well for low noise rate, for high noise regimes it becomes less precise. One of the issues affecting the precision of the classification of clean samples from the training set is that they usually rely on an estimation of clean and noisy sets using the loss from the latest training epoch and do not consider the stability of the classification of clean samples over several epochs.</p><p>Another technique being studied for noisy-label learning is the use of multiple models to improve the robustness of sample noise characterisation. <ref type="bibr" target="#b8">Han et al. (2018b)</ref> propose Co-teaching, which trains two models simultaneously, where each model estimates the clean sample set to be used by the other model. However, with an increase in the number of epochs, both networks converge to a consensus and show little difference between their estimated clean sets. Co-teaching+ <ref type="bibr" target="#b41">(Yu et al., 2019)</ref> relies on small loss samples that disagree on the predictions to select the data for the other model. Although this multiple model strategy shows better results for filtering clean samples, noisy samples are usually ignored during training, decreasing the effectiveness of the approach.</p><p>After distinguishing between clean and noisy samples, methods either disregard the noisy samples during training <ref type="bibr" target="#b33">(Thulasidasan et al., 2019;</ref><ref type="bibr" target="#b8">Han et al., 2018b)</ref>, or use both the clean and noisy samples in a semi-supervised learning (SSL) approach <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b0">Arazo Sanchez et al., 2019;</ref><ref type="bibr" target="#b30">Sachdeva et al., 2021)</ref>, where SSL-based methods tend to show better results on benchmarks. One particularly successful technique that relies on SSL is DivideMix <ref type="bibr" target="#b21">(Li et al., 2020)</ref> that relies on MixMatch <ref type="bibr" target="#b1">(Berthelot et al., 2019)</ref> to linearly combine training samples classified as clean or noisy for the EVR minimisation <ref type="bibr" target="#b45">(Zhang et al., 2017)</ref>. The generalisation of the EVR minimisation has been theoretically shown to depend on a large training set . However, recent methods, such as DivideMix <ref type="bibr" target="#b21">(Li et al., 2020)</ref>, constrain this training set to be of the same size as the clean set, which tends to be small in large noise rate scenarios. Our approach removes this constraint, allowing a better generalisation of the EVR minimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider the training set</head><formula xml:id="formula_0">D = {(x i , y i )} |D| i=1</formula><p>, where x i ‚àà S is the i th image and y i ‚àà {0, 1} |Y| is a one-hot vector representing the noisy label, with Y ‚àà {1, ..., |Y|} denoting the set of labels, and c‚ààY y i (c) = 1. The label y i may differ from the unknown true label≈∑ i as a result of a noise process represented by</p><formula xml:id="formula_1">y i ‚àº p(y|x i , Y,≈∑ i ), with p(y(j)|x i , Y,≈∑ i (c)) = Œ∑ jc (x i ),</formula><p>where the j, c ‚àà Y are the classes, Œ∑ jc (x i ) ‚àà [0, 1] the probability of flipping the class c to j, and j‚ààY Œ∑ jc (x i ) = 1. We assume that this noise process can be of three types, namely symmetric , asymmetric <ref type="bibr" target="#b28">(Patrini et al., 2017)</ref>, and semantic . The symmetric noise, also called uniform noise, refers to a noise type that the hidden label flips to a random class with a fixed probability Œ∑, where the true label is included into the label flipping options, which means that in Œ∑ jc (x i ) = Œ∑ |Y|‚àí1 , ‚àÄj ‚àà Y, such that j = c, and Œ∑ cc (x i ) = 1 ‚àí Œ∑. The asymmetric noise is based on flipping labels between similar classes <ref type="bibr" target="#b28">(Patrini et al., 2017)</ref>, where Œ∑ jc (x i ) depends only on the classes j, c ‚àà Y, but not on x i . For example, using CIFAR-10 data set <ref type="bibr" target="#b17">(Krizhevsky et al., 2009)</ref>, the asymmetric noise maps truck ‚Üí automobile, bird ‚Üí plane, deer ‚Üí horse, as mapped by <ref type="bibr" target="#b47">(Zhang &amp; Sabuncu, 2018)</ref>. The semantic noise ) depends on both the classes j, c ‚àà Y and the image x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Background</head><p>We only consider 2-stage noisy-label learning approaches <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b5">Ding et al., 2018;</ref><ref type="bibr" target="#b16">Kong et al., 2019)</ref> that hold state-of-the-art (SOTA) results on all benchmarks -these approaches are based on: 1) an unsupervised learning classifier that characterises training samples as clean or noisy; and 2) a semi-supervised learning classifier that assumes that the training samples classified as clean are labelled, and the samples classified as noisy are unlabelled. The SOTA noise-robust classifier <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b27">Nguyen et al., 2019)</ref> is formed by an ensemble of two classifiers, each represented by f : S √ó Œò ‚Üí [0, 1] |Y| , where the classifier structure is the same, but their parameters are denoted by Œ∏(1), Œ∏(2) ‚àà Œò. The training for Œ∏(1) influences Œ∏(2) and vice-versa, where this can be achieved by co-training <ref type="bibr" target="#b21">(Li et al., 2020)</ref> or student-teacher <ref type="bibr" target="#b27">(Nguyen et al., 2019)</ref> approaches. Our training relies on co-training.</p><p>The unsupervised learning classifier predicts the clean and noisy samples based on their loss values (Arazo <ref type="bibr" target="#b0">Sanchez et al., 2019;</ref><ref type="bibr" target="#b21">Li et al., 2020;</ref><ref type="bibr" target="#b18">Lee et al., 2019;</ref><ref type="bibr">Jiang et al., 2020)</ref>. Formally, assuming that the training is minimising the empirical risk 1 |D| |D| i=1 (f (x i ; Œ∏), y i ), the set of clean and noisy samples are respectively defined by</p><formula xml:id="formula_2">X = {(x i , y i ) : p (clean| i , Œ≥) ‚â• œÑ } , U = {(x i , y * i ) : p (clean| i , Œ≥) &lt; œÑ } ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">y * i = f (x i ; Œ∏), i = (f (x i ; Œ∏), y i )</formula><p>represents a classification loss (e.g., cross entropy), and p (clean| (f (x i ; Œ∏), y i ), Œ≥) is a function that computes the probability that the training sample (x i , y i ) is clean based on its loss i <ref type="bibr">(Jiang et al., 2020;</ref><ref type="bibr" target="#b21">Li et al., 2020;</ref><ref type="bibr" target="#b48">Zhang et al., 2020;</ref><ref type="bibr" target="#b27">Nguyen et al., 2019)</ref> and parameterised by Œ≥ (in this paper, this probability function computes the posterior of the smaller-mean component of a bi-modal GMM, where this smaller mean represents the clean GMM component <ref type="bibr" target="#b21">(Li et al., 2020)</ref>). To learn Œ∏(1) and Œ∏(2), co-training uses the clean and noisy sets from model Œ∏(1) to train Œ∏(2), and vice-versa.</p><p>The semi-supervised learning based on MixMatch (Berthelot et al., 2019) mixes the elements of X and U to minimise the empirical vicinal risk (EVR) <ref type="bibr" target="#b45">(Zhang et al., 2017)</ref>:</p><formula xml:id="formula_4">EV R = 1 |X | (x i ,·ªπ i ) ‚ààX (X ) (f (xi; Œ∏),·ªπi)+ Œª (U ) |U | (x i ,·ªπ i ) ‚ààU (U ) (f (xi; Œ∏),·ªπi),<label>(2)</label></formula><p>where Œª (U ) weights the noisy set loss, (X ) (.) and (U ) (.) denote the losses in the clean and noisy sets, respectively defined as</p><formula xml:id="formula_5">X = {(xi,·ªπi) : (xi,·ªπi) ‚àº v(x,·ªπ|xi, yi), (xi, yi) ‚àà X } U = {(xi,·ªπi) : (xi,·ªπi) ‚àº v(x,·ªπ|xi, yi), (xi, yi) ‚àà U},<label>(3)</label></formula><p>with</p><formula xml:id="formula_6">v(x,·ªπ|xi, yi) = 1 |X ‚à™ U| (x j ,y j ) ‚ààX ‚à™U E Œª [Œ¥ (x = xi + (1 ‚àí Œª)xj,·ªπ = Œªyi + (1 ‚àí Œª)yj)] ,<label>(4)</label></formula><p>where Œ¥ is a Dirac mass centered at (x,·ªπ), Œª ‚àº Beta(Œ±, Œ±), and Œ± ‚àà (0, ‚àû). In <ref type="bibr" target="#b21">(Li et al., 2020)</ref>, the noisy set size |U | and clean set size |X | are constrained to be equal to |X |, which means that |X | = |U | = |X |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Our Hypothesis</head><p>We hypothesise that the generalisation of 2-stage noisy-label learning methods depend on: 1) the precision of the classification of clean samples to be included in X in (1), and 2) the size of the clean set denoted by |X |. In particular, a large |X | with a high proportion of positives will reduce the bound of the difference between the estimated and vicinal risks , improving the semi-supervised classification accuracy.</p><p>Let us begin with our proposed method to increase the precision in the classification of clean samples in X . Our idea is to classify as clean, the samples that consistently show p(clean| i , Œ≥) ‚â• œÑ for Œ∂ epochs. Assuming that P cc denotes the probability of classifying a clean sample as clean, P nc the probability of classifying a clean sample as noisy (i.e., P cc + P nc = 1). Similarly, P nn represents the probability of classifying a noisy sample as noisy, P cn the probability of classifying a noisy sample as clean (i.e., P nn + P cn = 1). Also, P c and P n denote the proportion of clean and noisy samples in the training set, with P n + P c = 1. The probability of a clean sample being in the clean set X after Œ∂ epochs is P Œ∂ cc , and the probability of a noisy sample being in the clean set after Œ∂ epochs is P Œ∂ cn = (1 ‚àí P nn ) Œ∂ . Lemma 3.1. Assuming that P cc ‚àà (0.5, 1.0) (so P nc ‚àà (0.0, 0.5)) and P nn ‚àà (0.5, 1.0) (so P cn ‚àà (0.0, 0.5)), the classification precision of clean samples in X tends to 1 and recall tends to 0, as Œ∂ increases.</p><p>Proof. The precision and recall are calculated with:</p><formula xml:id="formula_7">Precision = P Œ∂ cc √ó P c P Œ∂ cc √ó P c + P Œ∂ cn √ó P n , Recall = P Œ∂ cc √ó P c P Œ∂ cc √ó P c + (1 ‚àí P Œ∂ cc ) √ó P c .<label>(5)</label></formula><p>Given the assumption that P cn ‚àà (0.0, 0.5) and P cc ‚àà (0.5, 1.0) and that lim Œ∂‚Üí‚àû (P Œ∂ cn /P Œ∂ cc ) ‚Üí 0, Precision tends to 1, and similarly, given that lim Œ∂‚Üí‚àû ((1 ‚àí P Œ∂ cc )/P Œ∂ cc ) ‚Üí ‚àû, Recall tends to 0.</p><p>In low noise rate problems, P cc tends to be large and P cn , small, so even for small values of Œ∂, Precision will be close to one with a relatively high Recall in <ref type="formula" target="#formula_7">(5)</ref>, allowing for a large |X |. According to Theorem 8 in , a large |X | will decrease the bound for vicinal risk minimisation. On the other hand, P cc tends to be small and P cn large, in high noise rate scenarios, which means that Œ∂ needs to increase to push the Precision to be close to one, but that can reduce the Recall to very low values, resulting in a potentially small |X |, which will increase the vicinal risk minimisation bound . Therefore, Œ∂ is a hyper-parameter that needs to be estimated to achieve a good trade-off between Precision and Recall. Nevertheless, for high noise rate scenarios, even with a careful estimation of Œ∂, |X | can still be small. Hence, we propose that X must be sampled with replacement when mixing up X and U in <ref type="formula" target="#formula_5">(3)</ref>, such that |X | = |U | = |D|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LongReMix</head><p>Our proposed LongReMix algorithm is divided into two stages ( <ref type="figure">Figure 1</ref>). The first stage, comprising the High Confidence Training (HCT), trains the model to find a high confidence set of clean samples with high precision. Next, in the second stage, we build a core set of clean samples using the largest high confidence set obtained from the first stage. With this core set, we retrain the model. Moreover, we propose a new way to build the data sets X and U in (3), called LongMix, which enables the number of MixUp operations to be proportional to |D| instead of |X |, as described in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">First Stage: High Confidence Training</head><p>The high confidence training (HCT) aims to increase the precision of the unsupervised classification of clean and noisy training samples. Following the idea presented in Sec. 3.3, we re-define how to form the sets of clean and noisy samples, originally defined in (1), as follows:</p><formula xml:id="formula_8">X (e) 1 = (xi, yi, wi) : wi = p(clean| (e) i , Œ≥) ‚â• œÑ, ‚àÄe ‚àà E , U (e) 1 = (xi, y * i , wi) : wi = p(clean| (e) i , Œ≥) &lt; œÑ, ‚àÉe ‚àà E ,<label>(6)</label></formula><p>where (e) i represents the loss of sample (x i , y i ) at training epoch e and E denotes the confidence window comprising the current and the previous (Œ∂ ‚àí 1) epochs -this is represented by the block "filter" that produces the high confidence samples in <ref type="figure">Fig. 1</ref>. Hence, a sample to be in the clean set X (e) 1 must be classified as clean for Œ∂ epochs in a row, resulting in a more consistent, but smaller, set of clean samples, containing fewer noisy samples than the set in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Second Stage: Guided Training</head><p>The second stage of the training depends on the core set of clean samples estimated from the first training stage with</p><formula xml:id="formula_9">H = arg max X (e) :e‚àà{ E 2 ,...,E} |X (e) 1 |,<label>(7)</label></formula><p>where E is the total number of training epochs for the first stage of training. In the second stage of training, we define the labelled and unlabelled sets as in <ref type="formula" target="#formula_2">(1)</ref>, but we use H to update these sets as follows:</p><formula xml:id="formula_10">X (e) 2 = (x i , y i , w i ) : w i = 1, if (x i , y i ) ‚àà H; or w i = p clean| (e) i , Œ≥ ‚â• œÑ, otherwise , U (e) 2 = (x i , y * i , w i ) : w i = p clean| (e) i , Œ≥ &lt; œÑ, and (x i , y i ) / ‚àà H .<label>(8)</label></formula><p>During the second stage of LongReMix, we retrain the model from scratch 1 using the core set of clean samples H from <ref type="formula" target="#formula_9">(7)</ref> included in the predicted clean and with the original labels from D.</p><p>As explained in Sec. 3.3, we hypothesise that by sampling the clean set with replacement, we increase the number of MixUp operations in the EVR loss in <ref type="formula" target="#formula_4">(2)</ref>, resulting in a smaller bound of the difference be-tween estimated and vicinal risks . Therefore, we propose LongMix that increases the number of MixUp operations to be |D|, instead of the number of predicted clean samples. A criticism faced by LongMix is that adding more MixUp iterations per epoch may be equivalent to a simple increase in the number of epochs, but we show in the experiments that this is not true. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Loss</head><p>The training loss for our proposed LongReMix is <ref type="bibr" target="#b21">(Li et al., 2020)</ref>:</p><formula xml:id="formula_11">= EV R + Œª (reg) (reg) ,<label>(9)</label></formula><p>where EV R denotes the empirical vicinal error defined in <ref type="formula" target="#formula_4">(2)</ref></p><formula xml:id="formula_12">, with (X ) (f (x i ; Œ∏),·ªπ i ) = ‚àí·ªπ i log(f (x i ; Œ∏)), (U ) (f (x i ; Œ∏),·ªπ i ) = ·ªπ i ‚àí f (x i ; Œ∏) 2 2 , Œª (reg) weights the regularisation loss, and (reg) = KL Ô£Æ Ô£∞ œÄ |Y| 1 |X | + |U | x‚àà(X U ) f (x; Œ∏) Ô£π Ô£ª ,<label>(10)</label></formula><p>with œÄ |Y| denoting a vector of |Y| dimensions with values equal to 1/|Y|, and KL[a||b] representing the Kullback Leibler divergence between a and b. The pseudo-code for the training of LongReMix is shown in Algorithm 1 in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare LongReMix with related approaches on five noisy-label learning benchmarks. We also analyze the performance of LongReMix on a number of ablation studies. All comparisons are performed with the same network architecture and trained for the same number of epochs as the compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Sets</head><p>We conduct our experiments on the data sets CIFAR-10, CIFAR-100 <ref type="bibr" target="#b17">(Krizhevsky et al., 2009</ref>), Clothing1M <ref type="bibr" target="#b37">(Xiao et al., 2015)</ref>, WebVision <ref type="bibr" target="#b22">(Li et al., 2017)</ref> and Food101-N <ref type="bibr" target="#b19">(Lee et al., 2018)</ref>. CIFAR-10 and CIFAR-100 have 50000 training and 10000 testing images of size 32 √ó 32 pixels, where CIFAR-10 has 10 classes and CIFAR-100 has 100 classes and all training and testing sets have a perfectly balanced number of images per classes. As CIFAR-10 and CIFAR-100 data sets originally do not contain label noise, a common approach is to add synthetic noise to eval-uate the models. For CIFAR-10/CIFAR-100 we investigated three noise types: symmetric, asymmetric and semantic, as defined in Sec. 3.1. The symmetric noise is generated using Œ∑ ‚àà {0.2, 0.5, 0.8, 0.9}, with Œ∑ defined in Sec. 3.1. The asymmetric noise is produced following the mapping used in <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b28">Patrini et al., 2017)</ref>, with Œ∑ jc ‚àà {0.4, 0.49} (note that we study Œ∑ jc = 49% because it is close to the theoretical limit of 50% for this type of noise). We also evaluate the semantic noise scenario, where we follow the setup from  to generate semantically noisy labels based on a trained VGG <ref type="bibr" target="#b31">(Simonyan &amp; Zisserman, 2015)</ref>, DenseNet (DN), and ResNet (RN) on CIFAR-10 and CIFAR-100.</p><p>Clothing1M consists of 1 million training images acquired from online shopping websites and it is composed of 14 classes.As the images from the data set vary in size, we resized the images to 256 √ó 256 for training, as used in <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b9">Han et al., 2019)</ref>. The data set is heavily imbalanced and most of the noise is asymmetric <ref type="bibr" target="#b39">(Yi &amp; Wu, 2019)</ref>, with noise rate estimated to be around 40% <ref type="bibr" target="#b37">(Xiao et al., 2015)</ref>. The data set provide additional clean sets for training, validation, and test of 50k, 14k and 10k images, respectively. For our experiments we do not use any of the clean training or validation sets, but we use the test set for evaluation.</p><p>WebVision contains 2.4 million images collected from the internet, with the same 1000 classes from ILSVRC12 <ref type="bibr" target="#b4">(Deng et al., 2009</ref>) and images resized to 256 √ó 256 pixels. It provides a clean test set of 50k images, with 50 images per class. We compare our model using the first 50 classes of the Google image subset, as used in <ref type="bibr" target="#b21">(Li et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref>.</p><p>Food101-N <ref type="bibr" target="#b19">(Lee et al., 2018)</ref> contains 310,009 training images of food recipes classified in 101 classes and 25,000 images for the testing set. The images from this data set were resized to 256 √ó 256. This data set is based on the Food101 data set <ref type="bibr" target="#b2">(Bossard et al., 2014)</ref>, but it has more images with noisy labels. The test set is the same provided by the original Food101 <ref type="bibr" target="#b2">(Bossard et al., 2014)</ref>, which is a clean test set of 25K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>The model f (x; Œ∏) is represented by a 18-layer PreAct ResNet (PRN18) <ref type="bibr" target="#b11">(He et al., 2016b)</ref> for CIFAR-10 and CIFAR-100, InceptionV2 <ref type="bibr" target="#b32">(Szegedy et al., 2017)</ref> for We-bVision (this is the model used by competing approaches), and ResNet-50 <ref type="bibr" target="#b10">(He et al., 2016a)</ref> for Clothing1M and Food-101N. The models are trained with stochastic gradient descent with momentum of 0.8, weight decay of 0.0005 and batch size of 64. The learning rate is 0.02 which is reduced to 0.002 in the middle of the training. The WarmUp and total number of epochs is defined according to each data set, as defined in <ref type="bibr" target="#b21">(Li et al., 2020)</ref>. For CIFAR-10 and CIFAR-100, PRN18 is based on a WarmUp stage of 30 epochs, with 300 epochs of total training. For WebVision, the In-ceptionV2 is trained for 100 epochs, with a WarmUp stage of 1 epoch. For Clothing1M, ResNet-50 is trained for 80 epochs with WarmUp stage of 1 epoch. For Food-101N, we also use ResNet-50 and rely on the same training protocol as in , consisting of training for 30 epochs, WarmUp stage of 1 epoch and reducing the learning rate by a factor of 10 every 10 epochs. The MixMatch parameter is Œ± = 4 in (4), and the regularisation weight for the loss in (9) is Œª (reg) = 1 for symmetric noise and Œª (reg) = 0 for asymmetric noise-these two parameters are as defined in (Li et al., 2020). We used a confidence window of Œ∂ = 5 in <ref type="formula" target="#formula_8">(6)</ref>, which was defined empirically for all the experiments. In <ref type="table" target="#tab_1">Table 1</ref> of supplementary material we show that, in general, Precision increases and Recall decreases with larger Œ∂ values. Also, classification accuracy reaches a peak for large noise rates (symmetric at 90% and asymmetric ‚â• 40%) at Œ∂ = 5, and for lower noise rates, accuracy does not change much with different values of Œ∂ ‚àà {1, ..., 10}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Precision and Recall of the Clean Set</head><p>We evaluate the precision and recall of the clean set X  <ref type="formula" target="#formula_2">(1)</ref>, where TP refers to the samples correctly predicted as clean, FP denotes the noisy samples incorrectly predicted as clean, and FN denotes the clean samples incorrectly predicted as noisy. <ref type="figure" target="#fig_0">Figure 2-(a)</ref> shows the Precision vs Recall of predicted clean set for CIFAR-10 with 40% asymmetric noise, where results are obtained by varying the threshold œÑ applied to p(clean| , Œ≥) to form X (e) 1 and X . We highlight the value of œÑ = 0.5, which is the default value <ref type="bibr" target="#b21">(Li et al., 2020)</ref> that we use to split the clean and noisy samples. Notice that in this highly asymmetric noise scenario, the curve from HCT shows a better trade-off than the Baseline. <ref type="figure" target="#fig_0">Figure 2-(b,c)</ref> shows that X (e) 1 from HCT trades off a higher precision for a lower recall, compared with X from the Baseline for several types of noise, As shown below, this has a large influence on the training efficacy of LongReMix. <ref type="figure" target="#fig_1">Figure 3</ref> shows the test accuracy versus training steps (iterations) for LongMix compared to the baseline <ref type="bibr" target="#b21">(Li et al., 2020)</ref>, for CIFAR-10 at 90% symmetric noise -this figure shows that adding more MixUp iterations per epoch, as in  <ref type="formula" target="#formula_5">(3)</ref> are |D|) and the baseline in <ref type="bibr" target="#b21">(Li et al., 2020)</ref> (with sizes of X and U being |X |), using the same number of iterations on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20% to 90% and asymmetric (ranging from 40% and 49%) noise.  <ref type="table">Table 2</ref>. Results using PRN18 on CIFAR-10 and CIFAR-100 under symmetric (ranging from 20% to 90% and asymmetric (ranging from 40% and 49%) noises. Results from related approaches are as presented in <ref type="bibr" target="#b21">(Li et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">LongMix Analysis</head><p>LongMix, is not equivalent to adding more epochs, as in baseline <ref type="bibr" target="#b21">(Li et al., 2020)</ref>. This shows evidence for the claim in Sec. 4.2 that a simple increase in the number of epochs is not equivalent to adding more MixUp iterations, as we propose for LongMix. <ref type="table" target="#tab_1">Table 1</ref> shows further evidence for this claim by comparing LongMix and baseline <ref type="bibr" target="#b21">(Li et al., 2020)</ref> using the same number of training iterations for different noisy rates on CIFAR-10 and CIFAR-100 -results show that LongMix is more accurate for most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the State-of-the-Art</head><p>For CIFAR-10 and CIFAR-100, we evaluate our model using different levels of symmetric label noise ranging from 20% to 90%. We also consider asymmetric noisy, with noise rates of 40% and 49%. We report both the best test accuracy across all epochs and the averaged test accuracy over the last 10 epochs of training, similar to <ref type="bibr" target="#b21">(Li et al., 2020)</ref>. <ref type="table">Table 2</ref> shows that for CIFAR-10 and CIFAR-100 data sets, our method obtains better results for all evaluated noisy rates. LongReMix displays a higher improvement for large symmetric noise and asymmetric noise scenarios, which can be considered as the most challenging cases. We believe that the improvement over higher noise rates is due to the LongMix approach, which runs a large number of MixUp operations proportional to the size of the training set. The retraining with high confidence samples also improves the results for asymmetric noise. The results for semantic noise  in <ref type="table">Table 4</ref> shows again the superiority of our approach compared to the related work.</p><p>Also, we evaluate our method on large-scale data sets. For WebVision, <ref type="table">Table 5</ref> shows the Top-1 and Top-5 accuracy, where LongReMix displays better results than competing methods. For the Clothing1M evaluation, the competing methods rely on a pre-trained ImageNet model for training on Clothing1M. In our experiments, we did not observe any improvement with pre-trained models, and therefore we trained from scratch with 128k images from Clothing1M. The results in <ref type="table">Table 6</ref> show that our model, trained from  <ref type="table">Table 4</ref>. Results for Semantic Noise. Results from baseline methods are as presented in . Methods marked by * denote re-implementations based on public code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top 1 Top 5</p><p>Decoupling <ref type="bibr" target="#b25">(Malach &amp; Shalev-Shwartz, 2017)</ref> 62.54 84.74 D2L  62.68 84.00 MentorNet <ref type="bibr" target="#b13">(Jiang et al., 2018)</ref> 63.00 81.40 Co-teaching <ref type="bibr" target="#b8">(Han et al., 2018b)</ref> 63.58 85.20 Iterative-CV  65.24 85.34 DivideMix <ref type="bibr" target="#b21">(Li et al., 2020)</ref> 77.32 91.64 LongReMix <ref type="bibr">[ours]</ref> 78.92 92.32 <ref type="table">Table 5</ref>. Results for WebVision <ref type="bibr" target="#b22">(Li et al., 2017)</ref>. Results from baseline methods are as presented in <ref type="bibr" target="#b21">(Li et al., 2020)</ref>.</p><p>scratch and with a reduced training set, obtained comparable results to the competing approaches. Lastly, <ref type="table">Table 7</ref> summarizes the results for Food-101N. For this problem, we evaluate our approach with a pre-trained model and trained from scratch, and LongReMix outperforms all other approaches in both scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>We analyze the effect of the different components of our proposal in an ablation study, shown in <ref type="table">Table 3</ref>. Below, "Retrain" denotes the high-confidence training explained in Sec. 4.1 which increases the accuracy of the classifier Method Test Accuracy</p><p>Cross-Entropy <ref type="bibr" target="#b21">(Li et al., 2020)</ref> 69.21 M-correction (Arazo <ref type="bibr" target="#b0">Sanchez et al., 2019)</ref> 71.00 PENCIL <ref type="bibr" target="#b39">(Yi &amp; Wu, 2019)</ref> 73.49 DeepSelf  74.45 CleanNet <ref type="bibr" target="#b19">(Lee et al., 2018)</ref> 74.69 DivideMix <ref type="bibr" target="#b21">(Li et al., 2020)</ref> 74.76 LongReMix ‚Ä† [ours] 74.38 <ref type="table">Table 6</ref>. Results for Clothing1M <ref type="bibr" target="#b37">(Xiao et al., 2015)</ref>. Results from baseline methods are as presented in <ref type="bibr" target="#b21">(Li et al., 2020)</ref>. The marker ‚Ä† denotes the model is trained from scratch.  <ref type="table">Table 7</ref>. Results for Food-101N <ref type="bibr" target="#b19">(Lee et al., 2018)</ref>. Methods marked by * denote re-implementations based on public code. that distinguishes between clean and noisy samples; and "LongMix" represents the guided training from Sec. 4.2 that increases the number of MixUp operations. We first evaluate our approach without LongMix -this approach is referred to as "Retrain". Then we evaluate training only with the LongMix, without the second stage of re-training, and the whole model is denoted as LongReMix. In general, we can observe that the LongReMix is competitive for all noise scenarios (being best or second best for all cases), but it is generally better for the large-scale data sets. Considering different data sets and noise rates, LongReMix shows the best average rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented LongReMix, a new 2-stage noisy-label learning algorithm based on an unsupervised learning stage to classify clean and noisy training samples, followed by an SSL stage to minimise the EVR using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. Our LongReMix improves the precision of the unsupervised learning stage and improves the generalisation of the EVR minimisation. We show that LongReMix reaches state-of-the-art performance on several benchmarks, and is robust to over-fitting in high label noise problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Precision versus Recall for our proposed X (e) 1 from (6) (denoted by HCT) and X from (1) (Baseline), for 40% asymmetric noise on CIFAR-10, where œÑ ‚àà [0, 1] (denoted by th) for p(clean| , Œ≥). (b) Precision and (c) Recall for different noise rates, for CIFAR-10, using œÑ = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Test accuracy versus number of training steps (or iterations) for CIFAR-10 at 90% symmetric noise for our proposed LongMix (where sizes of X and U in (3) are |D|) and the baseline (with sizes of X and U being |X |<ref type="bibr" target="#b21">(Li et al., 2020)</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in the last epoch of the first stage of training (HCT), compared to the clean set X from (1) that relies on the small loss result from the last epoch (Baseline). We assess that by computing Precision = TP TP+FP and Recall =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our proposed LongReMix method is composed of two stages: the High Confidence Training and the Guided Training. The High Confidence Training is responsible for finding the high confidence samples. The Guided Training uses the high confidence samples to update the predicted clean set and train the model.</figDesc><table><row><cell>warmup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>warmup</cell><cell></cell><cell>Core set of clean samples</cell><cell></cell></row><row><cell>CNN</cell><cell>GMM</cell><cell>Predicted clean</cell><cell>Filter</cell><cell>High confidence clean samples</cell><cell>CNN</cell><cell>GMM</cell><cell>Predicted clean</cell><cell>Updated clean</cell></row><row><cell></cell><cell></cell><cell>Predicted noisy</cell><cell></cell><cell>MixMatch</cell><cell></cell><cell></cell><cell>Predicted noisy</cell><cell>Updated Noisy</cell><cell>LongMix</cell></row><row><cell cols="3">First Stage: High Confidence Training</cell><cell></cell><cell></cell><cell cols="3">Second Stage: Guided Training</cell><cell></cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Li et al., 2020) Best 96.22 94.93 93.33 76.49 93.24 82.90 78.03 74.87 62.74 29.79 Last 96.01 94.68 92.99 75.45 91.79 75.57 77.43 74.23 62.01 29.37 Comparison of the test accuracy between LongMix (where sizes of X and U in</figDesc><table><row><cell>Data set</cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Noise type</cell><cell></cell><cell cols="2">sym.</cell><cell></cell><cell cols="2">asym.</cell><cell></cell><cell cols="2">sym.</cell><cell></cell></row><row><cell>Method/ noise ratio</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell><cell>90%</cell><cell>40%</cell><cell>49%</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>Baseline (LongMix [ours]</cell><cell cols="6">Best 96.18 95.19 94.09 85.33 93.38 83.23 Last 95.98 94.79 93.73 84.71 91.87 77.18</cell><cell cols="4">78.03 75.84 62,24 33.54 77.56 74.87 61.60 33.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>LongReMixBest96.25 95.01 93.88 81.98 94.64 84.68 77.82 75.59 62.92 33.80 78.92 74.38 87.39 1.46 Last 96.02 94.72 93.37 81.35 94.32 76.08 77.52 75.11 62.34 33.25 78.00 73.00 87.29 1.69 LongMix Best 96.18 95.19 94.09 85.33 93.38 83.23 78.03 75.84 62.24 33.54 78.44 74.05 87.21 1.92 Last 95.98 94.79 93.73 84.71 91.87 77.18 77.56 74.87 61.60 33.00 77.72 73.25 87.12 1.69 Retrain Best 96.23 94.85 92.86 78.47 94.59 85.10 77.20 74.41 60.29 30.61 77.84 74.30 87.16 2.61 Last 95.89 94.60 92.54 77.51 94.31 80.88 76.89 73.89 59.88 30.37 77.84 73.21 86.98 2.61 Table 3. Ablation Study Results. The italic bold, bold, and regular numbers represent respectively the ranking of first, second and third results in accuracy. Last column shows the average rank of each approach (smaller is better). 81.61 85.71 68.40 66.28 66.84 LongReMix [ours] 85.13 82.51 85.90 69.03 66.70 67.42</figDesc><table><row><cell>Data set</cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell>Webv. Cloth. Food.</cell><cell>Mean Rank</cell></row><row><cell>Noise type</cell><cell></cell><cell cols="2">sym.</cell><cell></cell><cell cols="2">asym.</cell><cell>sym.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Method/ n. ratio</cell><cell cols="6">20% 50% 80% 90% 40% 49%</cell><cell cols="2">20% 50% 80% 90% -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Data set</cell><cell cols="3">CIFAR-10</cell><cell cols="3">CIFAR-100</cell><cell></cell></row><row><cell cols="2">Method/ noise ratio DN</cell><cell>RN</cell><cell>VGG</cell><cell>DN</cell><cell>RN</cell><cell>VGG</cell><cell></cell></row><row><cell></cell><cell>(32%)</cell><cell>(38%)</cell><cell>(34%)</cell><cell>(34%)</cell><cell>(37%)</cell><cell>(37%)</cell><cell></cell></row><row><cell>CE + RoG</cell><cell cols="6">68.33 64.15 70.04 61.14 53.09 53.64</cell><cell></cell></row><row><cell>Bootstrap + RoG</cell><cell cols="6">68.38 64.03 70.11 54.71 53.30 53.76</cell><cell></cell></row><row><cell>Forward + RoG</cell><cell cols="6">68.20 64.24 70.09 53.91 53.36 53.63</cell><cell></cell></row><row><cell>Backward + RoG</cell><cell cols="6">68.66 63.45 70.18 54.01 53.03 53.50</cell><cell></cell></row><row><cell>D2L + RoG</cell><cell cols="6">68.57 60.25 59.94 31.67 39.92 45.42</cell><cell></cell></row><row><cell>DivideMix*</cell><cell>84.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We compared if we should fine-tune the model trained from the first stage or train from scratch, and the latter approach showed the best results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arazo</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixmatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05040</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A semisupervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr√©nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pumpout: A meta approach for robustly training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photometric transformer networks and label adjustment for breast density prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jaehwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Donggeun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hyo-Eun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nlnl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with noisy labels in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recycling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="66998" to="67005" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11300</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cleannet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dividemix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<title level="m">Learning with noisy labels as semi-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>S√°nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rboost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2216" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Self</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01842</idno>
		<title level="m">Learning to filter noisy labels with self-ensembling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning with combined open-set and closed-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evidentialmix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3607" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6234" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust learning at noisy labeled medical images: Applied to skin lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1280" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Iterative cross learning on noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcmains</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Generalization bounds for vicinal risk minimization principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04351</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to hallucinate clean representations for noisy-labeled visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metacleaner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9294" to="9303" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
