<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point Convolutional Neural Networks by Extension Operators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point Convolutional Neural Networks by Extension Operators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension</head><p>Convolution Restriction <ref type="figure">Figure 1</ref>: A new framework for applying convolution to functions defined over point clouds: First, a function over the point cloud (in this case the constant one) is extended to a continuous volumetric function over the ambient space; second, a continuous volumetric convolution is applied to this function (without any discretization or approximation); and lastly, the result is restricted back to the point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper presents Point Convolutional Neural Networks (PCNN): a novel framework for applying convolutional neural networks to point clouds. The framework consists of two operators: extension and restriction, mapping point cloud functions to volumetric functions and viseversa. A point cloud convolution is defined by pull-back of the Euclidean volumetric convolution via an extensionrestriction mechanism.</p><p>The point cloud convolution is computationally efficient, invariant to the order of points in the point cloud, robust to different samplings and varying densities, and translation invariant, that is the same convolution kernel is used at all points. PCNN generalizes image CNNs and allows readily adapting their architectures to the point cloud setting.</p><p>Evaluation of PCNN on three central point cloud learning benchmarks convincingly outperform competing point cloud learning methods, and the vast majority of methods working with more informative shape representations such as surfaces and/or normals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The huge success of deep learning in image analysis motivates researchers to generalize deep learning techniques to work on 3D shapes. Differently from images, 3D data has several popular representation, most notably surface meshes and points clouds. Surface-based methods exploit connectivity information for 3D deep learning based on rendering <ref type="bibr" target="#b40">[39]</ref>, local and global parameterization <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b23">23]</ref>, or * equal contribution spectral properties <ref type="bibr" target="#b46">[45]</ref>. Point cloud methods rely mostly on points' locations in three-dimensional space and need to implicitly infer how the points are connected to form the underlying shape.</p><p>The goal of this paper is to introduce Point Cloud Convolutional Neural Networks (PCNN) generalizing deep learning techniques, and in particular Convolutional Neural Networks (CNN) <ref type="bibr" target="#b22">[22]</ref>, to point clouds. As a point cloud X ⊂ R 3 is merely an approximation to some underlying shape S, the main challenges in building point cloud networks are to achieve: (i) Invariance to the order of points supplied in X; (ii) Robustness to sampling density and distribution of X in S; and (iii) Translation invariance of the convolution operator (i.e., same convolution kernel is used at all points) .</p><p>Invariance to point order in X was previously tackled in <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b47">46]</ref> by designing networks that are composition of euuquivariant layers (i.e., commute with permutations) and a final symmetric layer (i.e., invariant to permutations). As shown in <ref type="bibr" target="#b35">[34]</ref>, any linear equivariant layer is a combination of scaled identity and constant linear operator and therefore missing many of the degrees of freedom existing in standard linear layers such as fully connected and even convolutional.</p><p>Volumetric grid methods <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35]</ref> use 3D occupancy grid to deal with the point order in X and provide translation invariance of the convolution operator. However, they quantize the point cloud to a 3D grid, usually producing a crude approximation to the underlying shape (i.e., piecewise constant on voxels) and are confined to a fixed 3D grid structure.</p><p>Our approach toward these challenges is to define CNN on a point cloud X using a pair of operators we call extension E X and restriction R X . The extension operator maps functions defined over the point cloud X to volumetric functions (i.e., functions defined over the entire ambient space R 3 ), where the restriction operator does the inverse action. Using E X , R X we can translate operators such as Euclidean volumetric convolution to point clouds, see <ref type="figure">Figure 1</ref>. In a nutshell, if O is an operator on volumetric functions then its restriction to the point cloud X would be</p><formula xml:id="formula_0">O X = R X • O • E X .<label>(1)</label></formula><p>We take E X to be a Radial Basis Function (RBF) approximation operator, and R X to be a sampling operator, i.e., sample a volumetric function at the points in X. As O we take continuous volumetric convolution operators with general kernels κ represented in the RBF basis as-well. In turn (1) is calculated using a sparse linear tensor combining the learnable kernel weights k, function values over the point cloud X, and a tensor connecting the two, defined directly from the point cloud X.</p><p>Since our choice of E X is invariant to point order in X, and R X is an equivariant operator (w.r.t. X) we get that O X in (1) is equivariant. This construction leads to new equivariant layers, in particular convolutions, with more degrees of freedom compared to <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b47">46]</ref>. The second challenge of robustness to sampling density and distribution is addressed by the approximation power of the extension operator E X . Given a continuous function defined over a smooth surface, f : S → R, we show that the extension of its restriction to X approximates the restriction of f to S, namely</p><formula xml:id="formula_1">E X • R X [f ] ≈ f S .</formula><p>This means that two different samplings X, X ⊂ S of the same surface function are extended to the same volumetric function, up to an approximation error. In particular, we show that extending the simplest, constant one function over the point cloud, E X <ref type="bibr" target="#b0">[1]</ref>, approximates the indicator function of the surface S, while the gradient, ∇E X <ref type="bibr" target="#b0">[1]</ref>, approximates the mean curvature normal field over the surface. Then, the translation invariance and robustness of our convolution operator naturally follows from the fact that the volumetric convolution is translation invariant and the extension operator is robust. PCNN provides a flexible framework for adapting standard image-based CNNs to the point cloud setting, while maintaining data only over the point cloud on the one hand, and learning convolution kernels robust to sampling on the other. We have tested our PCNN framework on standard classification, segmentation and normal estimation datasets where PCNN outperformed all other point cloud methods and the vast majority of other methods that use more informative shape representations such as surface connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>We review different aspects of geometric deep learning with a focus on the point cloud setting. For a more comprehensive survey on geometric deep learning we refer the reader to <ref type="bibr" target="#b7">[7]</ref>.</p><p>Deep learning on point clouds. PointNet <ref type="bibr" target="#b32">[31]</ref> pioneered deep learning for point clouds with a Siamese, per-point network composed with a symmetric max operator that guarantees invariance to the points' order. PointNet was proven to be a universal approximator (i.e., can approximate arbitrary continuous functions over point clouds). A follow up work <ref type="bibr" target="#b34">[33]</ref> suggests a hierarchical application of the PointNet model to different subsets of the point cloud; this allows capturing structure at different resolutions when applied with a suitable aggregation mechanism. In <ref type="bibr" target="#b16">[16]</ref> the PointNet model is used to predict local shape properties from point clouds. In a related work <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b47">46]</ref> suggest to approximate set function, with equivariant layers composed with a symmetric function such as max. Most related to our work is the recent work of <ref type="bibr" target="#b21">[21]</ref> that suggested to generalize convolutional networks to point clouds by defining convolutions directly on kd-trees built out of the point clouds <ref type="bibr" target="#b3">[3]</ref>, and <ref type="bibr" target="#b37">[36]</ref> that suggested a convolutional architecture for modeling quantum interactions in molecules represented as point clouds, where convolutions are defined by multiplication with continuous filters. The main difference to our work is that we define the convolution of a point cloud function using an exact volumetric convolution with an extended version of the function. The approximation properties of the extended function facilitate a robust convolution on point clouds.</p><p>Volumetric methods. Another strategy is to generate a tensor volumetric representation of the shape restricted to a regular grid (e.g., by using occupancy indicators, or a distance function) <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b33">32]</ref>. The main limitation of these methods is the approximation quality of the underlying shape due to the low resolution enforced by the three dimensional grid structure. To overcome this limitation a few methods suggested to use sparse three dimensional data structures such as octrees <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b36">35]</ref>. Our work can be seen as a generalization of these volumetric methods in that it allows replacing the grid cell's indicator functions as the basis for the network's functions and convolution kernels with more general basis functions (e.g., radial basis functions).</p><p>Deep learning on Graphs. Shapes can be represented as graphs, namely points with neighboring relations. In spectral deep learning the convolution is being replaced by a diagonal operator in the graph-Laplacian eigenbasis <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b18">18]</ref>. The main limitation of these methods in the context of geometric deep learning is that different graphs have different spectral bases and finding correspondences between the bases or common bases is challenging. This problem was recently targeted by <ref type="bibr" target="#b46">[45]</ref> using the functional map framework.</p><p>Deep learning on surfaces. Other approaches to geometric deep learning work with triangular meshes that posses also connectivity and normal information, in addition to the point locations. One class of methods use rendering and 2D projections to reduce the problem to the image setting <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b19">19]</ref>. Another line of works uses local surface representations <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b27">26]</ref> or global parameterizations of surfaces <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b23">23]</ref> for reducing functions on surfaces to the planar domain or for defining convolution operators directly over the surfaces.</p><p>RBF networks. RBF networks are a type of neural networks that use RBF functions as an activation layer, see <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b29">28]</ref>. This model was first introduced in <ref type="bibr" target="#b8">[8]</ref>, and was used, among other things, for function approximation and time series prediction. Usually, these networks have three layers and their output is a linear combination of radial basis functions. Under mild conditions this model can be shown to be a universal approximator of functions defined on compact subsets of R d <ref type="bibr" target="#b31">[30]</ref>. Our use of RBFs is quite different: RBFs are used in our extension operator solely for the purpose of defining point cloud operators, whereas the ReLU is used as an activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Notations. We will use tensor (i.e., multidimensional arrays) notation, e.g., a ∈ R I×I×J×L×M . Indexing a particular entry is done using corresponding lower-case letters,</p><formula xml:id="formula_2">a ii jlm , where 1 ≤ i, i ≤ I, 1 ≤ j ≤ J, etc. When summing tensors c = ijl a ii jlm b ijl , where b ∈ R I×J×L the dimensions of the result tensor c are defined by the free indices, in this case c = c i m ∈ R I×M .</formula><p>Goal. Our goal is to define convolutional neural networks on point clouds</p><formula xml:id="formula_3">X = {x i } I i=1 ∈ R I×3 .</formula><p>Our approach to defining point cloud convolution is to extend functions on point clouds to volumetric functions, perform standard Euclidean convolution on these functions and sample them back on the point cloud.</p><p>We define an extension operator</p><formula xml:id="formula_4">E X : R I×J → C(R 3 , R J ),<label>(2)</label></formula><p>where R I×J represents the collection of functions f : X → R J , and C(R 3 , R J ) volumetric functions ψ :</p><formula xml:id="formula_5">R 3 → R J .</formula><p>Together with the extension operator we define the restriction operator</p><formula xml:id="formula_6">R x : C(R 3 , R M ) → R I×M .<label>(3)</label></formula><p>Given a convolution operator O :</p><formula xml:id="formula_7">C(R 3 , R J ) → C(R 3 , R M ) we adapt O to the point cloud X via (1)</formula><p>. We will show that a proper selection of such point cloud convolution operators possess the following desirable properties: In the next paragraphs we define these operators and show how they are used in defining the main building blocks of PCNN, namely: convolution, pooling and upsampling. We discuss the above theoretical properties in Section 4.</p><formula xml:id="formula_8">1. Efficiency: O X is computationally efficient. 2. Invariance: O X is indifferent to the order of points in X, that is, O X is equivariant. 3. Robustness: Assuming X ⊂ S is a sampling of an underlying surface S, and f ∈ C(S, R), then E X • R X [f ] ∈ C(R 3 , R) approximates f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension operator</head><p>The extension operator E X : R I×J → C(R 3 , R J ) is defined as an operator of the form,</p><formula xml:id="formula_9">E X [f ](x) = i f ij i (x),<label>(4)</label></formula><p>where f ∈ R I×J , and i ∈ C(R 3 , R) can be thought of as basis functions defined per evaluation point x. One important family of bases are the Radial Basis Functions (RBF), that were proven to be useful for surface representation <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b11">11]</ref>. For example, one can consider interpolating bases (i.e., satisfying i (x i ) = δ ii ) made out of an RBF Φ : R + → R. Unfortunately, computing (4) in this case amounts to solving a dense linear system of size I × I. Furthermore, it suffers from bad condition number as the number of points is increased <ref type="bibr" target="#b43">[42]</ref>. In this paper we will advocate a novel approximation scheme of the form where c is a constant depending on the RBF Φ and ω i can be thought of the amount of shape area corresponding to point</p><formula xml:id="formula_10">i (x) = cω i Φ(|x − x i |),<label>(5)</label></formula><formula xml:id="formula_11">x i . A practical choice of ω i is ω i = 1 c i Φ(|x i − x i |) .<label>(6)</label></formula><p>Note that although this choice resembles the Nadaraya-Watson kernel estimator <ref type="bibr" target="#b28">[27]</ref>, it is in fact different as the denominator is independent of the approximation point x; this property will be useful for the closed-form calculation of the convolution operator. As we prove in Section 4, the point cloud convolution operator, O X , defined using the extension operator, (4)-(5), satisfies the properties (1)-(4) listed above, making it suitable for deep learning on point clouds. In fact, as we show in Section 4, robustness is the result of the extension operator E X approximating a continuous, sampling independent operator over the underlying surface S denoted E S . This continuous operator applied to a function f , E S [f ], is proved to approximate the restriction of f to the surface S. <ref type="figure" target="#fig_0">Figure 2</ref> demonstrates the robustness of our extension operator E X ; applying it to the constant one function, evaluated on three different sampling densities of the same shape, results in approximately the same shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel model</head><p>We consider a continuous convolution operator O :</p><formula xml:id="formula_12">C(R 3 , R J ) → C(R 3 , R M ) applied to vector valued func- tion ψ ∈ C(R 3 , R J ), O[ψ](x) = ψ * κ (x) = R 3 j ψ j (y) κ jm (x − y) dy, (7)</formula><p>where κ ∈ C(R 3 , R J×M ) is the convolution kernel that is also represented in the same RBF basis:</p><formula xml:id="formula_13">κ jm (z) = l k ljm Φ(|z − y l |),<label>(8)</label></formula><p>where with a slight abuse of notation we denote by k ∈ R L×J×M the tensor representing the continuous kernel in the RBF basis. Note, that k represents the network's learnable parameters, and has similar dimensions to the convolution parameters in the image case (i.e., spatial dimensions × input channels × output channels).</p><p>The translations {y l } L l=1 ⊂ R 3 are also a degree of freedom and can be chosen to generate a regular 3 × 3 × 3 grid or any other point distribution such as spherical equispaced points. The translations can be predefined by the user or learned (with some similarly to <ref type="bibr" target="#b13">[13]</ref>). See inset for illustration of some possible translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restriction operator</head><p>Our restriction operator R X : C(R 3 , R J ) → R I×J is the sampling operator over the point cloud X,</p><formula xml:id="formula_14">R X [ψ] = ψ j (x i ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_15">ψ ∈ C(R 3 , R J ). Note that R X [ψ] ∈ R I×J .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse extrinsic convolution</head><p>We want to compute the convolution operator O X : R I×J → R I×M restricted to the point cloud X as defined in <ref type="formula" target="#formula_0">(1)</ref> with the convolution operator O from <ref type="bibr" target="#b7">(7)</ref>. First, we</p><formula xml:id="formula_16">compute E X [f ] * k E X [f ] * k (x) = c ijl f ij k ljm w i R 3 Φ(|y − x i |)Φ(|x − y − y l |) dy</formula><p>Applying our restriction operator finally gives our point cloud convolution operator:</p><formula xml:id="formula_17">O X [f ] = c ijl f ij k ljm w i q ii l ,<label>(10)</label></formula><p>where q = q(X) ∈ R I×I×L is the tensor defined by</p><formula xml:id="formula_18">q ii l = R 3 Φ(|y − x i |)Φ(|x i − y − y l |) dy.<label>(11)</label></formula><p>Note that O X [f ] ∈ R I×M , as desired. Equation <ref type="formula" target="#formula_0">(10)</ref> shows that the convolution's weights k ljm are applied to the data f ij using point cloud-dependent weights w, q that can be seen as "translators" of k to the point cloud geometry X. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the computational flow of the convolution operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of RBF</head><p>Our choice of radial basis function Φ stems from two desired properties: First, we want the extension operator (4) to have approximation properties; second, we want the computation of the convolution of a pair of RBFs in <ref type="bibr" target="#b11">(11)</ref> to have an efficient closed-form solution. A natural choice satisfying these requirements is the Gaussian:</p><formula xml:id="formula_19">Φ σ (r) = exp − r 2 2σ 2<label>(12)</label></formula><p>To compute the tensor q ∈ R I×I×L in (11) we make use of the following convolution rule for Gaussians, proved in Appendix A for completeness:</p><formula xml:id="formula_20">Lemma 1.</formula><p>Let Φ denote the Gaussian as in <ref type="bibr" target="#b12">(12)</ref>. Then,</p><formula xml:id="formula_21">Φ α (| · −a|) * Φ β (| · −b|) ∝ Φ γ (| · −a − b|),<label>(13)</label></formula><p>where γ = α 2 + β 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Up-sampling and pooling</head><p>Aside from convolutions, there are two operators that perform spatially and need to be defined for point clouds: up-sampling U X,X * : R I×J → R I * ×J , and pooling P X,X * : R I×J → R I * ×J , where X * ⊂ R 3 is superset of X (i.e., I * &gt; I) in the upsampling case and subset of X (i.e., I * &lt; I) in the pooling case. The upsample operator is defined by</p><formula xml:id="formula_22">U X,X * [f ] = R X * • E X [f ].<label>(14)</label></formula><p>Pooling does not require the extension/restriction operators and (similarly to <ref type="bibr" target="#b34">[33]</ref>) is defined by</p><formula xml:id="formula_23">P X,X * [f ](x * i ) = max i∈V i * f ij ,<label>(15)</label></formula><p>where V i * ⊂ {1, 2, . . . , I} denotes the set of indices of points in X that are closer in Euclidean distance to x * i than any other point in X * . The point cloud X * ⊂ X in the next layer is calculated using farthest point sampling of the input point cloud X.</p><p>Lastly, similarly to <ref type="bibr" target="#b20">[20]</ref> we implement deconvolution layers by an upsampling layer followed by a regular convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Properties</head><p>In this section we discuss the properties of the point cloud operators we have defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Invariance and equivariance</head><p>Given a function f ∈ R I×J on a point cloud X ∈ R I×3 , an equivariant layer L :</p><formula xml:id="formula_24">R I×J → R I×M satisfies L(πf ) = πL(f ),</formula><p>where π ∈ Π I ⊂ R I×I is an arbitrary permutation matrix. Equivariant layers have been suggested in <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b47">46]</ref> to learn data in the form of point clouds (or sets in general).</p><p>The key idea is that equivariant layers can be used to represent a set function F : 2 R 3 → R. Indeed, a set function restricted to sets of fixed size (say, I) can be represented as a symmetric function (i.e., invariant to the order of its arguments). A rich class of symmetric functions can be built by composing equivariant layers and a final symmetric layer.</p><p>The equivariance of our point cloud operators O X stems from the invariance property of the extension operator and equivariance property of the restriction operator. We will next show these properties. </p><formula xml:id="formula_25">E π(X) [f ] = i f π(i)j π(i) = i f ij i = E X [f ], and R X [ψ] = ψ j (x π(i) ) = ΠR X [ψ].</formula><p>A consequence of this lemma is that any convolution O acting on volumetric functions in R 3 translates to an equivariant operator O X ,</p><formula xml:id="formula_26">Theorem 1. Let O : C(R 3 , R J ) → C(R 3 , R M ) be a vol- umetric function operator. Then O X : R I×J → R I×M defined by (1) is equivariant. Namely, O πX [f ] = πO X [f ].</formula><p>Theorem 1 applies in particular to convolutions <ref type="bibr" target="#b7">(7)</ref>, and therefore our point cloud convolutions are all equivariant by construction. Note that this model provides "datadependent" equivariant operator that are more general than those suggest in <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b47">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness</head><p>Overview. Robustness is the key property that allows applying the same convolution kernel to functions over different irregular point clouds. The key idea is to make the extension operator produce approximately the same volumetric function when applied to different samplings of the same underlying shape function. To make things concrete, let X ∈ R I×3 , X * ∈ R I * ×3 be two different point clouds samples of a compact smooth surface S ⊂ R 3 . Let f ∈ C(S, R J ) be some function over S and R X [f ], R X * [f ] its sampling on the points clouds X, X * , respectively.</p><p>We will show the following:</p><p>1. We introduce a continuous extension operator E S from surface functions to volumetric functions. We show that E S has several favorable properties.</p><p>2. We show that (under mild assumptions) our extension operator E X , defined in (4)-(5) converges to E S ,</p><formula xml:id="formula_27">E X • R X [f ] ≈ E S [f ].<label>(16)</label></formula><p>3. We deduce that (under mild assumptions) the properties of E S are inherited by E X and in particular we have:</p><formula xml:id="formula_28">E X • R X [f ] ≈ E X * • R X * [f ].<label>(17)</label></formula><p>Continuous extension operator. We define E S : C(S, R J ) → C(R 3 , R J ) which is an extension operator from surface functions to volumetric functions so that E S [f ]| S ≈ f and E S [f ] → 0 away from S:</p><formula xml:id="formula_29">E S [f ](x) = 1 2πσ 2 S f (y)Φ σ (|x − y|) da(y),<label>(18)</label></formula><p>where da is the area element of the surface S.</p><p>The operator E S enjoys several favorable approximation properties: First,</p><formula xml:id="formula_30">E S [f ](x) σ→0 − −− → f (x) x ∈ S 0 otherwise .<label>(19)</label></formula><p>That is, E S [f ] approximates f over S and decays to zero away from S. In particular, this implies that the constant one function, 1 : S → R, satisfies</p><formula xml:id="formula_31">E S [1](x) → χ S (x),<label>(20)</label></formula><p>where χ S (x) is the volumetric indicator function of S ⊂ R 3 . Interestingly, E S [1] provides also higher-order geometric information of the surface S,</p><formula xml:id="formula_32">∇E S [1] S → −H · n,<label>(21)</label></formula><p>where H : S → R is the mean curvature function of S and n : S → S 2 (S 2 ⊂ R 3 is the unit sphere) is the normal field to S. We prove that the approximation quality in <ref type="bibr" target="#b16">(16)</ref> improves as the point cloud sample X ⊂ S densifies S, and the operator E X becomes more and more consistent. In that case E X <ref type="bibr" target="#b0">[1]</ref> furnishes an approximation to the indicator function of the surface S and its gradient, ∇E X <ref type="bibr" target="#b0">[1]</ref>, to the mean curvature vectors of S. This demonstrates that given the simplest, all ones input data 1 ∈ R I×1 , the network can already reveal the indicator function and the mean curvature vectors of the underlying surface by simple linear operators corresponding to specific choices of the kernel k in <ref type="bibr" target="#b8">(8)</ref>.</p><p>These results are summarized in the following theorem which is proved in appendix A. </p><p>and</p><formula xml:id="formula_34">ω i = area(Ω i ), (23a) Ω i = {y ∈ S | d S (y − x i ) ≤ d S (y − x i ), ∀i } , (23b)</formula><p>the Voronoi cell of x i ∈ S, where d S denotes the distance function of points on S, satisfies</p><formula xml:id="formula_35">E X • R X [f ](x) → E S [f ](x),<label>(24)</label></formula><p>where X ⊂ S is a δ-net and δ → 0. Furthermore, E S satisfies the approximation and mean curvature properties as defined in <ref type="formula" target="#formula_0">(19)</ref>, <ref type="bibr" target="#b20">(20)</ref>, <ref type="bibr" target="#b21">(21)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Revisiting image CNNs</head><p>Our model is a generalization of image CNNs. Images can be viewed as point clouds in regular grid configuration, X = {x i } ⊂ R 3 , with image intensities I i as functions over this point cloud,</p><formula xml:id="formula_36">E X (I) = i I i Φ(x − x i ),</formula><p>where Φ is the indicator function over one square grid cell (i.e., pixel). In this case the extension operator reproduces the image as a volumetric function over R 2 . Writing the convolution kernel also in the basis Φ with regular grid translations leads to <ref type="bibr" target="#b0">(1)</ref> reproducing the standard image discrete convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolu�on layers</head><p>Convolu�on layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully conected layers</head><p>Deconvolu�on layers  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip layer connec�ons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have tested our PCNN framework on the problems of point cloud classification, point cloud segmentation, and point cloud normal estimation. We also evaluated the different design choices and network variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Point cloud classification</head><p>We tested our method on the standard ModelNet40 and ModelNet10 benchmarks <ref type="bibr" target="#b44">[43]</ref>. ModelNet40 is composed of 9843 train models and 2468 test models in 40 different classes, such as guitar, cone, laptop etc. ModelNet 10 consists 3991 train and 908 test models from ten different classes. The models are originally given as triangular meshes. The upper part of <ref type="table">Table 1</ref> compares our classification results versus state of the art learning algorithms that use only the point clouds (i.e., coordinates of points in R 3 ) as input: PointNet <ref type="bibr" target="#b32">[31]</ref>, PointNet++ <ref type="bibr" target="#b34">[33]</ref>, deep sets <ref type="bibr" target="#b47">[46]</ref>, ECC <ref type="bibr" target="#b38">[37]</ref> and kd-network <ref type="bibr" target="#b21">[21]</ref>. For completeness we also provide results of state of the art algorithms taking as input additional data such as meshes and normals. Our method outperforms all point cloud methods and all other non-ensemble methods.</p><p>We use the point cloud data of <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b34">33]</ref> that sampled a point cloud from each model using farthest point sampling.</p><p>In the training we randomly picked 1024 farthest point sample out of a fixed set of 1200 farthest point sample for each model. As in <ref type="bibr" target="#b21">[21]</ref> we also augment the data with random anisotropic scaling in the range [−0.66, 1.5] and uniform translations in the range [−0.2, 0.2]. As input to the network we provide the constant one tensor, together with the coordinate functions of the points, namely (1, x) ∈ R I×4 . The σ parameter controls the variance of the RBFs (both in the convolution kernels and the extension operator) and is chosen to be σ = I −1/2 . The translations of the convolution are chosen to be regular 3 × 3 × 3 grid with size 2σ.</p><p>At test time, similarly to <ref type="bibr" target="#b21">[21]</ref> we use voting: we sample ten different samples of size 1024 from 1200 points on each point cloud, apply anisotropic scaling, propagate it through the net and sum the label probability vectors before taking the label with the maximal probability.</p><p>We used standard convolution architecture, see <ref type="figure" target="#fig_5">Figure 4</ref>:</p><p>conv block(1024, 256, 64) → conv block(256, 64, 256)</p><p>→ conv block(64, 1, 1024) → fully connected block,</p><p>where conv block(#points in, #points out ,#channels) consists of a convolution layer, batch normalization, Relu activation and pooling. The fully connected block is a concatenation of a two fully connected layers with dropout after each one.</p><p>Robustness to sampling. The inset compares our method with <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b34">33]</ref>   The favorable robustness of our method to subsampling can be possibly explained by the fact that our extension operator possess approximation power, even with sparse samples, e.g., for smooth shapes, see <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Method variants. We evaluate the performance of our algorithm subject to the variation in: the number of points I, the kernel translations {y l }, the input tensor f , different bases i in (4), the choice of σ, and number of learnable parameters. Points were randomly sampled by the same ratio as in the above (e.g. 512 out of 600). <ref type="table" target="#tab_3">Table 2</ref> presents the results. Using the constant one as input f = 1 ∈ R I×1 provides almost comparable results to using the full coordinates of the points f = (1, x) ∈ R I×4 . This observation is partially supported by the theoretical analysis shown in section 4 which states that our extension operator applied to the constant one tensor already provides good approximation to the underlying surface and its normal as well as curvature. Using interpolation basis { i } in the extension operator (4), although heavier computationally, does not provide better results. Applying too small σ provides worse classification result. This can be explained by the observation that small σ results in separated Gaussians centered at the points which deteriorates the approximation (X and σ should be related). Interestingly, using a relatively small network size of 1.4M parameters provides comparable classification result.   <ref type="figure" target="#fig_8">Figure 5</ref> visualizes the features learned in the first layer of PCNN on a few shapes from the ModelNet40 dataset. As in the case of images, the features learned on the first layer are mostly edge detectors and directional derivatives. Note that the features are consistent through different sampling and shapes. <ref type="figure">Figure 6</ref> shows 9 different features learned in the third layer of PCNN. In this layer the features capture more semantically meaningful parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Point cloud segmentation</head><p>Our method can also be used for part segmentation: given a point cloud that represents a shape the task is to label each point with a correct part label. We evaluate PCNN performance on ShapeNet part dataset <ref type="bibr" target="#b45">[44]</ref>. ShapeNet contains 16,881 shapes from 16 different categories, and total of 50 part labels. <ref type="table">Table 3</ref> compares per-category and mean IoU(%) scores of PCNN with state of the art point cloud methods: Point-Net <ref type="bibr" target="#b32">[31]</ref>, kd-network <ref type="bibr" target="#b21">[21]</ref>, and 3DCNN (results taken from <ref type="bibr" target="#b32">[31]</ref>). Our method outperforms all of these methods. For completeness we also provide results of other methods that use additional shape features or mesh normals as input. <ref type="figure" target="#fig_9">Figure 7</ref> depicts several of our segmentation results.</p><p>For this task we used standard convolution segmentation architecture, see <ref type="figure" target="#fig_5">Figure 4</ref>: where deconv block(#points in,#points out,#features) consists of an upsampling layer followed by a convolution block. In order to provide the last layers with raw features we also add skip-layers connections, see <ref type="figure" target="#fig_5">Figure 4</ref>(b). This is a common practice in such architectures where fine details are needed at the output layer (e.g., <ref type="bibr" target="#b12">[12]</ref>).  We use the data from <ref type="bibr" target="#b32">[31]</ref> (2048 uniformly sampled points on each model). As done in <ref type="bibr" target="#b32">[31]</ref> we use a single network to predict segmentations for each of the object classes and concatenate a hot-one encoding of the object's label to the bottleneck feature layer. At test time, we use only the part labels that correspond to the input shape (as in <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b21">21]</ref>).</p><formula xml:id="formula_37">conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Normal estimation</head><p>Estimating normals of a point cloud is a central subproblem of the 3D reconstruction problem. We cast this problem as supervised regression problem and employ segmentation network with the following changes: the output layer is composed of 3 channels instead of 50 which are then normalized and fed into cosine-loss with the ground truth normals.</p><p>We have trained and tested our network on the standard train/test splits of the ModelNet40 dataset (we used the data generator code by <ref type="bibr" target="#b34">[33]</ref>). <ref type="table" target="#tab_6">Table 4</ref> compares the mean cosine loss (distance) of PCNN and the normal estimation of <ref type="bibr" target="#b32">[31]</ref> Figure 6: High level features learned by PCNN's third convolution layer and visualized on the input point cloud. As expected, the features are less geometrical than the first layer's features (see <ref type="figure" target="#fig_8">Figure 5</ref>) and seem to capture more semantically meaningful shape parts. and <ref type="bibr" target="#b34">[33]</ref>. <ref type="figure" target="#fig_10">Figure 8</ref> depicts normal estimation examples from this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training details, timings and network size</head><p>We implemented our method using the TensorFlow library <ref type="bibr" target="#b0">[1]</ref> in Python. We used the Adam optimization method with learning rate 0.001 and decay rate 0.7. The models were trained on Nvidia p100 GPUs. <ref type="table" target="#tab_8">Table 5</ref> summarizes running times and network sizes. Our smaller classification network achieves state of the art result (see <ref type="table" target="#tab_3">Table  2</ref>, previous to last row) and has only 1.4M parameters with a total model size of 17 MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper describes PCNN: a methodology for defining convolution of functions over point clouds that is efficient, invariant to point cloud order, robust to point sampling and density, and posses translation invariance. The key idea is to translate volumetric convolution to arbitrary point clouds using extension and restriction operators.</p><p>Testing PCNN on standard point cloud benchmarks show state of the art results using compact networks. The main limitation of our framework compared to image CNN is the extra computational burden due to the computation of farthest point samples in the network and the need to compute input mean aero bag cap car chair ear-p guitar knife lamp laptop motor mug pistol rocket skate  <ref type="table">Table 3</ref>: ShapeNet segmentation results by point cloud methods (top) and methods using additional input data (sf -shape features; nors -normals). The methods compared to are: PointNet <ref type="bibr" target="#b32">[31]</ref>; kd-network <ref type="bibr" target="#b21">[21]</ref>; 3DCNN <ref type="bibr" target="#b32">[31]</ref>; SyncSpecCNN <ref type="bibr" target="#b46">[45]</ref>; Yi <ref type="bibr" target="#b45">[44]</ref>; PointNet++ <ref type="bibr" target="#b34">[33]</ref>; OCNN (+CRF refinement) <ref type="bibr" target="#b41">[40]</ref>.  the "translating" tensor q ∈ R I×I×L which is a function of the point cloud X. Still, we believe that a sparse efficient implementation can alleviate this limitation and mark it as a future work. Other venues for future work is to learn kernel translations per channel similarly to <ref type="bibr" target="#b13">[13]</ref>, and apply the method to data of higher dimension than d = 3 which seems to be readily possible. Lastly, we would like to test this framework on different problems and architectures.   where µ = µ 1 + µ 2 , σ = σ 2 1 + σ 2 2 and C(σ 1 , σ 2 ) =</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Applying the extension operator to the constant 1 function over three airplane point clouds in different sampling densities: 2048, 1024 and 256 points. Note how the extended functions resemble the airplane indicator function, and hence similar to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Point cloud convolution operator, computational flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 2 .</head><label>2</label><figDesc>The extension operators defined in (4) is invariant to permutations, i.e., E πX [f ] = E X [f ], for all permutations π ∈ Π I . The restriction operator (9) is equivariant to permutations, R πX [ψ] = πR X [ψ], for all π ∈ Π I . Proof. The properties follow from the definitions of the operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 .</head><label>2</label><figDesc>Let f ∈ C(S, R J ) be a continuous function defined on a compact smooth surface S ⊂ R 3 . The extension operator (4),(5) with c = 1 2πσ 2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Different architectures used in the paper: (a) classification network; and (b) segmentation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Our point cloud convolution is translation invariant and robust to sample size and density: (a) shows feature activations of two kernels (rows) learned by our network's first convolution layer on different shapes (columns). The features seems consistent across the different models; (b) shows another pair of kernels (rows) on a single model with varying sampling density (from left to right): 10K points, 5K points (random sampling), 1K points (farthest point sampling) and 1K (random sampling). Note that the convolution captures the same geometric properties on all models regardless of the sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Results of PCNN on the part segmentation benchmark from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Normal estimation in ModelNet40. We show normal estimation of four models (rows) with blow-ups. Normals are colored by one of their coordinates for better visualization. Note that competing methods sometimes fail to recognize the outward normal direction (examples indicated by red arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification with variations to the PCNN model.</figDesc><table><row><cell>Feature visualizations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table</head><label></label><figDesc></figDesc><table><row><cell>Point clouds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>2K pnts</cell><cell cols="3">85.1 82.4 80.1 85.5 79.5</cell><cell>90.8</cell><cell>73.2</cell><cell>91.3</cell><cell>86.0</cell><cell>85.0</cell><cell>95.7</cell><cell>73.2</cell><cell>94.8</cell><cell>83.3</cell><cell>51.0</cell><cell>75.0</cell><cell>81.8</cell></row><row><cell>PointNet</cell><cell>2K pnts</cell><cell cols="3">83.7 83.4 78.7 82.5 74.9</cell><cell>89.6</cell><cell>73</cell><cell>91.5</cell><cell>85.9</cell><cell>80.8</cell><cell>95.3</cell><cell>65.2</cell><cell>93</cell><cell>81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell>kd-network</cell><cell>4K pnts</cell><cell cols="3">82.3 80.1 74.6 74.3 70.3</cell><cell>88.6</cell><cell>73.5</cell><cell>90.2</cell><cell>87.2</cell><cell>81</cell><cell>94.9</cell><cell>57.4</cell><cell>86.7</cell><cell>78.1</cell><cell>51.8</cell><cell>69.9</cell><cell>80.3</cell></row><row><cell>3DCNN</cell><cell></cell><cell cols="2">79.4 75.1 72.8 73.3</cell><cell>70</cell><cell>87.2</cell><cell>63.5</cell><cell>88.4</cell><cell>79.6</cell><cell>74.4</cell><cell>93.9</cell><cell>58.7</cell><cell>91.8</cell><cell>76.4</cell><cell>51.2</cell><cell>65.3</cell><cell>77.1</cell></row><row><cell>Additional input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SyncSpecCNN sf</cell><cell>84.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yi</cell><cell>sf</cell><cell>81.4</cell><cell cols="2">81 78.4 77.7 75.7</cell><cell>87.6</cell><cell>61.9</cell><cell>92.0</cell><cell>85.4</cell><cell>82.5</cell><cell>95.7</cell><cell>70.6</cell><cell>91.9</cell><cell>85.9</cell><cell>53.1</cell><cell>69.8</cell><cell>75.3</cell></row><row><cell>PointNet++</cell><cell>pnts, nors</cell><cell cols="3">85.1 82.4 79.0 87.7 77.3</cell><cell>90.8</cell><cell>71.8</cell><cell>91.0</cell><cell>85.9</cell><cell>83.7</cell><cell>95.3</cell><cell>71.6</cell><cell>94.1</cell><cell>81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell cols="2">OCNN (+CRF) nors</cell><cell cols="3">85.9 85.5 87.1 84.7 77.0</cell><cell>91.1</cell><cell>85.1</cell><cell>91.9</cell><cell>87.4</cell><cell>83.3</cell><cell>95.4</cell><cell>56.9</cell><cell>96.2</cell><cell>81.6</cell><cell>53.5</cell><cell>74.1</cell><cell>84.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Normal estimation in ModelNet40.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Timing and network size. Training time is measured in minuets per epoch.then Φ µ1,σ1 * Φ µ2,σ2 = C(σ 1 , σ 2 ) · Φ µ,σ</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This research was supported in part by the European Research Council (ERC Consolidator Grant, "Lift-Match" 771136), the Israel Science Foundation (Grant No. 1830/17). We would like thank the authors of PointNet <ref type="bibr" target="#b32">[31]</ref> for sharing their code and data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>and B(σ) = 1 (2πσ 2 ) <ref type="bibr">3 2</ref> Proof. It is well known <ref type="bibr" target="#b42">[41]</ref> that the convolution of two normal distributions is again a normal distribution:</p><p>The result above follows from the linearity of the convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Theoretical properties of the extension operator</head><p>Proof of theorem 2. Let us first show <ref type="bibr" target="#b24">(24)</ref>.</p><p>To show <ref type="bibr" target="#b19">(19)</ref> first let x / ∈ S. Then as σ → 0 we have</p><p>Indeed, let &gt; 0. Since f is uniformly continuous, take δ &gt; 0 sufficiently small so that |f (</p><p>Lastly, to show (21) note that</p><p>Using an argument from <ref type="bibr" target="#b2">[2]</ref> (see Section 4.2) where we take σ 2 = 2t in their notation and get convergence to − 1 2 ∆ S x, where ∆ S is the Laplace-Beltrami operator on surfaces S. To finish the proof remember that [10]</p><p>Lemma 3. Let S ⊂ R 3 be a compact smooth surface. Then,</p><p>Proof. Denote T x S the tangent plane to S centered at x ∈ S. Let y = y(u) : T x S → S be the local parameterization to S over T x S, where u is the local coordinate at T x S. Since S is smooth and compact we have that ∀u ∈ Υ δ = T x S ∩ B(x, δ),</p><p>where |dy(u)| is the pulled-back area element of S <ref type="bibr" target="#b15">[15]</ref>. We break the error to 1</p><p>. First, we note that (iii) = 0. Now, take δ = σ 1−τ , for some fixed 0 &lt; τ &lt; 1, where σ &gt; 0. Then, (i) = O(σ), (iv) = O(σ). Lastly (ii)</p><p>where we used Lemma 5 in the last inequality. Taking any τ &lt; 1/4 proves the result. </p><p>where in the last inequality we used ||x − y| − |x − y || ≤ |y − y | and Lemma 5. Since f, |·| are both uniformly continuous over S (as continuous functions over a compact surface), f is bounded, i.e., |f | ∞ &lt; ∞, equicontinuity of {g x } is proved.</p><p>where in the last inequality we used the fact that te − t 2 2σ 2 ≤ σe −1/2 . Lastly, to justify (6) let us use Theorem 2 and consider f (x) ≡ 1,</p><p>whereω i is an average of values of ω i (note that Φ(|x i − x i |) are fast decaying weights away from x i ). Hence,</p><p>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">GT Ours Pointnet Pointnet++</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards a theoretical foundation for laplacian-based manifold methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3559</biblScope>
			<biblScope unit="page" from="486" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="301" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Radial basis functions, multi-variable functional interpolation and adaptive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Signals and Radar Establishment Malvern</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Geometry III: theory of surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Zalgaller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstruction and representation of 3d objects with radial basis functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Cherrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Fright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06211</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Deformable convolutional networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Carmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pinkall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Reckziegel</surname></persName>
		</author>
		<title level="m">Differential geometry</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="155" to="180" />
		</imprint>
	</monogr>
	<note>Mathematical Models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04954</idno>
		<title level="m">Pcpnet: Learning local shape properties from raw point clouds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05695</idno>
		<title level="m">Fusionnet: 3d object classification using multiple data representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Averkiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02808</idno>
		<title level="m">3d shape segmentation with projective convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01222</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<title level="m">Convolutional neural networks on surfaces via seamless toric covers. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08402</idno>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recent advances in radial basis function networks. Institute for Adaptative and Neural Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Orr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to radial basis function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Orr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05009</idno>
		<title level="m">Learning deep 3d representations at high resolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moleculenet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02901</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning 3d shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Normal sum distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scattered data approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wendland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Deep sets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">B(σ1)B(σ2) B(σ)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
