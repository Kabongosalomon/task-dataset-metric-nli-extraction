<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Core Semantic First: A Top-down Approach for AMR Parsing *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Core Semantic First: A Top-down Approach for AMR Parsing *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The core semantic first principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract Meaning Representation (AMR) <ref type="bibr" target="#b4">(Banarescu et al., 2013)</ref> is a semantic formalism that encodes the meaning of a sentence as a rooted labeled directed graph. As illustrated by an example in <ref type="figure" target="#fig_0">Figure 1</ref>, AMR abstracts away from the surface forms in text, where the root serves as a rudimentary representation of the overall focus while the details are elaborated as the depth of the graph increases. AMR has been proved useful for many downstream NLP tasks, including text summarization <ref type="bibr" target="#b23">(Liu et al., 2015;</ref><ref type="bibr" target="#b17">Hardy and Vlachos, 2018)</ref> and question answering <ref type="bibr" target="#b28">(Mitra and Baral, 2016)</ref>.</p><p>The task of AMR parsing is to map natural language strings to AMR semantic graphs automatically. Compared to constituent parsing <ref type="bibr">(Zhang *</ref> The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14204418). The first author is grateful for the discussions with Zhisong Zhang and Zhijiang Guo. and Clark, 2009) and dependency parsing <ref type="bibr" target="#b20">(Kübler et al., 2009</ref>), AMR parsing is considered more challenging due to the following characteristics:</p><p>(1) The nodes in AMR have no explicit alignment to text tokens; (2) The graph structure is more complicated because of frequent reentrancies and non-projective arcs; (3) There is a large and sparse vocabulary of possible node types (concepts). Many methods for AMR parsing have been developed in the past years, which can be categorized into three main classes: Graph-based parsing <ref type="bibr" target="#b11">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b26">Lyu and Titov, 2018</ref>) uses a pipeline design for concept identification and relation prediction. Transition-based parsing <ref type="bibr" target="#b39">(Wang et al., 2016;</ref><ref type="bibr" target="#b8">Damonte et al., 2017;</ref><ref type="bibr" target="#b3">Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b16">Guo and Lu, 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref>) processes a sentence from left-to-right and constructs the graph incrementally. The third class is seq2seq-based parsing <ref type="bibr" target="#b5">(Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b19">Konstas et al., 2017;</ref><ref type="bibr" target="#b30">van Noord and Bos, 2017)</ref>, which views parsing as sequence-to-sequence transduction by a linearization (depth-first traversal) of the AMR graph.</p><p>While existing graph-based models cannot sufficiently model the interactions between individual decisions, the autoregressive nature of transitionbased and seq2seq-based models makes them suffer from error propagation, where later decisions can easily go awry, especially given the complexity of AMR. Since capturing the core semantics of a sentence is arguably more important and useful in practice, it is desirable for a parser to have a global view and a priority for capturing the main ideas first. In fact, AMR graphs are organized in a hierarchy that the core semantics stay closely to the root, for which a top-down parsing scheme can fulfill the desiderata. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the subgraph in the red box already conveys the core meaning "an earthquake suddenly struck at a particular time", and the subgraph in the blue box further informs that "the earthquake was big" and "the time was of prosperity and happiness".</p><p>We propose a novel framework for AMR parsing known as Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that, to our knowledge, it is the first top-down AMR parser. 1 GSP performs parsing in an incremental, root-to-leaf fashion, but still maintains a global view of the sentence and the previously derived graph. At each step, it generates the connecting arcs between the existing nodes and the coming new node, upon which the type of the new node (concept) is jointly decided. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. Compared to previous graph-based methods, our model is capable of capturing more complicated intra-graph interactions, while reducing the number of parsing steps to be linear in the sentence length. 2 Compared to transition-based methods, our model removes the left-to-right restriction and avoids sophisticated oracle design for handling the complexity of AMR graphs.</p><p>Notably, most existing methods including the state-the-of-art parsers often rely on heavy graph re-categorization for reducing the complexity of the original AMR graphs. For graph recategorization, specific subgraphs of AMR are grouped together and assigned to a single node with a new compound category <ref type="bibr">(Werling et al.,</ref><ref type="bibr">1</ref> Depth-first traversal in seq2seq models does not produce a strictly top-down order due to the reentrancies in AMR.</p><p>2 Since the size of AMR graph is approximately linear in the length of sentence. 2015; <ref type="bibr" target="#b12">Foland and Martin, 2017;</ref><ref type="bibr" target="#b26">Lyu and Titov, 2018;</ref><ref type="bibr" target="#b14">Groschwitz et al., 2018;</ref><ref type="bibr" target="#b16">Guo and Lu, 2018)</ref>. The hand-crafted rules for re-categorization are often non-trivial, requiring exhaustive screening and expert-level manual efforts. For instance, in the re-categorization system of <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref>, the graph fragment "temporal-quantity</p><formula xml:id="formula_0">:ARG3−of −→ rate-entity-91 :unit −→ year :quant −→ 1"</formula><p>will be replaced by one single nested node "rate-entity-3(annual-01)". There are hundreds of such manual heuristic rules. This kind of re-categorization has been shown to have considerable effects on the performance <ref type="bibr" target="#b16">Guo and Lu, 2018)</ref>. However, one issue is that the precise set of re-categorization rules differs among different models, making it difficult to distinguish the performance improvement from model optimization or carefully designed rules. In fact, some work will become totally infeasible when removing this re-categorization step. For example, the parser of <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref> requires tight integration with this step as it is built on the assumption that an injective alignment exists between sentence tokens and graph nodes.</p><p>We evaluate our parser on the latest AMR sembank and achieve competitive results to the stateof-the-art models. The result is remarkable since our parser directly operates on the original AMR graphs and requires no manual efforts for graph recategorization. The contributions of our work are summarized as follows:</p><p>• We propose a new method for learning AMR parsing that produces high-quality core semantics.</p><p>• Without the help of heuristic graph recategorization which requires expensive expert-level manual efforts for designing re-categorization rules, our method achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Currently, most AMR parsers can be categorized into three classes: (1) Graph-based methods <ref type="bibr" target="#b11">(Flanigan et al., 2014</ref><ref type="bibr" target="#b10">(Flanigan et al., , 2016</ref><ref type="bibr" target="#b41">Werling et al., 2015;</ref><ref type="bibr" target="#b12">Foland and Martin, 2017;</ref><ref type="bibr" target="#b26">Lyu and Titov, 2018;</ref><ref type="bibr" target="#b42">Zhang et al., 2019)</ref> adopt a pipeline approach for graph construction. It first maps continuous text spans into AMR concepts, then calculates the scores of possible edges and uses a max-imum spanning connected subgraph algorithm to select the final graph. The major deficiency is that the concept identification and relation prediction are strictly performed in order, yet the interactions between them should benefit both sides <ref type="bibr" target="#b44">(Zhou et al., 2016)</ref>. In addition, for computational efficacy, usually only first-order information is considered for edge scoring.</p><p>(2) Transition-based methods <ref type="bibr" target="#b39">(Wang et al., 2016;</ref><ref type="bibr" target="#b8">Damonte et al., 2017;</ref><ref type="bibr" target="#b3">Ballesteros and Al-Onaizan, 2017;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b31">Peng et al., 2018;</ref><ref type="bibr" target="#b16">Guo and Lu, 2018;</ref><ref type="bibr" target="#b29">Naseem et al., 2019)</ref> borrow techniques from shift-reduce dependency parsing. Yet the non-trivial nature of AMR graphs (e.g., reentrancies and non-projective arcs) makes the transition system even more complicated and difficult to train <ref type="bibr" target="#b16">(Guo and Lu, 2018)</ref>.</p><p>(3) Seq2seq-based methods <ref type="bibr" target="#b5">(Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b33">Peng et al., 2017;</ref><ref type="bibr" target="#b19">Konstas et al., 2017;</ref><ref type="bibr" target="#b30">van Noord and Bos, 2017)</ref> treat AMR parsing as sequence-to-sequence problem by linearizing AMR graphs, thus existing seq2seq models <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b25">Luong et al., 2015)</ref> can be readily utilized. Despite its simplicity, the performance of the current seq2seq models lag behind when the training data is limited. The first reason is that seq2seq models are often not as effective on smaller datasets. The second reason is that the linearized AMRs add the challenges of making use of the graph structure information.</p><p>There are also some notable exceptions. <ref type="bibr" target="#b32">Peng et al. (2015)</ref> introduce a synchronous hyperedge replacement grammar solution. <ref type="bibr" target="#b35">Pust et al. (2015)</ref> regard the task as a machine translation problem, while <ref type="bibr" target="#b0">Artzi et al. (2015)</ref> adapt combinatory categorical grammar. <ref type="bibr" target="#b14">Groschwitz et al. (2018)</ref>; <ref type="bibr" target="#b22">Lindemann et al. (2019)</ref> view AMR graphs as the structure AM algebra.</p><p>Most AMR parsers require an explicit alignment between tokens in the sentences and nodes in the AMR graph during training. Since such information is not annotated, a pre-trained aligner <ref type="bibr" target="#b11">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b34">Pourdamghani et al., 2014;</ref><ref type="bibr" target="#b24">Liu et al., 2018)</ref> is often required. More recently, <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref> demonstrate that the alignments can be treated as latent variables in a joint probabilistic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background of Multi-head Attention</head><p>The multi-head attention mechanism introduced by <ref type="bibr" target="#b38">Vaswani et al. (2017)</ref> is used as a basic building block in our framework. The multi-head attention consists of H attention heads, and each of which learns a distinct attention function. Given a query vector x and a set of vectors {y 1 , y 2 , . . . , y m } or in short y 1:m , for each attention head, we project x and y 1:m into distinct query, key, and value representations q ∈ R d , K ∈ R m×d and V ∈ R m×d respectively, where d is the dimension of the vector space. Then we perform scaled dot-product attention <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>:</p><formula xml:id="formula_1">a = softmax (Kq) √ d attn = aV</formula><p>where a ∈ R m is the attention vector (a distribution over all input y 1:m ) and attn is the weighted sum of the value vectors. Finally, the outputs of all attention heads are concatenated and projected to the original dimension of x. For brevity, we will denote the whole attention procedure described above as a function T (x, y 1:m ).</p><p>Based on the multi-head attention, the Transformer encoder <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> uses self-attention for context information aggregation when given a set of vectors (e.g., word embeddings in a sentence or node embeddings in a graph). <ref type="figure" target="#fig_1">Figure 2</ref> depicts the major neural components in our proposed framework: The Sentence Encoder component and the Graph Encoder component are designed for token-level sentence representation and node-level graph representation respectively. Given an input sentence w = (w 1 , w 2 , . . . , w n ), where n is the sentence length, the Sentence Encoder component will first read the whole sentence and encode each word w i into the hidden state s i . The initial graph G 0 is always initialized with one dummy node d * and a previously generated concept c j is encoded into the hidden state v j by the Graph Encoder component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>At each time step t, the Focus Selection component reads both the sentence representation s 1:n and the graph representation v 0:t−1 of G t−1 repeatedly, generates the initial parser state h t . The</p><formula xml:id="formula_2">w 1 w 2 w n s 1 s 2 s n c 1 c 2 c 3 v 1 v 2 v 3 Focus Selection … … h t {a g i t } k i= 1 a s t G t−1</formula><p>Sentence Encoder Graph Encoder × k parser state 1 2 3 4 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Prediction Relation Classification Relation Identification</head><p>? c t d* <ref type="figure" target="#fig_5">Figure 5</ref>: A single layer of the Recursive Neural Tensor Network. Each dashed box represents one of d-many slices and can capture a type of influence a child can have on its parent.</p><p>The RNTN uses this definition for computing p1:</p><formula xml:id="formula_3">p1 = f  b c T V [1:d]  b c + W  b c ! ,</formula><p>where W is as defined in the previous models. The next parent vector p2 in the tri-gram will be computed with the same weights:</p><formula xml:id="formula_4">p2 = f  a p1 T V [1:d]  a p1 + W  a p1 ! .</formula><p>The main advantage over the previous RNN model, which is a special case of the RNTN when V is set to 0, is that the tensor can directly relate input vectors. Intuitively, we can interpret each slice of the tensor as capturing a specific type of composition.</p><p>An alternative to RNTNs would be to make the compositional function more powerful by adding a second neural network layer. However, initial experiments showed that it is hard to optimize this model and vector interactions are still more implicit than in the RNTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tensor Backprop through Structure</head><p>We describe in this section how to train the RNTN model. As mentioned above, each node has a softmax classifier trained on its vector representation to predict a given ground truth or target vector t. We assume the target distribution vector at each node has a 0-1 encoding. If there are C classes, then it has length C and a 1 at the correct label. All other entries are 0.</p><p>We want to maximize the probability of the correct prediction, or minimize the cross-entropy error between the predicted distribution y i 2 R C⇥1 at node i and the target distribution t i 2 R C⇥1 at that node. This is equivalent (up to a constant) to minimizing the KL-divergence between the two distributions. The error as a function of the RNTN parameters ✓ = (V, W, Ws, L) for a sentence is:</p><formula xml:id="formula_5">E(✓) = X i X j t i j log y i j + k✓k 2<label>(2)</label></formula><p>The derivative for the weights of the softmax classifier are standard and simply sum up from each node's error. We define x i to be the vector at node i (in the example trigram, the x i 2 R d⇥1 's are (a, b, c, p1, p2)). We skip the standard derivative for Ws. Each node backpropagates its error through to the recursively used weights V, W . Let i,s 2 R d⇥1 be the softmax error vector at node i:</p><formula xml:id="formula_6">i,s = W T s (y i t i ) ⌦ f 0 (x i ),</formula><p>where ⌦ is the Hadamard product between the two vectors and f 0 is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f (x i ).</p><p>The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes. The full derivative for V and W is the sum of the derivatives at each of the nodes. We define the complete incoming error messages for a node i as i,com . The top node, in our case p2, only received errors from the top node's softmax. Hence, p2,com = p2,s which we can use to obtain the standard backprop derivative for W <ref type="bibr">(Goller and Küchler, 1996;</ref><ref type="bibr">Socher et al., 2010)</ref>. For the derivative of each slice k = 1, . . . , d, we get:</p><formula xml:id="formula_7">@E p2 @V [k] = p2,com k  a p1  a p1 T ,</formula><p>where p2,com k is just the k'th element of this vector. Now, we can compute the error message for the two <ref type="figure" target="#fig_5">Figure 5</ref>: A single layer of the Recursive Neural Tensor Network. Each dashed box represents one of d-many slices and can capture a type of influence a child can have on its parent.</p><p>The RNTN uses this definition for computing p1:</p><formula xml:id="formula_8">p1 = f  b c T V [1:d]  b c + W  b c ! ,</formula><p>where W is as defined in the previous models. The next parent vector p2 in the tri-gram will be computed with the same weights:</p><formula xml:id="formula_9">p2 = f  a p1 T V [1:d]  a p1 + W  a p1 ! .</formula><p>The main advantage over the previous RNN model, which is a special case of the RNTN when V is set to 0, is that the tensor can directly relate input vectors. Intuitively, we can interpret each slice of the tensor as capturing a specific type of composition.</p><p>An alternative to RNTNs would be to make the compositional function more powerful by adding a second neural network layer. However, initial experiments showed that it is hard to optimize this model and vector interactions are still more implicit than in the RNTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tensor Backprop through Structure</head><p>We describe in this section how to train the RNTN model. As mentioned above, each node has a softmax classifier trained on its vector representation to predict a given ground truth or target vector t. We assume the target distribution vector at each node has a 0-1 encoding. If there are C classes, then it has length C and a 1 at the correct label. All other entries are 0.</p><p>We want to maximize the probability of the correct prediction, or minimize the cross-entropy error between the predicted distribution y i 2 R C⇥1 at node i and the target distribution t i 2 R C⇥1 at that node. This is equivalent (up to a constant) to minimizing the KL-divergence between the two distributions. The error as a function of the RNTN parameters ✓ = (V, W, Ws, L) for a sentence is:</p><formula xml:id="formula_10">E(✓) = X i X j t i j log y i j + k✓k 2<label>(2)</label></formula><p>The derivative for the weights of the softmax classifier are standard and simply sum up from each node's error. We define x i to be the vector at node i (in the example trigram, the x i 2 R d⇥1 's are (a, b, c, p1, p2)). We skip the standard derivative for Ws. Each node backpropagates its error through to the recursively used weights V, W . Let i,s 2 R d⇥1 be the softmax error vector at node i:</p><formula xml:id="formula_11">i,s = W T s (y i t i ) ⌦ f 0 (x i ),</formula><p>where ⌦ is the Hadamard product between the two vectors and f 0 is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f (x i ).</p><p>The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes. The full derivative for V and W is the sum of the derivatives at each of the nodes. We define the complete incoming error messages for a node i as i,com . The top node, in our case p2, only received errors from the top node's softmax. Hence, p2,com = p2,s which we can use to obtain the standard backprop derivative for W <ref type="bibr">(Goller and Küchler, 1996;</ref><ref type="bibr">Socher et al., 2010)</ref>. For the derivative of each slice k = 1, . . . , d, we get:</p><formula xml:id="formula_12">@E p2 @V [k] = p2,com k  a p1  a p1 T ,</formula><p>where p2,com k is just the k'th element of this vector. Now, we can compute the error message for the two f <ref type="figure" target="#fig_5">Figure 5</ref>: A single layer of the Recursive Neural Tensor Network. Each dashed box represents one of d-many slices and can capture a type of influence a child can have on its parent.</p><p>The RNTN uses this definition for computing p1:</p><formula xml:id="formula_13">p1 = f  b c T V [1:d]  b c + W  b c ! ,</formula><p>where W is as defined in the previous models. The next parent vector p2 in the tri-gram will be computed with the same weights:</p><formula xml:id="formula_14">p2 = f  a p1 T V [1:d]  a p1 + W  a p1 ! .</formula><p>The main advantage over the previous RNN model, which is a special case of the RNTN when V is set to 0, is that the tensor can directly relate input vectors. Intuitively, we can interpret each slice of the tensor as capturing a specific type of composition.</p><p>An alternative to RNTNs would be to make the compositional function more powerful by adding a second neural network layer. However, initial experiments showed that it is hard to optimize this model and vector interactions are still more implicit than in the RNTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tensor Backprop through Structure</head><p>We describe in this section how to train the RNTN model. As mentioned above, each node has a softmax classifier trained on its vector representation to predict a given ground truth or target vector t. We assume the target distribution vector at each node has a 0-1 encoding. If there are C classes, then it has length C and a 1 at the correct label. All other entries are 0.</p><p>We want to maximize the probability of the correct prediction, or minimize the cross-entropy error between the predicted distribution y i 2 R C⇥1 at node i and the target distribution t i 2 R C⇥1 at that node. This is equivalent (up to a constant) to minimizing the KL-divergence between the two distributions. The error as a function of the RNTN parameters ✓ = (V, W, Ws, L) for a sentence is:</p><formula xml:id="formula_15">E(✓) = X i X j t i j log y i j + k✓k 2<label>(2)</label></formula><p>The derivative for the weights of the softmax classifier are standard and simply sum up from each node's error. We define x i to be the vector at node i (in the example trigram, the x i 2 R d⇥1 's are (a, b, c, p1, p2)). We skip the standard derivative for Ws. Each node backpropagates its error through to the recursively used weights V, W . Let i,s 2 R d⇥1 be the softmax error vector at node i:</p><formula xml:id="formula_16">i,s = W T s (y i t i ) ⌦ f 0 (x i ),</formula><p>where ⌦ is the Hadamard product between the two vectors and f 0 is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f (x i ).</p><p>The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes. The full derivative for V and W is the sum of the derivatives at each of the nodes. We define the complete incoming error messages for a node i as i,com . The top node, in our case p2, only received errors from the top node's softmax. Hence, p2,com = p2,s which we can use to obtain the standard backprop derivative for W <ref type="bibr">(Goller and Küchler, 1996;</ref><ref type="bibr">Socher et al., 2010)</ref>. For the derivative of each slice k = 1, . . . , d, we get:</p><formula xml:id="formula_17">@E p2 @V [k] = p2,com k  a p1  a p1 T ,</formula><p>where p2,com k is just the k'th element of this vector. Now, we can compute the error message for the two f + <ref type="figure" target="#fig_5">Figure 5</ref>: A single layer of the Recursive Neural Tensor Network. Each dashed box represents one of d-many slices and can capture a type of influence a child can have on its parent.</p><p>The RNTN uses this definition for computing p1:</p><formula xml:id="formula_18">p1 = f  b c T V [1:d]  b c + W  b c ! ,</formula><p>where W is as defined in the previous models. The next parent vector p2 in the tri-gram will be computed with the same weights:</p><formula xml:id="formula_19">p2 = f  a p1 T V [1:d]  a p1 + W  a p1 ! .</formula><p>The main advantage over the previous RNN model, which is a special case of the RNTN when V is set to 0, is that the tensor can directly relate input vectors. Intuitively, we can interpret each slice of the tensor as capturing a specific type of composition.</p><p>An alternative to RNTNs would be to make the compositional function more powerful by adding a second neural network layer. However, initial experiments showed that it is hard to optimize this model and vector interactions are still more implicit than in the RNTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tensor Backprop through Structure</head><p>We describe in this section how to train the RNTN model. As mentioned above, each node has a softmax classifier trained on its vector representation to predict a given ground truth or target vector t. We assume the target distribution vector at each node has a 0-1 encoding. If there are C classes, then it has length C and a 1 at the correct label. All other entries are 0.</p><p>We want to maximize the probability of the correct prediction, or minimize the cross-entropy error between the predicted distribution y i 2 R C⇥1 at node i and the target distribution t i 2 R C⇥1 at that node. This is equivalent (up to a constant) to minimizing the KL-divergence between the two distributions. The error as a function of the RNTN parameters ✓ = (V, W, Ws, L) for a sentence is:</p><formula xml:id="formula_20">E(✓) = X i X j t i j log y i j + k✓k 2<label>(2)</label></formula><p>The derivative for the weights of the softmax classifier are standard and simply sum up from each node's error. We define x i to be the vector at node i (in the example trigram, the x i 2 R d⇥1 's are (a, b, c, p1, p2)). We skip the standard derivative for Ws. Each node backpropagates its error through to the recursively used weights V, W . Let i,s 2 R d⇥1 be the softmax error vector at node i:</p><formula xml:id="formula_21">i,s = W T s (y i t i ) ⌦ f 0 (x i ),</formula><p>where ⌦ is the Hadamard product between the two vectors and f 0 is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f (x i ).</p><p>The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes. The full derivative for V and W is the sum of the derivatives at each of the nodes. We define the complete incoming error messages for a node i as i,com . The top node, in our case p2, only received errors from the top node's softmax. Hence, p2,com = p2,s which we can use to obtain the standard backprop derivative for W <ref type="bibr">(Goller and Küchler, 1996;</ref><ref type="bibr">Socher et al., 2010)</ref>. For the derivative of each slice k = 1, . . . , d, we get:</p><formula xml:id="formula_22">@E p2 @V [k] = p2,com k  a p1  a p1 T ,</formula><p>where p2,com k is just the k'th element of this vector. Now, we can compute the error message for the two parser state carries the most useful information and serves as a writable memory during the expansion step. Next, the Relation Identification component decides which specific head nodes to expand by computing the multiple attention scores {a g i t } k i=1 over the existing nodes. New arcs are generated according to the attention scores. Then the Concept Prediction component updates the parser state h t with arc information, computes the attention vector a s t over the sentence and accordingly chooses a specific part to generate the new concept c t . Finally, the Relation Classification component is used to predict the relation labels between the newly generated concept and its predecessors. Consequently an updated graph G t is produced and G t will be processed for the next time step. The whole decoding procedure is terminated if the newly generated concept is the special stop concept .</p><p>Our method expands the graph in a root-toleaf fashion, nodes with shorter distances to the root will be introduced first. It follows a similar way that humans grasp the meaning: first seeking the main concepts then proceeding to the substructures governed by certain head concepts <ref type="bibr" target="#b4">(Banarescu et al., 2013)</ref>.</p><p>During training, we use breadth-first search to decide the order of nodes. However, for nodes with multiple children, there still exist multiple valid selections. In order to define a deterministic decoding process, we sort sibling nodes by their relations to the head node. We will present more discussions on the choice of sibling order in 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Framework Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence &amp; Graph Representation</head><p>Transformer encoder architecture is employed for both the Sentence Encoder and the Graph encoder components. For sentence encoding, a special token (£) is prepended to the input word sequence, whose final hidden state s 0 is regarded as an aggregated summary of the whole sentence and used as the initial state in parsing steps.</p><p>The Graph Encoder component takes previously generated concept sequence (c 0 , c 1 , . . . , c t−1 ) (c 0 is the dummy node d * ) as input. For computation efficiency and reducing error propagation, instead of encoding the edge information explicitly, we use the Transformer encoder to capture the interactions between nodes. Finally, the encoder outputs a sequence of node representations (v 0 , v 1 , . . . , v t−1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Focus Selection</head><p>At each time step t, the Focus Selection component will read the sentence and the partially constructed graph repeatedly for gradually locating and collecting the most relevant information for the next expansion. We simulate the repeated reading by multiple levels of attention. Formally, the following recurrence is applied by L times:</p><formula xml:id="formula_23">x (l+1),1 t = LN(h (l) t + T (l+1),1 (h (l) t , s 1:n )) x (l+1),2 t = LN(x (l+1),1 t + T (l+1),2 (x (l+1),1 t , v 0:t−1 )) h (l+1) t = max(x (l+1),2 t W l+1 1 + b l+1 1 )W l+1 2 + b l+1 2</formula><p>where T (·, ·) is the multi-head attention function. LN is the layer normalization <ref type="bibr">(Ba et al., 2016)</ref> and</p><formula xml:id="formula_24">h (0)</formula><p>t is always initialized with s 0 . For clarity, we denote the last hidden state h (L) t as h t , as the parser state at the time step t. We now proceed to present the details of each decision stage of one parsing step, which is also illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation Identification</head><p>Our Relation Identification component is inspired by a recent attempt of exposing auxiliary supervision on attention mechanism <ref type="bibr" target="#b37">(Strubell et al., 2018)</ref>. It can be considered as another attention layer over the existing graph, yet the attention weights explicitly indicate the likelihood of the new node being attached to a specific node. In other words, its aim is to answer the question of where to expand. Since a node can be attached to multiple nodes by playing different semantic roles, we utilize multi-head attention and take the maximum over different heads as the final arc probabilities.</p><p>Formally, through a multi-head attention mechanism taking h t and v 0:t−1 as input, we obtain a set of attention weights {a g i t } k i=1 , where k is the number of attention heads and a g i t is the i-th probability vector. The probability of the arc between the new node and the node v j is then computed by a g t,j = max i (a g i t,j ). Intuitively, each head is in charge of a set of possible relations (though not explicitly specified). If certain relations do not exist between the new node and any existing node, the probability mass will be assigned to the dummy node d * . The maximum pooling reflects that the arc should be built once one relation is activated. <ref type="bibr">3</ref> The attention mechanism passes the arc decisions to later layers by the update of the parser state as follows:</p><formula xml:id="formula_25">h t = LN (h t + W arc t−1 j=0</formula><p>a g t,j v j ) <ref type="bibr">3</ref> We also found that there may exist more than one relation between two distinct nodes, however, it rarely happens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Concept Prediction</head><p>Our Concept Prediction component uses a soft alignment between words and the new concept. Concretely, a single-head attention a s t is computed based on the parser state h t and the sentence representation s 1:n , where a s t,i denotes the attention weight of the word w i in the current time step. This component then updates the parser state with the alignment information via the following equation:</p><formula xml:id="formula_26">h t = LN (h t + W conc n i=1 a s t,i s i )</formula><p>The probability of generating a specific concept c from the concept vocabulary V is calculated as gen</p><formula xml:id="formula_27">(c|h t ) = exp(x c T h t )/ c ∈V exp(x c T h t )</formula><p>, where x c (for c ∈ V) denotes the model parameters. To address the data sparsity issue in concept prediction, we introduce a copy mechanism in similar spirit to . Besides generation, our model can either directly copy an input token w i (e.g, for entity names) or map w i to one concept m(w i ) according to the alignment statistics 4 in the training data (e.g., for "went", it would propose go). Formally, the prediction probability of a concept c is given by:</p><formula xml:id="formula_28">P (c|h t ) =P (copy|h t ) n i=1 a s t,i [[w i = c]] +P (map|h t ) n i=1 a s t,i [[m(w i ) = c]] +P (gen|h t )gen(c|h t )</formula><p>where <ref type="bibr">[[. . .]</ref>] is the indicator function. P (copy|h t ), P (map|h t ) and P (gen|h t ) are the probabilities of three prediction modes respectively, computed by a single layer neural network with softmax activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Relation Classification</head><p>Lastly, the Relation Classification component employs a multi-class classifier for labeling the arcs detected in the Relation Identification component. The classifier uses a biaffine function to score each label, given the head concept representation v i and the child vector h t as input:</p><formula xml:id="formula_29">e i t = h T t W v i + U T h t + V T v i + b</formula><p>where W, U, V, b are model parameters. As suggested by <ref type="bibr" target="#b9">Dozat and Manning (2016)</ref>, we project v i and h t to a lower dimension for reducing the computation cost and avoiding the overfitting of the model. The label probabilities are computed by a softmax function over all label scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Reentrancies</head><p>AMR reentrancy is employed when a node participates in multiple semantic relations (with multiple parent nodes), and that is why AMRs are graphs, rather than trees. The reentrancies are often hard to treat. While previous work often either remove them <ref type="bibr" target="#b16">(Guo and Lu, 2018)</ref> or relies on rule-based restoration in the postprocessing stage <ref type="bibr" target="#b26">(Lyu and Titov, 2018;</ref><ref type="bibr" target="#b30">van Noord and Bos, 2017)</ref>, our model provides a new and principled way to deal with reentrancies. In our approach, when a new node is generated, all its connections to already existing nodes are determined by the multi-head attention.</p><p>For example, for a node with k parent nodes, k different heads will point to the those parent nodes respectively. For a better understanding of our model, a pseudocode is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Training and Inference</head><p>Our model is trained to maximize the log likelihood of the gold AMR graphs given sentences, i.e. log P (G|w), which can be factorized as:</p><formula xml:id="formula_30">log P (G|w) = m t=1 log P (c t |G t−1 , w) + i∈pred(t) log P (arc it |G t−1 , w) + i∈pred(t) log P (rel arc it |G t−1 , w)</formula><p>where m is the total number of vertices. The set of predecessor nodes of c t is denoted as pred(t). arc it denotes the arc between c i and c t , and rel arc it indicates the arc label (relation type). As mentioned, GSP is an autoregressive model, such as seq2seq models and transition models, but it factors the distribution according to a top-down graph structure rather than a depth-first traversal or a left-to-right chain. Meanwhile, GSP has a clear separation of node, arc and relation label probabilities, interacting in a more interpretable and tighten manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Graph Spanning based Parsing</head><p>Input: the input sentence w = (w 1 , w 2 , . . . , w n ) Output: the AMR graph G corresponds to w.</p><p>Q Learning Sentence Representation 1: w = (w 0 = £) + (w 1 , w 2 , . . . , w n ) 2: s 0 , s 1 , s 2 , . . . , s n = Transformer(w) Q Initialization 3: initialize the graph G 0 (c 0 = d * ) for i ∈ pred(t) do 14:</p><p>Relation Classification (h t , v i )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>decide the edge type between c t and c i 15:</p><p>end for 16:</p><formula xml:id="formula_31">update G t−1 to G t 17: end while 18: return G t−1</formula><p>At the operational or testing time, the prediction for the input w is obtained viaĜ = arg max G P (G |w). Rather than iterating over all possible graphs, we adopt a beam search to approximate the best graph. Specifically, for each partially constructed graph, we only consider the top-K concepts obtaining the best single-step probability (a product of the corresponding concept, arc, and relation label probability), where K is the beam size. Only the best K graphs at each time step are kept for the next expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We focus on the most recent LDC2017T10 dataset, as it is the largest AMR corpus. It consists of 36521, 1368, and 1371 sentences in the training, development, and testing sets respectively.</p><p>We use Stanford CoreNLP <ref type="bibr" target="#b27">(Manning et al., 2014)</ref> for text preprocessing, including tokenization, lemmatization, part-of-speech, and namedentity tagging. The input for sentence encoder consists of the randomly initialized lemma, partof-speech tag, and named-entity tag embeddings, as well as the output from a learnable CNN with character embeddings as inputs. The graph encoder uses randomly initialized concept embeddings and another char-level CNN. Model hyperparameters are chosen by experiments on the development set. The details of the hyper-parameter settings are provided in the Appendix. During testing, we use a beam size of 8 for generating graphs. <ref type="bibr">5</ref> Conventionally, the quality of AMR parsing results is evaluated using the Smatch tool , which seeks for the maximum number of overlaps between two AMR annotations after decomposing AMR graphs into triples. However, the ordinary Smatch metric treats all triples equally regardless of their roles in the composition of the whole sentence meaning. We refine the ordinary Smatch metric to take into consideration the notion of core semantics. Specifically, we compute:</p><p>• Smatch-weighted: This metric weights different triples by their importance of composing the core ideas. The root distance d of a triple is defined as the minimum root distance of its involving nodes, the weight of the triple is then computed as:</p><formula xml:id="formula_32">w = min(−d + d thr , 1)</formula><p>In other words, the weight has a linear decay in root distance until d thr . If two triples are matched, the minimum importance score of them is obtained. In our experiments, d thr is set to 5.</p><p>• Smatch-core: This metric only compares the subgraphs representing the main meaning. Precisely, we cut down AMR graphs by setting a maximum root distance d max and only keep the nodes and edges within the threshold. d max is set to 4 in our experiments, of which the remaining subgraphs still have a broad coverage of the original meaning, as illustrated by the distribution of root distance in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Besides, we also evaluate the quality by computing the following metrics.</p><p>jcyk/AMR-parser. • complete-match (CM): This metric counts the number of parsing results that are completely correct.</p><p>• root-accuracy (RA): This metric measures the accuracy of the root concept identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results and Case Study</head><p>The main result is presented in <ref type="table">Table 1</ref>. We compare our method with the best-performing models in each category as discussed in 2. Concretely, van Noord and Bos <ref type="formula" target="#formula_5">(2017)</ref> is a character-level seq2seq model that achieves very competitive result. However, their model is very data demanding as it requires to train on additional 100K sentence-AMR pairs generated by other parsers. <ref type="bibr" target="#b16">Guo and Lu (2018)</ref> is a transitionbased parser with refined search space for AMR. Certain concepts and relations (e.g., reentrancies) are removed to reduce the burdens during training. <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref> is a graph-based method that achieves the best-reported result evaluated by the ordinary Smatch metric. Their parser uses different LSTMs for concept prediction, relation identification, and root identification sequentially. Also, the relation identification stage has the time complexity of O(m 2 log m) where m is the number of concepts. <ref type="bibr" target="#b14">Groschwitz et al. (2018)</ref> views AMR as terms of the AM algebra <ref type="bibr" target="#b13">(Groschwitz et al., 2017)</ref>, which allows standard tree-based parsing techniques to be applicable. The complexity of their projective decoder is O(m 5 ). Last but not least, all these models except for that of van Noord and Bos (2017) require hand-crafted heuristics for graph re-categorization.</p><p>We consider the Smatch-weighted metric as the most suitable metric for measuring the parser's quality on capturing core semantics. The comparison shows that our method significantly outperforms all other methods. The Smatch-core metric also demonstrates the advantage of our Model Graph Smatch(%) RA(%) CM(%) Re-ca. weighted core ordinary <ref type="bibr" target="#b6">Buys and Blunsom (2017)</ref> No --61.9 -van Noord and Bos <ref type="formula" target="#formula_5">(2017)</ref>  The solution is functional human patterns and a balance between transport system capacity and land use generated travel demand. Input: method in capturing the core ideas. Besides, our model achieves the highest root-accuracy (RA) and complete-match (CM), which further confirms the usefulness of a global view and the core semantic first principle.</p><p>Even evaluated by the ordinary Smatch metric, our model yields better results than all previously reported models with the exception of <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref>, which relies on a tremendous amount of manual heuristics for designing rules for graph re-categorization and adopts a pipeline approach. Note that our parser constructs the AMR graph in an end-to-end fashion with a better (quadratic) time complexity.</p><p>We present a case study in <ref type="figure" target="#fig_4">Figure 4</ref> with comparison to the output of Lyu and Titov (2018)'s parser. As seen, both parsers make some mistakes. Specifically, our method fails to identify the concept generated-01. While Lyu and Titov (2018)'s parser successfully identifies it, their parser mistakenly treats it as the root of the whole AMR. It leads to a serious drawback of making the sentence meaning be interpreted in a wrong way. In contrast, our method shows a strong capacity in capturing the main idea "the solution is about some patterns and a balance". However, on the ordinary Smatch met-  ric, their graph obtains a higher score (68% vs. 66%), which indicates that the ordinary Smatch is not a proper metric for evaluating the quality of capturing core semantics. If we adopt the Smatch-weighted metric, our method achieves a better score i.e. 74% vs. 61%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">More Results</head><p>To reveal our parser's ability for grasping meanings at different levels of granularity, we plot the Smatch-core scores in <ref type="figure" target="#fig_5">Figure 5</ref> by varying the maximum root distance d max , compared with several strong baselines and the state-of-the-art model. It demonstrates that our method is better at abstracting the core ideas of a sentence. As discussed in 3, there could be multiple valid generation orders for sibling nodes in an AMR graph. We experiment with the following traversal variants: (1) random, which sorts the sibling nodes in completely random order. (2) relation freq., which sorts the sibling nodes according to their relations to the head node. We assign higher priorities to relations that occur more frequently, which drives our parser always to seek for the most common relation first. (3) combined, which combines the above two strategies by using random and relation freq. with equal chance. As seen in Table 2, the deterministic order strategy for training (relation freq.) achieves better performance than random order. Interestingly, the combined strategy significantly boosts the performance. <ref type="bibr">6</ref> The reason is that the random order potentially produces a larger set of training pairs since each random order strategy can be considered as a different training pair. On the other hand, the deterministic order stabilizes the maximum likelihood estimate training. Therefore, the combined strategy benefits from both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We presented the first top-down AMR parser. Our proposed parser builds a AMR graph incrementally in a root-to-leaf manner. Experiments show that our method has a better capability of capturing the core semantics in a sentence compared with previous state-of-the-art methods. In addition, we overcome the need of heuristics for graph recategorization employed in most previous work, which makes our method much more transferable to other semantic representations or languages.</p><p>Our methods follows the intuition that humans tend to grasp the core meaning of a sentence first. However, some cognitive theories (Langacker, 2008) also suggest that human language understanding is often presented as a circular, abductive process (hermeneutic circle). It is interesting to explore the use of some revision mechanisms when the initial steps go wrong.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>In all experiments, we use the same char-level CNN settings in the sentence encoder and the graph encoder. In addition, all Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017</ref>) layers in our model share the same hyper-parameter settings. For computation efficiency, we only allow each concept to attend to its previously generated concepts in the graph encoder. 7 <ref type="table" target="#tab_4">Table 3</ref> summarizes the chosen hyper-parameters after we tuned on the development set. To mitigate overfitting, we also apply dropout <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref> with the drop rate 0.2 between different layers. We use a special UNK token to replace the input lemmas, POS tags, and NER tags with a rate of 0.33. Parameter optimization is performed with the Adam optimizer (Kingma and <ref type="bibr" target="#b18">Ba, 2014)</ref> with β 1 = 0.9 and β 2 = 0.999. The same learning rate schedule of <ref type="bibr" target="#b38">(Vaswani et al., 2017</ref>) is adopted in our experiments. We use early stopping on the development set for choosing the best model. Following <ref type="bibr" target="#b26">Lyu and Titov (2018)</ref>, for word sense disambiguation, we simply use the most frequent sense in the training set, or -01 if not presented. For wikification, we look-up in the training set for the most frequent one and default to "-.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AMR for the sentence "During a time of prosperity and happiness, such a big earthquake suddenly struck.", where the subgraphs close to the root represent the core semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of GSP, together with the decoding procedure at the time step t, where the read and write operations around the parser state h t follow the order 1 → 2 → 3 → 4 → 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4: initialize time step t = 1 Q Entering Main Spanning Loop 5: while True do 6:h 0 , . . . , v t−1 = Transformer(c 0:t−1 ) 7: h t = Focus Selection (s 0 , v 0:t−1 , s 1:n ) 8: h t = Relation Identification (h t , v 0:t−1 ) 8:decide the parents nodes pred(t) of c t 9:h t = Concept Prediction (h t , s 1:n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of root distance of concepts in the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Smatch scores with different root distances. vN17 is van Noord and Bos (2017)'s parser with 100K additional training pairs. GL'18 is Guo and Lu (2018)'s parser. L'18 is Lyu and Titov (2018)'s parser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: Comparison with state-of-the-art methods (results on the test set). Results relying on heuristic rules for graph re-categorization are marked "Yes" in the Graph Re-ca. column.</figDesc><table><row><cell>+ 100K</cell><cell>No</cell><cell>68.8</cell><cell>67.6</cell><cell>71.0</cell><cell></cell><cell cols="2">75.8</cell><cell>10.2</cell></row><row><cell>Guo and Lu (2018)</cell><cell>Yes</cell><cell>63.5</cell><cell>62.3</cell><cell>69.8</cell><cell></cell><cell cols="2">63.6</cell><cell>9.4</cell></row><row><cell>Lyu and Titov (2018) Groschwitz et al. (2018)</cell><cell>Yes Yes</cell><cell>66.6 -</cell><cell>67.1 -</cell><cell>74.4 71.0</cell><cell></cell><cell cols="2">59.1 -</cell><cell>10.2 -</cell></row><row><cell>Ours</cell><cell>No</cell><cell>71.3</cell><cell>70.2</cell><cell>73.2</cell><cell></cell><cell cols="2">76.9</cell><cell>11.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARG1</cell><cell cols="2">generate-01</cell><cell>ARG0</cell><cell>ARG1-of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">demand-01 ARG1</cell><cell>op1</cell><cell>and</cell><cell>op2</cell><cell>function-01 ARG2-of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>travel-01</cell><cell></cell><cell>pattern</cell><cell>balance-01</cell><cell>solve-01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">mod</cell><cell></cell><cell>ARG1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">settle-02 ARG1</cell><cell cols="2">op1</cell><cell>and</cell><cell>op2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>human</cell><cell></cell><cell cols="2">capacity</cell><cell>use-01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">op1-of mod mod</cell><cell>ARG1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">between</cell><cell cols="2">system</cell><cell>transport</cell><cell>land</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The effect of different sibling orders.</figDesc><table><row><cell>model component char-level CNN Transformer Sentence Encoder Graph Encoder Focus Selection Relation Identification Relation Classification</cell><cell>hyper-parameter number of filters width of filters char embedding size final hidden size number of heads hidden state size feed-forward hidden size 1024 value 256 3 32 128 8 512 Transformer layers 4 lemma embedding size 200 POS tag embedding size 32 NER tag embedding size 16 Transformer layers 1 concept embedding size 300 attention layers 3 number of heads 8 hidden state size 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameters settings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Based on the alignments provided by<ref type="bibr" target="#b24">Liu et al. (2018)</ref>, for each word, the most frequently aligned concept (or its lemma if it has empty alignment) is used for direct mapping.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our code can be found at https://github.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We note there are many other ways to generate a deterministic order. For example, van Noord and Bos (2017) uses the order of aligned words in the sentence. However, we use the relation frequency method for its simplicity and not relying on external resources (e.g, an aligner).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Otherwise, we will need to re-compute the hidden states for all existing nodes at each parsing step.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AMR parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1269" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RIGA at SemEval-2016 task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1143" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Oxford at semeval-2017 task 9: Neural amr parsing with pointeraugmented attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abstract meaning representation parsing using lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Foland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A constrained graph algebra for semantic parsing with AMRs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWCS 2017 -12th International Conference on Computational Semantics -Long papers</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AMR dependency parsing with a typed semantic algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1831" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better transitionbased amr parsing with refined search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guided neural language generation for abstractive summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hardy</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="768" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cognitive Grammar: A Basic Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langacker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing across graphbanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4576" to="4585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An AMR aligner tuned by transition-based parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2422" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">AMR parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rewarding Smatch: Transition-based AMR parsing with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4586" to="4592" />
		</imprint>
	</monogr>
	<note>Radu Florian, Salim Roukos, and Miguel Ballesteros</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Amr parsing with cache transition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aligning english strings with abstract meaning representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="425" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parsing english into abstract meaning representation using syntaxbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1143" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linguistically-informed self-attention for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5027" to="5038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Camr at semeval-2016 task 8: An extended transition-based amr parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Getting the most out of amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust subgraph generation improves abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">AMR parsing as sequence-tograph transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transition-based parsing of the chinese treebank using a global discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AMR parsing with an incremental joint model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="680" to="689" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
