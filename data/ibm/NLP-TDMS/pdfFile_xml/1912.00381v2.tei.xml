<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gate-Shift Networks for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sergio@maia.ub.es</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universitat de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
							<email>lanz@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gate-Shift Networks for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep 3D CNNs for video action recognition are designed to learn powerful representations in the joint spatiotemporal feature space. In practice however, because of the large number of parameters and computations involved, they may under-perform in the lack of sufficiently large datasets for training them at scale. In this paper we introduce spatial gating in spatial-temporal decomposition of 3D kernels. We implement this concept with Gate-Shift Module (GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. With GSM plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. We perform an extensive evaluation of the proposed module to study its effectiveness in video action recognition, achieving state-of-the-art results on Something Something-V1 and Diving48 datasets, and obtaining competitive results on EPIC-Kitchens with far less model complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video action recognition is receiving increasing attention due to its potential applications in video surveillance, media analysis, and robotics, just to mention a few. Although great advances have been achieved during last years, action recognition models have not yet achieved the success of image recognition models, and the 'AlexNet momentum for video' has still to come.</p><p>A key challenge lies in the space-time nature of the video medium that requires temporal reasoning for finegrained recognition. Methods based on temporal pooling of frame-level features (TSN <ref type="bibr" target="#b41">[42]</ref>, ActionVLAD <ref type="bibr" target="#b12">[13]</ref>) process the video as a (order-less) set of still images and work well enough when the action can be discerned from objects and scene context (UCF-101, Sports-1M, THUMOS). More akin to the time dimension in video, late temporal aggregation of frame-level features can be formulated as sequence <ref type="figure">Figure 1</ref>: 3D kernel factorization for spatio-temporal learning in video. Existing approaches decompose into channelwise (CSN), spatial followed by temporal (S3D, TSM), or grouped spatial and spatio-temporal (GST). In all these, spatial, temporal, and channel-wise interaction is hardwired. Our Gate-Shift Module (GSM) leverages group spatial gating (blocks in green) to control interactions in spatial-temporal decomposition. GSM is lightweight and a building block of high performing video feature extractors. learning (LRCN <ref type="bibr" target="#b4">[5]</ref>, VideoLSTM <ref type="bibr" target="#b23">[24]</ref>) and with attention (Attentional Pooling <ref type="bibr" target="#b11">[12]</ref>, LSTA <ref type="bibr" target="#b33">[34]</ref>). At the other hand, early temporal processing is used to fuse short term motion features from stack of flow fields (Two-Stream <ref type="bibr" target="#b32">[33]</ref>) or predicted directly from the encoded video (DMC-Net <ref type="bibr" target="#b31">[32]</ref>).</p><p>Fine-grained recognition can benefit from deeper temporal modeling. Full-3D CNNs (C3D <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16]</ref>) process the video in space-time by expanding the kernels of a 2D ConvNet along the temporal dimension. Deep C3Ds are designed to learn powerful representations in the joint spatiotemporal feature space with more parameters (3D kernels) and computations (kernels slide over 3 densely sampled dimensions). In practice however, they may under-perform due to the lack of sufficiently large datasets for training them at scale. To cope with these issues arising from the curse of dimension one can narrow down network capacity by design. <ref type="figure">Fig. 1</ref> shows several C3D kernel decomposition approaches proposed for spatio-temporal feature learning in video. A most intuitive approach is to factorize 3D spatio-temporal kernels into 2D spatial plus 1D temporal, resulting in a structural decomposition that disentangles spatial from temporal interactions (P3D <ref type="bibr" target="#b30">[31]</ref>, R(2+1)D <ref type="bibr" target="#b39">[40]</ref>, S3D <ref type="bibr" target="#b45">[46]</ref>). An alternative design is separating channel interactions and spatio-temporal interactions via group convolution (CSN <ref type="bibr" target="#b38">[39]</ref>), or modeling both spatial and spatiotemporal interactions in parallel with 2D and 3D convolution on separated channel groups (GST <ref type="bibr" target="#b26">[27]</ref>). Temporal convolution can be constrained to hard-coded time-shifts that move some of the channels forward in time or backward (TSM <ref type="bibr" target="#b24">[25]</ref>). All these existing approaches learn structured kernels with a hard-wired connectivity and propagation pattern across the network. There is no data dependent decision taken at any point in the network to route features selectively through different branches, for example, groupand-shuffle patterns are fixed by design and learning how to shuffle is combinatorial complexity.</p><p>In this paper we introduce spatial gating in spatialtemporal decomposition of 3D kernels. We implement this concept with Gate-Shift Module (GSM) as shown in <ref type="figure">Fig. 1</ref>. GSM is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. The GSM first applies 2D convolution, then decomposes the output tensor using a learnable spatial gating into two tensors: a gated version of it, and its residual. The gated tensor goes through a 1D temporal convolution while its residual is skip-connected to its output. We implement spatial gating as group spatio-temporal convolution with single output plane per group. We use hard-coded time-shift of channel groups instead of learnable temporal convolution. With GSM plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. For example, when GSM is plugged into TSN <ref type="bibr" target="#b41">[42]</ref>, an absolute gain of +32 percentage points in accuracy is obtained on Something Something-V1 dataset with just 0.48% additional parameters and 0.55% additional Floating Point Operations (FLOPs).</p><p>The contributions of this paper can be summarized as follows: (i) We propose a novel spatio-temporal feature extraction module that can be plugged into existing 2D Convolutional Neural Network (CNN) architectures with negligible overhead in terms of computations and memory; (ii) We perform an extensive ablation analysis of the proposed module to study its effectiveness in video action recognition; (iii) We achieve state-of-the-art or competititve results on public benchmarks with less parameters and FLOPs compared to existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Inspired by the performance improvements obtained with deep convolutional architectures in image recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>, much effort has gone into extending these for video action recognition.</p><p>Fusing appearance and flow. A popular extension of 2D CNNs to handle video is the Two-Stream architecture by Simonyan and Zisserman <ref type="bibr" target="#b32">[33]</ref>. Their method consists of two separated CNNs (streams) that are trained to extract features from a sampled RGB video frame paired with the surrounding stack of optical flow images, followed by a late fusion of the prediction scores of both streams. The image stream encodes the appearance information while the optical flow stream encodes the motion information, that are often found to complement each other for action recognition. Several works followed this approach to find a suitable fusion of the streams at various depths <ref type="bibr" target="#b8">[9]</ref> and to explore the use of residual connections between them <ref type="bibr" target="#b7">[8]</ref>. These approaches rely on optical flow images for motion information, and a single RGB frame for appearance information, which is limiting when reasoning about the temporal context is required for video understanding.</p><p>Video as a set or sequence of frames. Later, other approaches were developed using multiple RGB frames for video classification. These approaches sparsely sample multiple frames from the video, which are applied to a 2D CNN followed by a late integration of frame-level features using average pooling <ref type="bibr" target="#b41">[42]</ref>, multilayer perceptrons <ref type="bibr" target="#b48">[49]</ref>, recurrent aggregation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, or attention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>. To boost performance, most of these approaches also combine video frame sequence with externally computed optical flow. This shows to be helpful, but computationally intensive.</p><p>Modeling short-term temporal dependencies. Other research has investigated the middle ground between late aggregation (of frame features) and early temporal processing (to get optical flow), by modeling short-term dependencies. This includes differencing of intermediate features <ref type="bibr" target="#b28">[29]</ref> and combining Sobel filtering with feature differencing <ref type="bibr" target="#b35">[36]</ref>. Other works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> develop a differentiable network that performs TV-L1 <ref type="bibr" target="#b46">[47]</ref>, a popular optical flow extraction technique. The work of <ref type="bibr" target="#b20">[21]</ref> instead uses a set of fixed filters for extracting motion features, thereby greatly reducing the number of parameters. DMC-Nets <ref type="bibr" target="#b31">[32]</ref> leverage motion vectors in the compressed video to synthesize discriminative motion cues for two-stream action recognition at low computational cost compared to raw flow extraction.</p><p>Video as a space-time volume. Unconstrained modeling and learning of action features is possible when considering video in space-time. Since video can be seen as a temporally dense sampled sequence of images, expanding 2D convolution operation in 2D-CNNs to 3D convolution is a most intuitive approach to spatio-temporal feature learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. The major drawback of 3D CNNs is the huge number of parameters involved. This results in increased computations and the requirement of large scale datasets for pre-training. Carreira and Zisserman <ref type="bibr" target="#b1">[2]</ref> addressed this limitation by inflating video 3D kernels with the 2D weights of a CNN trained for image recognition. Several other approaches focused on reducing the number of parameters by disentangling the spatial and temporal feature extraction operations. P3D <ref type="bibr" target="#b30">[31]</ref> proposes three different choices for separating the spatial and temporal convolutions and develops a 3D-ResNet architecture whose residual units are a sequence of such three modules. R(2+1)D <ref type="bibr" target="#b39">[40]</ref> and S3D-G <ref type="bibr" target="#b45">[46]</ref> also show that a 2D convolution followed by 1D convolution is enough to learn discriminative features for action recognition. CoST <ref type="bibr" target="#b21">[22]</ref> performs 2D convolutions, with shared parameters, along the three orthogonal dimensions of a video sequence. MultiFiber <ref type="bibr" target="#b2">[3]</ref> uses multiple lightweight networks, the fibers, and multiplexer modules that facilitate information flow using point-wise convolutions across the fibers.</p><p>Spatial-temporal modeling. Recently, the focus of research is moving to the development of efficient (from a computational point of view) and effective (from a performance point of view) architectures. CNNs provide different levels of feature abstractions at different layers of the hierarchy. It has been found that the bottom layer features are less useful for extracting discriminative motion cues <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b45">46]</ref>. In <ref type="bibr" target="#b34">[35]</ref> it is proposed to apply 1D convolution layers on top of a 2D CNN for video action recognition. The works of <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b45">[46]</ref> show that it is more effective to apply full 3D and separable 3D convolutions at the top layers of a 2D CNN for extracting spatio-temporal features. These approaches resulted in performance improvement over full 3D architectures with less parameters and computations. Static features from individual frames represent scenes and objects and can also provide important cues in identifying the action. This is validated by the improved performance obtained with two-path structures that apply a parallel 2D convolution in addition to the 3D convolution <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b26">27]</ref>. MiCT <ref type="bibr" target="#b49">[50]</ref> is designed by adding 3D convolution branches in parallel to the 2D convolution branches of a BN-Inception-like CNN. GST <ref type="bibr" target="#b26">[27]</ref> makes use of the idea of grouped convolutions for developing an efficient architecture for action recognition. They separate the features at a hierarchy across the channel dimension and separately perform 2D and 3D convolutions followed by a concatenation operation. In this way, the performance is increased while reducing the number of parameters. STM <ref type="bibr" target="#b17">[18]</ref> proposes two parallel blocks for extracting motion features and spatio-temporal features. Their network rely only on 2D and 1D convolutions and feature differencing for encoding motion and spatio-temporal features. TSM <ref type="bibr" target="#b24">[25]</ref> proposes to shift the features across the channel dimension as a way to perform temporal interaction between the features from adjacent frames of a video. This parameter-less approach has resulted in similar performance to 3D CNNs. However, in all previous approaches, spatial, temporal, and channelwise interaction is hard-wired. Here, we propose the Gate-Shift Module (GSM), which control interactions in spatialtemporal decomposition and learns to adaptively route features thought time and combine them, at almost no additional parameters and computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Gate-Shift Networks</head><p>In this section we present Gate-Shift Networks for finegrained action recognition. We first describe their building block, Gate-Shift Module (GSM), that turns a 2D CNN into a high performing spatio-temporal feature extractor, with minimal overhead. We then discuss and motivate the design choices leading to our final GSM architecture used in the experiments.  <ref type="figure">Fig. 1)</ref> that have been successfully applied to video action recognition. S3D, or R(2+1)D, P3D, decompose 3D convolutions into 2D spatial plus 1D temporal convolutions. TSM replaces 1D temporal convolution with parameter-free channel-wise temporal shift operations. GST uses group convolution where one group applies 2D spatial and the other 3D spatio-temporal convolution. GST furthermore applies point-wise convolution before and after the block to allow for interactions between spatial and spatio-temporal groups, and for channel reduction and up-sampling. In these modules, the feature flow is hard-wired by design, meaning that features are forwarded from one block to the next without data-dependent pooling, gating or routing decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gate-Shift Module</head><p>GSM design, in <ref type="figure" target="#fig_0">Fig. 2</ref>, is inspired by GST and TSM but replaces the hard-wired channel split with a learnable spatial gating block. The function of gate block, paired with fuse block, is selectively routing gated features through time-shifts and merging them with the spatially convolved residual to inject spatio-temporal interactions adaptively. GSM is lightweight as it uses 2D kernels, parameter-free time-shifts and few additional parameters to compute the spatial gating planes.</p><p>Based on the conceptual design in <ref type="figure" target="#fig_0">Fig. 2</ref>, we instantiate GSM as in <ref type="figure" target="#fig_2">Fig. 3</ref>. GSM first applies spatial convolution on the layer input; this is the operation inherited from the 2D CNN base model where GSM is build in. Then, grouped spatial gating is applied, that is, gating planes are obtained for each of two channel groups, and applied on them. This separates the 2D convolution output into group-gated features and residual. The gated features are group-shifted for-  ward and backward in time, and zero-padded. These are finally fused (added) with the residual and propagated to the next layer. This way, GSM selectively mixes spatial and temporal information through a learnable spatial gating. Gating is implemented with a single spatio-temporal 3D kernel and tanh activation. With a 3D kernel we utilize short-range spatio-temporal information in the gating. tanh provides spatial gating planes with values in the range (−1, +1) and is motivated as follows. When the gating value at a feature location is +1 and that of the time-shifted feature was +1, then a temporal feature averaging is performed at that location. If the gating value of the timeshifted feature was -1 instead, then a temporal feature differencing is performed. Using tanh, the gating can thus learn to apply either of the two modes, location-wise. It is also found in our ablation study that tanh provides better results than e.g. sigmoid that would be the standard choice with gating, see Sec. 4.3 last paragraph.</p><p>GSM layer implementation. Let tensor X be the GSM input after 2D convolution (output of blue block in <ref type="figure" target="#fig_2">Fig. 3)</ref>, of shape C ×T ×W ×H where C is the number of channels and W H, T are the spatial and temporal dimensions. Let X = [X 1 , X 2 ] be the group=2 split of X along the channel dimension, and</p><formula xml:id="formula_0">W = [W 1 , W 2 ] be the two C /2 × 3 × 3 × 3 shaped gating kernels. Then, the GSM output Z = [Z 1 , Z 2 ] is computed as Y 1 = tanh(W 1 * X 1 ) X 1 (1) Y 2 = tanh(W 2 * X 2 ) X 2 (2) R 1 = X 1 − Y 1 (3) R 2 = X 2 − Y 2 (4) Z 1 = shift_fw(Y 1 ) + R 1 (5) Z 2 = shift_bw(Y 2 ) + R 2<label>(6)</label></formula><p>where ' * ' represents convolution, ' ' is Hadamard product, and shift_fw, shift_bw is forward, backward temporal shift. Note that the parameter count in this GSM implementation is 2 × (27 · C /2) = 27 · C; this is far less than that of a typical C2D block. E.g., the 1×3×3 block in <ref type="figure" target="#fig_2">Fig. 3</ref> has C kernels of size (9 · C in ) where typically C ≥ C in 3.</p><p>Relation to Residual Architectures. It should be noted that Eqns. 3 and 5 can be reformulated as R 1 = shift fw(Y 1 )−Y 1 and Z 1 = X 1 +R 1 . This is in analogy to the residual learning of ResNet. In GSM, the residual is the learned spatio-temporal features that are added to the input X 1 to generate discriminative spatio-temporal features for identifying an action class. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we insert GSM inside one of the branches of Inception blocks. We analyze the branch to which GSM is to be applied, empirically. From the experiments we conclude that adding GSM to the branch with the least number of convolution layers performs the best. A hypothesis is that the other branches consist of spatial convolutions with larger kernel sizes and applying GSM on those branches will affect the spatial learning capacity of the network. This hypothesis is strengthened further by observing a reduced performance when GSM was added inside all the branches. Because of the presence of additional branches in Inception blocks, that encode spatial information, there is no need for a separate spatial convolution operation in GSM. That is, the GSM blocks in <ref type="figure" target="#fig_3">Fig. 4</ref> are as in <ref type="figure" target="#fig_2">Fig. 3</ref> without the 1 × 3 × 3 spatial convolution block.</p><p>For clip level action classification we follow the approach of TSN, that is, we predict the action by average pooling the frame level (now spatio-temporal) scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>This section presents an extensive set of experiments to evaluate GSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate Gate-Shift Module (GSM) on three standard action recognition benchmarks, Something Something-V1 <ref type="bibr" target="#b14">[15]</ref> (Something-V1), Diving48 <ref type="bibr" target="#b22">[23]</ref> and EPIC-Kitchens <ref type="bibr" target="#b3">[4]</ref>. Something-V1 consists of 100K videos with 174 fine-grained object manipulation actions. Performance is reported on the validation set. Diving48 dataset contains around 18K videos with 48 fine-grained dive classes. EPIC-Kitchens dataset comprises 34K egocentric videos with fine-grained activity labels. We report the performance obtained on the two standard test splits. Since the test labels are withheld, the recognition scores are obtained from the submission server after we submitted the prediction scores.  <ref type="table">Table 1</ref>: Ablation analysis done to determine the Inception branch that is most suitable for plugging in GSM.</p><p>All the three considered datasets are diverse in nature and require strong spatio-temporal reasoning for predicting the action classes. For instance, Something-V1 dataset does not distinguish among the objects being handled. On the other hand, EPIC-Kitchens dataset require strong spatio-temporal reasoning as well as information about the objects being handled. The videos in Diving48 dataset generally contain a uniform background with fine-grained diving actions and require strong understanding of temporal dynamics of the human body in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>As explained in Sec. 3.2, we choose BN-Inception and InceptionV3 as the CNN backbones. GSM is added inside each Inception block of the respective CNNs. Thus a total of 10 GSMs are added. We initialize the 3D convolution in the gating layer with zeros. Thus the model starts as a standard TSN architecture and the gating is learned during training. All models are initialized with ImageNet pre-trained weights. The entire network is trained end-toend using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.01 and momentum 0.9. We use a cosine learning rate schedule <ref type="bibr" target="#b25">[26]</ref>. The network is trained for 60 epochs on Something-V1 and EPIC-Kitchens while Diving48 is trained for 20 epochs. The first 10 epochs are used for gradual warm-up <ref type="bibr" target="#b13">[14]</ref>. The batch size is 32 for Something-V1 and EPIC-Kitchens, and 8 for Diving48. Dropout is applied at the classification layer at a rate of 0.5 for Something-V1 and EPIC-Kitchens and 0.7 for Diving48 dataset. Random scaling, cropping and flipping are applied as data augmentation during training. The dimension of the input is 224 × 224 and 229 × 229 for BN-Inception and InceptionV3, respectively. The reduced input dimension to InceptionV3 reduces the computational complexity without degradation in performance. If not specified, we use just the center crop during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Analysis</head><p>In this section, we report the ablation analysis performed on the validation set of Something-V1 dataset. In all the experiments, we apply 8 frames as input to the network. We first conducted an analysis to determine the Inception branch that is most suitable for adding GSM. The results of this experiment are reported in Tab. 1. We number each  <ref type="figure" target="#fig_3">Fig. 4</ref>. In these two plots the 10 action categories described in <ref type="bibr" target="#b14">[15]</ref> are visualized. In (d) we list the action classes with the highest improvement over TSN baseline. X-axis shows the number of corrected samples for each class. Y-axis labels are in the format true label (GSM)/predicted label (TSN). In (c) we visualize the corresponding t-SNE plot.  branch from left to right. It can be seen that the best performing model is obtained when GSM is added in branch 4. When GSM is added inside all the branches, we observed the lowest performance as this adversely affects the spatial modeling capacity of the network. From the above experiments, we conclude that GSM is most suited to be added inside the branch which contains the least number of convolutions. We follow the same design choice for InceptionV3 as well. More details regarding the architecture of Incep-tionV3 is provided in the supplementary material.</p><p>We then compared the performance improvement by adding GSM on BN-Inception. Tab. 2 shows the ablation results. Baseline is the standard TSN architecture, with an accuracy of 17.25%. We then applied GSM at the last In-ception block of the CNN. This improved the recognition performance by 5%. Increasing the number of GSMs added to the backbone consistently improved the recognition performance of the network. The final model, in which GSM is applied in all Inception blocks results in a recognition accuracy of 47.24%, i.e., +30% absolute improvement over TSN baseline, with only 0.48% and 0.55% overhead in parameters and complexity, respectively.</p><p>Since sigmoid is the general choice used in gating mechanims, we also analyzed the performance of GSM when sigmoid non-linearity is used inside the gating function. Compared to tanh non-linearity, sigmoid unperforms by absolute 3% (47.24% vs 44.75%) proving the suitability of tanh for gate calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">State-of-the-Art Comparison</head><p>Something-V1. The recognition performance obtained by GSM is compared with state-of-the-art approaches that just use RGB frames in Tab. 3. We also report the number of frames used by each approach during inference and the corresponding computational complexity in terms of FLOPs. The first block in the table lists the approaches that use  2D CNN and efficient 3D CNN implementations. The second block shows the approaches that use Full-3D CNNs. From the table, it can be seen that GSM results in an absolute gain of +32% (17.52% vs 49.56%) over the TSN baseline. GSM performs better than 3D CNNs or heavier backbones and also those approaches that use external data for pre-training, with considerably less number of FLOPs. GSM performs comparatively to the top performing method <ref type="bibr" target="#b27">[28]</ref> with less number of FLOPs. It should be noted that the FLOPs of the architecture described in <ref type="bibr" target="#b27">[28]</ref> is computed assuming a single clip of 16 frames. It can also be seen that by using InceptionV3, which is a larger backbone than BN-Inception, the performance of the proposed approach improves. We also evaluated the performance of ensemble of models trained with different number of input frames and achieved a state-of-the-art recognition accuracy of 55.16% 1 .</p><p>Diving48. Tab. 4 compares performance of GSM on Diving48 dataset with state-of-the-art approaches. We train the network using 16 frames and sample two clips during inference. We use InceptionV3 as the CNN backbone. In this dataset, the actions cannot be recognized from the scene context alone and require strong spatio-temporal reasoning. GSM achieves a recognition accuracy of 40.27%, an improvement of +1.3% over previous state-of-the-art <ref type="bibr" target="#b26">[27]</ref>.</p><p>EPIC-Kitchens. In EPIC-Kitchens, the labels are provided as verb-noun pairs and the performance is evaluated on verb, noun and action recognition accuracies. For this dataset, we train GSM as a multi-task problem for verb, noun and action prediction. In the classifica-1 See supplementary document for more details on model ensembles. tion layers, we apply action scores as bias to the verb and noun classifiers, as done in LSTA <ref type="bibr" target="#b33">[34]</ref>. We use BN-Inception as the backbone CNN. The network is trained with 16 frames. Two clips consisting of 16 frames are sampled from each video during inference. We report the recognition accuracy obtained on the two standard test splits, S1 (seen) and S2 (unseen), in Tab. 5. The first block in the table shows the methods that use both RGB frames and optical flow as inputs while the second block lists the approaches that only use RGB images. From the table, it can be seen that GSM performs better than other approaches that use optical flow images for explicit motion encoding. The only two methods that beat GSM, R(2+1)D <ref type="bibr" target="#b10">[11]</ref> and LFB <ref type="bibr" target="#b44">[45]</ref>, train two separate networks, one trained for verb and the other for noun classification, and leverage additional data for pre-training. GSM uses a single network for predicting all three labels from a video, thereby making it faster and more memory efficient. In fact, GSM performs better than R(2+1)D model pre-trained on Sports-1M dataset, suggesting that GSM can also improve its performance by pre-training on external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>In <ref type="figure">Fig. 5d</ref>, we show the top 10 action classes that improved the most by adding GSM to the CNN backbone of TSN. From the figure, it can be seen that the network has enhanced its ability to distinguish between action classes that are similar in appearance, such as Unfolding something and Folding something, Putting something infront of something and</p><p>Removing something, revealing something behind, etc. Sample frames from some Folding something</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfolding something</head><p>Plugging something into something Plugging something into something but pulling it ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Taking one of many similar things on the table</head><p>Putting something similar to other things that are ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Putting something infront of something</head><p>Removing something, revealing something behind <ref type="figure">Figure 6</ref>: Sample frames from Something-V1 videos that belong to the most improved classes when GSM is added. of these most improved classes are shown in <ref type="figure">Fig. 6</ref>. From these frames, we can see that reversing the order of the frames changes the action and thus the orderless pooling present in TSN fails to identify the action. On the other hand, GSM is able to improve the recognition score on these classes, providing a strong spatio-temporal reasoning. In order to validate the temporal encoding capability of GSM, we evaluated its performance by applying the video frames in the reverse order. This resulted in a drastic degradation in the recognition performance from 47.24 to 15.38%. On the other hand, there was no change in the recognition performance of TSN when frames reversed in time were applied.</p><p>The t-SNE plots of the features from the final layer of the CNN corresponding to the 10 action groups described in <ref type="bibr" target="#b14">[15]</ref> are shown in <ref type="figure">Fig. 5a and 5b. Fig. 5c</ref> shows the t-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pre-training Accuracy (%) TSN <ref type="bibr" target="#b41">[42]</ref>    SNE visualization of the most improved classes compared to TSN. We sample 1800 videos from the validation split for the t-SNE visualization. It can be seen that the features from GSM show a lower intra-class and higher inter-class variability compared to those from TSN. We also analyzed the memory requirement and computational complexity of GSM and various state-of-the-art approaches. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the accuracy, parameter and complexity trade-off computed on the validation set of Somthing-V1 dataset. The graph plots accuracy vs GFLOPs and the area of the bubbles indicate the number of parameters present in each method. From the plot, it can be seen that GSM performs competitively to the state-of-theart <ref type="bibr" target="#b27">[28]</ref> with less than one tenth the number of parameters and half the number of FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed Gate-Shift Module (GSM), a novel temporal interaction block that turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. GSM introduces spatial gating to decide on exchanging information with neighboring frames. We performed an extensive evaluation to study its effectiveness in video action recognition, achieving state-of-the-art results on Something Something-V1 and Diving48 datasets, and obtaining competitive results on EPIC-Kitchens with far less model complexity. For example, when GSM is plugged into TSN, an absolute gain of +32% in recognition accuracy is obtained on Something Something-V1 dataset with just 0.48% additional parameters and 0.55% additional FLOPs. <ref type="table">Table 6</ref>: Gate-Shift BN-Inception Architecture. All convolution layers are followed by BN layer and ReLU nonlinearity. C is the number of classes in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. InceptionV3</head><p>The architecture of GSM InceptionV3 is shown in Tab. 7 along with the size of the outputs after each layer. We apply an input of size 229 × 229 instead of the standard size of 299 × 299. This reduces the computational complexity without affecting the performance of the model. The Inception blocks with GSM used in the model are shown in <ref type="figure">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. t-SNE</head><p>We first visualize the t-SNE plot of features for the models used in the ablation study, i.e., model with no GSM <ref type="figure">(Fig. 9a)</ref>, model with 1 GSM <ref type="figure">(Fig. 9b)</ref>, model with 5 GSM <ref type="figure">(Fig. 9c</ref>) and model with 10 GSM <ref type="figure">(Fig. 9d)</ref>  <ref type="figure">(Fig. 1a)</ref> 27 × 27 × 256 Inception-GSM <ref type="figure">(Fig. 1a)</ref> 27 × 27 × 288 Inception-GSM <ref type="figure">(Fig. 1c)</ref> 27 × 27 × 288 Inception-GSM <ref type="figure">(Fig. 1b)</ref> 13 × 13 × 768 Inception-GSM <ref type="figure">(Fig. 1b)</ref> 13 × 13 × 768 Inception-GSM <ref type="figure">(Fig. 1b)</ref> 13 × 13 × 768 Inception-GSM <ref type="figure">(Fig. 1b)</ref> 13 × 13 × 768 Inception-GSM <ref type="figure">(Fig. 1b)</ref> 13 × 13 × 768 Inception-GSM <ref type="figure">(Fig. 1d)</ref> 6 × 6 × 1280 Inception-GSM <ref type="figure">(Fig. 1e)</ref> 6 × 6 × 2048 Inception-GSM <ref type="figure">(Fig. 1e</ref>  <ref type="table">Table 7</ref>: Gate-Shift InceptionV3 Architecture. All convolution layers are followed by BN layer and ReLU nonlinearity. C is the number of classes in the dataset.</p><p>the features of the 10 action groups presented in <ref type="bibr" target="#b14">[15]</ref>. From the figures, one can see that adding GSM into the CNN results in a reduction of intra-class variability and in an increase of inter-class variability. <ref type="figure" target="#fig_7">Fig. 10</ref> shows the t-SNE plot of features from the last four Inception blocks of BN-Inception with 10 GSM. From the figure, we can see that the semantic separation increases as we move towards the top layers of the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ensemble Results</head><p>Tab. 8 lists the action recognition accuracy, number of parameters and FLOPS obtained by ensembling the models presented in this work on Something Something-V1 dataset. The first and second blocks in the table list the accuracy obtained with individual models when evaluated using 1 and 2 clips, respectively. The third block shows the recognition accuracy with different ensemble models. Ensembling is done by combining GSM InceptionV3 models that are trained with different number of input frames. We average the prediction scores obtained from individual models to compute the performance of the ensemble. From the table, it can be seen that the accuracy is increasing as more models are being added. Using models with different number of input frames enables the ensemble to encode the video with different temporal resolutions. Such an ensemble has some analogy with SlowFast <ref type="bibr" target="#b6">[7]</ref>. With an ensemble of models trained on 8, 12, 16 and 24 frames, we achieve a stateof-the-art recognition accuracy of 55.16%. We include the parameter and complexity trade-off in <ref type="figure" target="#fig_8">Fig. 11. From the figure</ref>, we can see that the ensemble of GSM family achieves the state-of-the-art recognition performance with fewer parameters than previous state-of-the-art <ref type="bibr" target="#b27">[28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head><p>We show 'visual explanations' for the decisions made by GSM. We use the approach of saliency tubes [?] for generating the visualizations. In this approach, the frames and their corresponding regions that are used by the model for making a decision are visualized in a form of saliency map. <ref type="figure" target="#fig_0">Figs. 12 and 13</ref> compare the saliency tubes generated by the TSN baseline and the proposed GSM approach on sample videos from the validation set of Something Something-V1 dataset. We use the models with BNInception backbone trained using 16 frames for generating the visualizations. Each column in the figures show the 16 frames that are applied as input to the respective networks with the saliency tubes overlaid on top. We show TSN on the left side and GSM on the right side. The classes that improved the most by plugging in GSM on TSN are chosen   for visualization. These classes require strong temporal reasoning for understanding the action. From the figures, we can see that TSN focuses on the objects present in the video irrespective of where and when the action takes place, while GSM enables temporal reasoning by focusing on the active object(s) where and when an action is taking place. For example, in <ref type="figure" target="#fig_0">Fig. 12a</ref>, an example from the class putting something in front of something, TSN focuses on the object that is present in the scene, the pen in the first few frames and the cup in the later frames. On the other hand, GSM makes the decision from the frames where the cup is introduced into the video. Similarly, in the example from the class taking one of many similar things on the table shown in <ref type="figure" target="#fig_0">Fig. 12d</ref>, TSN is focusing on the object, the matchbox, in all the frames while GSM makes the decision based on those frames where the action is taking place.</p><p>putting something in front of something putting something similar to other things that are already on the table (d) <ref type="figure" target="#fig_2">Figure 13</ref>: Saliency tubes generated by TSN (left) and GSM (right) on sample videos taken from the validation set of Something Something-V1 dataset. Action labels are shown as text on columns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>illustrates the network schematics of 3D kernel factorization approaches (cf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>C3D decomposition approaches in comparison to GSM schematics. GSM is inspired by GST and TSM but replaces the hard-wired channel split with a learnable spatial gating block.shift fw shift bw tanh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>GSM implementation with group gating and forward-backward temporal shift. A gate is a single 3D convolution kernel with tanh calibration, thus very few parameters are added when GSM is used to turn a C2D base model into a spatio-temporal feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>BN-Inception blocks with GSM. The kernel size and stride of convolutional and pooling layers are annotated inside each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) t-SNE plot of TSN features (b) t-SNE plot of GSM features (c) t-SNE of most improved classes (d) Most improved classes Figure 5: t-SNE plots of the output layer features preceding the final fully connected layers for (a) TSN with BN-Inception, and (b) same TSN but with GSM built-in as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>+32%Figure 7 :</head><label>7</label><figDesc>Accuracy-vs-complexity of state-of-the-art on Something-V1, from Tab. 3. Size indicates number of parameters (M, in millions). GSM outperforms or competes in recognition performance with far less model complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Inception blocks with GSM used in the InceptionV3 architecture. t-SNE visualization of features from networks that use (a) No GSM, (b) 1 GSM, (c) 5 GSMs and (d) 10 GSMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>t-SNE visualization of features from intermediate layer of GSM BN-Inception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Accuracy-vs-complexity of state-of-the-art on Something-V1. Size indicates number of parameters (M, in millions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Saliency tubes generated by TSN (left) and GSM (right) on sample videos taken from the validation set of Something Something-V1 dataset. Action labels are shown as text on columns.plugging something into something but pulling it right out as you remove your hand(c)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Recognition Accuracy by varying the number of Gate-Shift Modules (GSMs) added to the backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison to state-of-the-art on Something-V1.* : Computed assuming a single clip of 16 frames as input.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison to state-of-the-art on Diving48.</figDesc><table><row><cell>Method</cell><cell>Pre-train</cell><cell>S1 Verb Noun Action Verb Noun Action S2</cell></row><row><cell>TSN [42]</cell><cell cols="2">ImageNet 45.68 36.8 19.86 34.89 21.82 10.11</cell></row><row><cell>TBN [20]</cell><cell cols="2">ImageNet 60.87 42.93 30.31 49.61 25.68 16.80</cell></row><row><cell cols="3">LSTA [34] ImageNet 59.55 38.35 30.33 47.32 22.16 16.63</cell></row><row><cell cols="3">RU-LSTM [10] ImageNet 56.93 43.05 33.06 43.67 26.77 19.49</cell></row><row><cell>LFB [45]</cell><cell cols="2">Kinetics 60.0 45 32.7 50.9 31.5 21.2</cell></row><row><cell cols="3">R(2+1)D [11] Sports-1M 59.6 43.7 31.0 47.2 28.7 18.3</cell></row><row><cell cols="3">R(2+1)D [11] External 65.2 45.1 34.5 58.4 36.9 26.1</cell></row><row><cell>GSM</cell><cell cols="2">ImageNet 59.41 41.83 33.45 48.28 26.15 20.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparison to state-of-the-art on EPIC-Kitchens.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Recognition Accuracy obtained on Something Something-V1 dataset by ensembling different models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Relation to existing approaches. GSM is a generalization of several existing approaches. With gating=0, GSM collapses to TSN<ref type="bibr" target="#b41">[42]</ref>; with gating=1, converges to TSM<ref type="bibr" target="#b24">[25]</ref> style; with gating=1 and replacing temporal shift with expensive 3D convolution, converges to GST<ref type="bibr" target="#b26">[27]</ref> style.3.2. Gate-Shift ArchitectureWe adopt TSN as reference architecture for action recognition. TSN performs temporal pooling of frame-level features using C2D backbone. We choose BN-Inception and InceptionV3 as backbone options with TSN, and describe here how we GSM them.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by ICREA under the ICREA Academia programme.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This section includes details about the backbone CNN architecture, additional t-SNE plots, results on Something Something-V1 dataset using ensemble of models, and visualization samples using saliency tubes. We also provide a supplementary video with the visualization samples. Code and models are available at https://github.com/ swathikirans/GSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Details</head><p>We provide the details of the CNN architectures used in our GSM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. BN-Inception</head><p>Tab. 6 shows the architecture of GSM BN-Inception. The Inception modules used are shown in <ref type="figure">Fig. 4</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04172</idno>
		<title level="m">Learning Discriminative Motion Features Through Detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Longterm recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SlowFast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">STM: Spa-tioTemporal and Motion Encoding for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attentive Spatio-Temporal Representation Learning for Diving Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative Spatiotemporal Feature Learning for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RESOUND: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VideoLSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grouped Spatial-Temporal Aggregation for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with spatial-temporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal difference networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DMC-Net: Generating discriminative motion cues for fast compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LSTA: Long Short-Term Attention for Egocentric Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video Classification With Channel-Separated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03349</idno>
		<title level="m">Video Modeling with Correlation Networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Trajectory Convolution for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">MiCT: Mixed 3d/2d convolutional tube for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Approximated Bilinear Modules for Temporal Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolutional Network for Online Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
