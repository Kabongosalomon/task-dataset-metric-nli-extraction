<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIANet: Dense-and-Implicit Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzhan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">New Oriental AI Research Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senwei</forename><surname>Liang</surname></persName>
							<email>liangsenwei@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfu</forename><surname>Liang</surname></persName>
							<email>mingfuliang2020@u.northwestern.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DIANet: Dense-and-Implicit Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention networks have successfully boosted the performance in various vision problems. Previous works lay emphasis on designing a new attention module and individually plug them into the networks. Our paper proposes a novel-andsimple framework that shares an attention module throughout different network layers to encourage the integration of layer-wise information and this parameter-sharing module is referred as Dense-and-Implicit-Attention (DIA) unit. Many choices of modules can be used in the DIA unit. Since Long Short Term Memory (LSTM) has a capacity of capturing long-distance dependency, we focus on the case when the DIA unit is the modified LSTM (refer as DIA-LSTM). Experiments on benchmark datasets show that the DIA-LSTM unit is capable of emphasizing layer-wise feature interrelation and leads to significant improvement of image classification accuracy. We further empirically show that the DIA-LSTM has a strong regularization ability on stabilizing the training of deep networks by the experiments with the removal of skip connections or Batch Normalization (Ioffe and Szegedy 2015) in the whole residual network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Attention, a cognitive process that selectively focuses on a small part of information while neglects other perceivable information <ref type="bibr" target="#b0">(Anderson 2005)</ref>, has been used to effectively ease neural networks from learning large information contexts from sentences <ref type="bibr" target="#b12">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b1">Britz et al. 2017;</ref><ref type="bibr" target="#b2">Cheng, Dong, and Lapata 2016)</ref>, images <ref type="bibr" target="#b19">(Xu et al. 2015;</ref><ref type="bibr" target="#b9">Luong, Pham, and Manning 2015)</ref> and videos <ref type="bibr" target="#b9">(Miech, Laptev, and Sivic 2017)</ref>. Especially in computer vision, deep neural networks (DNNs) incorporated with special operators that mimic the attention mechanism can process informative regions in an image efficiently. These operators are modularized and plugged into networks as attention modules <ref type="bibr" target="#b14">Wang et al. 2018;</ref><ref type="bibr" target="#b7">Hu et al. 2018;</ref><ref type="bibr" target="#b1">Cao et al. 2019)</ref>.</p><p>Previous works lay emphasis on designing a new attention module and individually plug them into networks. Generally, the attention module can be divided into three parts: extraction, processing and recalibration. First, the added plugin module extracts internal features of a network which can * Equal contribution be squeezed channel-wise information <ref type="bibr" target="#b8">Li et al. 2019)</ref> or spatial information <ref type="bibr" target="#b14">(Wang et al. 2018;</ref>). Next, the module processes the extraction and generates a mask to measure the importance of the features via fully connected layer , convolution layer <ref type="bibr" target="#b14">(Wang et al. 2018)</ref>. Last, the mask is applied to recalibrate the features. Previous works focus on designing effective ways to process the extracted features. There is one obvious common ground where the attention modules are individually plugged into each layer throughout DNNs <ref type="bibr" target="#b14">Wang et al. 2018)</ref>.</p><p>Our Framework. Differently, we proposes a novel-andsimple framework that shares an attention module throughout different network layers to encourage the integration of layer-wise information and this parameter-sharing module is referred as Dense-and-Implicit-Attention (DIA) unit. The structure and computation flow of a DIA unit is visualized in <ref type="figure">Figure 2</ref>. There are also three parts: extraction ( 1 ), processing ( 2 ) and recalibration ( 3 ) in the DIA unit. The 2 is the main module in the DIA unit to model network attention and is the key innovation of the proposed method where the parameters of the attention module is shared. Characteristics and Advantages. (1) As shown in <ref type="figure">Figure 2</ref>, the DIA unit is placed parallel to the network backbone, and it is shared with all the layers in the same stage (the collection of successive layers with same spatial size, as defined in <ref type="bibr" target="#b4">(He et al. 2016a</ref>)) to improve the interaction of layers at different depth. (2) As the DIA unit is shared, the number of parameter increment from the DIA unit remains roughly constant as the depth of the network increases.</p><p>We show the feasibility of our framework by applying SE <ref type="figure">Figure 2</ref>: DIA units. F ext means the operation for extracting different scales of features. F emp means the operation for emphasizing features.</p><p>module  in DIA unit. SE module, a representative of attention mechanism, is used for each block individually in its original design. In our framework, we share the same SE module (refer as DIA-SE) throughout all layers in the same stage. It is easy to see that DIA-SE has the same computation cost as SE, but in <ref type="table" target="#tab_1">Table 1</ref>  Implicit and Dense Connection. We illustrate how the DIA unit connects all layers in the same stage implicitly and densely. Consider a stage consisting many layers in <ref type="figure" target="#fig_0">Figure 1</ref> (Left). It is an explicit structure with a DIA unit and one layer seems not to connect the other layers except the network backbone. In fact, the different layers use the parameter-sharing attention module and the layer-wise information jointly influences the update of learnable parameters in the module, which causes implicit connections between layers with the help of the shared DIA unit as in <ref type="bibr">Figure 1 (Right)</ref>. Since there is communication between every pair of layers, the connections over all layers are dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIA-LSTM</head><p>The idea of parameter sharing also used in Recurrent Neural Network (RNN) to capture contextual information so we consider apply RNN in our framework to model the layer-wise interrelation. Since Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) is capable of capturing long-distance dependency, we mainly focus on the case when we use LSTM in DIA unit (DIA-LSTM) and the remainder of our paper studies DIA-LSTM. <ref type="figure" target="#fig_1">Figure 3</ref> is the showcase of DIA-LSTM. A global average pooling (GAP) layer (as the 1 in <ref type="figure">Figure 2</ref>) is used to extract global information from current layer. A LSTM module (as the 2 in <ref type="figure">Figure 2</ref>) is used to integrate multi-scale information and there are three inputs passed to the LSTM: the extracted global information from current raw feature map, the hidden state vector h t−1 , and cell state vector c t−1 from previous layers. Then the LSTM outputs the new hidden state vector h t and the new cell state vector c t . The cell state vector c t stores the information from the t th layer and its preceding layers. The new hidden state vector h t (dubbed as attention vector in our work) is then applied back to the raw feature map by channel-wise multiplication (as the 3 in <ref type="figure">Figure</ref> 2) to recalibrate the feature.</p><p>The LSTM in the DIA unit plays a role to bridge the current layer and preceding layers such that the DIA unit can adaptively learn the non-linearity relationship between features in two different dimensions. The first dimension of features is the internal information of the current layer. The second dimension represents the outer information, regarded as layer-wise information, from the preceding layers. The non-linearity relationship between these two dimensions will benefit attention modeling for the current layer. The multiple dimension modeling enables DIA-LSTM to have regularization effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contribution</head><p>We summary our contribution as followed, 1. We proposes a novel-and-simple framework that shares an attention module throughout different network layers to encourage the integration of layer-wise information.</p><p>2. We propose incorporating LSTM in DIA unit (DIA-LSTM) and show the effectiveness of DIA-LSTM for image classification by conducting experiments on benchmark datasets and popular networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Attention Mechanism in Computer Vision. <ref type="bibr" target="#b9">(Mnih et al. 2014;</ref><ref type="bibr" target="#b21">Zhao et al. 2017</ref>) use attention mechanism in image classification via utilizing a recurrent neural network to select and process local regions at high resolution sequentially. Concurrent attention-based methods tend to construct operation modules to capture non-local information in an image <ref type="bibr" target="#b14">(Wang et al. 2018;</ref><ref type="bibr" target="#b1">Cao et al. 2019)</ref>, and model the interrelationship between channel-wise features <ref type="bibr" target="#b7">Hu et al. 2018)</ref>. The combination of multilevel attentions are also widely studied . Prior works <ref type="bibr" target="#b14">(Wang et al. 2018;</ref><ref type="bibr" target="#b1">Cao et al. 2019;</ref><ref type="bibr" target="#b7">Hu, Shen, and Sun 2018;</ref><ref type="bibr" target="#b7">Hu et al. 2018;</ref>) usually insert an attention module in each layer individually. In this work, the DIA unit is innovatively shared for all the layers in the same stage of the network, and the existing attention modules can be composited into the DIA unit readily. Besides, we adopt a global average pooling in part 1 to extract global information and a channelwise multiplication in part 3 to recalibrate features, which is similar to SENet  Multi-Dimension Feature Integration. <ref type="bibr" target="#b16">(Wolf and Bileschi 2006)</ref> experimentally analyzes that even the simple aggregation of low-level visual features sampled from wide inception field can be efficient and robust for context representation, which inspires <ref type="bibr" target="#b6">Hu et al. 2018)</ref> to incorporate multi-level features to improve the network representation. <ref type="bibr" target="#b9">(Li, Ouyang, and Wang 2016)</ref> also demonstrates that by biasing the feature response in each convolutional layers using different activation functions, the deeper layer could achieve the better capacity of capturing the abstract pattern in DNN. In DIA unit, the high nonlinearity relationship between multi-dimension features are learned and integrated via the LSTM module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense-and-Implicit Attention Network</head><p>In this section, we will formally introduce the DIA-LSTM unit. We use the modified LSTM module in the DIA unit. Afterwards, a DIANet is referred to a network built with DIA-LSTM units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formulation of DIA-LSTM unit</head><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref> when a DIA-LSTM unit is built with a residual network <ref type="bibr" target="#b4">(He et al. 2016a)</ref>, the input of the t th layer is x t ∈ R W ×H×N , where W, H and N mean width, height and the number of channels, respectively. f (·; θ (t) 1 ) is the residual mapping at the t th layer with parameters θ</p><formula xml:id="formula_0">(t) 1 as introduced in (He et al. 2016a). Let a t = f (x t ; θ (t) 1 ) ∈ R W ×H×N .</formula><p>Next, a global average pooling de-noted as GAP(·) is applied to a t to extract global information from features in the current layer. Then GAP(a t ) ∈ R N is passed to LSTM along with a hidden state vector h t−1 and a cell state vector c t−1 ( h 0 and c 0 are initialized as zero vectors). The LSTM finally generates a current hidden state vector h t ∈ R N and a cell state vector c t ∈ R N as</p><formula xml:id="formula_1">(h t , c t ) = LSTM(GAP(a t ), h t−1 , c t−1 ; θ 2 ).</formula><p>(1) In our model, the hidden state vector h t is regarded as attention vector to adaptively recalibrate feature maps. We apply channel-wise multiplication ⊗ to enhance the importance of features, i.e., a t ⊗ h t and obtain x t+1 after skip connection, i.e., x t+1 = x t + a t ⊗ h t . <ref type="table" target="#tab_3">Table 2</ref> shows the formulation of ResNet, SENet, and DIANet, and Part (b) is the main difference between them. The LSTM module is used repeatedly and shared with different layers in parallel to the network backbone. Therefore the number of parameters θ 2 in a LSTM does not depend on the number of layers in the backbone, e.g., t. SENet utilizes a attention-module consisted of fully connected layers to model the channel-wise dependency for each layer individually . The total number of parameters brought by the addin modules depends on the number of layers in the backbone and increases with the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modified LSTM Module</head><p>Now we introduce the modified LSTM module used in <ref type="figure">Figure</ref> 3. The design of attention module usually requires the value of the attention vector in range [0, 1] and also requires small parameter increment. We conducts some modifications in LSTM module used in DIA-LSTM. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, compared to the standard LSTM (Hochreiter and Schmidhuber 1997) module, there are two modifications in our purposed LSTM: 1) a shared linear transformation to reduce input dimension of LSTM; 2) a careful selected activation function for better performance.</p><p>(1) Parameter Reduction. A standard LSTM consists of four linear transformation layers as shown in <ref type="figure" target="#fig_3">Figure 4</ref> (Left). Since y t , h t−1 and h t are of the same dimension N , the standard LSTM may cause 8N 2 parameter increment as shown in Appendix. When the number of channels is large, e.g., N = 2 10 , the parameter increment of added-in LSTM module in the DIA unit will be over 8 million, which can hardly be tolerated.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (Top), h t−1 and y t are passed to four linear transformation layers with the same input and ResNet SENet DIANet (ours) output dimension N . In the DIA-LSTM, a linear transformation layer (denoted as "Linear1" in <ref type="figure" target="#fig_3">Figure 4 (Bottom)</ref>) with a smaller output dimension are applied to h t−1 and y t . We use reduction ratio r in the Linear1. Specifically, we reduce the dimension of the input from 1×1×N to 1×1×N/r and then apply the ReLU activation function to increase nonlinearity in this module. The dimension of the output from ReLU function are changed back to 1 × 1 × N when the output is passed to those four linear transformation functions. This modification can enhance the relationship between the inputs for different parts in DIA-LSTM and also effectively reduce the number of parameters by sharing a linear transformation for dimension reduction. The number of parameter increment reduces from 8N 2 to 10N 2 /r as shown in the Appendix, and we find that when we choose an appropriate reduction ratio r, we can make a better trade-off between parameter reduction and the performance of DIANet. Further experimental results will be discussed in the ablation study later.</p><formula xml:id="formula_2">(a) at = f (xt; θ (t) 1 ) at = f (xt; θ (t) 1 ) at = f (xt; θ (t) 1 ) (b) - ht = FC(GAP(at); θ (t) 2 ) (ht, ct) = LSTM(GAP(at), ht−1, ct−1; θ2) (c) xt+1 = xt + at xt+1 = xt + at ⊗ ht xt+1 = xt + at ⊗ ht</formula><p>(2) Activation Function. Sigmoid function (σ(z) = 1/(1 + e −z )) is used in many attention-based methods like SENet , CBAM  to generate attention maps as a gate mechanism. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (Bottom), we change the activation function of the output layer from Tanh to Sigmoid. Further discussion will be presented in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we evaluate the performance of the DIA-LSTM unit in image classification task and empirically demonstrate its effectiveness. We conduct experiments on popular networks for benchmark datasets. Since SENet  is also a channel-specific attention model, we compare DIANet with SENet. For a fair comparison, we adjust the reduction ratio such that the number of parameters of DIANet is similar to that of SENet. Image Classification. As shown in <ref type="table">Table 3</ref>, DIANet improves the testing accuracy significantly over the original networks and consistently comparing with SENet for different datasets. In particular, the performance improvement of the ResNet with the DIA unit is most remarkable. Due to the popularity of ResNet, the DIA unit may be applied in other computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we conduct ablation experiments to explore how to better embed DIA-LSTM units in different neural network structures and gain a deeper understanding of the role of components in the unit. All experiments are performed on CIFAR100 with ResNet. For simplicity, DI-ANet164 is denoted as a 164-layer ResNet built with DIA-LSTM units. Reduction ratio. The reduction ratio is the only hyperparameter in DIANet. The main advantage of our model is  <ref type="table">Table 3</ref>: Testing accuracy (%) on CIFAR10, CIFAR100 and ImageNet 2012. "#P(M)" means the number of parameters (million). The rightmost "r" indicates the reduction ratio of DIANet.</p><p>to improve the generalization ability with a light parameter increment. The smaller reduction ratio causes a higher parameter increment and model complexity. This part investigates the trade-off between the model complexity and performance. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the number of parameters of the DIANets decreases with the increasing reduction ratio, but the testing accuracy declines slightly, which suggests that the model performance is not sensitive to the reduction ratio.   <ref type="table" target="#tab_8">Table 5</ref> show that as the depth increases from 83 to 407 layers, the DIANet with a smaller number of parameters can achieve higher classification accuracy than the SENet. Moreover, the DIANet83 can achieve a similar performance as the SENet245, and DIANet164 outperforms all the SENets with at least 1.13% and at most 58.8% parameter reduction. They imply that the DIANet is of higher parameter efficiency than SENet. The results also suggest that: for DIANet, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the DIA-LSTM unit will pass more layers recurrently with a deeper depth. The DIA-LSTM can handle the interrelationship between the information of different layers in much deeper DNN and figure out the long-distance dependency between layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>SENet  Activation function and the number of stacking LSTM cells. We choose different activation functions in the output layer of LSTM in <ref type="figure" target="#fig_3">Figure 4</ref> (Bottom) and different numbers of stacking LSTM cells to explore the effects of these two factors. In <ref type="table">Table 6</ref>, we find that the performance has been significantly improved after replacing tanh in the standard LSTM with sigmoid. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (Bottom), this activation function is located in the output layer and directly changes the effect of memory unit c t on the output of the output gate. In fact, the sigmoid function is used in many attention-based methods like SENet as a gate mechanism. The test accuracy of different choices of LSTM activation functions in <ref type="table">Table 6</ref> shows that sigmoid better helps LSTM as a gate to rescale channel features. <ref type="table" target="#tab_1">Table 12</ref> in the SENet paper  shows the performance of different activation functions like: sigmoid &gt; tanh &gt; ReLU (bigger is better), which coincides to our reported results.</p><p>Figure 5: Visualization of feature integration for each stage by random forest. Each row presents the importance of source layers h n , 1 ≤ n &lt; t contributing to the target layer h t .</p><p>When we use sigmoid in the output layer of LSTM, the increasing number of stacking LSTM cells does not necessarily lead to performance improvement but may lead to performance degradation. However, when we choose tanh, the situation is different. It suggest that, through the stacking of LSTM cells, the scale of the information flow among them is changed, which may effect the performance. #P(M) Activation #LSTM cells top1-acc.  <ref type="table">Table 6</ref>: Test accuracy (%) with DIANet164 of different activation function at the output layer in the modified LSTM and different number of stacking LSTM cells on CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In this section, we study some properties of DIANet, including feature integration and regularization effect on stabilizing training. Firstly, the layers are connected by DIA-LSTM unit in DIANet and we can use the random forest model <ref type="bibr" target="#b3">(Gregorutti, Michel, and Saint-Pierre 2017)</ref> to visualize how the current layer depends on the preceding layers. Secondly, we study the stabilizing training effect of DIANet by removing all the Batch Normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy 2015)</ref> or the skip connection in the residual networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Integration</head><p>Here we try to understand the dense connection from the numerical perspective. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> and 1, the DIA-LSTM bridges the connections between layers by propagating the information forward through h t and c t . Moreover, h t at different layers are also integrating with h t , 1 ≤ t &lt; t in DIA-LSTM. Notably, h t is applied directly to the features in the network at each layer t. Therefore the relationship between h t at different layers somehow reflects connection degree of different layers. We explore the nonlinear relationship between the hidden state h t of DIA-LSTM and the preceding hidden state h t−1 , h t−2 , ..., h 1 , and visualize how the information coming from h t−1 , h t−2 , ..., h 1 contributes to h t . To reveal this relationship, we consider using the random forest to visualize variable importance. The random forest can return the contributions of input variables to the output separately in the form of importance measure, e.g., Gini importance <ref type="bibr" target="#b3">(Gregorutti, Michel, and Saint-Pierre 2017)</ref>. The computation details of Gini importance can be referred to the Appendix. Take h n , 1 ≤ n &lt; t as input variables and h t as output variable, we can get the Gini importance of each variable h n , 1 ≤ n &lt; t. ResNet164 contains three stages, and each stage consists of 18 layers. We conduct three Gini importance computation to each stage separately. As shown in <ref type="figure">Figure 5</ref>, each row presents the importance of source layers h n , 1 ≤ n &lt; t contributing to the target layer h t . In each sub-graph of <ref type="figure">Figure 5</ref>, the diversity of variable importance distribution indicates that the current layer uses the information of the preceding layers. The interaction between shallow and deep layers in the same stage reveals the effect of implicitly dense connection. In particular, taking h 17 in stage 1 (the last row) as an example, h 16 or h 15 does not intuitively provide the most information for h 17 , but h 5 does. We conclude that the DIA unit can adaptively integrate information between multiple layers.  over, in <ref type="figure" target="#fig_1">Figure 5 (stage 3)</ref>, the information interaction with previous layers in stage 3 is more intense and frequent than that of the first two stages. Correspondingly, as shown in Table 7, in the experiments when we remove the DIA-LSTM unit in stage 3, the classification accuracy decreases from 76.67 to 75.40. However, when it in stage 1 or 2 is removed, the performance degradation is very similar, falling to 76.27 and 76.25 respectively. Also note that for DIANet, the number of parameter increment in stage 2 is larger than that of stage 1. It implies that the significant performance degradation after the removal of stage 3 may be not only due to the reduction of the number of parameters but due to the lack of dense feature integration.  <ref type="table">Table 8</ref>: Testing accuracy (%). We train models of different depth without BN on CIFAR-100. "nan" indicates the numerical explosion.   <ref type="table">Table 8</ref>, at the same depth, SENet has larger number of parameters than DIANet but still comes to numerical explosion without BN, which means that the number of parameter is not the case for stabilization of training but sharing mechanism we proposed may be the case. Besides, comparing with <ref type="table" target="#tab_8">Table 5</ref>, the testing accuracy of DIANet without BN still can keep up to 70%. The scaling learned by DIANet integrates the information from preceding layers and enables the network to choose a better scaling for feature maps of current layer.</p><p>Removal of skip connection. The skip connection has become a necessary structure for training DNNs <ref type="bibr" target="#b5">(He et al. 2016b)</ref>. Without skip connection, the DNN is hard to train due to the reasons like the gradient vanishing <ref type="bibr" target="#b0">(Bengio et al. 1994;</ref><ref type="bibr" target="#b2">Glorot and Bengio 2010;</ref><ref type="bibr" target="#b11">Srivastava, Greff, and Schmidhuber 2015a)</ref>. We conduct the experiment where all the skip connections are removed in ResNet56 and count the absolute value of gradient at the output tensor of each stage. As shown in <ref type="figure" target="#fig_4">Figure 6</ref> which presents the gradient distribution with all skip connection removal, DIANet (blue) obviously enlarges the mean and variance of the gradient distri-  bution, which enables larger absolute value and diversity of gradient and relieves gradient degradation to some extent. Without data augment. Explicit dense connections may help bring more efficient usage of parameters, which makes the neural network less prone to overfit <ref type="bibr" target="#b7">(Huang et al. 2017)</ref>.</p><p>Although the dense connections in DIA-LSTM are implicit, the DIANet still shows the ability to reduce overfitting. To verify it, We train the models without data augment to reduce the influence of regularization from data augment. As shown in <ref type="table" target="#tab_15">Table 9</ref>, DIANet achieves lower testing error than ResNet164 and SENet. To some extent, the implicit and dense structure of DIANet may have regularization effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposes a novel-and-simple framework that shares an attention module throughout different network layers to encourage the integration of layer-wise information. The parameter-sharing module is called Dense-and-Implicit Attention (DIA) unit. We propose incorporating LSTM in DIA unit (DIA-LSTM) and show the effectiveness of DIA-LSTM for image classification by conducting experiments on benchmark datasets and popular networks. We further empirically show that the DIA-LSTM has a strong regularization ability on stabilizing the training of deep networks by the experiments with the removal of skip connections or Batch Normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy 2015)</ref> in the whole residual network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: explicit structure of DIANet. Right: implicit connection of DIA unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The showcase of DIA-LSTM. In the LSTM cell, c t is the cell state vector and h t is the hidden state vector. GAP means global average pool over channels and ⊗ means channel-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Dataset and Model. We conduct experiments on CI-FAR10, CIFAR100 (Krizhevsky and Hinton 2009), and Im-ageNet 2012 (Russakovsky et al. 2015) using ResNet (He et al. 2016a), PreResNet (He et al. 2016b), WRN (Zagoruyko and Komodakis 2016) and ResNeXt (Xie et al. 2017). CIAFR10 or CIFAR100 has 50k train images and 10k test images of size 32 by 32, but has 10 and 100 classes respectively. ImageNet 2012 (Russakovsky et al. 2015) comprises 1.28 million training and 50k validation images from 1000 classes, and the random cropping of size 224 by 224 is used in our experiments. The details can be found in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Top. The standard LSTM cell. Bottom. The modified LSTM cell in DIA-LSTM unit. We highlight the modified component in the modified LSTM. "σ" means the sigmoid activation. "Linear" means the linear transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The distribution of gradient in each stage of ResNet56 without all the skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Testing accuracy (mean ± std%) on CIFAR100 and ResNet164 with different attention modules. "Org" means the original backbone of ResNet164. #P (M) means the num- ber of parameters (million).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Formulation for the structure of ResNet, SENet, and DIANet. f is the convolution layer. FC means fully connected layer and GAP indicates global average pooling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>In the case of r = 16, the DIANet164 has 0.05M parameter increment compared to the original ResNet164 but the testing accuracy of the DIANet164 is 76.50% while that of the ResNet164 is 73.43%.</figDesc><table><row><cell>Ratio r</cell><cell>#P(M)</cell><cell>top1-acc.</cell></row><row><cell>1</cell><cell>2.59 (+0.86)</cell><cell>76.88</cell></row><row><cell>4</cell><cell>1.95 (+0.22)</cell><cell>76.67</cell></row><row><cell>8</cell><cell>1.84 (+0.11)</cell><cell>76.42</cell></row><row><cell>16</cell><cell>1.78 (+0.05)</cell><cell>76.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) with different reduction ratio on CIFAR100 with ResNet164.</figDesc><table><row><cell>The depth of the neural network. Generally, in practice,</cell></row><row><cell>DNNs with a larger number of parameters do not guarantee</cell></row><row><cell>sufficient performance improvement. Deeper networks may</cell></row><row><cell>contain extreme feature and parameter redundancy (Huang</cell></row><row><cell>et al. 2017). Therefore, designing a new structure of deep</cell></row><row><cell>neural networks (He et al. 2016a; Huang et al. 2017; Sri-</cell></row><row><cell>vastava, Greff, and Schmidhuber 2015a; Hu, Shen, and Sun</cell></row><row><cell>2018; Hu et al. 2018; Wang et al. 2018) is necessary. Since</cell></row><row><cell>DIA units change the topology of DNN backbones, evalu-</cell></row><row><cell>ating the effectiveness of DIANet structure is of great im-</cell></row><row><cell>portance. Here we investigate how the depth of DNNs influ-</cell></row><row><cell>ences DIANets in two aspects: (1) the performance of DI-</cell></row><row><cell>ANets compared to SENets of various depth; (2) the param-</cell></row><row><cell>eter increment of DIANets.</cell></row><row><cell>The results in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Test accuracy (%) with ResNet of different depth</cell></row><row><cell>on CIFAR100.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The test accuracy (%) of DIANet164 with the removal of DIA-LSTM unit in different stage.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>top1-acc. #P(M) top1-acc. #P(M) top1-acc.</figDesc><table><row><cell></cell><cell cols="2">original</cell><cell>SENet</cell><cell></cell><cell cols="2">DIANet(r = 16)</cell></row><row><cell cols="2">#P(M) ResNet83 0.88</cell><cell>nan</cell><cell>0.98</cell><cell>nan</cell><cell>0.94</cell><cell>70.58</cell></row><row><cell>ResNet164</cell><cell>1.70</cell><cell>nan</cell><cell>1.91</cell><cell>nan</cell><cell>1.76</cell><cell>72.36</cell></row><row><cell>ResNet245</cell><cell>2.53</cell><cell>nan</cell><cell>2.83</cell><cell>nan</cell><cell>2.58</cell><cell>72.35</cell></row><row><cell>ResNet326</cell><cell>3.35</cell><cell>nan</cell><cell>3.75</cell><cell>nan</cell><cell>3.41</cell><cell>nan</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>The effect on stabilizing trainingRemoval of Batch Normalization. Small changes in shallower hidden layers may be amplified as the information propagates within the deep architecture and sometimes result in a numerical explosion. Batch Normalization (BN)<ref type="bibr" target="#b8">(Ioffe and Szegedy 2015)</ref> is widely used in the deep networks since it stabilizes the training by normalization the input of each layer. DIA-LSTM unit recalibrates the feature maps by channel-wise multiplication, which plays a role of scaling similar to BN. As shown inTable 8, different models trained with varying depth in CIFAR100 and BNs are removed in these networks. The experiments are conducted on a single GPU with batch size 128 and initial learning rate 0.1. Both the original ResNet, SENet face problem of numerical explosion without BN while the DIANet can be trained with depth up to 245. In</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy (%) of the models without data augment with ResNet164.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>S. Liang and H. Yang gratefully acknowledge the support of National Supercomputing Center (NSCC) SINGAPORE and High Performance Computing (HPC) of National University of Singapore for providing computational resources, and the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. Sincerely thank Xin Wang from Tsinghua University for providing personal computing resource. H. Yang thanks the support of the start-up grant by the Department of Mathematics at the National University of Singaporet, the Ministry of Education in Singapore for the grant MOE2018-T2-2-147.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introdcution of Implementation detail</head><p>The hyper-parameter settings of CIFAR and ImageNet are shown in <ref type="table">Table 10</ref> and <ref type="table">Table 11</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gini importance</head><p>We present the algorithm of computing the Gini importance used in our paper in Algorithm 1.</p><p>Algorithm 1 Calculate features integration by Gini importance from Random Forest Input: H: composed of h 1 ,h 2 ,...,h t from stage i;</p><p>#b z denotes the batch size of h t #c z denotes the number of the feature maps' channel in current stage #f z denotes the number of layers in current stage Output: The hotmap G about the features integration for stage i; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameter of LSTM</head><p>Suppose the input yt is of size N and the hidden state vector ht−1 is also of size N .</p><p>Standard LSTM As shown in <ref type="figure">Figure (</ref>3) (Left), in the standard LSTM, there requires 4 linear transformation to control the information flow with input yt and ht−1 respectively. The output size is set to be N . To simplify the calculation, the bias is omitted. Therefore, for the yt, the number parameters of 4 linear transformation is equal to 4 × n × n. Similarly, the number parameters of 4 linear transformation with input ht−1 is equal to 4 × n × n. The total of parameters equals to 8n 2 .</p><p>DIA-LSTM As shown in <ref type="figure">Figure (</ref>3) (Right), there is a linear transformation to reduce the dimension at the beginning. The dimension of input yt will reduce from N to N/r after the first linear transformation. The number of parameters for the linear transformation is equal to n × n/r. Then the output will be passed into 4 linear transformation same as the standard LSTM. the number parameters of 4 linear transformation is equal to 4 × n/r × n. Therefore, for input yt and reduction ratio r, the number of parameters is equal to 5n 2 /r. Similarly, the number of parameters with input ht−1 is the same as that concerning yt. The total of parameters equals to 10n 2 /r.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Cognitive psychology and its implications. Macmillan</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Britz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<idno>arXiv:1904.11492</idno>
	</analytic>
	<monogr>
		<title level="m">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cao et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Long short-term memory-networks for machine reading</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlation and variable importance in random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gregorutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saint-Pierre ;</forename><surname>Gregorutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saint-Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="659" to="678" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Selective kernel networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-bias non-linear activation in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouyang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan ;</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pham</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<idno>arXiv:1706.06905</idno>
	</analytic>
	<monogr>
		<title level="m">Effective approaches to attention-based neural machine translation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06514</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bam: Bottleneck attention module</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greff</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greff</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards universal object detection by domain attention</title>
		<idno>abs/1904.04402</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A critical view of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="261" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Zagoruyko and Komodakis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
