<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>yangl@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
							<email>albanie@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LIU, ALBANIE, NAGRANI, ZISSERMAN: COLLABORATIVE EXPERTS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets 'in the wild' vary a lot in terms of degree of specificity, with some queries describing 'specific details' such as the names of famous identities, content from speech, or text available on the screen. Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended.</p><p>For this we exploit existing knowledge in the form of pre-trained semantic embeddings which include 'general' features such as motion, appearance, and scene features from visual content. We also explore the use of more 'specific' cues from ASR and OCR which are intermittently available for videos and find that these signals remain challenging to use effectively for retrieval. We propose a collaborative experts model to aggregate information from these different pre-trained experts and assess our approach empirically on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data can be found at www. robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper contains a correction to results reported in the previous version.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Videos capture the world in two important ways beyond a simple image: first, video contains temporal information -semantic concepts, actions and interactions evolve over time; Second, video may also contain information from multiple modalities, such as an accompanying audio track. This makes videos both richer and more informative, but also more challenging to represent. Our goal in this paper is to embed the information from multiple modalities and multiple time steps of a video segment into a compact fixed-length representation. Such a compact representation can then be used for a number of video understanding tasks, such as video retrieval, clustering and c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. Details of the results correction can found in the appendix. *Equal contribution. summarisation. In particular, we focus on retrieval; our objective is to be able to retrieve video clips using a free form text query that may contain both general and specific information. Learning a robust and compact representation tabula rasa for this task is made extremely challenging by the high dimensionality of the sensory data contained in videos-to do so with discriminative training would require prohibitively expensive textual annotation of a vast number of videos. The primary hypothesis underpinning our approach is the following: the discriminative content of the multi-modal video embedding can be well approximated by the set of semantic representations of the video data learnt by individual experts (in audio, scenes, actions, etc). In essence, this approximation enables us to exploit knowledge from existing individual sources where the cost of annotation is significantly reduced (e.g. classification labels for objects and scenes in images, labels for actions in videos etc.) and where consequently, there exist very large-scale labelled datasets. These large-scale datasets can then be used to train independent "experts" for different perception tasks, which in turn provide a robust, low-dimensional basis for the discriminative query-content approximation described above.</p><p>The two key aspects of this idea that we explore in this paper are: (i) General and specific features: in addition to using generic video descriptors (e.g. objects and actions) we investigate encodings of quite specific information from the clip, for example, text from overlaid captions and text from speech to provide effective coverage of the "queryable content" of the video <ref type="figure" target="#fig_0">(Fig. 1,  left)</ref>. While such features may be highly discriminative for humans, they may not always be available <ref type="figure" target="#fig_0">(Fig. 1, right)</ref> and as we show through experiments (Sec. 4.3), making good use of these cues is challenging. We therefore also propose (ii) Collaborative experts: a framework that seeks to make effective use of embeddings from different 'experts' (e.g. objects, actions, speech) by learning their combination in order to render them more discriminative. Each expert is filtered via a simple dynamic attention mechanism that considers its relation to all other experts to enable their collaboration. This pairwise approach enables, for instance, the sound of a dog barking to inform the modulation of the RGB features, selecting the features that have encoded the concept of the dog. As we demonstrate in the sequel, this idea yields improvements in the retrieval performance.</p><p>Concretely, we make the following three contributions: (i) We propose the Collaborative Experts framework for learning a joint embedding of video and text by combining a collection of pretrained embeddings into a single, compact video representation. Our joint video embeddings are independent of the retrieval text-query and can be pre-computed offline and indexed for efficient retrieval; (ii) We explore the use of both general video features such as motion, image classification and audio features, and specific video features such as text embedded on screen and speech obtained using OCR and ASR respectively. We find that strong generic features deliver good performance, but that specific, rarely available features remain challenging to use for retrieval 1 . (iii) We assess the performance of the representation produced by combining all available cues on a number of retrieval benchmarks, in several cases achieving an advance over prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cross-Modal Embeddings: A range of prior work has proposed to jointly embed images and text into the same space <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>, enabling cross-modal retrieval. More recently, several works have also focused on audio-visual cross-modal embeddings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref>, as well as audio-text embeddings <ref type="bibr" target="#b6">[7]</ref>. Our goal in this work, however, is to embed videos and natural language sentences (sometimes multiple sentences) into the same semantic space, which is made more challenging by the high dimensional content of videos. Video-Text Embeddings: While a large number of works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b61">62]</ref> have focused on learning visual semantic embeddings for video and language, many of these existing approaches are based on image-text embedding methods by design and typically focus on single visual frames. Mithun el al. <ref type="bibr" target="#b41">[42]</ref> observe that a simple adaptation of a state-of-the-art image-text embedding method <ref type="bibr" target="#b15">[16]</ref> by mean-pooling features from video frames provides a better result than many prior video-text retrieval approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>. However, such methods do not take advantage of the rich and varied additional information present in videos, including motion dynamics, speech and other background sounds, which may influence the concepts in human captions to a considerable extent. Consequently, there has been a growing interest in fusing information from other modalities- <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref> utilise the audio stream (but do not exploit speech content) and use models pretrained for action recognition to extract motion features. These methods do not make use of speech-to-text or OCR for additional cues, which have nevertheless been used successfully to understand videos in other domains, particularly lecture retrieval <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b62">63]</ref> (where the videos consist of slide shows) and news broadcast <ref type="bibr" target="#b20">[21]</ref> retrieval, where a large fraction of the content is displayed on screen in the form of text. Our approach draws particular inspiration from the powerful joint embedding proposed by <ref type="bibr" target="#b38">[39]</ref> (which in turn, builds on the classical Mixtures-of-Experts model <ref type="bibr" target="#b27">[28]</ref>) and extends it to investigate additional cues (such as speech and text) and make more effective use of pretrained features via the robust collaborative gating mechanism described in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation scarcity:</head><p>A key challenge for video-retrieval is the small size of existing training datasets, due to the high cost of annotating videos with natural language. We therefore propose to use the knowledge from existing embeddings pretrained on a wide variety of other tasks. This idea is not new: semantic projections of visual inputs in the form of 'experts' was used by <ref type="bibr" target="#b14">[15]</ref> for the task of image retrieval and has also been central to modern video retrieval methods such as <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref>. More recently, alternative approaches to addressing the issue of annotation scarcity have been explored, which include self-supervised <ref type="bibr" target="#b53">[54]</ref> and weakly-supervised [70] video-text models. The proposed Collaborative Experts framework for learning a joint video-text embedding (coloured boxes denote learnable parameters). The information provided by each pretrained "expert" (potentially with multiple experts from a single domain) is temporally aggregated as it enters the video encoder and then refined through the use of a collaborative gating mechanism (right) to obtain the video-embedding (for visual clarity, we show the interaction of just a single expert with three others, though in practice all experts are used-see Sec. 3.1 for details). Note that to maintain retrieval efficiency, collaboration occurs only between video experts (the text-query and video embeddings are computed independently).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collaborative Experts</head><p>Given a set of videos with corresponding text captions, we would like to create a pair of functions φ v and φ t that map sensory video data and text into a joint embedding space that respects this correspondence-embeddings for paired text and video should lie close together, while embeddings for text and video that do not match should lie far apart. We would also like φ v and φ t to be independent of each other to enable efficient retrieval: the process of querying then reduces to a distance comparison between the embedding of the query and the embeddings of the collection to be searched (which can be pre-computed offline). The proposed Collaborative Experts framework for learning these functions is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this work, we pay particular attention to the design of the video encoder φ v and the process of combining information from different video modalities (Sec. 3.1). To complete the framework, we then discuss how the query text is encoded and the ranking loss used to learn the joint embedding space (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Encoder</head><p>To construct the video encoder φ v , we draw on a collection of pretrained, single-modality experts. These operate on the video sensory data v and project it to a collection of n variable-length task-specific embeddings {Ψ</p><formula xml:id="formula_0">(1) var (v), ... , Ψ (n) var (v)}. Here Ψ (i)</formula><p>var represents the i th expert (we use the "var" subscript to denote a variable-length output when applied to a sequence of frames) whose parameters have been learned on a prior task such as object classification and then frozen. Each element of this collection is then aggregated along its temporal dimension to produce fixed-size, task-specific embeddings per video {Ψ <ref type="bibr" target="#b0">(1)</ref> (v),...,Ψ (n) (v)}. Any temporal aggregation function may be used here-in this work, we use simple average pooling to aggregate "slow" visual features such as objects and scenes, and NetVLAD <ref type="bibr" target="#b2">[3]</ref> to aggregate more dynamic audio and word features (see Sec. 4.1 for further details). Next, to enable their combination, we apply linear projections to transform these task-specific embeddings to a common dimensionality. Our goal when fusing the resulting representations together into a single condensed video representation is to capture the valuable complementary information between task-specific projections while simultaneously filtering out irrelevant noise and resolving individual expert conflicts on a per-sample basis. To do so, this we propose a collaborative gating module, described next.</p><p>Collaborative Gating: The collaborative gating module comprises two operations: (1) Prediction of attention vectors for every expert projection T = {T (1) (v),...,T (n) (v)}; and (2) modulation of expert responses. Inspired by the relational reasoning module proposed by <ref type="bibr" target="#b50">[51]</ref> for visual question answering, we define the attention vector of the i th expert projection T i as follows:</p><formula xml:id="formula_1">T (i) (v)=h φ ( ∑ j =i g θ (Ψ (i) (v),Ψ ( j) (v))),<label>(1)</label></formula><p>where functions h φ and g θ are used to model the pairwise relationship between projection Ψ (i) and projection Ψ ( j) . Of these, g θ is used to infer pairwise task relationships, while h φ maps the sum of all pairwise relationships into a single attention vector. In this work, we instantiate both h φ and g θ as multi-layer perceptrons (MLPs). Note that the functional form of Equation <ref type="formula" target="#formula_1">(1)</ref> dictates that the attention vector of any expert projection should consider the potential relationships between all pairs associated with this expert. That is to say, the quality of each expert Ψ ( j) should contribute in determining and selecting the information content from Ψ (i) in the final decision. It is also worth noting that the collaborative gating module uses the same functions g θ and h φ (shared weights) to compute all pairwise relationships. This mode of operation encourages greater generalisation, since g θ and h φ are encouraged not to over-fit to features of any particular pair of tasks. After the attention vectors T ={T <ref type="bibr" target="#b0">(1)</ref> (v),...,T (n) (v)} have been computed, each expert projection is modulated follows:</p><formula xml:id="formula_2">Ψ (i) (v)=Ψ (i) (v)•σ(T (i) (v)),<label>(2)</label></formula><p>where σ is an element-wise sigmoid activation and • is the element-wise multiplication (Hadamard product). This gating function re-calibrates the strength of different activations of Ψ (i) (v) and selects which information is highlighted or suppressed, providing the model with a powerful mechanism for dynamically filtering content from different experts. A diagram of the mechanism is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (right). The final video embedding is then obtained by passing the modulated responses of each expert through a Gated Embedding Module (GEM) <ref type="bibr" target="#b38">[39]</ref> (note that this operation produces l2-normalized outputs) before concatenating the outputs together into a single fixed-length vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Query Encoder and Training Loss</head><p>To construct the text embeddings, query sentences are first mapped to a sequence of feature vectors with pretrained contextual word-level embeddings (see Sec. 4.1 for details)-as with the video experts, the parameters of this first stage are frozen. These are then aggregated, again using NetVLAD <ref type="bibr" target="#b2">[3]</ref>. Following aggregation, we follow the text encoding architecture proposed by <ref type="bibr" target="#b38">[39]</ref>, which projects the aggregated features to separate subspaces for each expert using GEMs (as with the video encoder, producing l2-normalized outputs). Each projection is then scaled by a mixture weight (one scalar weight per expert projection), which is computed by applying a single linear layer to the aggregated text-features, and passing the result through a softmax to ensure that the mixture weights sum to one (see <ref type="bibr" target="#b38">[39]</ref> for further details). Finally, the scaled outputs are concatenated, producing a vector of dimensionality that matches that of the video embedding.</p><p>With the video encoder φ v and text encoder φ t as described, the similarity s j i of the i th video, v i , and the and j th caption, t j , can then be directly computed as the cosine of the angle between their</p><formula xml:id="formula_3">respective embeddings φ v (v i ) T φ t (t j )</formula><p>. During optimisation, the parameters of the video encoder (including the collaborative gating module) and text query encoder (the coloured regions of <ref type="figure" target="#fig_1">Fig. 2</ref>) are learned jointly. Training proceeds by sampling a sequence of minibatches of corresponding video-text pairs {v i ,t i } N B i=1 and minimising a Bidirectional Max-margin Ranking Loss <ref type="bibr" target="#b52">[53]</ref>:</p><formula xml:id="formula_4">L r = 1 N B N B ∑ i=1, j =i max(0,m+s j i −s i i )+max(0,m+s i j −s i i )<label>(3)</label></formula><p>where N B is the batch size, and m is a fixed constant which is set as a hyperparameter. When assessing retrieval performance, at test time the embedding distances are simply computed via their inner product, as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Missing Experts</head><p>When a set of expert features are missing, such as when there is no speech in the audio track, we simply zero-pad the missing experts when estimating the similarity score. To compensate for the implicit scaling introduced by missing experts (the similarity is effectively computed between shorter embeddings), we follow the elegant approach proposed by <ref type="bibr" target="#b38">[39]</ref> and simply remove the mixture weights for missing experts, then renormalise the remaining weights such that they sum to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our model on five benchmarks for video retrieval tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets, Implementation Details and Metrics</head><p>Datasets:</p><p>We perform experiments on five video datasets: MSR-VTT <ref type="bibr" target="#b60">[61]</ref>, LSMDC <ref type="bibr" target="#b49">[50]</ref>, MSVD <ref type="bibr" target="#b7">[8]</ref>, DiDeMo <ref type="bibr" target="#b0">[1]</ref> and ActivityNet-captions <ref type="bibr" target="#b31">[32]</ref>, covering a challenging set of domains which include videos from YouTube, personal collections and movies. Expert Features: In order to capture the rich content of a video, we draw on existing powerful representations for a number of different semantic tasks. These are first extracted at a frame-level, then aggregated to produce a single feature vector per modality per video. RGB "object" framelevel embeddings of the visual data are generated with two models: an SENet-154 model <ref type="bibr" target="#b23">[24]</ref> (pretrained on ImageNet for the task of image classification), and a ResNext-101 <ref type="bibr" target="#b59">[60]</ref> pretrained on Instagram hashtags <ref type="bibr" target="#b36">[37]</ref>. Motion embeddings are generated using the I3D inception model <ref type="bibr" target="#b5">[6]</ref> and a 34-layer R(2+1)D model <ref type="bibr" target="#b55">[56]</ref> trained on IG-65m <ref type="bibr" target="#b18">[19]</ref>. Face embeddings are extracted in two stages: (1) Each frame is passed through an SSD face detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> to extract bounding boxes; <ref type="bibr" target="#b1">(2)</ref> The image region of each box is passed through a ResNet50 <ref type="bibr" target="#b21">[22]</ref> that has been trained for the task of face classification on the VGGFace2 dataset <ref type="bibr" target="#b4">[5]</ref>. Audio embeddings are obtained with a VGGish model, trained for audio classification on the YouTube-8m dataset <ref type="bibr" target="#b22">[23]</ref>. Speech-to-Text features are extracted using the Google Cloud speech API, to extract word tokens from the audio stream, which are then encoded via pretrained word2vec embeddings <ref type="bibr" target="#b40">[41]</ref>. Optical Character Recognition is done in two stages: (1) Each frame is passed through the Pixel Link <ref type="bibr" target="#b9">[10]</ref> text detection model to extract bounding boxes for text; <ref type="bibr" target="#b1">(2)</ref> The image region of each box is passed through a model <ref type="bibr" target="#b35">[36]</ref> that has been trained for scene text recognition on the Synth90K dataset <ref type="bibr" target="#b26">[27]</ref>. The text is then encoded via a pretrained word2vec embedding model <ref type="bibr" target="#b40">[41]</ref>. Temporal Aggregation: We adopt a simple approach to aggregating the features described above.</p><p>For appearance, motion, scene and face embeddings, we average frame-level features along the temporal dimension to produce a single feature vector per video (we found max-pooling to perform similarly). For speech, audio and OCR features, we adopt the NetVLAD mechanism proposed by <ref type="bibr" target="#b2">[3]</ref>, which has proven effective in the retrieval setting <ref type="bibr" target="#b37">[38]</ref>. As noted in Sec. 3.1, all aggregated features are projected to a common size (768 dimensions).</p><p>Text: Each word is encoded using pretrained word2vec word embeddings <ref type="bibr" target="#b40">[41]</ref> and then passed through a pretrained OpenAI-GPT model <ref type="bibr" target="#b47">[48]</ref> to extract contextual word embeddings. Finally, the word embeddings in each sentence are aggregated using NetVLAD. Dataset-specific details: Except where noted otherwise for ablation purposes, we use each of the embeddings described above for the MSR-VTT, ActivityNet and DiDeMo datasets. For MSVD, we extract the subset of features which do not require an audio stream (since no audio is available with the dataset). For LSMDC, we re-use the existing face, text and audio features made available by <ref type="bibr" target="#b38">[39]</ref>, and combine them with the remaining features described above.</p><p>Training Details: The CE framework is implemented with PyTorch <ref type="bibr" target="#b46">[47]</ref>. Optimisation is performed with the Lookahead solver <ref type="bibr" target="#b28">[29]</ref> in combination with RAdam <ref type="bibr" target="#b33">[34]</ref> (implementation by <ref type="bibr" target="#b58">[59]</ref>). Optimisation settings and the hyperparameter selection procedure is described in the appendix. Evaluation Metrics: We follow prior work (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr">65,</ref><ref type="bibr">66]</ref>) and report standard retrieval metrics (where existing work enables comparison) including median rank (lower is better), mean rank (lower is better) and R@K (recall at rank K-higher is better). When computing videoto-sentence metrics for datasets with multiple independent sentences per video (MSR-VTT and MSVD), we follow the evaluation protocol used in prior work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref> which corresponds to reporting the minimum rank among all valid text descriptions for a given video query. For each benchmark, we report the mean and standard deviation of three randomly seeded runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to Prior State-of-the-Art</head><p>We first compare the proposed method with the existing state-of-the-art on the MSR-VTT benchmark for the tasks of sentence-to-video and video-to-sentence retrieval Tab. 1. Driven by strong expert features, we observe that Collaborative Experts (CE) consistently improves retrieval performance for both sentence and video queries. We next evaluate the performance of the CE framework on the LSMDC benchmark for sentence-to-video retrieval (Tab. 2, left) and observe that CE matches or outperforms all prior work, including the prior state-of-the-art method <ref type="bibr" target="#b38">[39]</ref> which incorporates additional training images and captions from the COCO benchmark during training, but uses fewer experts. We observe similar trends in the results for the MSVD retrieval benchmark (Tab. 2, right).  In Tab. 4, we compare with prior work on the ActivityNet paragraph-video retrieval benchmark (note that we compare to methods which use the same level of annotation as our approach i.e. video-level annotation), and see that CE is competitive. Finally, in Tab. 3 we provide a comparison with previously reported numbers on the DiDeMo benchmark and see that CE again outperforms prior work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, we provide ablation studies to empirically assess: (1) the effectiveness of the proposed collaborative experts framework vs other aggregation strategies; (2) the importance of using of a diverse range of experts with differing levels of specificity; (3) the relative value of using experts in comparison to simply having additional annotated training data.</p><p>Text  <ref type="table">Table 4</ref>: Comparison of paragraph-video retrieval methods trained with video-level information on the ActivityNet-captions dataset (val1 test-split).</p><note type="other">=⇒ Video Video =⇒ Text Method R@1 R@5 R@50 MdR MnR R@1 R@5 R@50 MdR</note><p>Aggregation method: We compare the use of collaborative experts with several other baselines (with access to the same experts) for embedding aggregation including: (1) simple expert concatenation; (2) CE without projecting to a common dimension, without mixture weights and without the collaborative gating module described in Sec. 3.1; (3) the state of the art MoEE <ref type="bibr" target="#b38">[39]</ref> method (equivalent to CE without the common projection and collaborative gating) and (4) CE without collaborative gating. The results, presented in Tab. 6 (left), demonstrate the contribution of collaborative gating which improves performance and leads to a more efficient parameterisation than the prior state of the art. Importance of different experts: The value of different experts is assessed in Tab. 5 (note that since several experts are not present in all videos, we combine them with features produced by a "scene" expert pretrained on Places365 [69]-the expert with the lowest performance that is consistently available as a baseline to enable a more meaningful comparison). There is considerable variance in the effect produced by different choices of expert. Using stronger features within a given modality (pretraining on Instagram <ref type="bibr" target="#b36">[37]</ref> rather than Kinetics <ref type="bibr" target="#b5">[6]</ref> (resp. ImageNet) <ref type="bibr" target="#b10">[11]</ref> for actions (resp. object) experts can yield a significant boost in performance). The cues from scarce features (such as speech, face and OCR) which are often missing from videos (see <ref type="figure" target="#fig_0">Fig. 1</ref>, right) provide significantly weaker cues and bring a limited improvement to performance when used in combination.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Captions in training:</head><p>An emerging idea in our community is that many machine perception tasks might be solved through the combination of simple models and large-scale training sets, reminiscent of the "big-data" hypothesis <ref type="bibr" target="#b19">[20]</ref>. In this section, we perform an ablation study to assess the relative importance of access to pretrained experts and additional video description annotations. To do so, we measure the performance of the CE model as we vary <ref type="formula" target="#formula_1">(1)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduced collaborative experts, a framework for learning a joint video-text embedding for efficient retrieval. We have shown that using a range of pretrained features and combining them through an appropriate gating mechanism can boost retrieval performance. In future work, we plan to explore the use of collaborative experts for other video understanding tasks such as clustering and summarisation.</p><p>Acknowledgements: Funding for this research is provided by the EPSRC Programme Grant Seebibyte EP/M013774/1 and EPSRC grant EP/R03298X/1. A.N. is supported by a Google PhD Fellowship. We would like to thank Antoine Miech, YoungJae Yu and Bowen Zhang for their assistance with experiment details. We would like to particularly thank Valentin Gabeur for identifying a bug in the software implementation that was responsible for the inaccurate results reported in the initial version of the paper. We would also like to thank Zak Stone and Susie Lim for their help with cloud computing.</p><p>[65] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471-487, 2018.</p><p>[66] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text.</p><p>In Proceedings of the European Conference on Computer Vision (ECCV), pages 374-390, 2018.</p><p>[67] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps forward, 1 step back. In Advances in Neural Information Processing Systems, pages 9593-9604, 2019.</p><p>[68] Yujie Zhong, Relja Arandjelović, and Andrew Zisserman. Ghostvlad for set-based face recognition.</p><p>In Asian Conference on Computer Vision, pages 35-50. Springer, 2018.</p><p>[69] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.</p><p>[70] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. arXiv preprint arXiv:1903.08225, 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Paper update, result corrections and summary of differences</p><p>Following the release of the initial version of this paper (which can be viewed for reference at https://arxiv.org/abs/1907.13487v1), a bug was discovered in our open-source software implementation which resulted in: (i) an overestimate of model performance; (ii) inaccurate conclusions about the relative importance of different experts on retrieval performance. This correction to the paper contains repeats of each of the experiments reported in the initial paper, with the following changes: (1) the removal of the bug which affected previous results; (2) a systematic approach to hyperparameter selection (discussed in more detail below); (3) the inclusion of additional "expert" pretrained features (described in Sec. A.5) to assess the influence of feature strength within a modality. In addition to results, the written analysis has also been updated to reflect the corresponding changes in results. The authors would like to express their gratitude to Valentin Gabeur who identified the bug in the software implementation and enabled this correction. Bug details: The bug caused information about feature availability in the ground truth target video to become available to the query encoder during both training and testing when computing embedding distances. The leak occurred through incorrect weighting of the embedding distances due to: (1) a leaking broadcasting operation in an existing open-source library <ref type="bibr" target="#b38">[39]</ref> that was imported into our codebase; (2) incorrect NaN handling (introduced in our codebase), producing the same effect. The bug has now been patched in each of the open-source codebases that were known to have used this implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Detailed Description of Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSR-VTT [61]</head><p>: This large-scale dataset comprises approximately 200K unique video-caption pairs (10K YouTube video clips, each accompanied by 20 different captions). The dataset is particularly useful because it contains a good degree of video diversity, but we noted a reasonably high degree of label noise (there are a number of duplicate annotations in the provided captions). The dataset allocates 6513, 497 and 2990 videos for training, validation and testing, respectively. To enable a comparison with as many methods as possible, we also report results across other train/test splits used in prior work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr">65]</ref>. In particular, when comparing with <ref type="bibr" target="#b38">[39]</ref> (on splits which do not provide a validation set), we follow their evaluation protocol, measuring performance after training has occurred for a fixed number of epochs (100 in total). MSVD <ref type="bibr" target="#b7">[8]</ref>: The MSVD dataset contains 80K English descriptions for 1,970 videos sourced from YouTube with a large number of captions per video (around 40 sentences each). We use the standard split of 1,200, 100, and 670 videos for training, validation, and testing <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref> 2 . Differently from the other datasets, the MSVD videos do not have audio streams. LSMDC <ref type="bibr" target="#b49">[50]</ref>: This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets, as outlined by the Large Scale Movie Description Challenge (LSMDC). <ref type="bibr" target="#b2">3</ref> ActivityNet-captions <ref type="bibr" target="#b31">[32]</ref>: ActivityNet Captions consists of 20K videos from YouTube, coupled with approximately 100K descriptive sentences. We follow the paragraph-video retrieval protocols described in [66] training up to 200 epochs and reporting performance on val1 (this train/test split allocates 10,009 videos for training and 4,917 videos for testing). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Optimisation details and hyperparameter selection</head><p>For each dataset, a grid search was first performed (using the Lookahead solver <ref type="bibr" target="#b58">[59,</ref><ref type="bibr">67]</ref>) over batch sizes <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref>, learning rates (0.1, 0.01) and weight decay (1E-3, 5E-5) for each dataset using a single expert to determine appropriate optimisation parameters. Next, an experiment on MSR-VTT compared several choices for the dimensionality of the projection operation applied to the features (described in Sec. 3.1) (choosing among 512, 768 and 1024 dimensions), which suggested that 768 was most effective. This was then fixed for all remaining experiments (this represents a difference from the original paper, in which 512 was used). Further ablations (provided below) indicate that performance is not sensitive to this hyperparameter. Next, Asynchronous Hyperband <ref type="bibr" target="#b32">[33]</ref> was used to select all remaining hyperparameters on MSR-VTT by partially evaluating 1k configurations on the validation sets for each dataset. These hyperparameters consisted of: the number of VLAD clusters and ghost clusters [68] used for different experts, the zero-padding length applied to variable-length experts, the margin hyperparameter m in Eq. 3, the Collaborative Gating architecture (whether to use batch normalization <ref type="bibr" target="#b25">[26]</ref>, the number of layers used to form the MLP, and the choice of activation function). The architecture choices were then fixed for all datasets. Note that to ensure a fair comparison on MSR-VTT with the MoEE method of <ref type="bibr" target="#b38">[39]</ref> in Tab. 6, MoEE was also provided with a budget of 1k sampled configurations. To determine zero-padding, margin and VLAD clusters for DiDeMo, MSVD and LSMDC further Asynchronous Hyperband searches were conducted, each with a budget of 500 sampled configurations. Since, differently from the other datasets with available validation and test sets, the validation set itself is used to assess performance on ActivityNet, hyperparameters were copied from the DiDeMo configuration. The configurations, experts, pretrained models and logs for each of the experiments reported in this paper are made available as part of the updated open-source implementation at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/.     <ref type="table" target="#tab_2">Table 10</ref>: Ablation study of the importance of model capacity by varying the shared embedding dimension used by CE on MSR-VTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Ablation Studies -Full Tables</head><p>as Obj <ref type="figure">(IN)</ref>. Features are collected from the final global average pooling layer of both models, and have a dimensionality of 2048. Action embeddings are similarly generated from two models, Action(KN) and Action(IG). Action <ref type="figure">(KN)</ref> is an I3D inception model that computes features following the procedure described by <ref type="bibr" target="#b5">[6]</ref>. Frames extracted at 25fps and processed with a window length of 64 frames and a stride of 25 frames. Each frame is first resized to a height of 256 pixels (preserving aspect ratio), before a 224 × 224 centre crop is passed to the model. Each temporal window produces a (1024x7)-matrix of features. Action(IG) is a 34-layer R(2+1)D model <ref type="bibr" target="#b55">[56]</ref> trained on IG-65m <ref type="bibr" target="#b18">[19]</ref> which processes clips of 8 consecutive 112 × 112 pixel frames, extracted at 30 fps (we use the implementation provided by <ref type="bibr" target="#b8">[9]</ref>). Face embeddings are extracted in two stages: (1) Each frame (also extracted at 25 fps) is resized to 300 × 300 pixels and passed through an SSD face detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> to extract bounding boxes;</p><p>(2) The image region of each box is resized such that the minimum dimension is 224 pixels and a centre crop is passed through a ResNet50 <ref type="bibr" target="#b21">[22]</ref> that has been trained for task of face classification on the VGGFace2 dataset <ref type="bibr" target="#b4">[5]</ref>, producing a 512-dimensional embedding for each detected face.</p><p>Audio embeddings are obtained with a VGGish model, trained for audio classification on the YouTube-8m dataset <ref type="bibr" target="#b22">[23]</ref>. To produce the input for this model, the audio stream of each video is re-sampled to a 16kHz mono signal, converted to an STFT with a window size of 25ms and a hop of 10ms with a Hann window, then mapped to a 64 bin log mel-spectrogram. Finally, the features are parsed into non-overlapping 0.96s collections of frames (each collection comprises 96 frames, each of 10ms duration), which is mapped to a 128-dimensional feature vector.</p><p>Scene embeddings of 2208 dimensions are extracted from 224×224 pixel centre crops of frames extracted at 1fps using a DenseNet-161 <ref type="bibr" target="#b24">[25]</ref> model pretrained on Places365 <ref type="bibr">[69]</ref>. Speech to Text The audio stream of each video is re-sampled to a 16kHz mono signal. We then obtained transcripts of the spoken speech for MSR-VTT, MSVD and ActivityNet using the Google Cloud Speech to Text API 4 from the resampled signal. The language for the API is specified as English. For reference, of the 10,000 videos contained in MSR-VTT, 8,811 are accompanied by audio streams. Of these, we detected speech in 5,626 videos. Optical Character Recognition is extracted in two stages: (1) Each frame is resized to 800×400 pixels) and passed through Pixel Link <ref type="bibr" target="#b9">[10]</ref> text detection model to extract bounding boxes for texts;</p><p>(2) The image region of each box is resized to 32×256 and then pass through a model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref> that has been trained for text of scene text recognition on the Synth90K dataset <ref type="bibr" target="#b26">[27]</ref>, producing a character sequence for each detect box. They are then encoded via a pretrained word2vec embedding model <ref type="bibr" target="#b40">[41]</ref>.</p><p>Text We encode each word using the Google News 5 trained word2vec word embeddings <ref type="bibr" target="#b40">[41]</ref>. All the word embeddings are then pass through a pretrained OpenAI-GPT model to extract the context-specific word embeddings (i.e., not only learned based on word concurrency but also the sequential context). Finally, all the word embeddings in each sentence are aggregated using NetVLAD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>arXiv:1907.13487v2 [cs.CV] 14 Feb 2020 O b je c t s A c t io n s S c e n e s S o u n d s F a c e s S p e e c h O (Left): Unconstrained videos 'in the wild' convey information in various different ways, including (clockwise from upper-left), clues from distinctive speech, names of individuals on screen, other text clues embedded in the video and audio. (Right): For the five video datasets considered in this work, the chart portrays the video-level availability of "expert" embeddings from different domains (with potentially multiple experts per domain): certain generic embeddings can almost always be extracted via pretrained object/action/scene classification networks. Other features such as sounds, faces, speech and OCR are less consistently available and are more challenging to exploit (Sec. 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(Left):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative Results on MSR-VTT: For each query, we show frames from the top three ranked videos (where present, the ground truth video is indicated by a green box around the similarity score). Top row: (left) Even for imperfect rankings, the model retrieves reasonable videos; Failure case (right) the embeddings can fail to differentiate between certain signals (in this case, ranking cars of the wrong colour above the ground truth video). Bottom row: (left) the videos retrieved by the proposed model (which assigns its second highest similarity to the correct video); (right) removing the proposed CE component produces a nosier ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>DiDeMo [ 1 ]:</head><label>1</label><figDesc>DiDeMo contains 10,464 unedited, personal videos in diverse visual settings with roughly 3-5 pairs of descriptions and distinct moments per video. The videos are collected in an open-world setting and include diverse content such as pets, concerts, and sports games. The total number of sentences is 40,543. While the moments are localised with time-stamp annotations, we do not use time stamps in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The description of datasets, implementation details and evaluation metric are provided in Sec. 4.1. A comprehensive comparison on general video retrieval benchmarks is reported in Sec. 4.2. We present an ablation study in Sec. 4.3 to explore how the performance of the proposed method is affected by different model configurations, including the aggregation methods, importance of different experts and number of captions in training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>20.9 ±1.2 48.8 ±0.6 62.4 ±0.8 6 ±0 28.2 ±0.8 20.6 ±0.6 50.3 ±0.5 64.0 ±0.2 5.3 ±0.6 25.1 ±0.8 18.2 ±0.7 46.0 ±0.4 60.7 ±0.2 7 ±0 35.3 ±1.1 18.0 ±0.8 46.0 ±0.5 60.3 ±0.5 6.5 ±0.5 30.6 ±1.2 ±0.1 29.0 ±0.3 41.2 ±0.2 16 ±0 86.8 ±0.3 15.6 ±0.3 40.9 ±1.4 55.2 ±1.0 8.3 ±0.6 38.1 ±1.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Text =⇒ Video</cell><cell></cell><cell></cell><cell cols="3">Video =⇒ Text</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Test-set R@1 R@5 R@10 MdR MnR</cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="2">MdR MnR</cell></row><row><cell>JSFusion [65]</cell><cell cols="2">1k-A 10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">CE 1k-A MoEE [39] 1k-B 13.6</cell><cell>37.9</cell><cell>51.0</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">MoEE COCO [39] 1k-B 14.2</cell><cell>39.2</cell><cell>53.8</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CE 1k-B VSE [42] Full</cell><cell>5.0</cell><cell>16.4</cell><cell>24.6</cell><cell>47</cell><cell>215.1</cell><cell>7.7</cell><cell>20.3</cell><cell>31.2</cell><cell>28</cell><cell>185.8</cell></row><row><cell>VSE++ [42]</cell><cell>Full</cell><cell>5.7</cell><cell>17.1</cell><cell>24.8</cell><cell>65</cell><cell>300.8</cell><cell>10.2</cell><cell>25.4</cell><cell>35.1</cell><cell>25</cell><cell>228.1</cell></row><row><cell cols="2">Mithun et al. [42] Full</cell><cell>7.0</cell><cell>20.9</cell><cell>29.7</cell><cell>38</cell><cell>213.8</cell><cell>12.5</cell><cell>32.1</cell><cell>42.4</cell><cell>16</cell><cell>134.0</cell></row><row><cell>W2VV [13]</cell><cell>Full</cell><cell>6.1</cell><cell>18.7</cell><cell>27.5</cell><cell>45</cell><cell>-</cell><cell>11.8</cell><cell>28.9</cell><cell>39.1</cell><cell>21</cell><cell>-</cell></row><row><cell>Dual Enc. [14]</cell><cell>Full</cell><cell>7.7</cell><cell>22.0</cell><cell>31.8</cell><cell>32</cell><cell>-</cell><cell>13.0</cell><cell>30.8</cell><cell>43.3</cell><cell>15</cell><cell>-</cell></row><row><cell>E2E [40]</cell><cell>Full</cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>29.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE</cell><cell>Full</cell><cell>10.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Retrieval with sentences and videos on the MSR-VTT dataset. R@k denotes recall@k (higher is better), MdR and MnR denote median rank and mean rank resp. (lower is better).</figDesc><table /><note>Standard deviations are reported from three randomly seeded runs. 1k-A and 1k-B denote test sets of 1000 randomly sampled text-video pairs used by [65] and [39] resp.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>±0.4 26.9 ±1.1 34.8 ±2.0 25.3 ±3.1 ±0.3 49.0 ±0.3 63.8 ±0.1 6 ±0.0 23.1 ±0.3</figDesc><table><row><cell>Text =⇒ Video R@1 R@5 R@10 3.6 14.7 23.9 CCA [31] (rep. by [39]) 7.5 Method Yu et al. [64] † 21.7 31.0 JSFusion [65] ‡ 9.1 21.2 34.1 MoEE [39] 9.3 25.1 33.4 MoEE COCO [39] 10.1 25.6 34.6 CE 11.2 Text =⇒ Video MdR 50 33 36 27 27 Method R@1 R@5 R@10 MdR MnR CCA ([62]) ----245.3 JMDV [62] ----236.3 VSE [30] ([42]) 12.3 30.1 42.3 14 57.7 VSE++ [16] ([42]) 15.4 39.6 53.0 9 43.8 Multi. Cues [42] 20.3 47.8 61.1 6 28.3 CE 19.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>±0.7 36 ±0.8 78.9 ±1.6 11 -13.1 ±0.5 33.9 ±0.4 78.0 ±0.8 12 -CE 16.1 ±1.4 41.1 ±0.4 82.7 ±0.3 8.3 ±0.6 43.7 ±3.6 15.6 ±1.3 40.9 ±0.4 82.2 ±1.3 8.2 ±0.3 42.4 ±3.3</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Text =⇒ Video</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Video =⇒ Text</cell></row><row><cell>Method</cell><cell cols="3">R@1 R@5 R@50</cell><cell>MdR</cell><cell>MnR</cell><cell cols="3">R@1 R@5 R@50</cell><cell>MdR</cell><cell>MnR</cell></row><row><cell cols="2">S2VT [57] ([66]) 11.9</cell><cell>33.6</cell><cell>76.5</cell><cell>13</cell><cell>-</cell><cell>13.2</cell><cell>33.6</cell><cell>76.5</cell><cell>15</cell><cell>-</cell></row><row><cell>FSE [66]</cell><cell>13.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Text-to-Video retrieval results on the LSMDC dataset (left) and the MSVD dataset (right).†, ‡ denote the winners of the 2016 and 2017 LSMDC challenges, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of paragraph-video retrieval methods trained with video-level information on the DiDeMo dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>±0.2 44.8 ±0.4 89.1 ±0.3 7 -16.7 ±0.8 43.1 ±1.1 88.4 ±0.±0.3 47.7 ±0.6 91.4 ±0.4 6 ±0 23.1 ±0.5 17.7 ±0.6 46.6 ±0.7 90.9 ±0.2 6 ±0 24.4 ±0.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MnR</cell></row><row><cell cols="2">LSTM-YT [58] ([66]) 0.0</cell><cell>4.0</cell><cell>24.0</cell><cell>102</cell><cell>-</cell><cell>0.0</cell><cell>7.0</cell><cell>38.0</cell><cell>98</cell><cell>-</cell></row><row><cell>NOCTXT [57] ([66])</cell><cell>5.0</cell><cell>14.0</cell><cell>32.0</cell><cell>78</cell><cell>-</cell><cell>7.0</cell><cell>18.0</cell><cell>45.0</cell><cell>56</cell><cell>-</cell></row><row><cell>DENSE [32]</cell><cell>14.0</cell><cell>32.0</cell><cell>65.0</cell><cell>34</cell><cell>-</cell><cell>18.0</cell><cell>36.0</cell><cell>74.0</cell><cell>32</cell><cell></cell></row><row><cell>FSE [66]</cell><cell cols="9">18.2 3 7</cell><cell>-</cell></row><row><cell>HSE(4SEGS) [66] †</cell><cell>20.5</cell><cell>49.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.7</cell><cell>48.1</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>CE</cell><cell>18.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>±0.1 14.1 ±0.1 22.4 ±0.3 50.0 ±1.0 201.3 ±1.6 Scene+Speech 4.6 ±0.1 15.5 ±0.2 24.4 ±0.2 44.7 ±1.2 183.6 ±1.7 Scene+Audio 5.6 ±0.0 18.7 ±0.1 28.2 ±0.1 33.7 ±0.6 140.8 ±0.3 Scene+Action(KN) 5.3 ±0.3 17.6 ±0.8 27.1 ±0.9 36.0 ±1.7 158.7 ±1.6 Scene+Obj(IN) 5.0 ±0.2 16.6 ±0.7 25.5 ±1.0 40.7 ±2.1 173.1 ±3.3 Scene+Obj(IG) 7.2 ±0.1 22.3 ±0.3 33.0 ±0.2 25.3 ±0.6 125.1 ±0.1 Scene+Action(IG) 6.8 ±0.1 21.7 ±0.1 32.4 ±0.1 25.7 ±0.6 122.1 ±0.3 Scene+OCR 4.1 ±0.1 14.1 ±0.1 22.2 ±0.2 50.3 ±1.2 203.1 ±4.4 Scene+Face 4.1 ±0.1 14.2 ±0.3 22.4 ±0.4 49.7 ±0.6 194.2 ±5.1 ±0.1 14.1 ±0.1 22.4 ±0.3 50.0 ±1.0 201.3 ±1.6 Prev.+Speech 4.6 ±0.1 15.5 ±0.2 24.4 ±0.2 44.7 ±1.2 183.6 ±1.7 Prev.+Audio 5.8 ±0.1 19.0 ±0.3 28.8 ±0.2 32.3 ±0.6 136.8 ±1.2 Prev.+Action(KN) 6.7 ±0.2 21.8 ±0.4 32.5 ±0.5 25.3 ±0.6 115.9 ±1.0 Prev.+Obj(IN) 7.5 ±0.1 23.4 ±0.0 34.1 ±0.2 23.7 ±0.6 111.9 ±0.6 Prev.+Obj(IG) 9.5 ±0.2 27.7 ±0.1 39.4 ±0.1 18.0 ±0.0 92.6 ±0.4 Prev.+Action(IG) 9.9 ±0.1 28.6 ±0.3 40.7 ±0.1 17.0 ±0.0 86.4 ±0.4 Prev.+OCR 10.0 ±0.1 28.8 ±0.2 40.9 ±0.2 16.7 ±0.6 87.3 ±0.8 Prev.+Face 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0 86.8 ±0.3</figDesc><table><row><cell>Experts Scene</cell><cell>Text =⇒ Video R@10 MdR 4.0 Text =⇒ Video R@1 R@5 MnR Experts R@1 R@5 R@10 MdR Scene 4.0</cell><cell>MnR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The importance of different experts (Left): The value of different experts in combination with a baseline set for text-video retrieval and (right) their cumulative effect on MSR-VTT (here Prev. denotes the experts used in the previous row).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>±0.0 0.0 ±0.0 0.0 ±0.0 1495.5 ±0.0 369.72k CE -MW,P,CG 8.5 ±0.1 25.9 ±0.3 37.6 ±0.2 19.0 ±0.0 246.22M MoEE [39] 9.6 ±0.1 28.0 ±0.2 39.7 ±0.2 17.7 ±0.6 400.41 M CE -CG 9.7 ±0.1 28.1 ±0.2 40.2 ±0.1 17.0 ±0.0 181.07 M CE 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0 183.45 M ±0.1 9.3 ±0.4 15.0 ±0.7 101.3 ±15.5 Obj(IN) 20 4.9 ±0.1 16.5 ±0.2 25.3 ±0.4 40.7 ±1.2 All 1 4.8 ±0.2 16.2 ±0.5 25.0 ±0.7 43.3 ±4.0 All 20 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0</figDesc><table><row><cell>Aggreg. Concat</cell><cell>R@1 0.0 Expert Num. Captions R@1 R@5 R@10 MdR Params Obj(IN) 1 2.6</cell><cell>R@5 R@10</cell><cell>MdR</cell></row></table><note>the number of descriptions available per-video during training and (2) the number of experts it has access to. The results are shown in Tab. 6 (right). We observe that increasing the number of training captions</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>(Left): Aggregation methods for text-video retrieval on MSR-VTT; (Right): The relative value of training with additional captions vs the value of experts.per-video from 1 to 20 brings an improvement in performance, approximately comparable to adding the full collection of experts, suggesting that indeed, adding experts can help to compensate for a paucity of labelled data. When multiple captions and multiple experts are both available, they naturally lead to the most robust embedding. Some qualitative examples of videos retrieved by the multiple-expert, multiple-caption system are provided inFig. 3.</figDesc><table><row><cell cols="2">Query: Guy working on his engine with multiple parts</cell><cell></cell><cell cols="3">Query: shiny black sports car drives very slowly down road through orange</cell></row><row><cell>(GT rank: 29)</cell><cell></cell><cell></cell><cell cols="2">and white safety cones (GT rank: 10)</cell><cell></cell></row><row><cell>Similarity: 0.60</cell><cell>Similarity: 0.59</cell><cell>Similarity: 0.55</cell><cell>Similarity: 0.48</cell><cell>Similarity: 0.46</cell><cell>Similarity: 0.46</cell></row><row><cell cols="2">Query: Awareness of mosquitoe bites by doctors</cell><cell></cell><cell cols="3">Query: Query: Awareness of mosquitoe bites by doctors</cell></row><row><cell>(GT rank: 2)</cell><cell></cell><cell></cell><cell>(without CE) -(GT rank: 7)</cell><cell></cell><cell></cell></row><row><cell>Similarity: 0.38</cell><cell>Similarity: 0.36</cell><cell>Similarity: 0.34</cell><cell>Similarity: 0.34</cell><cell>Similarity: 0.33</cell><cell>Similarity: 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>±0.1 14.1 ±0.1 22.4 ±0.3 50.0 ±1.0 201.3 ±1.6 5.6 ±0.6 18.2 ±0.6 27.7 ±0.3 39.0 ±0.0 247.0 ±10.1 Scene+Speech 4.6 ±0.1 15.5 ±0.2 24.4 ±0.2 44.7 ±1.2 183.6 ±1.7 6.0 ±0.2 20.4 ±0.5 30.3 ±1.0 33.0 ±2.0 222.6 ±9.9 Scene+Audio 5.6 ±0.0 18.7 ±0.1 28.2 ±0.1 33.7 ±0.6 140.8 ±0.3 8.2 ±0.4 24.8 ±0.4 36.0 ±0.1 21.7 ±0.6 127.9 ±5.9 Scene+Action(KN) 5.3 ±0.3 17.6 ±0.8 27.1 ±0.9 36.0 ±1.7 158.7 ±1.6 7.3 ±0.6 22.3 ±1.4 33.4 ±1.7 25.2 ±2.0 151.7 ±11.6 Scene+Obj(IN) 5.0 ±0.2 16.6 ±0.7 25.5 ±1.0 40.7 ±2.1 173.1 ±3.3 6.9 ±0.5 21.2 ±0.9 31.1 ±1.9 28.7 ±3.8 188.3 ±4.7 Scene+Obj(IG) 7.2 ±0.1 22.3 ±0.3 33.0 ±0.2 25.3 ±0.6 125.1 ±0.1 10.1 ±0.3 29.7 ±0.5 41.9 ±0.7 15.2 ±0.9 91.3 ±2.4 Scene+Action(IG) 6.8 ±0.1 21.7 ±0.1 32.4 ±0.1 25.7 ±0.6 122.1 ±0.3 9.4 ±0.3 27.8 ±0.6 40.1 ±1.1 17.2 ±1.1 87.8 ±4.2 Scene+OCR 4.1 ±0.1 14.1 ±0.1 22.2 ±0.2 50.3 ±1.2 203.1 ±4.4 5.4 ±0.5 18.6 ±1.2 26.6 ±1.2 40.0 ±1.0 292.6 ±9.9 Scene+Face 4.1 ±0.1 14.2 ±0.3 22.4 ±0.4 49.7 ±0.6 194.2 ±5.1 5.6 ±1.0 17.9 ±0.7 26.7 ±0.8 39.1 ±2.6 273.5 ±6.3</figDesc><table><row><cell></cell><cell>Text =⇒ Video</cell><cell></cell><cell></cell><cell>Video =⇒ Text</cell><cell></cell></row><row><cell>Experts</cell><cell>R@1 R@5 R@10 MdR</cell><cell>MnR</cell><cell>R@1</cell><cell>R@5 R@10 MdR</cell><cell>MnR</cell></row><row><cell>Scene</cell><cell>4.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of importance of each expert when combined with Scene features. ±0.1 14.1 ±0.1 22.4 ±0.3 50.0 ±1.0 201.3 ±1.6 5.6 ±0.6 18.2 ±0.6 27.7 ±0.3 39.0 ±0.0 247.0 ±10.1 Prev.+Speech 4.6 ±0.1 15.5 ±0.2 24.4 ±0.2 44.7 ±1.2 183.6 ±1.7 6.0 ±0.2 20.4 ±0.5 30.3 ±1.0 33.0 ±2.0 222.6 ±9.9 Prev.+Audio 5.8 ±0.1 19.0 ±0.3 28.8 ±0.2 32.3 ±0.6 136.8 ±1.2 8.6 ±0.2 26.1 ±0.6 37.8 ±0.8 19.8 ±0.8 117.7 ±2.9 Prev.+Action(KN) 6.7 ±0.2 21.8 ±0.4 32.5 ±0.5 25.3 ±0.6 115.9 ±1.0 9.9 ±0.4 28.6 ±0.7 41.7 ±0.8 15.7 ±0.6 77.9 ±5.2 Prev.+Obj(IN) 7.5 ±0.1 23.4 ±0.0 34.1 ±0.2 23.7 ±0.6 111.9 ±0.6 11.2 ±0.3 32.1 ±0.8 45.4 ±0.6 13.7 ±0.6 68.0 ±1.4 Prev.+Obj(IG) 9.5 ±0.2 27.7 ±0.1 39.4 ±0.1 18.0 ±0.0 92.6 ±0.4 14.7 ±0.6 38.9 ±0.8 53.1 ±1.0 9.3 ±0.6 45.6 ±2.1 Prev.+Action(IG) 9.9 ±0.1 28.6 ±0.3 40.7 ±0.1 17.0 ±0.0 86.4 ±0.4 15.5 ±0.6 40.1 ±1.2 54.4 ±1.3 8.7 ±0.6 39.4 ±0.9 Prev.+ OCR 10.0 ±0.1 28.8 ±0.2 40.9 ±0.2 16.7 ±0.6 87.3 ±0.8 15.2 ±0.1 41.1 ±0.6 54.6 ±0.7 8.5 ±0.5 38.5 ±0.6 Prev.+ Face 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0 86.8 ±0.3 15.6 ±0.3 40.9 ±1.4 55.2 ±1.0 8.3 ±0.6 38.1 ±1.8</figDesc><table><row><cell></cell><cell></cell><cell>Text =⇒ Video</cell><cell></cell><cell></cell><cell>Video =⇒ Text</cell><cell></cell></row><row><cell>Experts</cell><cell>R@1</cell><cell>R@5 R@10 MdR</cell><cell>MnR</cell><cell>R@1</cell><cell>R@5 R@10 MdR</cell><cell>MnR</cell></row><row><cell>Scene</cell><cell>4.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of the importance experts on the MSR-VTT dataset. ±0.1 9.3 ±0.4 15.0 ±0.7 101.3 ±15.5 321.1 ±35.1 3.7 ±0.3 13.5 ±0.6 20.8 ±0.4 60.0 ±2.0 304.9 ±15.8 ±0.1 16.5 ±0.2 25.3 ±0.4 40.7 ±1.2 169.1 ±1.4 6.9 ±0.6 21.0 ±0.3 31.3 ±0.3 30.0 ±1.7 201.6 ±9.5 All 1 4.8 ±0.2 16.2 ±0.5 25.0 ±0.7 43.3 ±4.0 183.1 ±19.6 8.4 ±0.5 25.6 ±0.7 37.1 ±0.2 20.3 ±0.6 87.2 ±6.7 All 20 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0 86.8 ±0.3 15.6 ±0.3 40.9 ±1.4 55.2 ±1.0 8.3 ±0.6 38.1 ±1.8</figDesc><table><row><cell></cell><cell cols="2">Text =⇒ Video</cell><cell></cell><cell></cell><cell>Video =⇒ Text</cell><cell></cell></row><row><cell>Expert Num. Caps R@1</cell><cell>R@5 R@10</cell><cell>MdR</cell><cell>MnR</cell><cell>R@1</cell><cell>R@5 R@10 MdR</cell><cell>MnR</cell></row><row><cell>Obj(IN) 2.6 Obj(IN) 1 20 4.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of the number of captions in training on MSR-VTT ±0.2 27.8 ±0.4 39.8 ±0.4 17.7 ±0.6 88.8 ±0.5 14.0 ±0.5 38.7 ±0.5 52.7 ±1.4 9.3 ±0.6 41.8 ±1.0 88.62M 512 9.8 ±0.3 28.6 ±0.4 40.6 ±0.4 17.0 ±0.0 88.0 ±0.7 14.8 ±0.4 40.4 ±0.6 53.9 ±0.4 8.8 ±0.3 38.8 ±1.5 119.51M 640 10.1 ±0.1 28.8 ±0.1 40.9 ±0.2 16.7 ±0.6 87.6 ±0.2 15.6 ±0.6 41.3 ±0.7 55.0 ±0.5 8.3 ±0.6 37.3 ±1.8 151.12M 768 10.0 ±0.1 29.0 ±0.3 41.2 ±0.2 16.0 ±0.0 86.8 ±0.3 15.6 ±0.3 40.9 ±1.4 55.2 ±1.0 8.3 ±0.6 38.1 ±1.8 183.45M 1024 9.9 ±0.1 28.6 ±0.3 40.7 ±0.4 17.0 ±0.0 87.6 ±1.1 14.7 ±0.4 40.7 ±0.8 54.4 ±0.3 8.5 ±0.5 39.1 ±1.7 250.27M</figDesc><table><row><cell></cell><cell></cell><cell>Text =⇒ Video</cell><cell></cell><cell></cell><cell>Video =⇒ Text</cell></row><row><cell cols="2">Dimension R@1</cell><cell>R@5 R@10 MdR</cell><cell>MnR</cell><cell>R@1</cell><cell>R@5 R@10 MdR MnR Params.</cell></row><row><cell>384</cell><cell>9.4</cell><cell></cell><cell></cell><cell></cell></row></table><note>A.5 Implementation Details Object frame-level embeddings of the visual data are generated with two models, Obj(IN) and Obj(IG). Obj(IN) is an SENet-154 model [24] (pretrained on ImageNet for the task of image classi- fication) from frames extracted at 25 fps, where each frame is resized to 224 × 224 pixels. Obj(IG) is a ResNext-101 [60] pretrained on Instagram hashtags [37], using the same frame preparation</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this finding differs from the previous version of this paper (see appendix A.1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note: referred to by<ref type="bibr" target="#b41">[42]</ref> as the JMET-JMDV split 3 https://sites.google.com/site/describingmovies/lsmdc-2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://cloud.google.com/speech-to-text/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">GoogleNews-vectors-negative300.bin.gz found at: https://code.google.com/archive/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
		<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale content-based audio retrieval from text queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM international conference on Multimedia information retrieval</title>
		<meeting>the 1st ACM international conference on Multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Daniel</surname></persName>
		</author>
		<ptr target="https://github.com/moabitcoin/ig65m-pytorch" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Word2visualvec: Image and video to sentence matching by visual feature prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06838</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting visual features from text for image and video caption retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3377" to="3388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual dense encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining attributes and fisher vectors for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-modal information retrieval from broadcast video using ocr and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Alexander G Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobun Dorbin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>the 2nd ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="160" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jort</forename><forename type="middle">F</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.09430" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Malcolm Slaney, Ron Weiss, and Kevin Wilson</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert A</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05934</idno>
		<title level="m">Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Synthetically supervised feature learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learnable pins: Cross-modal embeddings for person identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning joint representations of videos and sentences with web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naokazu</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="651" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4594" to="4602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video retrieval using speech and text in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Inventive Computation Technologies (ICICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Niket Tandon, and Bernt Schiele. A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Less</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Project title</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Topic segmentation and retrieval system for lecture videos based on spontaneous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natsuo</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
