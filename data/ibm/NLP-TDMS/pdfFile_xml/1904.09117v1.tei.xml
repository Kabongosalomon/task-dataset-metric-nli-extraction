<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SelFlow: Self-Supervised Learning of Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SelFlow: Self-Supervised Learning of Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a self-supervised learning approach for optical flow. Our method distills reliable flow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical flow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better flow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical flow learning on the challenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. Our fine-tuned models achieve stateof-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimation is a core building block for a variety of computer vision systems <ref type="bibr" target="#b34">[30,</ref><ref type="bibr" target="#b12">8,</ref><ref type="bibr" target="#b43">39,</ref><ref type="bibr">4]</ref>. Despite decades of development, accurate flow estimation remains an open problem due to one key challenge: occlusion. Traditional approaches minimize an energy function to encourage association of visually similar pixels and regularize incoherent motion to propagate flow estimation from nonoccluded pixels to occluded pixels <ref type="bibr" target="#b17">[13,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b10">6,</ref><ref type="bibr" target="#b42">38]</ref>. However, this family of methods is often time-consuming and not applicable for real-time applications.</p><p>Recent studies learn to estimate optical flow end-toend from images using convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[10,</ref><ref type="bibr" target="#b39">35,</ref><ref type="bibr" target="#b19">15,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b47">43]</ref>. However, training fully supervised CNNs requires a large amount of labeled training data, which is extremely difficult to obtain for optical flow, especially when there are occlusions. Considering the recent performance improvements obtained when employing hundreds of millions of labeled images <ref type="bibr" target="#b44">[40]</ref>, it is obvious that the size of training data is a key bottleneck for optical flow estimation. * Work mainly done during an internship at Tencent AI Lab.</p><p>In the absence of large-scale real-world annotations, existing methods turn to pre-train on synthetic labeled datasets <ref type="bibr" target="#b14">[10,</ref><ref type="bibr" target="#b32">28]</ref> and then fine-tune on small annotated datasets <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b47">43]</ref>. However, there usually exists a large gap between the distribution of synthetic data and natural scenes. In order to train a stable model, we have to carefully follow specific learning schedules across different datasets <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b47">43]</ref>.</p><p>One promising direction is to develop unsupervised optical flow learning methods that benefit from unlabeled data. The basic idea is to warp the target image towards the reference image according to the estimated optical flow, then minimize the difference between the reference image and the warped target image using a photometric loss <ref type="bibr" target="#b24">[20,</ref><ref type="bibr" target="#b41">37]</ref>. Such idea works well for non-occluded pixels but turns to provide misleading information for occluded pixels. Recent methods propose to exclude those occluded pixels when computing the photometric loss or employ additional spatial and temporal smoothness terms to regularize flow estimation <ref type="bibr" target="#b33">[29,</ref><ref type="bibr" target="#b50">46,</ref><ref type="bibr" target="#b22">18]</ref>. Most recently, DDFlow <ref type="bibr" target="#b30">[26]</ref> proposes a data distillation approach, which employs random cropping to create occlusions for self-supervision. Unfortunately, these methods fails to generalize well for all natural occlusions. As a result, there is still a large performance gap comparing unsupervised methods with state-of-the-art fully supervised methods.</p><p>Is it possible to effectively learn optical flow with occlusions? In this paper, we show that a self-supervised approach can learn to estimate optical flow with any form of occlusions from unlabeled data. Our work is based on distilling reliable flow estimations from non-occluded pixels, and using these predictions to guide the optical flow learning for hallucinated occlusions. <ref type="figure">Figure 1</ref> illustrates our idea to create synthetic occlusions by perturbing superpixels. We further utilize temporal information from multiple frames to improve flow prediction accuracy within a simple CNN architecture. The resulted learning approach yields the highest accuracy among all unsupervised optical flow learning methods on Sintel and KITTI benchmarks.</p><p>Surprisingly, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. At the time of writing, our fine-tuned model achieves the <ref type="figure">Figure 1</ref>. A toy example to illustrate our self-supervised learning idea. We first train our NOC-model with the classical photometric loss (measuring the difference between the reference image (a) and the warped target image(d)), guided by the occlusion map (g). Then we perturbate randomly selected superpixels in the target image (b) to hallucinate occlusions. Finally, we use reliable flow estimations from our NOC-Model to guide the learning of our OCC-Model for those newly occluded pixels (denoted by self-supervision mask (i), where value 1 means the pixel is non-occluded in (g) but occluded in (h)). Note the yellow region is part of the moving dog. Our self-supervised approach learns optical flow for both moving objects and static scenes.</p><formula xml:id="formula_0">NOC Model OCC Model Guide Flow Flow +1 (a) Reference Image (b) Target Image +1 (c) Ground Truth Flow → +1 (d) Warped Target Image +1→ (e) SILC Superpixel (f) +1 (h) New Occlusion Map → +1 (i) Self-Supervision Mask → +1 (g) Occlusion Map → +1 +1 −1 1 2 1 ′ 2 ′ 1 2 −1</formula><p>highest reported accuracy (EPE=4.26) on the Sintel benchmark. Our approach also significantly outperforms all published optical flow methods on the KITTI 2012 benchmark, and achieves highly competitive results on the KITTI 2015 benchmark. To the best of our knowledge, it is the first time that a supervised learning method achieves such remarkable accuracies without using any external labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classical Optical Flow Estimation. Classical variational approaches model optical flow estimation as an energy minimization problem based on brightness constancy and spatial smoothness <ref type="bibr" target="#b17">[13]</ref>. Such methods are effective for small motion, but tend to fail when displacements are large. Later works integrate feature matching to initialize sparse matching, and then interpolate into dense flow maps in a pyramidal coarse-to-fine manner <ref type="bibr" target="#b10">[6,</ref><ref type="bibr" target="#b51">47,</ref><ref type="bibr" target="#b42">38]</ref>. Recent works use convolutional neural networks (CNNs) to improve sparse matching by learning an effective feature embedding <ref type="bibr" target="#b53">[49,</ref><ref type="bibr">2]</ref>. However, these methods are often compu-tationally expensive and can not be trained end-to-end. One natural extension to improve robustness and accuracy for flow estimation is to incorporate temporal information over multiple frames. A straightforward way is to add temporal constraints such as constant velocity <ref type="bibr" target="#b23">[19,</ref><ref type="bibr" target="#b26">22,</ref><ref type="bibr" target="#b45">41]</ref>, constant acceleration <ref type="bibr" target="#b49">[45,</ref><ref type="bibr">3]</ref>, low-dimensional linear subspace <ref type="bibr" target="#b20">[16]</ref>, or rigid/non-rigid segmentation <ref type="bibr" target="#b52">[48]</ref>. While these formulations are elegant and well-motivated, our method is much simpler and does not rely on any assumption of the data. Instead, our approach directly learns optical flow for a much wider range of challenging cases existing in the data.</p><p>Supervised Learning of Optical Flow. One promising direction is to learn optical flow with CNNs. FlowNet <ref type="bibr" target="#b14">[10]</ref> is the first end-to-end optical flow learning framework. It takes two consecutive images as input and outputs a dense flow map. The following work FlowNet 2.0 <ref type="bibr" target="#b19">[15]</ref> stacks several basic FlowNet models for iterative refinement, and significantly improves the accuracy. SpyNet <ref type="bibr" target="#b39">[35]</ref> proposes to warp images at multiple scales to cope with large displacements, resulting in a compact spatial pyramid network.  <ref type="figure">Figure 2</ref>. Our network architecture at each level (similar to PWC-Net <ref type="bibr" target="#b47">[43]</ref>).ẇ l denotes the initial coarse flow of level l andF l denotes the warped feature representation. At each level, we swap the initial flow and cost volume as input to estimate both forward and backward flow concurrently. Then these estimations are passed to layer l − 1 to estimate higher-resolution flow.</p><p>Recently, PWC-Net <ref type="bibr" target="#b47">[43]</ref> and LiteFlowNet <ref type="bibr" target="#b18">[14]</ref> propose to warp features extracted from CNNs and achieve state-ofthe-art results with lightweight framework. However, obtaining high accuracy with these CNNs requires pre-training on multiple synthetic datasets and follows specific training schedules <ref type="bibr" target="#b14">[10,</ref><ref type="bibr" target="#b32">28]</ref>. In this paper, we reduce the reliance on pre-training with synthetic data, and propose an effective self-supervised training method with unlabeled data. Unsupervised Learning of Optical Flow. Another interesting line of work is unsupervised optical flow learning. The basic principles are based on brightness constancy and spatial smoothness <ref type="bibr" target="#b24">[20,</ref><ref type="bibr" target="#b41">37]</ref>. This leads to the most popular photometric loss, which measures the difference between the reference image and the warped image. Unfortunately, this loss does not hold for occluded pixels. Recent studies propose to first obtain an occlusion map and then exclude those occluded pixels when computing the photometric difference <ref type="bibr" target="#b33">[29,</ref><ref type="bibr" target="#b50">46]</ref>. Janai et al. <ref type="bibr" target="#b22">[18]</ref> introduces to estimate optical flow with a multi-frame formulation and more advanced occlusion reasoning, achieving state-of-the-art unsupervised results. Very recently, DDFlow <ref type="bibr" target="#b30">[26]</ref> proposes a data distillation approach to learning the optical flow of occluded pixels, which works particularly well for pixels near image boundaries. Nonetheless, all these unsupervised learning methods only handle specific cases of occluded pixels. They lack the ability to reason about the optical flow of all possible occluded pixels. In this work, we address this issue by a superpixel-based occlusion hallucination technique. Self-Supervised Learning. Our work is closely related to the family of self-supervised learning methods, where the supervision signal is purely generated from the data itself. It is widely used for learning feature representations from unlabeled data <ref type="bibr" target="#b25">[21]</ref>. A pretext task is usually employed, such as image inpainting <ref type="bibr" target="#b38">[34]</ref>, image colorization <ref type="bibr" target="#b28">[24]</ref>, solving Forwardbackward consistency check <ref type="figure">Figure 3</ref>. Data flow for self-training with multiple-frame. To estimate occlusion map for three-frame flow learning, we use five images as input. This way, we can conduct a forward-backward consistency check to estimate occlusion maps between It and It+1, between It and It−1 respectively.</p><formula xml:id="formula_1">t−2 &amp; −1 &amp; t−1 &amp; &amp; +1 Model t &amp; +1 &amp; +2 −1→ −2 −1→ → −1 → +1 +1→ +1→ +2 −1→ → −1 → +1 +1→</formula><p>Jigsaw puzzles <ref type="bibr" target="#b36">[32]</ref>. Pathak et al. <ref type="bibr" target="#b37">[33]</ref> propose to explore low-level motion-based cues to learn feature representations without manual supervision. Doersch et al. <ref type="bibr" target="#b13">[9]</ref> combine multiple self-supervised learning tasks to train a single visual representation. In this paper, we make use of the domain knowledge of optical flow, and take reliable predictions of non-occluded pixels as the self-supervision signal to guide our optical flow learning of occluded pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present our self-supervised approach to learning optical flow from unlabeled data. To this end, we train two CNNs (NOC-Model and OCC-Model) with the same network architecture. The former focuses on accurate flow estimation for non-occluded pixels, and the latter learns to predict optical flow for all pixels. We distill reliable non-occluded flow estimations from NOC-Model to guide the learning of OCC-Model for those occluded pixels. Only OCC-Model is needed at testing. We build our network based on PWC-Net <ref type="bibr" target="#b47">[43]</ref> and further extend it to multi-frame optical flow estimation ( <ref type="figure">Figure 2</ref>). Before describing our approach in detail, we first define our notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>Given three consecutive RGB images I t−1 , I t , I t+1 , our goal is to estimate the forward optical flow from I t to I t+1 . Let w i→j denote the flow from I i to I j , e.g., w t→t+1 denotes the forward flow from I t to I t+1 , w t→t−1 denotes the backward flow from I t to I t−1 . After obtaining optical flow, we can backward warp the target image to reconstruct the reference image using Spatial Transformer Network <ref type="bibr" target="#b21">[17,</ref><ref type="bibr" target="#b50">46]</ref>. Here, we use I w j→i to denote warping I j to I i with flow w i→j . Similarly, we use O i→j to denote the occlusion map from I i to I j , where value 1 means the pixel in I i is not visible in I j .</p><p>In our self-supervised setting, we create the new target image I t+1 by injecting random noise on superpixels for occlusion generation. We can inject noise to any of three consecutive frames and even multiple of them as shown in <ref type="figure">Figure 1</ref>. For brevity, here we choose I t+1 as an example.  we set = 0.01, q = 0.4. For NOC-Model, only L p is employed.</p><p>For OCC-Model, we also estimate the optical flow of occluded pixels. To this end, a self-supervision loss for oc-cluded pixels L o is proposed. L o is only employed to those hand-crafted occluded pixels <ref type="figure">(Figure 1(h)</ref>). We use a selfsupervision mask M to represent these pixels and they can 6 If we let I t−1 , I t and I t+1 as input, then w, O, I w represent the generated optical flow, occlusion map and warped image respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CNNs for Multi-Frame Flow Estimation</head><p>In principle, our method can utilize any CNNs. In our implementation, we build on top of the seminar PWC-Net <ref type="bibr" target="#b47">[43]</ref>. PWC-Net employs pyramidal processing to increase the flow resolution in a coarse-to-fine manner and utilizes feature warping, cost volume construction to estimate optical flow at each level. Based on these principles, it has achieved state-of-the-art performance with a compact model size.</p><p>As shown in <ref type="figure">Figure 2</ref>, our three-frame flow estimation network structure is built upon two-frame PWC-Net with several modifications to aggregate temporal information. First, our network takes three images as input, thus produces three feature representations F t−1 , F t and F t+1 . Second, apart from forward flow w t→t+1 and forward cost volume, out model also computes backward flow w t→t−1 and backward cost volume at each level simultaneously. Note that when estimating forward flow, we also utilize the initial backward flow and backward cost volume information. This is because past frame I t−1 can provide very valuable information, especially for those regions that are occluded in the future frame I t+1 but not occluded in I t−1 . Our network combines all this information together and therefore estimates optical flow more accurately. Third, we stack initial forward flowẇ l t→t+1 , minus initial backward flow −ẇ l t+1→t , feature of reference image F l t , forward cost volume and backward cost volume to estimate the forward flow at each level. For backward flow, we just swap the flow and cost volume as input. Forward and backward flow estimation networks share the same network structure and weights. For initial flow at each level, we upscale optical flow of the next level both in resolution and magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Occlusion Estimation</head><p>For two-frame optical flow estimation, we can swap two images as input to generate forward and backward flow, then the occlusion map can be generated based on the forward-backward consistency prior <ref type="bibr" target="#b48">[44,</ref><ref type="bibr" target="#b33">29]</ref>. To make this work under our three-frame setting, we propose to utilize the adjacent five frame images as input as shown in <ref type="figure">Figure 3</ref>. Specifically, we estimate bi-directional flows between I t and I t+1 , namely w t→t+1 and w t+1→t . Similarly, we also estimate the flows between I t and I t−1 . Finally, we conduct a forward and backward consistency check to reason the occlusion map between two consecutive images.</p><p>For forward-backward consistency check, we consider one pixel as occluded when the mismatch between the forward flow and the reversed forward flow is too large. Take O t→t+1 as an example, we can first compute the reversed forward flow as follows,</p><formula xml:id="formula_2">w t→t+1 = w t+1→t (p + w t→t+1 (p)),<label>(1)</label></formula><p>A pixel is considered occluded whenever it violates the following constraint:</p><formula xml:id="formula_3">|w t→t+1 +ŵ t→t+1 | 2 &lt; α 1 (|w t→t+1 | 2 + |ŵ t→t+1 | 2 ) + α 2 ,<label>(2)</label></formula><p>where we set α 1 = 0.01, α 2 = 0.05 for all our experiments. Other occlusion maps are computed in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Occlusion Hallucination</head><p>During our self-supervised training, we hallucinate occlusions by perturbing local regions with random noise. In a newly generated target image, the pixels corresponding to noise regions automatically become occluded. There are many ways to generate such occlusions. straightforward way is to randomly select rectangle regions. However, rectangle occlusions rarely exist in real-world sequences. To address this issue, we propose to first generate superpixels <ref type="bibr">[1]</ref>, then randomly select several superpixels and fill them with noise. There are two main advantages of using superpixel. First, the shape of a superpixel is usually random and superpixel edges are often part of object boundaries. The is consistent with the real-world cases and makes the noise image more realistic. We can choose several superpixels which locate at different locations to cover more occlusion cases. Second, the pixels within each superpixel usually belong to the same object or have similar flow fields. Prior work has found low-level segmentation is helpful for optical flow estimation <ref type="bibr" target="#b53">[49]</ref>. Note that the random noise should lie in the pixel value range. <ref type="figure">Figure 1</ref> shows a simple example, where only the dog extracted from the COCO dataset <ref type="bibr" target="#b29">[25]</ref> is moving. Initially, the occlusion map between I t and I t+1 is (g). After randomly selecting several superpixels from (e) to inject noise, the occlusion map between I t and I t+1 change to (h). Next, we describe how to make use of these occlusion maps to guide our self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">NOC-to-OCC as Self-Supervision</head><p>Our self-training idea is built on top of the classical photometric loss <ref type="bibr" target="#b33">[29,</ref><ref type="bibr" target="#b50">46,</ref><ref type="bibr" target="#b22">18]</ref>, which is highly effective for nonoccluded pixels. <ref type="figure">Figure 1</ref> illustrates our main idea. Suppose pixel p 1 in image I t is not occluded in I t+1 , and pixel p 1 is its corresponding pixel. If we inject noise to I t+1 and let I t−1 , I t , I t+1 as input, p 1 then becomes occluded. Good news is we can still use the flow estimation of NOC-Model as annotations to guide OCC-Model to learn the flow of p 1 from I t to I t+1 . This is also consistent with real-world occlusions, where the flow of occluded pixels can be estimated based on surrounding non-occluded pixels. In the example of <ref type="figure">Figure 1</ref>, self-supervision is only employed to (i), which represents those pixels non-occluded from I t to I t+1 but become occluded from I t to I t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Functions</head><p>Similar to previous unsupervised methods, we first apply photometric loss L p to non-occluded pixels. Photometric  on these principles, our method significantly outperforms all existing unsupervised optical flow learning methods. After fine-tuning, we also achieve state-of-the-art supervised learning performance. Our results show that unsupervised pre-training is a promissing direction to achieve better or comparable supervised learning performance compared with pre-training on synthetic labeled datasets.   loss is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><formula xml:id="formula_4">L p = i,j ψ(I i − I w j→i ) (1 − O i ) (1 − O i )<label>(3)</label></formula><p>where ψ(x) = (|x|+ ) q is a robust loss function, denotes the element-wise multiplication. We set = 0.01, q = 0.4 for all our experiments. Only L p is necessary to train the NOC-Model.</p><p>To train our OCC-Model to estimate optical flow of occluded pixels, we define a self-supervision loss L o for those synthetic occluded pixels <ref type="figure">(Figure 1(i)</ref>). First, we compute a self-supervision mask M to represent these pixels,</p><formula xml:id="formula_5">M i→j = clip( O i→j − O i→j , 0, 1)<label>(4)</label></formula><p>Then, we define our self-supervision loss L o as,</p><formula xml:id="formula_6">L o = i,j ψ(w i→j − w i→j ) M i→j M i→j<label>(5)</label></formula><p>For our OCC-Model, we train with a simple combination of L p + L o for both non-occluded pixels and occluded pixels. Note our loss functions do not rely on spatial and temporal consistent assumptions, and they can be used for both classical two-frame flow estimation and multi-frame flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Supervised Fine-tuning</head><p>After pre-training on raw dataset, we use real-world annotated data for fine-tuning. Since there are only annotations for forward flow w t→t+1 , we skip backward flow estimation when computing our loss. Suppose that the ground truth flow is w gt t→t+1 , and mask V denotes whether the pixel has a label, where value 1 means that the pixel has a valid ground truth flow. Then we can obtain the supervised finetuning loss as follows,</p><formula xml:id="formula_7">L s = (ψ(w gt t→t+1 − w t→t+1 ) V )/ V (6)</formula><p>During fine-tuning, We first initialize the model with the pre-trained OCC-Model on each dataset, then optimize it using L s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate and compare our methods with stateof-the-art unsupervised and supervised learning methods on public optical flow benchmarks including MPI Sintel <ref type="bibr" target="#b11">[7]</ref>, KITTI 2012 <ref type="bibr" target="#b15">[11]</ref> and KITTI 2015 <ref type="bibr" target="#b34">[30]</ref>. To ensure reproducibility and advance further innovations, we make our code and models publicly available at https://github.com/ppliuboy/SelFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Data Preprocessing. For Sintel, we download the Sintel movie and extract ∼ 10, 000 images for self-training. We first train our model on this raw data, then add the official Sintel training data (including both "final" and "clean" versions). For KITTI 2012 and KITTI 2015, we use multi-view extensions of the two datasets for unsupervised pre-training, similar to <ref type="bibr" target="#b41">[37,</ref><ref type="bibr" target="#b50">46]</ref>. During training, we exclude the image pairs with ground truth flow and their neighboring frames (frame number 9-12) to avoid the mixture of training and testing data.   on these principles, our method significantly outperforms all existing unsupervised optical flow learning methods. After fine-tuning, we also achieve state-of-the-art supervised learning performance. Our results show that unsupervised pre-training is a promissing direction to achieve better or comparable supervised learning performance compared with pre-training on synthetic labeled datasets.  We rescale the pixel value from [0, 255] to [0, 1] for unsupervised training, while normalizing each channel to be standard normal distribution for supervised fine-tuning. This is because normalizing image as input is more robust for luminance changing, which is especially helpful for optical flow estimation. For unsupervised training, we apply Census Transform <ref type="bibr" target="#b55">[51]</ref> to images, which has been proved robust for optical flow estimation <ref type="bibr" target="#b16">[12,</ref><ref type="bibr" target="#b33">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>Training procedure. We train our model with the Adam optimizer <ref type="bibr" target="#b27">[23]</ref> and set batch size to be 4 for all experiments. For unsupervised training, we set the initial learning rate to be 10 −4 , decay it by half every 50k iterations, and use random cropping, random flipping, random channel swapping during data augmentation. For supervised fine-tuning, we employ similar data augmentation and learning rate schedule as <ref type="bibr" target="#b14">[10,</ref><ref type="bibr" target="#b19">15]</ref>.</p><p>For unsupervised pre-training, we first train our NOC-Model with photometric loss for 200k iterations. Then, we add our occlusion regularization and train for another 500k iterations. Finally, we initialize the OCC-Model with the trained weights of NOC-Model and train it with L p +L o for 500k iterations. Since training two models simultaneously will cost more memory and training time, we just generate the flow and occlusion maps using the NOC-Model in advance and use them as annotations (just like KITTI with sparse annotations).</p><p>For supervised fine-tuning, we use the pre-trained OCC-Model as initialization, and train the model using our supervised loss L s with 500k iterations for KITTI and 1, 000k iterations for Sintel. Note we do not require pre-training our model on any labeled synthetic dataset, hence we do not have to follow the specific training schedule (Fly-ingChairs <ref type="bibr" target="#b14">[10]</ref>→ FlyingThings3D <ref type="bibr" target="#b32">[28]</ref>) as <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b47">43]</ref>.</p><p>Evaluation Metrics. We consider two widely-used metrics to evaluate optical flow estimation: average endpoint error (EPE), percentage of erroneous pixels (Fl). EPE is the rank-ing metric on the Sintel benchmark, and Fl is the ranking metric on KITTI benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, we achieve state-of-the-art results for both unsupervised and supervised optical flow learning on all datasets under all evaluation metrics. <ref type="figure" target="#fig_1">Figure 4</ref> shows sample results from Sintel and KITTI. Our method estimates both accurate optical flow and occlusion maps.</p><p>Unsupervised Learning. Our method achieves the highest accuracy for unsupervised learning methods on leading benchmarks. On the Sintel final benchmark, we reduce the previous best EPE from 7.40 <ref type="bibr" target="#b30">[26]</ref> to 6.57, with 11.2% relative improvements. This is even better than several fully supervised methods including FlowNetS, FlowNetC <ref type="bibr" target="#b14">[10]</ref>, and SpyNet <ref type="bibr" target="#b39">[35]</ref>.</p><p>On the KITTI datasets, the improvement is more significant. For the training dataset, we achieve EPE=1.69 with 28.1% relative improvement on KITTI 2012 and EPE=4.84 with 15.3% relative improvement on KITTI 2015 compared with previous best unsupervised method DDFlow. On KITTI 2012 testing set, we achieve Fl-all=7.68%, which is better than state-of-the-art supervised methods including FlowNet2 <ref type="bibr" target="#b19">[15]</ref>, PWC-Net <ref type="bibr" target="#b47">[43]</ref>, ProFlow <ref type="bibr" target="#b31">[27]</ref>, and MFF <ref type="bibr" target="#b40">[36]</ref>. On KITTI 2015 testing benchmark, we achieve Fl-all 14.19%, better than all unsupervised methods. Our unsupervised results also outperform some fully supervised methods including DCFlow <ref type="bibr" target="#b53">[49]</ref> and ProFlow <ref type="bibr" target="#b31">[27]</ref>.</p><p>Supervised Fine-tuning. We further fine-tune our unsupervised model with the ground truth flow. We achieve stateof-the-art results on all three datasets, with Fl-all=6.19% on KITTI 2012 and Fl-all=8.42% on KITTI 2015. Most importantly, our method yields EPE=4.26 on the Sintel final dataset, achieving the highest accuracy on the Sintel benchmark among all submitted methods. All these show that our method reduces the reliance of pre-training with syn-  thetic datasets and we do not have to follow specific training schedules across different datasets anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To demonstrate the usefulness of individual technical steps, we conduct a rigorous ablation study and show the quantitative comparison in <ref type="table" target="#tab_6">Table 2</ref>. <ref type="figure" target="#fig_0">Figure 5</ref> and <ref type="figure" target="#fig_5">Figure 6</ref> show the qualitative comparison under different settings, where "W/O Occlusion" means occlusion handling is not considered, "W/O Self-Supervision" means occlusion handling is considered but self-supervision is not employed, "Rectangle" and "Superpixel" represent self-supervision is employed with rectangle and superpixel noise injection respectively. "Two-Frame Superpixel" means selfsupervision is conducted with only two frames as input. Two-Frame vs Multi-Frame. Comparing row 1 and row 2, row 3 and row 4 row 5 and row 7 in <ref type="table" target="#tab_6">Table 2</ref>, we can see that using multiple frames as input can indeed improve the performance, especially for occluded pixels. It is because multiple images provide more information, especially for those pixels occluded in one direction but non-occluded in the reverse direction. Occlusion Handling. Comparing the row 1 and row 3, row 2 and row 4 in <ref type="table" target="#tab_6">Table 2</ref>, we can see that occlusion handling can improve optical flow estimation performance over all pixels on all datasets. This is due to the fact that brightness constancy assumption does not hold for occluded pixels. Self-Supervision. We employ two strategies for our occlusion hallucination: rectangle and superpixel. Both strategies improve the performance significantly, especially for occluded pixels. Take superpixel setting as an example, EPE-OCC decrease from <ref type="bibr" target="#b30">26</ref> Such a big improvement demonstrates the effectiveness of our self-supervision strategy.</p><p>Comparing superpixel noise injection with rectangle noise injection, superpixel setting has several advantages. First, the shape of the superpixel is random and edges are more correlated to motion boundaries. Second, the pixels in the same superpixel usually have similar motion patterns. As a result, the superpixel setting achieves slightly better performance.</p><p>Self-Supervised Pre-training. <ref type="table" target="#tab_7">Table 3</ref> compares supervised results with and without our self-supervised pretraining on the validation sets. If we do not employ selfsupervised pre-training and directly train the model using only the ground truth, the model fails to converge well due to insufficient training data. However, after utilizing our self-supervised pre-training, it converges very quickly and achieves much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a self-supervised approach to learning accurate optical flow estimation. Our method injects noise into superpixels to create occlusions, and let one model guide the another to learn optical flow for occluded pixels. Our simple CNN effectively aggregates temporal information from multiple frames to improve flow prediction. Extensive experiments show our method significantly outperforms all existing unsupervised optical flow learning methods. After fine-tuning with our unsupervised model, our method achieves state-of-the-art flow estimation accuracy on all leading benchmarks. Our results demonstrate it is possible to completely reduce the reliance of pre-training on synthetic labeled datasets, and achieve superior performance by self-supervised pre-training on unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund). We thank anonymous reviewers for their constructive suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview</head><p>In this supplement, we first show occlusion estimation performance of SelFlow. Then we present screenshots (Nov. 23, 2018) of our submission on the public benchmarks, including MPI Sintel final pass, KITTI 2012, and KITTI 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Occlusion Estimation</head><p>Following <ref type="bibr" target="#b50">[46,</ref><ref type="bibr" target="#b22">18,</ref><ref type="bibr" target="#b30">26]</ref>, we also report the occlusion estimation performance using F-measure, which is the harmonic mean of precision and recall. We estimate occlusion map using forward-backward consistency check (no parameters to learn).</p><p>We compare our occlusion estimation performance with MODOF <ref type="bibr" target="#b54">[50]</ref>, OccAwareFlow <ref type="bibr" target="#b50">[46]</ref>, MultiFrameOccFlow-Soft <ref type="bibr" target="#b22">[18]</ref> and DDFlow. Note KITTI datasets only have sparse occlusion maps. As shown in <ref type="table" target="#tab_1">Table 1</ref>  <ref type="figure">Figure 1</ref> shows the screenshot of our submission on the MPI Sintel benchmark. Our unsupervised entry (CVPR-236) outperforms all the exiting unsupervised learning method, even outperforming supervised methods including FlowNetS+ft+v, FlowNetC+ft+v and SpyNet+ft. At the time of writing, our supervised fine-tuned entry (CVPR-236+ft) is the No. 1 among all submitted methods. In addition to the main ranking metric EPE-all, our method also achieves the best performance on EPE-matched, d10-60, s0-10, s10-40, and very competitive results on remaining metrics. This clearly demonstrates the effectiveness of our method. <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref> show the screenshots of KITTI 2012 and KITTI 2015 benchmark. Again, our unsupervised entry (CVPR-236) outperforms all the exiting unsupervised learning method on both benchmarks. On KITTI 2012, our unsupervised entry (CVPR-236) even outperforms the most recent fully supervised methods including ProFlow, ImpPB+SPCI, Flow-FieldCNN, IntrpNt-df. Our supervised fine-tuned entry (CVPR-236+ft) is the second best compared to published monocular optical flow estimation methods (only second to LiteFlowNet), while achieving better Out-All and Ave-All. On KITTI 2015, our unsupervised entry (CVPR-236) also outperforms several recent supervised methods including DCFlow, ProFlow, Flow-Fields++ and FlowFieldCNN. Our supervised fine-tuned entry (CVPR-236+ft) is the third best compared to published monocular optical flow estimation methods, only behind the concurrent work MFF, and the extended version of PWC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Screenshots on Benchmarks</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Sample unsupervised results on Sintel and KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Sample unsupervised results on Sintel and KITTI dataset. From top to bottom, we show samples from Sintel Final, KITTI 2012 and KITTI 2015. Our model can estimate both accurate flow and occlusion map. Note that on KITTI datasets, the occlusion maps are sparse, which only contain pixels moving out of the image boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling, multi-frame formulation and self-supervision consistently improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[ 5 ] 9 Figure 5 .</head><label>595</label><figDesc>Thomas Brox, Andrés Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on a Qualitative comparison of our model under different settings on Sintel Clean training and Sintel Final testing dataset. Occlusion handling, multi-frame formulation and self-supervision consistently improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison of our model under different settings on Sintel Clean training and Sintel Final testing dataset. Occlusion handling, multi-frame formulation and self-supervision consistently improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling, multi-frame formulation and self-supervision consistently improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This work is supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14210717 of the General Research Fund).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9 Figure 6 .</head><label>96</label><figDesc>Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling, multi-frame formulation and self-supervision consistently improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>From top to bottom, we show samples from Sintel Final, KITTI 2012 and KITTI 2015. Our model can estimate both accurate flow and occlusion map. Note that on KITTI datasets, the occlusion maps are sparse, which only contain pixels moving out of the image boundary. Comparison with state-of-the-art learning based optical flow estimation methods. Our method outperforms all unsupervised optical flow learning approaches on all datasets. Our supervised fine-tuned model achieves the highest accuracy on the Sintel Final dataset and KITTI 2012 dataset. All numbers are EPE except for the last column of KITTI 2012 and KITTI 2015 testing sets, where we report percentage of erroneous pixels over all pixels (Fl-all). Missing entries (-) indicate that the results are not reported for the respective method. Parentheses mean that the training and testing are performed on the same dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Sintel Clean</cell><cell cols="2">Sintel Final</cell><cell></cell><cell cols="2">KITTI 2012</cell><cell cols="2">KITTI 2015</cell></row><row><cell></cell><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell cols="2">test test(Fl)</cell><cell>train</cell><cell>test(Fl)</cell></row><row><cell></cell><cell>BackToBasic+ft [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.3</cell><cell>9.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Unsupervised</cell><cell cols="5">DSTFlow+ft [37] UnFlow-CSS [29] OccAwareFlow+ft [46] MultiFrameOccFlow-None+ft [18] (6.05) -(4.03) 7.95 (5.95) 9.15 -(7.91) 10.22 -(7.09) -MultiFrameOccFlow-Soft+ft [18] (3.89) 7.23 (5.52) 8.81</cell><cell>3.29 3.55 --</cell><cell>4 4.2 ---</cell><cell>-----</cell><cell>16.79 8.10 8.88 6.65 6.59</cell><cell>39% 23.30% 31.2% -22.94%</cell></row><row><cell></cell><cell>DDFlow+ft [26]</cell><cell cols="2">(2.92) 6.18</cell><cell>3.98</cell><cell>7.40</cell><cell>2.35</cell><cell>3.0</cell><cell>8.86%</cell><cell>5.72</cell><cell>14.29%</cell></row><row><cell></cell><cell>Ours</cell><cell cols="4">(2.96) 6.56 (3.87) 6.57</cell><cell>1.69</cell><cell>2.2</cell><cell>7.68%</cell><cell>4.84</cell><cell>14.19%</cell></row><row><cell></cell><cell>FlowNetS+ft [10]</cell><cell cols="4">(3.66) 6.96 (4.44) 7.76</cell><cell>7.52</cell><cell cols="2">9.1 44.49%</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FlowNetC+ft [10]</cell><cell cols="4">(3.78) 6.85 (5.28) 8.51</cell><cell>8.79</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SpyNet+ft [35]</cell><cell cols="4">(3.17) 6.64 (4.32) 8.36</cell><cell>8.25</cell><cell cols="2">10.1 20.97%</cell><cell>-</cell><cell>35.07%</cell></row><row><cell></cell><cell>FlowFieldsCNN+ft [2]</cell><cell>-</cell><cell>3.78</cell><cell>-</cell><cell>5.36</cell><cell>-</cell><cell cols="2">3.0 13.01%</cell><cell>-</cell><cell>18.68 %</cell></row><row><cell></cell><cell>DCFlow+ft [49]</cell><cell>-</cell><cell>3.54</cell><cell>-</cell><cell>5.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.83%</cell></row><row><cell>Supervised</cell><cell>FlowNet2+ft [15] UnFlow-CSS+ft [29] LiteFlowNet+ft-CVPR [14] LiteFlowNet+ft-axXiv [14] PWC-Net+ft-CVPR [43]</cell><cell cols="6">(1.45) 4.16 (2.01) 5.74 (1.28) 1.8 ----(1.14) 1.7 (1.64) 4.86 (2.23) 6.09 (1.26) 1.7 (1.35) 4.54 (1.78) 5.38 (1.05) 1.6 (2.02) 4.39 (2.08) 5.04 (1.45) 1.7</cell><cell cols="3">-8.42% (1.86) 11.11% (2.3) 11.48% -(2.16) 10.24% 7.27% (1.62) 9.38% 8.10% (2.16) 9.60%</cell></row><row><cell></cell><cell>PWC-Net+ft-axXiv [42]</cell><cell cols="6">(1.71) 3.45 (2.34) 4.60 (1.08) 1.5</cell><cell cols="2">6.82% (1.45)</cell><cell>7.90%</cell></row><row><cell></cell><cell>ProFlow+ft [27]</cell><cell cols="2">(1.78) 2.82</cell><cell>-</cell><cell cols="3">5.02 (1.89) 2.1</cell><cell cols="3">7.88% (5.22) 15.04%</cell></row><row><cell></cell><cell>ContinualFlow+ft [31]</cell><cell>-</cell><cell>3.34</cell><cell>-</cell><cell>4.52</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.03%</cell></row><row><cell></cell><cell>MFF+ft [36]</cell><cell>-</cell><cell>3.42</cell><cell>-</cell><cell>4.57</cell><cell>-</cell><cell>1.7</cell><cell>7.87%</cell><cell>-</cell><cell>7.17%</cell></row><row><cell></cell><cell>Ours+ft</cell><cell cols="6">(1.68) 3.74 (1.77) 4.26 (0.76) 1.5</cell><cell cols="2">6.19% (1.18)</cell><cell>8.42%</cell></row></table><note>(6.16) 10.41 (6.81) 11.27 10.43 12.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>.85) (1.53) (33.48) (5.28) (2.81) (36.83) 7.05 1.31 45.03 13.51 3.71 75.51 (3.67) (1.54) (30.80) (4.98) (2.68) (34.42) 6.52 1.11 42.44 12.13 3.47 66.91 (3.35) (1.37) (28.70) (4.50) (2.37) (31.81) 4.96 0.99 31.29 8.99 3.20 45.68 (3.20) (1.35) (26.63) (4.33) (2.32) (29.80) 3.32 0.94 19.117.66   Ablation study. We report EPE of our unsupervised results under different settings over all pixels (ALL), non-occluded pixels (NOC) and occluded pixels (OCC). Note that we employ Census Transform when computing photometric loss by default. Without Census Transform, the performance will drop.</figDesc><table><row><cell cols="5">Occlusion Multiple Self-Supervision Self-Supervision</cell><cell>Sintel Clean</cell><cell></cell><cell></cell><cell>Sintel Final</cell><cell></cell><cell cols="2">KITTI 2012</cell><cell>KITTI 2015</cell></row><row><cell>Handling</cell><cell>Frame</cell><cell>Rectangle</cell><cell>Superpixel</cell><cell>ALL</cell><cell>NOC</cell><cell>OCC</cell><cell>ALL</cell><cell>NOC</cell><cell>OCC</cell><cell cols="2">ALL NOC OCC ALL NOC OCC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(32.47 40.99</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(2.96) (1.33) (23.78) (4.06) (2.25) (27.19) 1.97 0.92</cell><cell>8.96</cell><cell>5.85</cell><cell>2.96 24.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(2.91) (1.37) (22.58) (3.99) (2.27) (26.01) 1.78 0.96</cell><cell>7.47</cell><cell>5.01</cell><cell>2.55 21.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(2.88) (1.30) (22.06) (3.87) (2.24) (25.42) 1.69 0.91</cell><cell>6.95</cell><cell>4.84</cell><cell>2.40 19.68</cell></row><row><cell cols="6">Unsupervised Pre-training Sintel Clean Sintel Final KITTI 2012 KITTI 2015</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Without</cell><cell>1.97</cell><cell>2.68</cell><cell>3.93</cell><cell>3.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>With</cell><cell>1.50</cell><cell>2.41</cell><cell>1.55</cell><cell>1.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Ablation study. We report EPE of supervised fine-tuning results on our validation datasets with and without unsupervised pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.63 to 22.06 on Sintel Clean, from 29.80 to 25.42 on Sintel Final, from 19.11 to 6.95 on KITTI 2012, and from 40.99 to 19.68 on KITTI 2015.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 1 .</head><label>1</label><figDesc>, we achieve the best occlusion estimation performance on Sintel Clean and Sintel Final, and comparable performance on KITTI 2012 and 2015. Comparison of occlusion estimation with F-measure. * marks cases where the occlusion annotation is sparse.</figDesc><table><row><cell>Method</cell><cell cols="4">Sintel Sintel KITTI KITTI Clean Final 2012 2015</cell></row><row><cell>MODOF</cell><cell>-</cell><cell>0.48</cell><cell>-</cell><cell>-</cell></row><row><cell>OccAwareFlow</cell><cell cols="3">(0.54) (0.48) 0.95  *</cell><cell>0.88  *</cell></row><row><cell cols="3">MultiFrameOccFlow-Soft (0.49) (0.44)</cell><cell>-</cell><cell>0.91  *</cell></row><row><cell>DDFlow</cell><cell cols="4">(0.59) (0.52) 0.94  *  0.86  *</cell></row><row><cell>Ours</cell><cell cols="3">(0.59) (0.52) 0.95  *</cell><cell>0.88</cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Figure 1. Screenshot of the Sintel benchmark on November 23th, 2018.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 2. Screenshot of the KITTI 2012 benchmark on November 23th, 2018.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 3. Screenshot of the KITTI 2015 benchmark on November 23th, 2018.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnnbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmanabhan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<idno>196:1- 196:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Andrés Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on a References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cnnbased patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmanabhan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<idno>196:1- 196:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moving object tracking using gaussian mixture model and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Krishan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Research in Computer Science and Software Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why is the census transform good for robust optic flow computation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Demetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-frame optical flow estimation using subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Yu Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06162</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="364" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ddflow: Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Proflow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Continual occlusions and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ochman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A fusion approach for multi-frame optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Layered image motion with explicit occlusions, temporal consistency, and depth ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05571</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modeling temporal coherence for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Accurate Optical Flow via Direct Cost Volume Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
