<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ISTR: End-to-End Instance Segmentation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<addrLine>2 Pinterest</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<addrLine>2 Pinterest</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<addrLine>2 Pinterest</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<addrLine>2 Pinterest</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<addrLine>2 Pinterest</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ISTR: End-to-End Instance Segmentation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code has been made available at: https://github. com/hujiecpp/ISTR.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end paradigms significantly improve the accuracy of various deep-learning-based computer vision models. To this end, tasks like object detection have been upgraded by replacing non-end-to-end components, such as removing non-maximum suppression by training with a set loss based on bipartite matching. However, such an upgrade is not applicable to instance segmentation, due to its significantly higher output dimensions compared to object detection. In this paper, we propose an instance segmentation Transformer, termed ISTR, which is the first end-to-end framework of its kind. ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss. Besides, ISTR concurrently conducts detection and segmentation with a recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the existing top-down and bottom-up frameworks. Benefiting from the proposed endto-end mechanism, ISTR demonstrates state-of-the-art performance even with approximation-based suboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using ResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO dataset. Quantitative and qualitative results reveal the promising potential of ISTR as a solid baseline for instance-level recognition. Code has been made available at: https://github. com/hujiecpp/ISTR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A growing trend in the recent development of computer vision is to remove the handcrafted components to enable end-to-end training and inference, which has demonstrated significant improvement in multiple fields. However, this end-to-end paradigm still lacks applications for instance segmentation that aims to jointly detect and segment each object in an image. Existing instance segmentation approaches either need a manually-designed post-processing step called non-maximum suppression (NMS) to remove duplicate predictions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>, or are early trials on small datasets and lack evaluation against modern baselines <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>. Popular approaches also rely on a topdown or bottom-up framework that decomposes instance segmentation into several dependent tasks, preventing them from being end-to-end.</p><p>Besides instance segmentation, object detection also faces similar challenges. Recent studies enable end-to-end object detection by introducing a set prediction loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56]</ref>, with optional use of Transformers <ref type="bibr" target="#b41">[42]</ref>. The set prediction loss enforces bipartite matching between labels and predictions to penalize redundant outputs, thus avoiding NMS during inference. However, enabling end-to-end instance segmentation is not as trivial as adding a mask branch and changing the loss. We conducted a proof-ofconcept experiment by adapting the end-to-end object detection approach to instance segmentation. The results in <ref type="table" target="#tab_4">Table 1a</ref> show the inferior performance of doing so.</p><p>We argue that the reason behind the failure is the insufficient number of samples for learning the mask head. On the one hand, the dimensions of masks are much higher than those of classes and boxes. For example, a mask usually has a 28 Ã— 28 or higher resolution on the COCO dataset, while a bounding box only needs two coordinates to represent. Therefore, the mask head requires more samples for training. On the other hand, the proposal bounding boxes obtained by the bipartite matching are usually on a small scale, which also raises the problem of sparse training samples. For example, Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> uses 512 proposal bounding boxes to extract the region of interest (RoI) features for training the mask head, while the number of proposal bounding boxes, i.e., ground truths per image on the COCO dataset, is only 7.7 on average after bipartite matching. The gap between the demand and supply of training samples makes the approach prone to failing.</p><p>While we could blindly augment the ground truth samples to alleviate the problem at the cost of longer training time, we argue that there might be a smarter way. <ref type="bibr">Not</ref>   <ref type="figure">Figure 1</ref>: Component analysis of masks, ranking the Top@100 components by energy. The majority of the mask information is embedded into the first few components.</p><p>28 Ã— 28 entries are likely to appear as a mask. The distribution of the natural masks may lie in a low-dimensional manifold instead of being uniformly scattered. Based on this intuition, we carried out several dimension reduction experiments on the masks from the training data, and surprisingly found even linear methods, such as principal component analysis (PCA), can do a decent job. The energy distribution of different components is shown in <ref type="figure">Fig. 1</ref>. We observe that the first few components can represent the majority of the mask information. Therefore, in this paper, we propose to achieve end-to-end instance segmentation by regressing low-dimensional embeddings instead of raw masks, which enables the training to be effectively conducted with a small number of matched samples. We also extend the definition of bipartite matching cost based on the mask embeddings. Furthermore, regressing with the embeddings enables us to design a recurrent refinement strategy that can process detection and segmentation concurrently. This provides a new way of instance segmentation compared to the top-down and bottom-up frameworks, and boosts the performance. Specifically, we propose a new end-to-end instance segmentation framework built upon a Transformer, termed ISTR. ISTR predicts low-dimensional mask embeddings, and then matches them with ground truth mask embeddings for the set loss. With the recurrent refinement strategy, ISTR updates the query boxes and refines the set of predictions. Benefiting from the proposed end-to-end mechanism, we find that even with the suboptimal mask embeddings obtained by the closed-form solution of PCA, ISTR can achieve state-of-the-art performance. With a single 1080Ti GPU, ISTR obtains a 46.8/38.6 box/mask AP with 13.8 fps using ResNet50-FPN, and a 48.1/39.9 box/mask AP with 11.0 fps using ResNet101-FPN on the test-dev split of the COCO dataset <ref type="bibr" target="#b25">[26]</ref>. Our contributions are summarized as follows:</p><p>â€¢ We propose a new framework, termed instance segmentation Transformer (ISTR). For the first time, we demonstrate the potential of using Transformers in end-to-end instance segmentation.</p><p>â€¢ ISTR predicts low-dimensional mask embeddings instead of high-dimensional masks, which facilitates the training with a small number of samples and inspires the design of a bipartite matching cost for masks.</p><p>â€¢ With a recurrent refinement strategy, ISTR concurrently detects and segments instances, providing a new perspective for achieving instance segmentation compared to the bottom-up and top-down frameworks.</p><p>â€¢ Without bells and whistles, ISTR demonstrates accuracy and run-time performance on par with the stateof-the-art methods on the challenging COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instance Segmentation: Instance segmentation requires instance-level and pixel-level predictions. Existing works can be summarized into three categories. Top-down methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> first detect and then segment the objects. Bottom-up methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref> view instance segmentation as a label-then-cluster problem, learning to classify each pixel and then clustering them into groups for each object. The latest work, SOLO <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>, deals with instance segmentation without dependence on box detection. The proposed ISTR provides a new perspective that directly predicts a set of bounding boxes and mask embeddings, which avoids decomposing instance segmentation into dependent tasks. Note that the idea of regressing mask embeddings is also investigated in MEInst <ref type="bibr" target="#b52">[53]</ref>. However, with redundant predictions in each pixel, MEInst obtains suboptimal performance compared to ISTR. End-to-End Instance-Level Recognition: Recent studies have revealed the great potential of end-to-end object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b54">55]</ref>. As such, the bipartite matching cost has become an essential component for achieving end-to-end object detection. For instance segmentation, the works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> explored the end-to-end mechanism with recurrent neural networks. However, these early trials were only evaluated on small datasets and not against current baselines. In contrast, ISTR uses the similarity metric of mask embeddings as the bipartite matching cost for masks, and, for the first time, incorporates Transformers <ref type="bibr" target="#b41">[42]</ref> to improve end-to-end instance segmentation.</p><p>Transformers in Computer Vision: The breakthroughs of Transformers <ref type="bibr" target="#b41">[42]</ref> in natural language processing have sparked great interest in the computer vision community. The critical component of Transformer is the multi-head attention, which can significantly enhance the capacity of models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. So far, Transformers have been successfully used for image recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref>, object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b36">37]</ref>, segmentation <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b50">51]</ref>, image superresolution <ref type="bibr" target="#b48">[49]</ref>, video understanding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13]</ref>, image generation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref>, visual question answering <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35]</ref>, and several other tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b49">50]</ref>. With the sequence information between frames, the contemporary work <ref type="bibr" target="#b46">[47]</ref> achieves end-toend video instance segmentation with a Transformer. With- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Embeddings</head><p>Boxes Classes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement Stages</head><p>RoI. out continuous frames, our study aims to segment instances for a single image, making its design entirely different from the framework of <ref type="bibr" target="#b46">[47]</ref>.</p><p>Multi-Task Learning: The benefit of learning detection and segmentation jointly was first studied in the work of <ref type="bibr" target="#b14">[15]</ref>. After that, Mask R-CNN <ref type="bibr" target="#b15">[16]</ref> also demonstrated that bounding box detection could benefit from multi-task learning. Recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2]</ref> have provided more complex mechanisms to improve the performance for multitasks. In our work, we also observe a performance boost when concurrently processing detection and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>ISTR aims to directly predict a set of mask embeddings, classes, and bounding boxes for each instance. To this end, we first introduce a generalized formulation to extract embeddings for representing and reconstructing the masks in Section 3.1. A bipartite matching cost and a set loss are introduced in Section 3.2 to pair and regress the predictions with ground truth labels. Finally, a model that predicts a set of outputs and learns their relations is proposed in Section 3.3. The overall framework of ISTR is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mask Embeddings</head><p>To provide a formulation that effectively extracts mask embeddings, we constrain the mutual information between the original and reconstructed masks:</p><formula xml:id="formula_0">max I M , f g(M ) ,<label>(1)</label></formula><p>where I(Â·, Â·) denotes the mutual information between two random variables, M denotes a set of masks {m i âˆˆ R s 2 |i = 1, ..., n}, s 2 is the dimension of masks, g(Â·) denotes the mask encoder for extracting embeddings and f (Â·) denotes the mask decoder for reconstructing masks. Eq. 1 guarantees that the encoding and decoding phases have minimal information loss, which implicitly encourages the embeddings to represent the masks. After derivation, we have a generalized objective function for the mask embeddings:</p><formula xml:id="formula_1">min n i=1 ||m i âˆ’ f (r i )|| 2 2 ,<label>(2)</label></formula><p>where r i = g m i denotes the mask embeddings, and ||Â·|| 2 is the L2-norm. By making the functions of the encoder and decoder simple linear transformations via a matrix D âˆˆ R s 2 Ã—l , i.e., f g(m i ) = DD T m i and DD T = I l , the objective function becomes:</p><formula xml:id="formula_2">D * = arg min D n i=1 ||m i âˆ’ DD T m i || 2 2 ,<label>(3)</label></formula><p>where l is the dimension of the mask embeddings, and I l denotes the l Ã— l unit matrix. Eq. 3 has the same formulation as the objective function of PCA, which provides a closed-form solution to learn the transformation. Note that the objective function in Eq. 2 can also be optimized by other models, such as an autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matching Cost and Prediction Loss</head><p>After obtaining the encoder and decoder for mask embeddings, we define a bipartite matching cost and a set prediction loss for end-to-end instance segmentation. Let us denote the ground truth bounding boxes, classes, and masks as Y = {b i , c i , m i |i = 1, ..., n}. The predicted bounding boxes, classes, and mask embeddings are denoted as</p><formula xml:id="formula_3">Y = { b i , c i , r i |i = 1, ..., k}, where k &gt; n.</formula><p>Bipartite Matching Cost: For the bipartite matching, we search for a permutation of n non-repeating integers Ïƒ âˆˆ {1, 2, ..., k} with the lowest cost, as:</p><formula xml:id="formula_4">Ïƒ * = arg min Ïƒ n i=1 C box (b i , b Ïƒ(i) ) +C cls (c i , c Ïƒ(i) ) + C mask (m i , r Ïƒ(i) ) .<label>(4)</label></formula><p>Inspired by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, we define the matching cost for bounding boxes as:</p><formula xml:id="formula_5">C box (b i , b Ïƒ(i) ) = Î» L1 Â· C L1 (b i , b Ïƒ(i) ) + Î» giou Â· C giou (b i , b Ïƒ(i) ),<label>(5)</label></formula><p>and the matching cost for classes as:</p><formula xml:id="formula_6">C cls (c i , c Ïƒ(i) ) = âˆ’Î» cls Â· p Ïƒ(i) (c i ),<label>(6)</label></formula><p>where Î» denotes the hyperparameters that balance the costs, C L1 (Â·, Â·) denotes the L1 cost, C giou (Â·, Â·) denotes the generalized IoU <ref type="bibr" target="#b31">[32]</ref> cost, and p Ïƒ(i) (c i ) is the probability of class c i . Instead of directly matching the high-dimensional masks, we use the similarity metric between mask embeddings to match them, which is defined as:</p><formula xml:id="formula_7">C mask (m i , r Ïƒ(i) ) = âˆ’ 1 2 Î» mask Â· &lt; r Ïƒ(i) || r Ïƒ(i) || 2 , g(m i ) ||g(m i )|| 2 &gt; +1 ,<label>(7)</label></formula><p>where the mask embeddings are L2 normalized, and the dot product between two normalized vectors is used to calculate the cosine similarity. We add 1 to the result and divide it by 2 to guarantee that the values are in the range of [0, 1]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Predict via ISTR encoder and heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Match predictions with labels via Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Calculate loss via Eq. 8 and train ISTR. Set Prediction Loss: For the set prediction loss, we use the matched predictions to regress the ground truth targets. The set prediction loss is defined as:</p><formula xml:id="formula_8">L set (Y , Y , Ïƒ * ) = 1 n n i=1 L box (b i , b Ïƒ * (i) ) +L cls (c i , c Ïƒ * (i) ) + L mask (m i , r Ïƒ * (i) ) ,<label>(8)</label></formula><p>where L box (Â·, Â·) is defined the same as C box (Â·, Â·), and L cls (Â·, Â·) is the focal loss <ref type="bibr" target="#b24">[25]</ref> for classification. For masks, we add the dice loss <ref type="bibr" target="#b29">[30]</ref> to improve the learned embeddings for reconstructing the masks. The mask loss is defined as:</p><formula xml:id="formula_9">L mask (m i , r Ïƒ * (i) ) =Î» mask Â· L L2 (g(m i ), r Ïƒ * (i) ) + L dice m i , f ( r Ïƒ * (i) ) ,<label>(9)</label></formula><p>where L L2 (Â·, Â·) denotes the L2 loss and L dice (Â·, Â·) denotes the dice loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance Segmentation Transformer</head><p>The architecture of ISTR is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. It contains four main components: a CNN backbone with FPN <ref type="bibr" target="#b23">[24]</ref> to extract features for each instance, a Transformer encoder with dynamic attention to learn the relations between objects, a set of prediction heads that conduct detection and segmentation concurrently, as well as the N -step recurrent update for refining the set of predictions.   <ref type="bibr" target="#b15">[16]</ref>. Image features P âˆˆ R kÃ—d are first extracted by averaging and summing the features from P2 to P5, and then expand the first dimension to k for each RoI feature. Learnable position embeddings E âˆˆ R kÃ—d are initialized randomly. Transformer Encoder and Dynamic Attention: The sum of image features and position embeddings is first transformed by three learnable weight matrices to obtain the inputs Q = (P + E)W Q , K = (P + E)W K , V = (P + E)W V for the self-attention module defined as:</p><formula xml:id="formula_10">Z = sof tmax( QK T âˆš d )V .<label>(10)</label></formula><p>The multi-head attention comprises multiple self-attention blocks, e.g., eight in the original Transformer <ref type="bibr" target="#b41">[42]</ref>, to encapsulate multiple complex relationships amongst different features. Inspired by <ref type="bibr" target="#b36">[37]</ref>, we add a dynamic attention module for better fusing the RoI and image features, which is defined as the attention conditioned on the RoI features U i in the i-th step:</p><formula xml:id="formula_11">O i = U i Â· f c(Z),<label>(11)</label></formula><p>where f c(Â·) denotes a fully connected layer to generate the dynamic parameters. The obtained features O i are then used in the prediction heads to produce the set of outputs.</p><p>Prediction Heads: The set of predictions is computed by the heads, including a class head, a box head, a mask head, and a fixed mask decoder. The box head predicts the residual values of normalized center coordinates, height, and width for updating the query boxes B i in the i-th step, and the class head predicts the classes using a softmax function. The mask head outputs the mask embeddings, which are then reconstructed to predict masks using the pre-learned mask decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Refinement Strategy:</head><p>The query boxes B i are recurrently updated by the predicted boxes, which refines the predictions and makes it possible to process the detection and segmentation concurrently. The overall process is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset and Evaluation Metrics: Our experiments are performed on the MS COCO dataset <ref type="bibr" target="#b25">[26]</ref>, which contains 123K images with 80-class instance labels. Our models are trained on the train2017 split (118K images), and the ablation study is carried out on the val2017 split  (5K images). Final results are reported on the test-dev split, which has no public labels and is evaluated on the server. We report the standard COCO metrics including AP (i.e., averaged over IoU thresholds), AP 50 , AP 75 , and AP S , AP M , AP L (i.e., AP at different scales) for both boxes and masks, denoted as AP b and AP m , respectively.</p><p>Training Details: We follow the strategy in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b38">39]</ref> to encode the ground truth mask embeddings. ResNet50 and ResNet101 <ref type="bibr" target="#b16">[17]</ref> pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> are used as our backbone networks. FPN <ref type="bibr" target="#b23">[24]</ref> is used to extract the feature pyramid. For the ablation study, all the models are trained over 12 epochs with learning rate decay, dividing by 10 at epoch 9 and 11, respectively. All results in the ablation study are tested with ResNet50-FPN and reported on the COCO val2017 split. The training schedule for the final models is 36 epochs with the learning rate divided by 10 at epoch 27 and 33, respectively. The mini-batch contains 16 images, and the models are trained with eight GPUs. Following <ref type="bibr" target="#b36">[37]</ref>, we use the AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer and an initial learning rate of 0.000025. The input images are resized such that the shortest side is at least 480 and at most 800 pixels, while the longest side is at most 1333. The number of predictions, i.e., k, is set to 300, and the number of self-attention blocks in the multi-head attention is set to 8. The number of recurrent refinement stages is set to 6.</p><p>Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56]</ref>, we set Î» cls , Î» L1 , and Î» giou to 2, 5, and 2, respectively. The hyperparameter Î» mask is set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Experiments</head><p>To analyze ISTR, we conduct ablation studies on the choices of mask embeddings, cost functions, loss functions, the effect of position embeddings, and pooling types. Results are shown in <ref type="table" target="#tab_4">Table 1</ref> and discussed in detail next.</p><p>Mask vs. Mask Embeddings: <ref type="table" target="#tab_4">Table 1a</ref> shows the models with various types of mask representations, including the original masks with 28 Ã— 28 dimensions and the mask embeddings with different dimensions l. We also expand the original masks to vectors to test the performance. Us-    <ref type="bibr" target="#b6">[7]</ref>.</p><p>ing raw masks for segmentation is implemented with the mask head from Mask R-CNN in a top-down manner, and other settings are the same as ISTR. We obtain the following results. First, predicting mask embeddings brings better performance to the mask APs than predicting masks. The results verify our concern that the high-dimensional masks cannot be effectively learned with a small number of matched samples. In contrast, the mask embeddings can be well regressed, as their dimensions are much lower than those of masks, e.g., 40, 60, 80 vs. 784. Second, the performance of mask embeddings improves when the dimension l=60 and saturates when the dimension l=80. Finally, regressing with the expanded masks, i.e., l=784, as embeddings has worse performance in mask APs.</p><p>Cost Functions: Appropriate cost functions can match high-quality predictions with ground truth labels for the set loss. In <ref type="table" target="#tab_4">Table 1b</ref>, we compare the performance with different choices of cost functions for the masks. Matching the predicted masks with ground truth masks by the dice loss does not produce expected gains compared to matching without the mask cost function. Using the L1 loss between embeddings slightly improves performance, and using the cosine similarity between embeddings as the mask cost function brings expected gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions:</head><p>We investigate two types of loss functions for ISTR: the dice loss at the pixel-level and the L2 loss at the embedding-level. The dice loss is calculated using the masks reconstructed by the mask decoder. As shown in <ref type="table" target="#tab_4">Table 1c</ref>, only calculating the dice loss at the pixel-level without constraining the mask embeddings has an inferior performance. Although the L2 loss between the predicted and encoded mask embeddings improves the performance, the learned mask embeddings are slightly suboptimal for reconstructing the original masks. Therefore, training with both the pixel-level dice loss and the embedding-level L2 loss produces better results.</p><p>Attention Type: We next study the effect of the dynamic attention module, which is used to fuse the RoI and image features, by replacing it with the multi-head attention module. As shown in <ref type="table" target="#tab_4">Table 1d</ref>, the dynamic attention module performs much better than the multi-head attention module. We believe this may be because the multi-projection in the multi-head attention complicates the fusion of RoI and image features, which is essential for learning the relations between objects. From the results, we infer that a single projection is more effective for learning such relations.</p><p>Pooling Type: In <ref type="table" target="#tab_4">Table 1e</ref>, we evaluate various strategies for obtaining the image features by extracting different information from input images. As can be seen, the global max-pooling does not obtain a high score, while the global average pooling performs better. We believe this is because the max-pooling extracts features from the highest activated pixel in the feature map, which usually corresponds to a single activated object. In contrast, the global average pooling extracts features that contain information about the whole image. We also find that the position embeddings are essential for achieving high results. By summing the position embeddings with averaged image features, the performance is significantly improved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stage Analysis</head><p>One of the essential components of ISTR is the recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the bottom-up and topdown strategies. The prediction heads infer the classes, bounding boxes, and masks for each instance using the query boxes updated in each stage. We investigate the performance of the recurrent refinement stages both quantitatively and qualitatively in <ref type="figure" target="#fig_3">Fig. 3</ref>. From the visualization of masks and bounding boxes shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>, we can see that both masks and boxes are refined from coarse to fine. The mask and box APs shown in <ref type="figure" target="#fig_3">Fig. 3b</ref> and <ref type="figure" target="#fig_3">Fig. 3c</ref> using different backbones also demonstrate that the results are gradually refined step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>We compare ISTR with the state-of-the-art instance segmentation methods as well as the latest end-to-end object detection methods to demonstrate its superior performance. Quantitative Results: From <ref type="table" target="#tab_6">Table 2</ref>, we can see that ISTR performs well, especially on small objects. For example, the AP m S of ISTR based on ResNet101-FPN outperforms SOLOv2 based on ResNet101-FPN by 5.5 points. We believe this is because the bipartite matching cost does not filter small objects for training. MEInst also uses mask em-beddings for instance segmentation. However, the performance of MEInst suffers significantly due to the redundant predictions of mask embeddings. For example, the AP m of ISTR based on ResNet101-FPN outperforms MEInst based on ResNet101-FPN by a large margin of 4.6 points. Besides, we also find performance gains of ISTR in detection when comparing the results with the state-of-the-art end-to-end object detection methods. We find that the AP b of ISTR outperforms DETR and sparse R-CNN based on ResNet101-FPN by 4.6 and 2.5 points, respectively. Overall, it is surprising that, despite the suboptimal mask embeddings from PCA, ISTR can still obtain such a good result. This demonstrates the strength of the proposed endto-end mechanism and shows the potential of concurrently conducting detection and segmentation with Transformers.</p><p>Qualitative Results: We show some examples comparing ISTR with Mask R-CNN in <ref type="figure" target="#fig_6">Fig. 4a</ref>. As can be seen, Mask R-CNN suffers inferior performance when NMS does not remove the duplicate predictions. More visualization results in <ref type="figure" target="#fig_6">Fig. 4b</ref> suggest that, although ISTR obtains state-of-theart mask APs, there is still room for further improvement by learning finer masks. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a new framework, termed instance segmentation Transformer (ISTR), to explore the end-to-end mechanism for instance segmentation. ISTR predicts low-dimensional mask embeddings instead of high-dimensional masks, which inspires the design of a mask matching cost and facilitates the regression. Besides, ISTR concurrently conducts detection and segmentation via a recurrent refinement strategy, which provides a new perspective to achieve end-to-end instance segmentation and boosts the performance of both tasks. On the challenging COCO dataset, the strong performance of ISTR demonstrates its potential for instance-level recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>â€¦Figure 2 :</head><label>2</label><figDesc>Framework of ISTR. Top: Overview of the pipeline. Input images are sent to a convolutional neural network (CNN) with a feature pyramid network (FPN)<ref type="bibr" target="#b23">[24]</ref> to produce the feature pyramid. The feature maps from the feature pyramid are cropped and aligned by learnable query boxes with RoIAlign<ref type="bibr" target="#b15">[16]</ref> to get the RoI features. Image features are obtained by summing and averaging the feature maps. Then, a Transformer encoder with dynamic attention fuses the image and RoI features for prediction heads. The predicted bounding boxes, classes, and masks are recurrently refined in N stages by updating the query boxes. During training, the predictions are matched with ground truth labels to calculate the set loss. During inference, the predictions are directly used as the final results without NMS. Bottom: Details of the modules. FFN denotes the feed-forward network, and the mask decoder is pre-learned and fixed during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>with the predicted boxes. 10 : 1 . 3 : 5 :</head><label>10135</label><figDesc>Until scheduled epochs. // ---Inference Phase ---Input: Images to be processed. Output: Detected and segmented objects. 1: For i = 1, 2, .., N stage: 2: Obtain RoI features by RoIAlign with B iâˆ’Obtain predictions via ISTR encoder and heads. Output the set of predictions in the final stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Qualitative Results: Masks and bounding boxes are from ISTR using ResNet101-FPN on the COCO val2017 split, with a threshold of 0.4. AP results on the COCO val2017 split with ResNet50-FPN. AP results on the COCO val2017 split with ResNet101-FPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Stage Analysis. We report quantitative and qualitative results of ISTR for N = 6 stages. Both results show that the bounding boxes and masks are refined stage by stage, and the performance saturates in the last two stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Mask R-CNN [16] (top) vs. ISTR (bottom) using ResNet101-FPN. Mask R-CNN suffers inferior segmentation when bad duplicate removal occurs.(b) More visualization results of ISTR, using ResNet101-FPN and running at 11.0 fps on a 1080Ti GPU, with 39.9 mask AP (Table 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative Results of ISTR on the COCO test-dev split. Predictions are shown with a threshold of 0.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>all arXiv:2105.00637v2 [cs.CV] 6 May 2021</figDesc><table><row><cell></cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Energy</cell><cell>50 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Energy Distribution</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Components</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Instance Segmentation Transformer // ---Training Phase ---Input: Images and ground truth labels. Output: Learned ISTR model. 1: Learn the mask encoder and decoder via Eq. 2. 2: Initialize the learnable query boxes B</figDesc><table><row><cell></cell><cell>0</cell><cell>.</cell><cell></cell></row><row><cell cols="2">3: Repeat</cell><cell></cell><cell></cell></row><row><cell cols="2">4: For i = 1, 2, .., N stage:</cell><cell></cell><cell></cell></row><row><cell>5:</cell><cell cols="2">Obtain RoI features by RoIAlign with B</cell><cell>iâˆ’1</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Mask vs. Mask Embeddings: Regression with mask embeddings instead of masks brings better performance to the mask APs. The performance improves when the embedding dimension l=60, and saturates when the dimension l=80. Directly expanding the mask as embeddings, i.e., l=784, has worse performance.</figDesc><table><row><cell cols="16">AP m 31.6 33.7 34.2 33.8 31.4 (a) AP m AP m 50 AP m 75 AP b AP b 50 AP b 75 mask 53.3 33.0 40.2 58.4 43.6 l=40 55.6 35.5 41.1 58.9 44.8 l=60 55.6 36.4 41.0 58.9 44.4 l=80 55.3 35.9 40.6 58.6 44.1 l=784 55.2 31.8 41.1 60.0 44.4 w/o 33.8 dice 33.9 L1 34.0 cosine 34.2 (b) Mask Cost Functions: Matching with the dice loss between the AP m 50 AP m 75 AP b AP b 50 AP b 75 55.5 35.7 40.7 58.8 44.1 55.4 35.8 40.8 58.8 44.3 55.5 35.6 40.9 58.9 44.3 55.6 36.4 41.0 58.9 44.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">predicted and ground truth masks performs slightly better than w/o</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">the mask cost. The L1 loss between the predicted and encoded mask</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">embeddings also has a slight improvement. Using cosine similarity as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">the mask cost function brings expected gains.</cell><cell></cell></row><row><cell></cell><cell cols="2">AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell><cell>AP m S</cell><cell>AP m M</cell><cell>AP m L</cell><cell></cell><cell></cell><cell>AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell><cell>AP b</cell><cell>AP b 50</cell><cell>AP b 75</cell></row><row><cell>dice</cell><cell>32.6</cell><cell></cell><cell>55.0</cell><cell>33.5</cell><cell>17.3</cell><cell>34.8</cell><cell>47.6</cell><cell cols="3">multi-head 22.0</cell><cell>43.9</cell><cell>19.6</cell><cell>31.2</cell><cell>50.5</cell><cell>32.5</cell></row><row><cell>L2</cell><cell>33.8</cell><cell></cell><cell>55.5</cell><cell>35.5</cell><cell>17.3</cell><cell>36.1</cell><cell>49.5</cell><cell cols="2">dynamic</cell><cell>34.2</cell><cell>55.6</cell><cell>36.4</cell><cell>41.0</cell><cell>58.9</cell><cell>44.4</cell></row><row><cell cols="2">L2+dice 34.2</cell><cell></cell><cell>55.6</cell><cell>36.4</cell><cell>17.6</cell><cell>36.5</cell><cell>50.6</cell><cell></cell><cell></cell><cell cols="3">+12.2 +11.7 +16.8</cell><cell>+9.8</cell><cell>+8.4</cell><cell>+11.9</cell></row><row><cell cols="8">(c) Loss Functions: Learning masks with both a pixel-level dice</cell><cell cols="8">(d) Attention Type: Dynamic attention brings significant gains com-</cell></row><row><cell cols="8">loss and a embedding-level L2 loss yields gains in mask APs.</cell><cell cols="8">pared with multi-head attention in fusing the RoI and image features.</cell></row><row><cell></cell><cell cols="4">img. type pos. AP m</cell><cell>AP m 50</cell><cell>AP m 75</cell><cell>AP m S</cell><cell>AP m M</cell><cell>AP m L</cell><cell>AP b</cell><cell>AP b 50</cell><cell>AP b 75</cell><cell>AP b S</cell><cell>AP b M</cell><cell>AP b L</cell></row><row><cell></cell><cell></cell><cell cols="2">max</cell><cell>33.3</cell><cell>54.6</cell><cell>35.2</cell><cell>17.4</cell><cell>35.6</cell><cell>48.5</cell><cell>40.0</cell><cell>57.6</cell><cell>43.5</cell><cell>24.2</cell><cell>42.6</cell><cell>52.6</cell></row><row><cell>features</cell><cell>-</cell><cell>avg -</cell><cell></cell><cell>33.6 34.0</cell><cell>55.3 55.3</cell><cell>35.5 36.3</cell><cell>17.3 17.4</cell><cell>36.4 36.4</cell><cell>49.5 50.5</cell><cell>40.6 40.9</cell><cell>58.8 58.8</cell><cell>44.1 44.3</cell><cell>23.9 24.3</cell><cell>43.2 43.2</cell><cell>54.1 55.0</cell></row><row><cell></cell><cell></cell><cell>avg</cell><cell></cell><cell>34.2</cell><cell>55.6</cell><cell>36.4</cell><cell>17.6</cell><cell>36.5</cell><cell>50.6</cell><cell>41.0</cell><cell>58.9</cell><cell>44.4</cell><cell>24.8</cell><cell>43.3</cell><cell>55.3</cell></row></table><note>(e) Pooling Type: The global average pooling yields better performance than the global max-pooling. Combining the image features with position embeddings increases the performance in both mask and box APs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Ablations. We train on the COCO train2017 split, and report mask as well as box APs on the val2017 split.Backbone: We use a CNN backbone with FPN to extract the features ranging from P2 to P5 level of the feature pyra- mid. Then, k learnable query boxes B0 = { b0 i |i = 1, ..., k} initially covering the whole images are used to extract k RoI features U 0 âˆˆ R kÃ—dÃ—tÃ—t via RoIAlign</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Results of ISTR on the COCO test-dev split. All the models are learned by multi-scale training. The results of FPS are measured with a single 1080Ti GPU. The performance of Mask R-CNN is the result of the modified version with implementation details in TensorMask</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Triply supervised decoder networks for joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>et al. A survey on visual transformer. Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorization transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hilaire Sean</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention. Arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">HervÃ©</forename><surname>JÃ©gou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sceneformer: Indoor scene generation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandan</forename><surname>Yeshwanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>NieÃŸner</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep variational instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mask encoding for single shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
