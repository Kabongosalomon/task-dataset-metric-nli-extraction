<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean Actor-Critic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Asadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melrose</forename><surname>Roderick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Providence, Seattle</settlement>
									<region>RI, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Konidaris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Littman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mean Actor-Critic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. We prove that this approach reduces variance in the policy gradient estimate relative to traditional actor-critic approaches. We show empirical results on two control domains and six Atari games, where MAC is competitive with state-of-the-art policy search methods. * These authors contributed equally. Please send correspondence to Cameron Allen &lt;csal@brown.edu&gt; and Kavosh Asadi &lt;kavosh@brown.edu&gt;. † This work was completed while at Microsoft Research. arXiv:1709.00503v2 [stat.ML] 22 May 2018 performing action a ∈ A in state s ∈ S as:</p><p>R(s, a) = E r t+1 s t = s, a t = a , and we denote the probability that performing action a in state s results in state s ∈ S as:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In reinforcement learning (RL), two important classes of algorithms are value-function-based methods and policy search methods. Value-function-based methods maintain an estimate of the value of performing each action in each state, and choose the actions associated with the most value in their current state <ref type="bibr" target="#b19">(Sutton and Barto 1998)</ref>. By contrast, policy search algorithms maintain an explicit policy, and agents draw actions directly from that policy to interact with their environment <ref type="bibr" target="#b20">(Sutton et al. 2000)</ref>. A subset of policy search algorithms, policy gradient methods, represent the policy using a differentiable parameterized function approximator (for example, a neural network) and use stochastic gradient ascent to update its parameters to achieve more reward.</p><p>To facilitate gradient ascent, the agent interacts with its environment according to the current policy and keeps track of the outcomes of its actions. From these (potentially noisy) sampled outcomes, the agent estimates the gradient of the objective function. A critical question here is how to compute an accurate gradient using these samples, which may be costly to acquire, while using as few sample interactions as possible.</p><p>Actor-critic algorithms compute the policy gradient using a learned value function to estimate expected future reward <ref type="bibr" target="#b20">(Sutton et al. 2000;</ref><ref type="bibr" target="#b11">Konda and Tsitsiklis 2000)</ref>. Since the expected reward is a function of the environment's dynamics, which the agent does not know, it is typically estimated by executing the policy in the environment. Existing algorithms compute the policy gradient using the value of states the agent visits, and critically, these methods take into account only the actions the agent actually executes during environmental interaction.</p><p>We propose a new policy gradient algorithm, Mean Actor-Critic (or MAC), for the discrete-action continuous-state case. MAC uses the agent's policy distribution to average the value function over all actions, rather than using the action-values of only the sampled actions. We prove that, under modest assumptions, this approach reduces variance in the policy gradient estimates relative to traditional actor-critic approaches. We implement MAC using deep neural networks, and we show empirical results on two control domains and six Atari games, where MAC is competitive with state-of-the-art policy search methods.</p><p>We note that the core idea behind MAC has also been independently and concurrently explored by <ref type="bibr" target="#b5">Ciosek and Whiteson (2017)</ref>. However, their results mainly focus on continuous action spaces and are more theoretical. We introduce a simpler proof of variance reduction that makes fewer assumptions, and we also show that the algorithm works well in discreteaction domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>In RL, we train an agent to select actions in its environment so that it maximizes some notion of longterm reward. We formalize the problem as a Markov decision process (MDP) <ref type="bibr" target="#b16">(Puterman 1990</ref>), which we specify by the tuple S, s 0 , A, R, T , γ , where S is a set of states, s 0 ∈ S is a fixed initial state, A is a set of discrete actions, the functions R : S × A → R and T : S × A × S →[0, 1] respectively describe the reward and transition dynamics of the environment, and γ ∈ [0, 1) is a discount factor representing the relative importance of immediate versus long-term rewards.</p><p>More concretely, we denote the expected reward for T (s, a, s ) = Pr(s t+1 = s s t = s, a t = a) .</p><p>In the context of policy search methods, the agent maintains an explicit policy π(a|s; θ) denoting the probability of taking action a in state s under the policy π parameterized by θ. Note that for each state, the policy outputs a probability distribution over the discrete set of actions: π : S → P (A). At each timestep t, the agent takes an action a t drawn from its policy π(·|s t ; θ), then the environment provides a reward signal r t and transitions to the next state s t+1 .</p><p>The agent's goal at every timestep is to maximize the sum of discounted future rewards, or simply return, which we define as:</p><formula xml:id="formula_0">G t = ∞ ∑ k=1 γ k−1 r t+k .</formula><p>In a slight abuse of notation, we will also denote the total return for a trajectory τ as G(τ), which is equal to G 0 for that same trajectory.</p><p>The agent's policy induces a value function over the state space. The expression for return allows us to define both a state value function, V π (s), and a stateaction value function, Q π (s, a). Here, V π (s) represents the expected return starting from state s, and following the policy π thereafter, and Q π (s, a) represents the expected return starting from s, executing action a, and then following the policy π thereafter:</p><formula xml:id="formula_1">V π (s) := E π G t s t = s , Q π (s, a) := E π G t s t = s, a t = a .</formula><p>Note that:</p><formula xml:id="formula_2">V π (s) = ∑ a∈A [π(a|s; θ)Q π (s, a)].</formula><p>The agent's goal is to find a policy that maximizes the return for every timestep, so we define an objective function J that allows us to score an arbitrary policy parameter θ:</p><formula xml:id="formula_3">J(θ) = E τ∼Pr(τ|θ) [G(τ)] = ∑ τ Pr(τ|θ)G(τ) ,</formula><p>where τ denotes a trajectory. Note that the probability of a specific trajectory depends on policy parameters as well as the dynamics of the environment. Our goal is to be able to compute the gradient of J with respect to the policy parameters θ:</p><formula xml:id="formula_4">∇ θ J(θ) = ∑ τ ∇ θ Pr(τ|θ)G(τ) = ∑ τ Pr(τ|θ) ∇ θ Pr(τ|θ) Pr(τ|θ) G(τ) = ∑ τ Pr(τ|θ)∇ θ log Pr(τ|θ)G(τ) = E s∼d π , a∼π [∇ θ log π(a|s; θ)G 0 ] = E s∼d π , a∼π [∇ θ log π(a|s; θ)G t ] = E s∼d π , a∼π [∇ θ log π(a|s; θ)Q π (s, a)] (1)</formula><p>where d π (s) = ∑ ∞ t=0 γ t Pr(s t = s|s 0 , π) is the discounted state distribution. In the second and third lines we rewrite the gradient term using a score function. In the fourth line, we convert the summation to an expectation, and use the G 0 notation in place of G(τ). Next, we make use of the fact that E[G 0 ] = E[G t ], given by <ref type="bibr" target="#b23">Williams (1992)</ref>. Intuitively this makes sense, since the policy for a given state should depend only on the rewards achieved after that state. Finally, we invoke the definition that Q π (s, a) = E[G t ].</p><p>A nice property of expectation (1) is that, given access to Q π , the expectation can be estimated through implementing policy π in the environment. Alternatively, we can estimate Q π using the return G t , which is an unbiased (and usually a high variance) sample of Q π . This is essentially the idea behind the REIN-FORCE algorithm <ref type="bibr" target="#b23">(Williams 1992)</ref>, which uses the following gradient estimator:</p><formula xml:id="formula_5">∇ θ J(θ) ≈ 1 T T ∑ t=1 G t ∇ θ log π(a t |s t ; θ).<label>(2)</label></formula><p>Alternatively, we can estimate Q π using some sort of function approximation: Q(s, a; ω) ≈ Q π (s, a), which results in variants of actor-critic algorithms. Perhaps the simplest actor-critic algorithm approximates (1) as follows:</p><formula xml:id="formula_6">∇ θ J(θ) ≈ 1 T T ∑ t=1 Q(s t , a t ; w)∇ θ log π(a t |s t ; θ).<label>(3)</label></formula><p>Note that value function approximation can, in general, bias the gradient estimation <ref type="bibr" target="#b2">(Baxter and Bartlett 2001)</ref>. One way of reducing variance in both REINFORCE and actor-critic algorithms is to use an additive control variate as a baseline <ref type="bibr" target="#b23">(Williams 1992;</ref><ref type="bibr" target="#b20">Sutton et al. 2000;</ref><ref type="bibr" target="#b7">Greensmith, Bartlett, and Baxter 2004)</ref>. The baseline function is typically a function that is fixed over actions, and so subtracting it from either the sampled returns or the estimated Q-values does not bias the gradient estimation. We refer to techniques that use such a baseline as advantage variations of the basic algorithms, since they approximate the advantage A(s, a) of choosing action a over some baseline representing "typical" performance for the policy in state s <ref type="bibr" target="#b1">(Baird 1994)</ref>. The update performed by advantage REINFORCE is:</p><formula xml:id="formula_7">θ ← θ + α T ∑ t=1 (G t − b)∇ θ log π(a t |s t ; θ) ,</formula><p>where b is a scalar baseline measuring the performance of the policy, such as a running average of the observed return over the past few episodes of interaction.</p><p>Advantage actor-critic uses an approximation of the expected value of each state s t as its baseline: V(s t ) := ∑ a π(a|s t ; θ) Q(s t , a; ω), which leads to the following update rule:</p><formula xml:id="formula_8">θ ← θ + α T ∑ t=1 Q(s t , a t ; ω) − V(s t ) ∇ θ log π(a t |s t ; θ) .</formula><p>Another way of estimating the advantage function is to use the TD-error signal δ = r t + γV(s ) − V(s). This approach is convenient, because it only requires estimating one set of parameters, namely for V. However, because the TD-error is a sample of the advantage function A(s, a) = Q π (s, a) − V π (s), this approach has higher variance (due to the environmental dynamics) than methods that explicitly compute Q(s, a) − V(s). Moreover, given Q and π, V can easily be computed as V = ∑ a π(a|s)Q(s, a), so in practice, it is still only necessary to estimate one set of parameters (for Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Actor-Critic</head><p>An overwhelming majority of recent actor-critic papers have computed the policy gradient using an estimate similar to Equation (3) <ref type="bibr" target="#b6">(Degris, White, and Sutton 2012;</ref><ref type="bibr" target="#b14">Mnih et al. 2016;</ref><ref type="bibr" target="#b22">Wang et al. 2016</ref>). This estimate samples both states and actions from trajectories executed according to the current policy in order to compute the gradient of the objective function with respect to the policy weights.</p><p>Instead of using only the sampled actions, Mean Actor-Critic (MAC) explicitly computes the probability-weighted average over all Q-values, for each state sampled from the trajectories. In doing so, MAC is able to produce an estimate of the policy gradient where the variance due to action sampling is reduced to zero. This is exactly the difference between computing the sample mean (whose variance is inversely proportional to the number of samples), and calculating the mean directly (which is simply a scalar with no variance).</p><p>MAC is based on the observation that expectation (1), which we repeat here, can be rewritten in the following way: We can estimate (4) by sampling states from a trajectory and using function approximation:</p><formula xml:id="formula_9">∇ θ J(θ) = E s∼d π , a∼π [∇ θ log π(a|s; θ)Q π (s, a)] = E s∼d π ∑ a∈A π(a|s; θ)∇ θ log π(a|s; θ)Q π (s, a) = E s∼d π ∑ a∈A ∇ θ π(a|s; θ)Q π (s, a) .<label>(4)</label></formula><formula xml:id="formula_10">∇ θ J(θ) ≈ 1 T T−1 ∑ t=0 ∑ a∈A ∇ θ π(a|s t ; θ) Q(s t , a; ω) .</formula><p>In our implementation, the inner summation is computed by combining two neural networks that represent the policy and state-action value function. The value function can be learned using a variety of methods, such as temporal-difference learning or Monte Carlo sampling. After performing a few updates to the value function, we update the parameters θ of the policy with the following update rule:</p><formula xml:id="formula_11">θ ← θ + α T−1 ∑ t=0 ∑ a∈A ∇ θ π(a|s t ; θ) Q(s t , a; ω).<label>(5)</label></formula><p>To improve stability, repeated updates to the value and policy networks are interleaved, as in Generalized Policy Iteration <ref type="bibr" target="#b19">(Sutton and Barto 1998)</ref>.</p><p>In traditional actor-critic approaches, which we refer to as sampled-action actor-critic, the only actions involved in the computation of the policy gradient estimate are those that were actually executed in the environment. In MAC, computing the policy gradient estimate will frequently involve actions that were not actually executed in the environment. This results in a trade-off between bias and variance. In domains where we can expect accurate Q-value predictions from our function approximator, despite not actually executing all of the relevant state-action pairs, MAC results in lower variance gradient updates and increased sample-efficiency. In domains where this assumption is not valid, MAC may perform worse than sampled-action actor-critic due to increased bias.</p><p>In some ways, MAC is similar to Expected Sarsa <ref type="bibr" target="#b21">(Van Seijen et al. 2009</ref>). Expected Sarsa considers all next-actions a t+1 , then computes the expected TDerror,</p><formula xml:id="formula_12">E[δ] = r t + γ E[Q(s t+1 , a t+1 )] − Q(s t , a t )</formula><p>, and uses the resulting error signal to update the Q function. By contrast, MAC considers all current-actions a t , and uses the corresponding Q(s t , a t ) values to update the policy directly.</p><p>It is natural to consider whether MAC could be improved by subtracting an action-independent baseline, as in sampled-action actor-critic and REINFORCE:</p><formula xml:id="formula_13">∇ θ J(θ) = E s∼d π ∑ a∈A ∇ θ π(a|s; θ) Q π (s, a) − V π (s) .</formula><p>However, we can simplify the expectation as follows:</p><formula xml:id="formula_14">∇ θ J(θ) = E s∼d π ∑ a∈A ∇ θ π(a|s; θ)Q π (s, a) − V π (s)∇ θ ∑ a∈A π(a|s; θ) .</formula><p>In doing so, we see that both V π (s) and the gradient operator can be moved outside of the summation, leaving just the sum of the action probabilities, which is always 1, and hence the gradient of the baseline term is always zero. This is true regardless of the choice of baseline, since the baseline cannot be a function of the actions or else it will bias the expectation. Thus, we see that subtracting a baseline is unnecessary in MAC, since it has no effect on the policy gradient estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Bias and Variance</head><p>In this section we prove that MAC does not increase variance over sampled-action actor-critic (AC), and also, that given a fixed Q, both algorithms have the same bias. We start with the bias result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1</head><p>If the estimated Q-values, Q(s, a; ω), for both MAC and AC are the same in expectation, then the bias of MAC is equal to the bias of AC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>See Appendix A.</head><p>This result makes sense because in expectation, AC will choose all of the possible actions with some probability according to the policy. MAC simply calculates this expectation over actions explicitly. We now move to the variance result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2</head><p>If the estimated Q-values, Q(s, a; ω), for both MAC and AC are the same in expectation, and if Q(s, a; ω) is independent of Q(s , a ; ω) for (s, a) = (s , a ), then Var[MAC] ≤ Var <ref type="bibr">[AC]</ref>. For deterministic policies, there is equality, and for stochastic policies the inequality is strict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>See Appendix B.</p><p>Intuitively, we can see that for cases where the policy is deterministic, MAC's formulation of the policy gradient is exactly equivalent to AC, and hence we can do no better than AC. For high-entropy policies, MAC will beat AC in terms of variance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>This section presents an empirical evaluation of MAC across three different problem domains. We first evaluate the performance of MAC versus popular policy gradient benchmarks on two classic control problems. We then evaluate MAC on a subset of Atari 2600 games and investigate its performance compared to state-ofthe-art policy search methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classic Control Experiments</head><p>In order to determine whether MAC's lower variance policy gradient estimate translates to faster learning, we chose two classic control problems, namely Cart Pole and Lunar Lander, and compared MAC's performance against four standard sampled-action policy gradient algorithms. We used the open-source implementations of Cart Pole and Lunar Lander provided by OpenAI Gym <ref type="bibr" target="#b4">(Brockman et al. 2016)</ref>, in which both domains have continuous state spaces and discrete action spaces. Screenshots of the two domains are provided in <ref type="figure" target="#fig_0">Figure 1</ref>. For each problem domain, we implemented MAC using two independent neural networks, representing the policy and Q function. We then performed a hyperparameter search to determine the best network architectures, optimization method, and learning rates. Specifically, the hyperparameter search considered: 0, 1, 2, or 3 hidden layers; 50, 75, 100, or 300 neurons per layer; ReLU, Leaky ReLU (with leak factor 0.3), or tanh activation; SGD, RMSProp, Adam, or Adadelta as the optimization method; and a learning rate chosen from 0.0001, 0.00025, 0.0005, 0.001, 0.005, 0.01, or 0.05. To find the best setting, we ran 10 independent trials for each combination of hyperparameters and chose the setting with the best asymptotic performance over the 10 trials. We terminated each episode after 200 and 1000 timesteps (in Cart Pole and Lunar Lander, respectively), regardless of the state of the agent.</p><p>We compared MAC against four standard benchmarks: REINFORCE, advantage REINFORCE, actorcritic, and advantage actor-critic. We implemented the REINFORCE benchmarks using just a single neural network to represent the policy, and we implemented the actor-critic benchmarks using two networks to represent both the policy and Q function. For each benchmark algorithm, we then performed the same hyperparameter search that we had used for MAC.</p><p>In order to keep the variance as low as possible for the advantage actor-critic benchmark, we explicitly computed the advantage function A(s, a) = Q(s, a) − V(s), where V(s) = ∑ a π(a|s)Q(s, a), rather than sampling it using the TD-error (see Section 2).</p><p>Once we had determined the best hyperparameter settings for MAC and each of the benchmark algorithms, we then ran each algorithm for 100 independent trials. <ref type="figure" target="#fig_1">Figure 2</ref> shows learning curves for the different algorithms, and <ref type="table">Table 1</ref> summarizes the results using the mean performance over trials and episodes. On Cart Pole, MAC learns substantially faster than all of the benchmarks, and on Lunar Lander, it performs competitively with the best benchmark algorithm, advantage actor-critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atari Experiments</head><p>To test whether MAC can scale to larger problem domains, we evaluated it on several Atari 2600 games using the Arcade Learning Environment (ALE) (Bellemare et al. 2013) and compared MAC's performance against that of state-of-the-art policy search methods, namely, Trust Region Policy Optimization (TRPO) <ref type="bibr" target="#b17">(Schulman et al. 2015)</ref>, Evolutionary Strategies (ES) <ref type="bibr" target="#b17">(Salimans et al. 2017)</ref>, and Advantage Actor-Critic (A2C) <ref type="bibr" target="#b24">(Wu et al. 2017</ref>). Due to the computational load inherent in training deep networks to play Atari games, we limited our experiments to a subset of six Atari games: Beamrider, Breakout, Pong, Q*bert, Seaquest and Space Invaders. These six games are commonly selected for tuning hyperparameters <ref type="bibr" target="#b13">(Mnih et al. 2015;</ref><ref type="bibr" target="#b24">Wu et al. 2017)</ref>, and thus provide a fair comparison against established benchmarks, de-spite our limited computational resources.</p><p>The MAC network architecture was derived from the OpenAI Baselines implementation of A2C <ref type="bibr" target="#b24">(Wu et al. 2017)</ref>. It uses three convolutional layers (size/stride/filters: 8/4/32, 4/2/64, 3/1/64), followed by a fully-connected layer (size 512), all with ReLU activation. A final fully-connected layer is split into two batches of N outputs each, where N is the number of actions. One batch uses a linear activation and corresponds to the Q-values; the other batch uses a softmax activation and corresponds to the policy. We used this architecture for both the MAC results and the A2C results. The TRPO and ES results are taken from their respective papers.</p><p>We trained the network using a variation of the multi-part loss function used in A2C <ref type="bibr" target="#b24">(Wu et al. 2017</ref>). The value loss at each timestep was equal to the mean squared error between the observed reward and the Q-value of the selected action. The policy entropy loss was simply the negative entropy of the policy at each timestep. For the A2C experiments, the policy improvement loss was the negative log probability of the selected action times its advantage value. For the MAC experiments, the policy improvement loss became the negative sum of action probabilities times their associated Q-values. The overall loss function was a linear combination of the policy improvement loss (coefficient 0.1), policy entropy loss (coefficient 0.001), and value loss (coefficient 0.5), and the network was trained using RMSProp with a learning rate of 1.5e-3. These coefficients trade off the importance of learning good Q-values, improving the policy, and preventing the policy from converging prematurely. This configuration of hyperparameters was found to perform well experimentally for both methods after a small hyperparameter search. The only difference between the A2C and MAC implementations was to replace A2C's  sampled-action policy improvement loss with MAC's sum-over-actions loss; the algorithms used exactly the same architecture and hyperparameters.</p><p>For A2C and MAC, we trained a network for each game on 50 million frames of play, across 16 parallel threads, pausing every 200K frames to evaluate performance and compute learning curves. In each evaluation, we ran 16 agents in parallel, for 4500 frames (5 minutes) each, or 50 total episodes, whichever came first, and averaged the scores of the completed (or timed-out) episodes. Agents were trained and evaluated under the typical random start condition, where the game is initialized with a random number of no-op ALE actions (between 0 and 30) <ref type="bibr" target="#b13">(Mnih et al. 2015)</ref>. The A2C and MAC results in <ref type="table" target="#tab_2">Table 2</ref> come from the final evaluation after all 50M frames, and they are averaged across 5 trials involving separately trained networks. Learning curves for each game can be found in Figure in the Appendix. In addition to A2C, we also compared MAC against TRPO (results from a single trial) <ref type="bibr" target="#b17">(Schulman et al. 2015)</ref>, and ES (results averaged over 30 trials) <ref type="bibr" target="#b17">(Salimans et al. 2017)</ref>, and found that MAC performed competitively with all three benchmark algorithms.</p><p>Note that MAC's performance on Pong and Q*bert was low relative to A2C. For Pong this was due to one of the five MAC trials obtaining a final score of -20.1 and pulling the average performance down significantly. The individual Pong scores for MAC were {20. <ref type="bibr">5, 19.7, 18.3, 14.7, -20.1}</ref>; the scores for A2C were <ref type="bibr">{19.4, 19.4, 19.3, 16.3, 15</ref>.6}. For Q*bert, the performance for both algorithms was much more variable. A2C scored 0.0 on 3 out of 5 trials, and MAC scored 0.0 on 2 out of 5 trials. The reason A2C's average score is so much higher than MAC's is that it had one lucky trial where it scored 7780.9 points. The individual Q*bert scores for MAC were {557.4, 504.7, 155.1, 0.0, 0.0}; the scores for A2C were {7780.9, 476.6, 0.0, 0.0, 0.0}. Additional hyperparameter tuning might lead to improved performance; however, the purpose of this Atari experiment was mainly to show that MAC is competitive with state-of-the-art policy search algorithms, and these results seem to indicate that it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>At its core, MAC offers a new way of computing the policy gradient that can substantially reduce variance and increase learning speed. There are a number of orthogonal improvements to policy gradient methods, such as using natural gradients <ref type="bibr" target="#b10">(Kakade 2002;</ref><ref type="bibr" target="#b15">Peters and Schaal 2008)</ref>, off-policy learning <ref type="bibr" target="#b22">(Wang et al. 2016;</ref><ref type="bibr" target="#b8">Gu et al. 2016;</ref><ref type="bibr" target="#b0">Asadi and Williams 2016)</ref>, secondorder methods <ref type="bibr" target="#b6">(Furmston, Lever, and Barber 2016)</ref>, and asynchronous exploration ). We have not investigated how MAC performs with these extensions; however, just as these improvements were added to basic actor-critic methods, they could be added to MAC as well, and we expect they would improve its performance in a similar way.</p><p>A typical use-case for actor-critic algorithms is for problem domains with continuous actions, which are awkward for value-function-based methods <ref type="bibr" target="#b19">(Sutton and Barto 1998)</ref>. One approach to dealing with continuous actions is Deterministic Policy Gradients (DPG) <ref type="bibr" target="#b18">(Silver et al. 2014;</ref><ref type="bibr" target="#b12">Lillicrap et al. 2015)</ref>, which uses a deterministic policy to perform off-policy policy gradient updates. However, in settings where on-policy learning is necessary, using a deterministic policy leads to sub-optimal behavior <ref type="bibr" target="#b19">(Sutton and Barto 1998)</ref>, and hence a stochastic policy is typically used instead. The recently-introduced Expected Policy Gradients (EPG) <ref type="bibr" target="#b5">(Ciosek and Whiteson 2017)</ref> addresses this problem by generalizing DPG for stochastic policies. However, while EPG has good experimental performance on domains with continuous action spaces, the authors do not provide experimental results for discrete domains. MAC's discrete results and EPG's continuous results are in some sense complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The basic formulation of policy gradient estimators presented here-where the gradient is estimated by averaging the state-action value function across actions-leads to a new family of actor-critic algorithms. This family has the advantage of not requiring an additional variance-reduction baseline, substantially reducing the design effort required to apply them. It is also a natural fit with deep neural network function approximators, resulting in a network architecture that is identical to some sampled-action actorcritic algorithms, but with less variance.</p><p>We prove that for stochastic policies, the MAC algorithm (the simplest member of the resulting family), reduces variance relative to traditional actor-critic approaches, while maintaining the same bias. Our neural network implementation of MAC either outperforms, or is competitive with, state-of-the-art policy search algorithms, and our experimental results show that MAC's lower variance lead to dramatically faster training in some cases. In future work, we aim to develop this family of algorithms further by including typical elaborations of the basic actor-critic architecture like natural or second-order gradients. Our results so far suggest that our new approach is highly promising, and that extensions to it will provide even further improvement in performance.</p><formula xml:id="formula_15">D [AC] − PG (6) Bias[MAC] = E D [MAC] − PG<label>(7)</label></formula><p>For clarity, we will rewrite the AC and MAC expectations <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_9">(4)</ref> to explicitly denote the way that each algorithm estimates the policy gradient, given a batch of data D (with size |D|): </p><formula xml:id="formula_16">AC = 1 |D| ∑ (s,a)∈D ∇ θ log π(a|s; θ) Q(s, a; ω)<label>(8)</label></formula><p>Substituting <ref type="formula" target="#formula_16">(8)</ref> and <ref type="formula">(1)</ref> into Eqn. <ref type="formula">(6)</ref> gives:</p><formula xml:id="formula_18">Bias[AC] = E D 1 |D| |D| ∑ t=1 ∇ θ log π(a t |s t ; θ) Q(s t , a t ; ω) − E s∼d π , a∼π ∇ θ log π(a|s; θ)Q π (s, a)<label>(10)</label></formula><p>Since D is sampled from trajectories that were carried out according to the policy, we can drop the dependence on t inside the expectation, and rewrite (10) as follows:</p><formula xml:id="formula_19">Bias[AC] = 1 |D| |D| ∑ t=1 E s∼d π , a∼π ∇ θ log π(a|s; θ) Q(s, a; ω) − E s∼d π , a∼π ∇ θ log π(a|s; θ)Q π (s, a) (11) = E s∼d π , a∼π ∇ θ log π(a|s; θ) Q(s, a; ω) − Q π (s, a)<label>(12)</label></formula><p>= E s∼d π ∑ a∈A π(a|s; θ)∇ θ log π(a|s; θ) Q(s, a; ω) − Q π (s, a)</p><p>Now we turn our attention to MAC, and substitute <ref type="formula" target="#formula_17">(9)</ref> and <ref type="formula">(1)</ref> into Eqn. <ref type="formula" target="#formula_15">(7)</ref>, to obtain: </p><formula xml:id="formula_21">Bias[MAC] = E D 1 |D| |D| ∑ t=1 ∑ a∈A π(a|s t ; θ)∇ θ log π(a|s t ; θ) Q(s t , a; ω) − E s∼d π ,</formula><formula xml:id="formula_22">= E s∼d π ∑ a∈A π(a|s; θ)∇ θ log π(a|s t ; θ) Q(s, a; ω) − Q π (s, a)<label>(17)</label></formula><p>Comparing <ref type="formula" target="#formula_6">(13)</ref> and <ref type="formula" target="#formula_15">(17)</ref>, we see that AC and MAC have the same bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 2</head><p>For any random variable Z, the variance Var[Z] can be written as:</p><formula xml:id="formula_23">Var[Z] = E Z 2 − E[Z] 2</formula><p>If we assume the estimated Q-values for MAC and AC are the same in expectation, then the squared expectation's contribution to the variance of each algorithm will be equal. We are only interested in determining which estimator has lower variance, so we can drop the second term and simply compare E Z 2 , the second moments.</p><p>Again we will employ the explicit definitions of the AC and MAC estimators, for a data set D, given by <ref type="formula" target="#formula_16">(8)</ref> and <ref type="formula" target="#formula_17">(9)</ref>, respectively.</p><p>For ease of notation, we define the following two functions:</p><p>X(s, a) = ∇ θ i log π(a|s; θ) Q(s, a; ω) (18) Y(s) = E π X(s, a) = ∑ a∈U(s) π(a|s; θ)X(s, a)</p><p>Here, θ i represents a single parameter of the parameter vector θ. We consider an arbitrary choice of i, so the following proof holds for all i.</p><p>The above expressions allow us to rewrite the AC and MAC estimators <ref type="bibr">(Eqn. 8 &amp; 9)</ref> in terms of X(s, a) and Y(s):</p><formula xml:id="formula_25">AC i = 1 |D| ∑ (s,a)∈D X(s, a)<label>(20)</label></formula><formula xml:id="formula_26">MAC i = 1 |D| ∑ s∈D ∑ a∈U(s) Y(s)<label>(21)</label></formula><p>For convenience, we drop the i subscript for the rest of this analysis.  By the assumption that Q(s, a; ω) is independent of Q(s , a ; ω) for (s, a) = (s , a ), we can distribute the expectation through E[X(s, a)X(s , a )] in line 3 on the left, to obtain E[X(s, a)] E[X(s , a )]. In the last line, we can drop the second term in each expression, because they are the same. At this point we just need to compare E s,a [X(s, a) 2 )] vs. E s [Y(s) 2 ]. In order to make this comparison, we make use of Jensen's Inequality <ref type="bibr" target="#b9">(Jensen 1906)</ref>, which says that for a convex function f and a vector Z ∈ R n :</p><formula xml:id="formula_27">E[ f (Z)] ≥ f (E[Z])</formula><p>We note that f (z) = z 2 is convex, and as such, the following holds: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Screenshots of the classic control domains Cart Pole (left) and Lunar Lander (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Performance comparison for CartPole (left) and Lunar Lander (right) of MAC vs. sampled-action policy gradient algorithms. Results are averaged over 100 independent trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves on six Atari games for A2C (blue) and MAC (orange). Vertical axis is score; horizontal axis is number of training frames (in millions). Results are averaged over 5 independent trials, and smoothed slightly for readability. Error bars represent standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>± 13.3 101.1 ± 10.5 Adv. REINFORCE 121.8 ± 11.2 114.7 ± 8.1 Actor-Critic 138.7 ± 13.2 124.6 ± 5.1 Adv. Actor-Critic 157.4 ± 6.4 162.8 ± 14.9 Table 1: Performance summary of MAC vs. sampledaction policy gradient algorithms. Scores denote the mean performance of each algorithm over all trials and episodes.</figDesc><table><row><cell>Algorithm</cell><cell>Cart Pole</cell><cell>Lunar Lander</cell></row><row><cell cols="2">REINFORCE 109.5 MAC 178.3 ± 7.6</cell><cell>163.5 ± 12.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Atari performance of MAC vs. policy search methods (random start condition). TRPO and ES results are from their respective papers<ref type="bibr" target="#b17">(Schulman et al. 2015;</ref><ref type="bibr" target="#b17">Salimans et al. 2017</ref>). A2C and MAC results were obtained with modified versions of the OpenAI Baselines implementation of A2C<ref type="bibr" target="#b24">(Wu et al. 2017)</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>Both AC and MAC are estimators of the true policy gradient (PG). Given a batch of data D, we can write the bias of AC and MAC as:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sample-efficient deep reinforcement learning for dialog control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06000</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning in continuous time: Advantage updating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE World Congress on Computational Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2448" to="2453" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Infinite-horizon policy-gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="319" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ciosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Expected policy gradients. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximate newton methods for policy search in markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furmston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.4839</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">227</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Off-policy actor-critic</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variance reduction techniques for gradient estimates in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Greensmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1471" to="1530" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Q-prop: Sample-efficient policy gradient with an off-policy critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02247</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sur les fonctions convexes et les inégalités entre les valeurs moyennes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L W V</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta mathematica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="193" />
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A natural policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1531" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1008" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural actor-critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="331" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolution strategies as a scalable alternative to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03864</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Trust region policy optimization. ICML-15</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
	<note>ICML-14</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of expected sarsa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL&apos;09. IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01224</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
