<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holistically-Nested Edge Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<email>s9xie@eng.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE and Dept</orgName>
								<orgName type="institution">CogSci University of California</orgName>
								<address>
									<addrLine>San Diego 9500 Gilman Drive</addrLine>
									<postCode>92093</postCode>
									<settlement>La Jolla</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of CogSci and Dept. of CSE</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego 9500 Gilman Drive, La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Holistically-Nested Edge Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a new edge detection algorithm that addresses two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4s per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we address the problem of detecting edges and object boundaries in natural images. This problem is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency, segmentation, object detection/recognition, tracking and motion analysis, medical imaging, structurefrom-motion and 3D reconstruction, to modern applications like autonomous driving, mobile computing, and image-totext analysis. It has been long understood that precisely localizing edges in natural images involves visual perception of various "levels" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. A relatively comprehensive data collection and cognitive study <ref type="bibr" target="#b27">[28]</ref> shows that while different subjects do have somewhat different preferences regarding where to place the edges and boundaries, there was nonetheless impressive consistency between subjects, e.g. reaching F-score 0.80 in the consistency study <ref type="bibr" target="#b27">[28]</ref>.</p><p>The history of computational edge detection is extremely rich; we now highlight a few representative works that have proven to be of great practical importance. Broadly speak-  (a) shows an example test image in the BSD500 dataset <ref type="bibr" target="#b27">[28]</ref>; (b) shows its corresponding edges as annotated by human subjects; (c) displays the HED results. In the second row: (d), (e), and (f), respectively, show side edge responses from layers 2, 3, and 4 of our convolutional neural networks. In the third row: (g), (h), and (i), respectively, show edge responses from the Canny detector <ref type="bibr" target="#b3">[4]</ref> at the scales σ = 2.0, σ = 4.0, and σ = 8.0. HED shows a clear advantage in consistency over <ref type="bibr">Canny.</ref> ing, one may categorize works into a few groups such as I: early pioneering methods like the Sobel detector <ref type="bibr" target="#b19">[20]</ref>, zerocrossing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>, and the widely adopted Canny detector <ref type="bibr" target="#b3">[4]</ref>; methods driven by II: information theory on top of features arrived at through careful manual design, such as Statistical Edges <ref type="bibr" target="#b21">[22]</ref>, Pb <ref type="bibr" target="#b27">[28]</ref>, and gPb <ref type="bibr" target="#b0">[1]</ref>; and III: learningbased methods that remain reliant on features of human design, such as BEL <ref type="bibr" target="#b4">[5]</ref>, Multi-scale <ref type="bibr" target="#b29">[30]</ref>, Sketch Tokens <ref type="bibr" target="#b23">[24]</ref>, and Structured Edges <ref type="bibr" target="#b5">[6]</ref>. In addition, there has been a recent wave of development using Convolutional Neural Networks that emphasize the importance of automatic hierarchical feature learning, including N 4 -Fields <ref type="bibr" target="#b9">[10]</ref>, Deep-Contour <ref type="bibr" target="#b33">[34]</ref>, DeepEdge <ref type="bibr" target="#b1">[2]</ref>, and CSCNN <ref type="bibr" target="#b18">[19]</ref>. Prior to this explosive development in deep learning, the Structured Edges method (typically abbreviated SE) <ref type="bibr" target="#b5">[6]</ref> emerged as one of the most celebrated systems for edge detection, thanks to its state-of-the-art performance on the BSD500 dataset <ref type="bibr" target="#b27">[28]</ref> (with, e.g., F-score of .746) and its practically significant speed of 2.5 frames per second. Recent CNNbased methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref> have demonstrated promising F-score performance improvements over SE. However, there still remains large room for improvement in these CNN-based methods, in both F-score performance and in speed -at present, time to make a prediction ranges from several seconds <ref type="bibr" target="#b9">[10]</ref> to a few hours <ref type="bibr" target="#b1">[2]</ref> (even when using modern GPUs).</p><p>Here, we develop an end-to-end edge detection system, holistically-nested edge detection (HED), that automatically learns the type of rich hierarchical features that are crucial if we are to approach the human ability to resolve ambiguity in natural image edge and object boundary detection. We use the term "holistic", because HED, despite not explicitly modeling structured output, aims to train and predict edges in an image-to-image fashion. With "nested", we emphasize the inherited and progressively refined edge maps produced as side outputs -we intend to show that the path along which each prediction is made is common to each of these edge maps, with successive edge maps being more concise. This integrated learning of hierarchical features is in distinction to previous multi-scale approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref> in which scale-space edge fields are neither automatically learned nor hierarchically connected. <ref type="figure" target="#fig_1">Figure 1</ref> gives an illustration of an example image together with the human subject ground truth annotation, as well as results by the proposed HED edge detector (including the side responses of the individual layers), and results by the Canny edge detector <ref type="bibr" target="#b3">[4]</ref> with different scale parameters. Not only are Canny edges at different scales not directly connected, they also exhibit spatial shift and inconsistency.</p><p>The proposed holistically-nested edge detector (HED) tackles two critical issues: (1) holistic image training and prediction, inspired by fully convolutional neural networks <ref type="bibr" target="#b25">[26]</ref>, for image-to-image classification (the system takes an image as input, and directly produces the edge map image as output); and (2) nested multi-scale feature learning, inspired by deeply-supervised nets <ref type="bibr" target="#b22">[23]</ref>, that performs deep layer supervision to "guide" early classification results. We find that the favorable characteristics of these underlying techniques manifest in HED being both accurate and computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Holistically-Nested Edge Detection</head><p>In this section, we describe in detail the formulation of our proposed edge detection system. We start by discussing related neural-network-based approaches, particularly those that emphasize multi-scale and multi-level feature learning. The task of edge and object boundary detection is inherently challenging. After decades of research, there have emerged a number of properties that are key and that are likely to play a role in a successful system: (1) carefully designed and/or learned features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>, (2) multi-scale response fusion <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref>, (3) engagement of different levels of visual perception <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17]</ref> such as mid-level Gestalt law information <ref type="bibr" target="#b6">[7]</ref>, (4) incorporating structural information (intrinsic correlation carried within the input data and output solution) <ref type="bibr" target="#b5">[6]</ref> and context (both short-and long-range interactions) <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b4">(5)</ref> making holistic image predictions (referring to approaches that perform prediction by taking the image contents globally and directly) <ref type="bibr" target="#b24">[25]</ref>, (6) exploiting 3D geometry <ref type="bibr" target="#b14">[15]</ref>, and (7) addressing occlusion boundaries <ref type="bibr" target="#b15">[16]</ref>.</p><p>Structured Edges (SE) <ref type="bibr" target="#b5">[6]</ref> primarily focuses on three of these aspects: using a large number of manually designed features (property 1), fusing multi-scale responses (property 2), and incorporating structural information (property 4). A recent wave of work using CNN for patch-based edge prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref> contains an alternative common thread that focuses on three aspects: automatic feature learning (property 1), multi-scale response fusion (property 2), and possible engagement of different levels of visual perception (property 3). However, due to the lack of deep supervision (that we include in our method), the multiscale responses produced at the hidden layers in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> are less semantically meaningful, since feedback must be back-propagated through the intermediate layers. More importantly, their patch-to-pixel or patch-to-patch strategy results in significantly downgraded training and prediction efficiency. By "holistically-nested", we intend to emphasize that we are producing an end-to-end edge detection system, a strategy inspired by fully convolutional neural networks <ref type="bibr" target="#b25">[26]</ref>, but with additional deep supervision on top of trimmed VGG nets <ref type="bibr" target="#b35">[36]</ref> (shown in <ref type="figure" target="#fig_3">Figure 3</ref>). In the absence of deep supervision and side outputs, a fully convolutional network <ref type="bibr" target="#b25">[26]</ref> (FCN) produces a less satisfactory result (e.g. F-score .745 on BSD500) than HED, since edge detection demands highly accurate edge pixel localization. One thing worth mentioning is that our image-to-image training and prediction strategy still has not explicitly engaged contextual information, since constraints on the neighboring pixel labels are not directly enforced in HED. In addition to the speed gain over patch-based CNN edge detection methods, the performance gain is largely due to three aspects: (1) FCN-like image-to-image training allows us to simultaneously train on a significantly larger amount of samples (see <ref type="table" target="#tab_5">Table 4</ref>); <ref type="bibr" target="#b1">(2)</ref> deep supervision in our model guides the learning of more transparent features (see <ref type="table" target="#tab_2">Table 2</ref>); (3) interpolating the side outputs in the end-to-end learning encourages coherent contributions from each layer (see <ref type="table">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Existing multi-scale and multi-level NN</head><p>Due to the nature of hierarchical learning in the deep convolutional neural networks, the concept of multi-scale and multi-level learning might differ from situation to situation. For example, multi-scale learning can be "inside" the neural network, in the form of increasingly larger receptive fields and downsampled (strided) layers. In this "inside" case, the feature representations learned in each layer are naturally multi-scale. On the other hand, multi-scale learning can be "outside" of the neural network, for example by "tweaking the scales" of input images. While these two variants have some notable similarities, we have seen both of them applied to various tasks. We continue by next formalizing the possible configurations of multi-scale deep learning into four categories, namely, multi-stream learning, skip-net learning, a single model running on multiple inputs, and training of independent networks. An illustration is shown in <ref type="figure" target="#fig_2">Fig 2.</ref> Having these possibilities in mind will help make clearer the ways in which our proposed holistically-nested network approach differs from previous efforts and will help to highlight the important benefits in terms of representation and efficiency.</p><p>Multi-stream learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> A typical multi-stream learning architecture is illustrated in <ref type="figure" target="#fig_2">Fig 2(a)</ref>. Note that the multiple (parallel) network streams have different parameter numbers and receptive field sizes, corresponding to multiple scales. Input data are simultaneously fed into multiple streams, after which the concatenated feature responses produced by the various streams are fed into a global output layer to produce the final result.</p><p>Skip-layer network learning: Examples of this form of network include <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10]</ref>. The key concept in "skip-layer" network learning is shown in <ref type="figure" target="#fig_2">Fig 2(b)</ref>. Instead of training multiple parallel streams, the topology for the skip-net architecture centers on a primary stream. Links are added to incorporate the feature responses from different levels of the primary network stream, and these responses are then combined in a shared output layer.</p><p>A common point in the two settings above is that, in both of the architectures, there is only one output loss function with a single prediction produced. However, in edge detection, it is often favorable (and indeed prevalent) to obtain multiple predictions to combine the edge maps together.</p><p>Single model on multiple inputs: To get multi-scale predictions, one can also run a single network (or networks with tied weights) on multiple (scaled) input images, as illustrated in <ref type="figure" target="#fig_2">Fig 2(c)</ref>. This strategy can happen at both the training stage (as data augmentation) and at the testing stage (as "ensemble testing"). One notable example is the tiedweight pyramid networks <ref type="bibr" target="#b7">[8]</ref>. This approach is also common in non-deep-learning based methods <ref type="bibr" target="#b5">[6]</ref>. Note that ensemble testing impairs the prediction efficiency of learning systems, especially with deeper models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Training independent networks: As an extreme variant to <ref type="figure" target="#fig_2">Fig 2(a)</ref>, one might pursue <ref type="figure" target="#fig_2">Fig 2(d)</ref>, in which multi-scale predictions are made by training multiple independent networks with different depths and different output loss layers. This might be practically challenging to implement as this duplication would multiply the amount of resources required for training.</p><p>Holistically-nested networks: We list these variants to help clarify the distinction between existing approaches and our proposed holistically-nested network approach, illustrated in <ref type="figure" target="#fig_2">Fig 2(e)</ref>. There is often significant redundancy in existing approaches, in terms of both representation and computational complexity. Our proposed holisticallynested network is a relatively simple variant that is able to produce predictions from multiple scales. The architecture can be interpreted as a "holistically-nested" version of the "independent networks" approach in <ref type="figure" target="#fig_2">Fig 2(d)</ref>, motivating our choice of name. Our architecture comprises a singlestream deep network with multiple side outputs. This architecture resembles several previous works, particularly the deeply-supervised net <ref type="bibr" target="#b22">[23]</ref> approach in which the authors show that hidden layer supervision can improve both optimization and generalization for image classification tasks. The multiple side outputs also give us the flexibility to add an additional fusion layer if a unified output is desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Formulation</head><p>Here we formulate our approach for edge prediction. Training Phase We denote our input training data set by</p><formula xml:id="formula_0">S = {(X n , Y n ), n = 1, . . . , N }, where sample X n = {x (n) j , j = 1, . . . , |X n |} denotes the raw input image and Y n = {y (n) j , j = 1, . . . , |X n |}, y (n) j ∈ {0, 1}</formula><p>denotes the corresponding ground truth binary edge map for image X n . We subsequently drop the subscript n for notational simplicity, since we consider each image holistically and independently. Our goal is to have a network that learns features from which it is possible to produce edge maps approaching the ground truth. For simplicity, we denote the collection of all standard network layer parameters as W. Suppose in the network we have M side-output layers. Each side-output layer is also associated with a classifier, in which the corresponding weights are denoted as w = (w <ref type="bibr" target="#b0">(1)</ref> , . . . , w (M ) ). We consider the objective function</p><formula xml:id="formula_1">L side (W, w) = M m=1 α m (m) side (W, w (m) ),<label>(1)</label></formula><p>where side denotes the image-level loss function for sideoutputs. In our image-to-image training, the loss function is computed over all pixels in a training image X = (x j , j = 1, . . . , |X|) and edge map Y = (y j , j = 1, . . . , |X|), y j ∈ {0, 1}. For a typical natural image, the distribution of edge/non-edge pixels is heavily biased: 90% of the ground truth is non-edge. A cost-sensitive loss function is proposed in <ref type="bibr" target="#b18">[19]</ref>, with additional trade-off parameters introduced for biased sampling. We instead use a simpler strategy to automatically balance the loss between positive/negative classes. We introduce a class-balancing weight β on a per-pixel term basis. Index j is over the image spatial dimensions of image X. Then we use this class-balancing weight as a simple way to offset this imbalance between edge and non-edge. Specifically, we define the following class-balanced cross-entropy loss function used in Equation (1)</p><formula xml:id="formula_2">(m) side (W, w (m) ) = −β j∈Y+ log Pr(y j = 1|X; W, w (m) ) − (1 − β) j∈Y− log Pr(y j = 0|X; W, w (m) )<label>(2)</label></formula><p>where β = |Y − |/|Y | and 1 − β = |Y + |/|Y |. |Y − | and |Y + | denote the edge and non-edge ground truth label sets, respectively. Pr(y j = 1|X; W, w (m) ) = σ(a (m) j ) ∈ [0, 1] is computed using sigmoid function σ(.) on the activation value at pixel j. At each side output layer, we then obtain edge map predictionsŶ</p><formula xml:id="formula_3">(m) side = σ(Â (m) side ), whereÂ (m) side ≡ {a (m) j</formula><p>, j = 1, . . . , |Y |} are activations of the side-output of layer m.</p><p>To directly utilize side-output predictions, we add a "weighted-fusion" layer to the network and (simultaneously) learn the fusion weight during training. Our loss function at the fusion layer L fuse becomes</p><formula xml:id="formula_4">L fuse (W, w, h) = Dist(Y,Ŷ fuse )<label>(3)</label></formula><formula xml:id="formula_5">whereŶ fuse ≡ σ( M m=1 h mÂ (m) side ) where h = (h 1 , . . . , h M ) is the fusion weight. Dist(·, ·)</formula><p>is the distance between the fused predictions and the ground truth label map, which we set to be cross-entropy loss. Putting everything together, we minimize the following objective function via standard (back-propagation) stochastic gradient descent:  See section 4 for detailed hyper-parameter and experiment settings.</p><formula xml:id="formula_6">(W, w, h) = argmin(L side (W, w) + L fuse (W, w, h))<label>(4)</label></formula><formula xml:id="formula_7">ℒ ℓ (1) ℓ (3) ℓ (2) ℓ (4) ℓ (5)</formula><p>Testing phase During testing, given image X, we obtain edge map predictions from both the side output layers and the weighted-fusion layer:</p><formula xml:id="formula_8">(Ŷ fuse ,Ŷ (1) side , . . . ,Ŷ (M ) side ) = CNN(X, (W, w, h) ),<label>(5)</label></formula><p>where CNN(·) denotes the edge maps produced by our network. The final unified output can be obtained by further aggregating these generated edge maps. The details will be discussed in section 4.</p><formula xml:id="formula_9">Y HED = Average(Ŷ fuse ,Ŷ (1) side , . . . ,Ŷ (M ) side )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>Next, we describe the network architecture of HED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trimmed network for edge detection</head><p>The choice of hierarchy for our framework deserves some thought. We need the architecture (1) to be deep, so as to efficiently generate perceptually multi-level features; and (2) to have multiple stages with different strides, so as to capture the inherent scales of edge maps. We must also keep in mind the potential difficulty in training such deep neural networks with multiple stages when starting from scratch. Recently, VGGNet <ref type="bibr" target="#b35">[36]</ref> has been seen to achieve state-ofthe-art performance in the ImageNet challenge, with great depth (16 convolutional layers), great density (stride-1 convolutional kernels), and multiple stages (five 2-stride downsampling layers). Recent work <ref type="bibr" target="#b1">[2]</ref> also demonstrates that fine-tuning deep neural networks pre-trained on the general image classification task is useful to the low-level edge detection task. We therefore adopt the VGGNet architecture but make the following modifications: (a) we connect our side output layer to the last convolutional layer in each stage, respectively conv1 2, conv2 2, conv3 3, conv4 3, conv5 3. The receptive field size of each of these convolutional layers is identical to the corresponding side-output layer; (b) we cut the last stage of VGGNet, including the 5th pooling layer and all the fully connected layers. The reason for "trimming" the VGGNet is two-fold. First, because we are expecting meaningful side outputs with different scales, a layer with stride 32 yields a too-small output plane with the consequence that the interpolated prediction map will be too fuzzy to utilize. Second, the fully connected layers (even when recast as convolutions) are computationally intensive, so that trimming layers from pool5 on can significantly reduce the memory/time cost during both training and testing. Our final HED network architecture has 5 stages, with strides 1, 2, 4, 8 and 16, respectively, and with different receptive field sizes, all nested in the VGGNet. See <ref type="table" target="#tab_1">Table 1</ref> for a summary of the configurations of the receptive fields and strides. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture alternatives</head><p>Below we discuss some possible alternatives in architecture design, and in particular, the role of deep supervision of HED for the edge detection task. FCN and skip-layer architecture The topology used in the FCN model differs from that in our HED model in several aspects. As we have discussed, while FCN reinterprets classification nets for per-pixel prediction, it has only one output loss function. Thus, in FCN, although the skip net structure is a DAG that combines coarse, high-layer information with fine low-layer information, it does not explicitly produce multi-scale output predictions. We explore how this architecture can be used for the edge detection task under the <ref type="figure">Figure 4</ref>. Two examples illustrating how deep supervision helps sideoutput layers to produce multi-scale dense predictions. Note that in the left column, the side outputs become progressively coarser and more "global", while critical object boundaries are preserved. In the right column, the predictions tends to lack any discernible order (e.g. in layers 1 and 2), and many boundaries are lost in later stages. same experimental setting as our HED model. We first try to directly apply the FCN-8s model by replacing the loss function with cross-entropy loss for edge detection. The results shown in first row of <ref type="table" target="#tab_2">Table 2</ref> are unsatisfactory, which is expected since this architecture is still not fine enough. We further explore whether the performance can be improved by adding even more links from low-level layers. We then create an FCN-2s network that adds additional links from the pool1 and pool2 layers. Still, directly applying the FCN skip-net topology falls behind our proposed HED architecture (see second row of <ref type="table" target="#tab_2">Table 2</ref>). With heavy tweaking of FCN, there is a possibility that one might be able to achieve competitive performance on edge detection, but the multiscale side-outputs in HED are seen to be natural and intuitive for edge detection. The role of deep supervision Since we incorporate a weighted-fusion output layer that connects each side-output layer, there is a need to justify the adoption of the deep supervision terms (specifically, side (W, w (m) ): now the entire network is path-connected and the output-layer parameters can be updated by back-propagation through the weighted-fusion layer error propagation path (subject to <ref type="figure" target="#fig_3">Equation 3</ref>). Here we show that deep supervision is important to obtain desired edge maps. The key characteristic of our proposed network is that each network layer is supposed to play a role as a singleton network responsible for producing an edge map at a certain scale. Here are some qualitative results based on the two variants discussed above: (1) training with both weighted-fusion supervision and deep supervision, and (2) training with weighted-fusion supervision only. We observe that with deep supervision, the nested side-outputs are natural and intuitive, insofar as the suc-cessive edge map predictions are progressively coarse-tofine, local-to-global. On the other hand, training with only the weighted-fusion output loss gives edge predictions that lack such discernible order: many critical edges are absent at the higher layer side output; under exactly same experimental setup, the result on the benchmark dataset (row three of <ref type="table" target="#tab_2">Table 2</ref>) differs only marginally in F-score but displays severely degenerated average precision; without direct control and guidance across multiple scales, this network is heavily biased towards learning large structure edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>w/o deep supervision w/ deep supervision w/o deep supervision w/ deep supervision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we discuss our detailed implementation and report the performance of our proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>We implement our framework using the publicly available Caffe Library and build on top of the publicly available implementations of FCN <ref type="bibr" target="#b25">[26]</ref> and DSN <ref type="bibr" target="#b22">[23]</ref>. Thus, relatively little engineering hacking is required. In our HED system, the whole network is fine-tuned from an initialization with the pre-trained VGG-16 Net model. Model parameters In contrast to fine-tuning CNN to perform image classification or semantic segmentation, adapting CNN to perform low-level edge detection requires special care. Differences in data distribution, ground truth distribution, and loss function all contribute to difficulties in network convergence, even with the initialization of a pretrained model. We first use a validation set and follow the evaluation strategy used in <ref type="bibr" target="#b5">[6]</ref> to tune the deep model hyper-parameters. The hyper-parameters (and the values we choose) include: mini-batch size (10), learning rate (1e-6), loss-weight α m for each side-output layer (1), momentum (0.9), initialization of the nested filters (0), initialization of the fusion layer weights (1/5), weight decay (0.0002), number of training iterations (10,000; divide learning rate by 10 after 5,000). We focus on the convergence behavior of the network. We observe that whenever training converges, the deviations in F-score on the validation set tend to be very small. In order to investigate whether including additional nonlinearity helps, we also consider a setting in which we add an additional layer (with 50 filters and a ReLU) before each side-output layer; we find that this worsens performance. On another note, we observe that our nested multi-scale framework is insensitive to input image scales; during our training process, we take advantage of this by resizing all the images to 400 × 400 to reduce GPU memory usage and to take advantage of efficient batch processing. In the experiments that follow, we fix the values of all hyper-parameters discussed above to explore the benefits of possible variants of HED. Consensus sampling In our approach, we duplicate the ground truth at each side-output layer and resize the (down-sampled) side output to its original scale. Thus, there exists a mismatch in the high-level side-outputs: the edge predictions are coarse and global, while the ground truth still contains many weak edges that could even be considered as noise. This issue leads to problematic convergence behavior, even with the help of a pre-trained model. We observe that this mismatch leads to back-propagated gradients that explode at the high-level side-output layers. We therefore adjust how we make use of the ground truth labels in the BSDS dataset to combat this issue. Specifically, the ground truth labels are provided by multiple annotators and thus, implicitly, greater labeler consensus indicates stronger ground truth edges. We adopt a relatively brute-force solution: only assign a pixel a positive label if it is labeled as positive by at least three annotators; regard all other labeled pixels as negatives. This helps with the problem of gradient explosion in high level side-output layers. For low level layers, this consensus approach brings additional robustness to edge classification and prevents the network from being distracted by weak edges. Although not fully explored in our paper, a careful handling of consensus levels of ground truth edges might lead to further improvement. Data augmentation Data augmentation has proven to be a crucial technique in deep networks. We rotate the images to 16 different angles and crop the largest rectangle in the rotated image; we also flip the image at each angle, leading to an augmented training set that is a factor of 32 larger than the unaugmented set. During testing we operate on an input image at its original size. We also note that "ensemble testing" (making predictions on rotated/flipped images and averaging the predictions) yields no improvements in F-score, nor in average precision. Different pooling functions Previous work <ref type="bibr" target="#b1">[2]</ref> suggests that different pooling functions can have a major impact on edge detection results. We conduct a controlled experiment in which all pooling layers are replaced by average pooling. We find that using average pooling decrease the performance to ODS=.741. In-network bilinear interpolation Side-output prediction upsampling is implemented with in-network deconvolutional layers, similar to those in <ref type="bibr" target="#b25">[26]</ref>. We fix all the deconvolutional layers to perform linear interpolation. Although it was pointed out in <ref type="bibr" target="#b25">[26]</ref> that one can learn arbitrary interpolation functions, we find that learned deconvolutions provide no noticeable improvements in our experiments.</p><p>Running time Training takes about 7 hours on a single NVIDIA K40 GPU. For a 320 × 480 image, it takes HED 400 ms to produce the final edge map (including the interface overhead), which is significantly faster than existing CNN-based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b1">2]</ref>. Some previous edge detectors also try to improve performance by the less desirable expedient of sacrificing efficiency (for example, by testing on input images from multiple scales and averaging the re-sults).  <ref type="figure">Figure 5</ref>. Results on the BSDS500 dataset. Our proposed HED framework achieves the best result (ODS=.782). Compared to several recent CNN-based edge detectors, our approach is also orders of magnitude faster. See <ref type="table" target="#tab_5">Table 4</ref> for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">BSDS500 dataset</head><p>We evaluate HED on the Berkeley Segmentation Dataset and Benchmark (BSDS 500) <ref type="bibr" target="#b0">[1]</ref> which is composed of 200 training, 100 validation, and 200 testing images. Each image has manually annotated ground truth contours. Edge detection accuracy is evaluated using three standard measures: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). We apply a standard non-maximal suppression technique to our edge maps to obtain thinned edges for evaluation. The results are shown in <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_5">Table 4</ref>. <ref type="table">Table 3</ref>. Results of single and averaged side output in HED on the BSDS 500 dataset. The individual side output contributes to the fused/averaged result. Note that the learned weighted-fusion (Fusion-output) achieves best F-score, while directly averaging all of the five layers (Average 1-5) produces better average precision. Merging those two readily available outputs further boost the performance. Side outputs To explicitly validate the side outputs, we summarize the results produced by the individual side-outputs at different scales in <ref type="table">Table 3</ref>, including different combinations of the multi-scale edge maps. We emphasize here that all the side-output predictions are obtained in one pass; this enables us to fully investigate different configurations of combining the outputs at no extra cost. There are several interesting observations from the results: for instance, combining predictions from multiple scales yields better performance; moreover, all the side-output layers contribute to the performance gain, either in F-score or averaged precision. To see this, in <ref type="table">Table 3</ref>, the side-output layer 1 and layer 5 (the lowest and highest layers) achieve similar relatively low performance. One might expect these two side-output layers to not be useful in the averaged results. However this turns out not to be the case -for example, the Average 1-4 achieves ODS=.760 and incorporating the side-output layer 5, the averaged prediction achieves an ODS=.774. We find similar phenomenon when considering other ranges. As mentioned above, the predictions obtained using different combination strategies are complementary, and a late merging of the averaged predictions with learned fusion-layer predictions leads to the best result. Another observation is, when compared to previous "non-deep" methods, performance of all "deep" methods drops more in the high recall regime. This might indicate that deep learned features are capable of (and favor) learning the global object boundary -thus many weak edges are omitted. HED is better than other deep learning based methods in the high recall regime because deep supervision helps us to take the low level predictions into account.  <ref type="bibr" target="#b8">[9]</ref> .610 .640 .560 10 BEL <ref type="bibr" target="#b4">[5]</ref> .660 * --1/10 gPb-owt-ucm <ref type="bibr" target="#b0">[1]</ref> .726 .757 .696 1/240 Sketch Tokens <ref type="bibr" target="#b23">[24]</ref> .727 .746 .780 1 SCG <ref type="bibr" target="#b30">[31]</ref> .739 .758 .773 1/280 SE-Var <ref type="bibr" target="#b5">[6]</ref> .746 .767 .803 2.5 OEF <ref type="bibr" target="#b12">[13]</ref> .749 .772 .817 -DeepNets <ref type="bibr" target="#b20">[21]</ref> .738 .759 .758 1/5 † N4-Fields <ref type="bibr" target="#b9">[10]</ref> .753 .769 .784 1/6 † DeepEdge <ref type="bibr" target="#b1">[2]</ref> .753 .772 .807 1/10 3 † CSCNN <ref type="bibr" target="#b18">[19]</ref> .756 .775 .798 -DeepContour <ref type="bibr" target="#b33">[34]</ref> . Late merging to boost average precision We find that the weighted-fusion layer output gives best performance in Fscore. However the average precision degrades compared to directly averaging all the side outputs. This might due to our focus on "global" object boundaries for the fusion-layer weight learning. Taking advantage of the readily available side outputs in HED, we merge the fusion layer output with the side outputs (at no extra cost) in order to compensate for the loss in average precision. This simple heuristic gives us the best performance across all measures that we report in <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODS</head><p>More training data Deep models have significantly advanced results in a variety of computer vision applications, at least in part due to the availability of large training data. In edge detection, however, we are limited by the number of training images available in the existing benchmarks. Here we want to explore whether adding more training data will help further improve the results. To do this, we expand the training set by randomly sampling 100 images from the test set. We then evaluate the result on the remaining 100 test images. We report the averaged result over 5 such trials. We observe that by adding only 100 training images, performance improves from ODS=.782 to ODS=.797 (±.003), nearly touching the human benchmark. This shows a potentially promising direction to further enhance HED by training it with a larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">NYUDv2 Dataset</head><p>The NYU Depth (NYUD) dataset <ref type="bibr" target="#b34">[35]</ref> has 1449 RGB-D images. This dataset was used for edge detection in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b10">[11]</ref>. Here we use the setting described in <ref type="bibr" target="#b5">[6]</ref> and evaluate HED on data processed by <ref type="bibr" target="#b10">[11]</ref>. The NYUD dataset is split into 381 training, 414 validation, and 654 testing images. All images are made to the same size and we train our network on full resolution images. As used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, during evaluation we increase the maximum tolerance allowed for correct matches of edge predictions to ground truth from .0075 to .011.   <ref type="table" target="#tab_8">Table 5</ref> for additional information.</p><p>Depth information encoding Following the success in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b25">[26]</ref>, we leverage the depth information by utilizing HHA features in which the depth information is embed-  <ref type="bibr" target="#b34">[35]</ref> .658</p><p>.661 -&lt;1/360 gPb+NG <ref type="bibr" target="#b10">[11]</ref> .687 .716 .629 1/375 SE <ref type="bibr" target="#b5">[6]</ref> .685 .699 .679 5 SE+NG+ <ref type="bibr" target="#b11">[12]</ref> . .746 .761 .786 1 † ded into three channels: horizontal disparity, height above ground, and angle of the local surface normal with the inferred direction of gravity . We use the same HED architecture and hyper-parameter settings as were used for BSDS 500. We train two different models in parallel, one on RGB images and another on HHA feature images, and report the results below. We directly average the RGB and HHA predictions to produce the final result by leveraging RGB-D information. We also tried other approaches to incorporate the depth information, for example, by training on the raw depth channel, or by concatenating the depth channel with the RGB channels before the first convolutional layer. None of these attempts yields notable improvement compared to the approach using HHA. The effectiveness of the HHA features shows that, although deep neural networks are capable of automatic feature learning, for depth data, carefully hand-designed features are still necessary, especially when only limited training data is available. <ref type="table" target="#tab_8">Table 5</ref> and <ref type="figure" target="#fig_4">Figure 6</ref> show the precision-recall evaluations of HED in comparison to other competing methods. Our network structures for training are kept the same as for BSDS. During testing we use the Average2-4 prediction instead of the Fusion-layer output as it yields the best performance. We do not perform late merging since combining two sources of edge map predictions (RGB and HHA) already gives good average precision. Note that the results achieved using the RGB modality only are already better than those of the previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have developed a new convolutionalneural-network-based edge detection system that demonstrates state-of-the-art performance on natural images at a speed of practical relevance (e.g., 0.4 seconds using GPU and 12 seconds using CPU). Our algorithm builds on top of the ideas of fully convolutional neural networks and deeply-supervised nets. We also initialize our network structure and parameters by adopting a pre-trained trimmed VGGNet. Our method shows promising results in performing image-to-image learning by combining multi-scale and multi-level visual responses, even though explicit contextual and high-level information has not been enforced. Source code and pretrained models are available online at https://github.com/s9xie/hed. Acknowledgment This work is supported by NSF IIS-1216528 (IIS-1360566), NSF award IIS-0844566 (IIS-1360568), and a Northrop Grumman Contextual Robotics grant. We gratefully thank Patrick Gallagher for helping improve this manuscript. We also thank Piotr Dollar and Yin Li for insightful discussions. We are grateful for the generous donation of the GPUs by NVIDIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Results</head><p>After the ICCV submission, we retrained our model with the following : (1) In data augmentation, we further triples the dataset by scaling the training images to 50%, 100%, 150% of its original size. <ref type="bibr" target="#b1">(2)</ref> In training phase, we use fullresolution images instead of resizing them to 400 × 400.</p><p>Updated results on BSDS500 benchmark dataset with this newly trained model are reported in <ref type="figure">Figure 7</ref> and Table 6.</p><p>In the new experiment settings, while we found that the gap in F-score narrows between models with/without deep supervision, we have similar qualitative and quantitative observations as illustrated in Section 3.2. <ref type="table">Table 6</ref>. Updated HED results on the BSDS500 dataset.  <ref type="figure">Figure 7</ref>. Updated results on the BSDS500 dataset. Our proposed HED framework achieves the best F-score (ODS=.790, OIS=.808, AP=.811), the late-merging variant achieves best average precision (ODS=.788, OIS=.808, AP=.840).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODS OIS AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changelog v2</head><p>Fix typos and reorganize formulations. Add <ref type="table" target="#tab_2">Table 2</ref> to discuss the role of deep supervision. Add appendix A for updated results on BSDS500 in a new experiment setting. Add links to publicly available repository for training/testing code, augmented data and pre-trained model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) original image (b) ground truth (c) HED: output (d) HED: side output 2 (e) HED: side output 3 (f) HED: side output 4 (h) Canny: = 4 (i) Canny: = 8 (g) Canny: = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the proposed HED algorithm. In the first row:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of different multi-scale deep learning architecture configurations: (a) multi-stream architecture; (b) skip-layer net architecture; (c) a single model running on multi-scale inputs; (d) separate training of different networks; (e) our proposed holistically-nested architectures, where multiple side outputs are added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our network architecture for edge detection, highlighting the error backpropagation paths. Side-output layers are inserted after convolutional layers. Deep supervision is imposed at each side-output layer, guiding the side-outputs towards edge predictions with the characteristics we desire. The outputs of HED are multi-scale and multi-level, with the side-output-plane size becoming smaller and the receptive field size becoming larger. One weighted-fusion layer is added to automatically learn how to combine outputs from multiple scales. The entire network is trained with multiple error propagation paths (dashed lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Precision/recall curves on NYUD dataset. Holistically-nested edge detection (HED) trained with RGB and HHA features achieves the best result (ODS=.746). See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>fusion-output (with deep supervision) .790 .808 .811 fusion-output (w/o deep supervision)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The receptive field and stride size in VGGNet<ref type="bibr" target="#b35">[36]</ref> used in HED.The bolded convolutional layers are linked to additional side-output layers.</figDesc><table><row><cell cols="2">layer c1 2</cell><cell>p1</cell><cell>c2 2</cell><cell>p2</cell><cell>c3 3</cell></row><row><cell>rf size</cell><cell>5</cell><cell>6</cell><cell>14</cell><cell>16</cell><cell>40</cell></row><row><cell>stride</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>4</cell></row><row><cell>layer</cell><cell>p3</cell><cell>c4 3</cell><cell>p4</cell><cell>c5 3</cell><cell>p5</cell></row><row><cell>rf size</cell><cell>44</cell><cell>92</cell><cell>100</cell><cell>196</cell><cell>212</cell></row><row><cell>stride</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>16</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of alternative architectures on BSDS dataset. The "fusion-output without deep supervision" result is learned w.r.t Eqn. 3. The "fusion-output with deep supervision" result is learned w.r.t. to Eqn. 4.</figDesc><table><row><cell></cell><cell>ODS OIS AP</cell></row><row><cell>FCN-8S</cell><cell>.697 .715 .673</cell></row><row><cell>FCN-2S</cell><cell>.738 .756 .717</cell></row><row><cell>Fusion-output (w/o deep supervision)</cell><cell>.771 .785 .738</cell></row><row><cell cols="2">Fusion-output (with deep supervision) .782 .802 .787</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results on BSDS500. * BSDS300 results, †GPU time</figDesc><table><row><cell></cell><cell cols="2">ODS OIS</cell><cell>AP</cell><cell>FPS</cell></row><row><cell>Human</cell><cell>.80</cell><cell>.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Canny</cell><cell cols="4">.600 .640 .580 15</cell></row><row><cell>Felz-Hutt</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Results on the NYUD dataset<ref type="bibr" target="#b34">[35]</ref> †GPU time</figDesc><table><row><cell></cell><cell>ODS</cell><cell>OIS</cell><cell>AP</cell><cell>FPS</cell></row><row><cell>gPb-ucm</cell><cell>.632</cell><cell>.661</cell><cell>.562</cell><cell>1/360</cell></row><row><cell>Silberman</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiscale convolutional neural networks for vision-based classification of cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buyssens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lézoray</surname></persName>
		</author>
		<idno>ACCV. 2013. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised learning of edges and object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ecological statistics of gestalt laws for the perceptual organization of contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6558</idno>
		<title level="m">N4-fields: Neural network nearest neighbor fields for image transforms</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4181</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Oriented edge forests for boundary detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recovering occlusion boundaries from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Boundary detection benchmarking: Beyond f-measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pixel-wise deep learning for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the accuracy of the sobel edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual boundary prediction: A deep neural prediction network and quality dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Technologies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Statistical edge detection: Learning and evaluating edge cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="57" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Theory of edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hildreth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B. Biological Sciences</title>
		<meeting>the Royal Society of London. Series B. Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1167" />
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="187" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale deep learning for gesture detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale improves boundary detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection draft version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>ECCV. 2012. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural mechanisms of form and motion processing in the primate visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scale-space filtering: A new approach to multiscale description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scaling theorems for zero crossings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
