<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Score-level Fusion Method for Eye Movement Biometrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Anjith</forename><surname>George</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Aurobinda</forename><surname>Routray</surname></persName>
						</author>
						<title level="a" type="main">A Score-level Fusion Method for Eye Movement Biometrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.patrec.2015.11.020</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Eye tracking</term>
					<term>Biometrics</term>
					<term>Eye movement biometrics</term>
					<term>Gaze tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel framework for the use of eye movement patterns for biometric applications. Eye movements contain abundant information about cognitive brain functions, neural pathways, etc. In the proposed method, eye movement data is classified into fixations and saccades. Features extracted from fixations and saccades are used by a Gaussian Radial Basis Function Network (GRBFN) based method for biometric authentication. A score fusion approach is adopted to classify the data in the output layer. In the evaluation stage, the algorithm has been tested using two types of stimuli: random dot following on a screen and text reading. The results indicate the strength of eye movement pattern as a biometric modality. The algorithm has been evaluated on BioEye 2015 database and found to outperform all the other methods. Eye movements are generated by a complex oculomotor plant which is very hard to spoof by mechanical replicas. Use of eye movement dynamics along with iris recognition technology may lead to a robust counterfeit-resistant person identification system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>B IOMETRICS is an active area of research in pattern recognition and machine learning community. Potential applications of biometrics include forensics, law enforcement, surveillance, personalized interaction, access control <ref type="bibr" target="#b0">[1]</ref>, etc. Physiological features like fingerprint, DNA, earlobe geometry, iris pattern, facial recognition, <ref type="bibr" target="#b1">[2]</ref> are widely used in biometrics. Recently, several behavioral biometric modalities have been proposed including gait, eye movement patterns, keystroke dynamics <ref type="bibr" target="#b2">[3]</ref> signature, etc. Even though many such parameters like brain signals <ref type="bibr" target="#b3">[4]</ref> (using electroencephalogram) and heart beats <ref type="bibr" target="#b4">[5]</ref> have been proposed as biometric modalities, their invasive nature limits their practical applications.</p><p>An effective biometric should have the following characteristics <ref type="bibr" target="#b0">[1]</ref>: 1) the features should be unique for each individual, 2) they should not change with time (template aging effects), 3) acquisition of parameters should be easy (low computational complexity and noninvasive), 4) accurate and automated algorithms should be available for classification, 5) counterfeit resistance, 6) low cost, and 7) ease of implementation. Other characteristics that might make the system more robust are portability and the ability to extract features from non-co-operative subjects.</p><p>Out of many biometric modalities, iris recognition has shown the most promising results <ref type="bibr" target="#b5">[6]</ref> obtaining Equal Error Rates (EER) close to 0.0011%. However, it can only be used when the user is co-operative. Such systems can be spoofed by contact lenses with printed patterns. Even though most of the biometric modalities perform well on evaluation databases, one may be able to spoof such systems with mechanical replicas or artificially fabricated models <ref type="bibr" target="#b6">[7]</ref>. In this regard, several approaches have been presented <ref type="bibr" target="#b7">[8]</ref> to detect the liveliness of tissues or body parts presented to the biometric system. However, such methods are also vulnerable to spoofing.</p><p>Biometrics using patterns obtained from eye movements is a relatively new field of research. Most of the conventional biometrics use physiological characteristics of the human body. Eye movement-based biometrics tries to identify the behavioral patterns as well as information regarding physiological properties of tissues and muscles generating eye movements <ref type="bibr" target="#b8">[9]</ref>. They provide abundant information about cognitive brain functions and neural signals controlling eye movements. Saccadic eye movement is the fastest movement (peak angular velocities up to 900 degrees per second) in the human body. Mechanically replicating such a complex oculomotor plant model is extremely difficult. These properties make eye movement patterns a suitable candidate for biometric applications. The dynamics of eye movement along with these properties can give inbuilt liveliness detection capability.</p><p>Initially, eye movement biometrics has been proposed as a soft biometric. However, with the high level of accuracy achieved, it seems there are more opportunities regarding its application as an independent biometric modality. Eye movement detection can be integrated easily into already existing iris recognition systems. A combination of iris recognition and eye movement pattern recognition may lead to a robust counterfeit-resistant biometric modality with embedded liveliness detection and continuous authentication properties. Eye movement biometrics can also be made taskindependent <ref type="bibr" target="#b9">[10]</ref> so that the movements can be captured even for non-co-operative subjects.</p><p>The rest of the paper is organized as follows. Section 2 describes previous works related to the use of eye movement as a biometric. Section 3 presents the proposed algorithm. Evaluation of the algorithm along with the results are outlined in section 4. Conclusions regarding eye movement biometrics and possible extensions are detailed in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Initial attempts to use eye movements as a biometric modality were carried out by Kasprowski and Ober <ref type="bibr" target="#b10">[11]</ref>. They recorded the eye movements of subjects following a jumping dot on a screen. Several frequency domain and Cepstral features were extracted from this data. They applied different classification methods like naive Bayes, C45 decision trees, SVM and KNN methods. The results obtained further motivated research in eye movement-based biometrics. Bednarik et al. <ref type="bibr" target="#b11">[12]</ref> conducted experiments on several tasks including text reading, moving cross stimulus tracking and free viewing of images. They used FFT and PCA on the eye movement data. Several combinations of such features were tried. However, the best results were obtained using the distance between eyes, which is not related to eye dynamics. Komogortsev et al. <ref type="bibr" target="#b12">[13]</ref> used an Oculomotor Plant Mathematical Model (OPMM) to model the complex dynamics of the oculomotor plant. The plant parameters were identified from the eye movement data. This approach was further extended in <ref type="bibr" target="#b13">[14]</ref>. Holland and Komogortsev <ref type="bibr" target="#b14">[15]</ref> evaluated the applicability of eye movement biometrics with different spatial and temporal accuracies and various types of stimuli. Several parameters of eye movements were extracted from fixations and saccades. Weighted components were used to compare different samples for biometric identification. A temporal resolution of 250 Hz and spatial accuracy of 0.5 degrees were identified as the minimum requirements for accurate gaze-based biometric systems. Kinnunen et al. <ref type="bibr" target="#b9">[10]</ref> presented a task-independent user authentication system based on eye movements. Gaussian mixture modeling of short-term gaze data was used in their approach. Even though the accuracy rates were fairly low, the study opened up possibilities for the development of task-independent eye movement-based verification systems. Rigas et al. <ref type="bibr" target="#b15">[16]</ref> explored variations in individual gaze patterns while observing human face images. Eye movements resulted were analyzed using a graph-based approach. The Multivariate Wald-Wolfowitz runs test was used to classify the eye movement data. This method achieved 70% rank-1 IR and 30% EER on a database of 15 subjects. Rigas et al. <ref type="bibr" target="#b16">[17]</ref> extended this method using features of velocity and acceleration calculated from fixations. The feature distributions were compared using Wald-Wolfowitz test. Zhang et al. <ref type="bibr" target="#b17">[18]</ref> used saccadic eye movements with machine learning algorithms for biometric verification. They used multilayer perceptron networks, support vector machines, radial basis function networks and logistic discriminant for the classification of eye movement data. Recently Cantoni et al. <ref type="bibr" target="#b18">[19]</ref> proposed a gaze analysis technique called GANT in which fixation patterns were denoted by a graph-based representation. For each user, a fixation model was constructed using the duration and number of visits at various points. Frobenius norm of the density maps was used to find the similarity between two recordings. Holland and Komogortsev presented an approach (CEM) <ref type="bibr" target="#b19">[20]</ref> using several scan path features including saccade amplitudes, average saccade velocities, average saccade peak velocities, velocity waveform, fixation counts, average duration of fixation, length of scan path, area of scan path, regions of interest, number of inflections, main sequence relationship, pairwise distances between fixations, amplitude duration relationship, etc. A comparison metric of the features was computed using Gaussian cumulative density function. Another similarity metric was obtained by comparing the scan paths. A weighted fusion of these parameters obtained the best case EER of 27%. Holland and Komogortsev proposed a method (CEM-B) <ref type="bibr" target="#b20">[21]</ref>, in which the fixation and saccade features were compared using statistical methods like Ansari-Bradley test, two-sample t-test, two-sample Kolmogorov-Smirnov test, and the two-sample Cramer-von Mises test. Their approach achieved 83% rank-1 IR and 16.5% EER on a dataset of 32 subjects.</p><p>To the best knowledge of the authors, the best case EER obtained is 16.5% <ref type="bibr" target="#b20">[21]</ref>. Most of the works presented in the literature were evaluated on smaller databases. The effect of template aging was not considered in these works. For the application of eye movement as a reliable biometric, the patterns should remain consistent with time. In this paper, we try to improve upon the existing methods. The proposed algorithm can reach an EER up to 2.59% and rank-1 accuracy of 89.54% in RAN 30min dataset of BioEye 2015 database <ref type="bibr" target="#b21">[22]</ref> containing 153 subjects. Template aging effect has also been studied using data taken after an interval of 1 year. The average EER obtained is 10.96% with a rank-1 accuracy of 81.08% with 37 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In the proposed approach, eye movement data from the experiment are classified into fixations and saccades, and their statistical features are used to characterize each individual. For each individual, the properties of saccades of same durations have been reported to be similar <ref type="bibr" target="#b22">[23]</ref>. We use this knowledge and extract the statistical properties of the eye movements for biometric identification. Different stages of the algorithm are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data pre-processing and noise removal</head><p>The data contains visual angles in both x and y directions along with stimulus angles. Information about the validity of samples is also available. Eye movement data has been captured at a sampling frequency of 1000Hz. The data obtained is decimated to 250Hz using an anti-aliasing filter. In the proposed feature extraction method, most of the parameters are computed with reference to the screen coordinate system. Hence, in the pre-processing stage, the data obtained is converted to screen coordinates based on head distance and geometry of the acquisition system as:</p><formula xml:id="formula_0">x screen = d * w pix w tan(θ x ) + w pix 2<label>(1)</label></formula><formula xml:id="formula_1">y screen = d * h pix h tan(θ y ) + h pix 2<label>(2)</label></formula><p>where, d, θ x and θ y denote distance from the screen and visual angles in x and y direction (in radian) respectively.</p><p>x screen and y screen denote the position of gaze on the screen. w pix , h pix ,w, h denote resolution and physical size of the screen in horizontal and vertical directions respectively. Raw eye gaze positions may contain noise. Most of the features used in this work are extracted from velocity and acceleration profiles. The presence of noise makes it difficult to estimate the velocity and acceleration parameters using differentiation operation. Eye movement signals contain high-frequency components, especially during saccades. High-frequency components would be more prominent in velocity and acceleration profiles <ref type="bibr" target="#b23">[24]</ref>. Savitzky-Golay filters are useful for filtering out the noise when the frequency span of the signal is large <ref type="bibr" target="#b24">[25]</ref>. They are reported to be optimal <ref type="bibr" target="#b25">[26]</ref> for minimizing the least-square error in fitting a polynomial to frames of the noisy data. We use this filter with polynomial order of 6 and frame size of 15 in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Eye movement classification and feature extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Eye movement classification</head><p>The I-VT (velocity threshold) algorithm <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> is used to classify the filtered eye movement data into a sequence of fixations and saccades (Algorithm 1). Most of the earlier works specify the velocity threshold for angular velocity. The angular velocity computed from the filtered data is used to classify the eye movements. A velocity of 50 degrees/second is used as the threshold in I-VT algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1: Gaze data and stimulus for RAN 30min sequence</head><p>A minimum duration threshold of 100 milliseconds has been chosen to reduce the false positives in fixation identification. Algorithm 1 returns the classification results for each data point as either fixation or saccade. Points that are not a part of fixations are considered as saccades in this stage. In the proposed approach, we consider saccades with their durations more than a specified threshold to minimize the effect of spurious saccade segments. From the results of Algorithm 1, a list containing starting index and duration of all fixations and saccades is created. A post-processing stage is carried out to remove small-duration saccades. Saccades with duration less than 12 milliseconds are removed in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature extraction</head><p>After the removal of small-duration saccades, each eye movement data is arranged into a sequence of fixations and saccades. The sequence of gaze locations and corresponding visual angles are also available for each fixation and saccade. Several statistical features are extracted from the position, velocity and acceleration profiles of the gaze sequence. Other features like duration, dispersion, path length and co-occurrence features are also extracted for both fixations and saccades. Earlier works <ref type="bibr" target="#b12">[13]</ref> suggested that saccades provide a rich amount of information about the dynamics of oculomotor plant. Hence, we extract several other parameters including the saccadic ratio, main sequence, angle, etc. Saccades in horizontal and vertical directions are generated by different areas of the brain <ref type="bibr" target="#b28">[29]</ref>. We use the statistical properties of the gaze data in x and y directions to incorporate this information. The distance and angle with the previous fixation/saccade are also used as features to leverage the temporal properties. The method used for computation of features is described below.</p><p>Let X = {x 1 , x 2 , x 3 , ..., x N } and Y = {y 1 , y 2 , y 3 , ..., y N } denote the set of coordinate positions of gaze in each fixation/saccade and let N denotes the number of data points in any fixation or saccade. (x i , y i ) denotes gaze location on the screen coordinate system and (θ x i , θ y i ) denotes the corresponding horizontal and vertical visual angles.</p><p>A large number of features are extracted from the gaze sequence in each fixation and saccade. Some features are derived from the angular velocity. The differentiation operation for finding velocity and acceleration is carried out using forward difference method on the smoothed data. List of features extracted from fixations and saccades along with the methods of computation are shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_4">Table  2</ref>. The features are extracted independently for each fixation and saccade.</p><p>The control mechanisms generating fixations and saccades are different. The number of fixations and saccades is also different in each recording. There is a total of 12 and 46 features extracted from fixations and saccades respectively. A feature normalization scheme is used to scale each feature into a common range to ensure equal contribution in the final classification stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feature selection</head><p>The large number of features extracted may contain redundancy and correlation. A backward feature selection algorithm, as shown in Algorithm 2 is used to retain a minimal set of discriminant features. We use the wrapperbased approach <ref type="bibr" target="#b29">[30]</ref> for selecting the features. An RBFN classifier is used for finding the Equal Error Rate (EER) in each iteration. Cross-validation has been carried out in the training set to avoid overfitting. We used a random 50% subset of the development dataset for the feature selection algorithm. Feature selection algorithm starts with a set of all the features. Now in each iteration, the EER with inclusion and exclusion of a particular feature is found. The feature is retained if the EER with the use of the feature is better than EER with exclusion. The procedure is repeated for all the features in a sequential manner. The feature selection algorithm is iterated ten times each time on a random 50% subset for cross-validation. After these iterations, a set of important features is retained. To evaluate the generalization ability of the selected features, we have tested the algorithm (with the selected features) on an entirely disjoint set that was not used in the feature selection process. The results with the evaluation set <ref type="bibr" target="#b21">[22]</ref>(as shown in the public results of BioEye 2015 competition) show the stability and generalization capability of the selected features. The subset of features selected were different for different stimuli (TEX and RAN sets). The list of features selected for TEX and RAN stimuli is shown in After obtaining the set of features from fixations and saccades, we develop a model to represent the data. It has been empirically observed that the performance of classification approaches with Kernel-based methods are better than linear classifiers. It has also been reported that the parameters like amplitude-duration and amplitude-peak velocity may vary with the angle of saccade <ref type="bibr" target="#b30">[31]</ref>. The nature of saccade dynamics may be different in different directions as the stimulus is changing randomly at various points on the screen. For each person, saccades of different amplitudes and directions form clusters in the feature space. In order to use the multi-mode nature of the data, we represent them by clustering them in the feature space. Representative vectors from each cluster are used to characterize each person. We use Gaussian Radial Basis Function Network (GRBFN) to model these data. The multiple cluster centers in the feature space are used as representative vectors in this approach. This vectors are selected using the K-means algorithm. Two different RBFNs are trained separately for fixation and saccade. Details about the structure of network and score fusion stage are described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RBF network</head><p>Radial Basis Function Network (RBFN) is a class of neural networks initially proposed by Broomhead and Lowe <ref type="bibr" target="#b31">[32]</ref>. Classification in RBFN is done by calculating the similarity between training and test vectors. Multiple prototype vectors corresponding to each class are stored in each neuron. The Euclidean distance between the input vector and the prototype vector is used to calculate neuron activations.</p><p>In the RBF network, input layer is made of feature vectors. ϕ(x) is a radial basis function that finds the Euclidean distance between the input vector and the prototype vector. A weighted combination of scores from the RBF layer is used to classify the input into different categories.</p><p>The number of prototypes per class can be defined by the user, and these vectors can be found from the data using different algorithms like K-means, Linde-Buzo-Gray (LBG) algorithm, etc.</p><p>The Gaussian activation function of each neuron is chosen as:</p><formula xml:id="formula_2">ϕ(x) = e −β x−µ 2<label>(3)</label></formula><p>where, µ is the mean of the distribution. The parameter β can be found from the data. </p><formula xml:id="formula_3">P ath Length = N −1 i=1 (x i+1 − x i ) 2 + (y i+1 − y i ) 2 Y Y</formula><p>Angle with previous fixation Angle with centroid of previous fixation Y Y Distance from the last fixation Euclidean distance from the previous fixation</p><formula xml:id="formula_4">Y Y Skewness(X) From Screen coordinates Y Y Skewness(Y) " N N Kurtosis(X) " Y Y Kurtosis(Y) " Y Y Dispersion Spatial spread during a fixation, Computed as D = (max(X) − min(X)) + (max(Y ) − min(Y )) Y</formula><p>Y Average Velocity AV = P ath Length/Duration Y and N denote inclusion or exclusion of the feature in the particular stimulus after feature selection  In this work, we have used K-means algorithm for selecting the representative vectors. For each person, 32 clusters for fixations and 32 cluster centers for saccades are kept, resulting in 32N clusters for each RBFN (where N is the number of persons in the dataset). The number of clusters to keep is obtained empirically. We have clustered the fixations/saccades of each individual separately to obtain a fixed number of representative vectors for each person. A maximum of 100 iterations is used to form the clusters. A standard K-means algorithm is used with squared Euclidean distance, and the centers are updated in each iteration. Each data point is assigned to the closest cluster center obtained from the K-means algorithm. For a particular neuron, the value of β is computed from the distance of all points belonging to that particular cluster as:</p><formula xml:id="formula_5">N −1 i=1 (x i+1 − x i ) 2 + (y i+1 − y i ) 2 Y Y Angle</formula><formula xml:id="formula_6">β = 1 2σ 2 (4)</formula><p>Where σ is the mean Euclidean distance of the points (assigned to the specific neuron) from the centroid of the corresponding cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Notations</head><p>The biometric identification problem is similar to a multiclass classification problem. Let there be n samples of a p dimensional data. Assume there are m classes (corresponding to m different individuals) with c samples per class (n = mc). Let y i be the label corresponding to i th sample. Let K be the number of representative vectors from each class. The value of K is chosen empirically (K = 32).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Network learning</head><p>The activations can be obtained as: A = ϕ i,j (x k ), i = 1, ..., K, j = 1, ..., m, k = 1, ..., n The output of the network can be represented as a linear combination of the RBF activations as:</p><formula xml:id="formula_7">f (x) = m j=1 w j ϕ j (x) (5)</formula><p>where, f (x) contains the class membership in vector form. Given the activations and output labels, the objective of the training stage is to find the weight parameters of the output layer. The weights are obtained by minimizing the sum of squared errors.</p><p>The output layer is represented by a linear system as:</p><p>Aŵ =ŷ</p><p>The optimal set of weights can be found using the Moore-Penrose pseudoinverse. Alternatively, these weights can be learned through gradient descent method. In the learning phase, features extracted from each fixation and saccade are used to train the model. Each fixation/saccade is treated as a sample in the training process. The method described here uses two-phase learning. RBF layer and weight layer trainings are carried out separately. However a joint training similar to back-propagation is also possible <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Training stage</head><p>Only the session 1 data from the datasets are used in the training stage. Cluster centers and corresponding β values are computed separately for each person (resulting in 32N neurons for both fixation and saccade RBFNs). The output weights (ŵ f ix andŵ sacc ) are found using all fixations and saccades from all the subjects in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Testing stage</head><p>Session 2 data is used in the testing stage. Parameters of RBFN are computed separately for fixations and saccades in the training session. The scores from both RBFNs are combined to obtain the final result. The overall configuration of the scheme is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>For an unlabeled probe, the activations for each fixation and saccade (A f ix and A sacc ) are found separately using the cluster centers obtained in the training stage. The final classification is carried out using the combined score obtained from all saccades and fixations. Let n f ix and n sacc be the number of fixations and saccades in an unlabeled gaze sequence. The combined score can be obtained as:</p><formula xml:id="formula_9">score = λ 1 n f ix n f ix i=1 A i f ixŵf ix + (1 − λ) 1 n sacc nsacc i=1</formula><p>A i saccŵsacc <ref type="bibr" target="#b6">(7)</ref> where, λ ∈ [0 1] is the weight used in the score fusion. The parameter λ decides the contribution of fixations and saccades in the final decision stage. This value can be obtained empirically. In the present work, λ value of 0.5 is used.</p><p>The label of the unknown sample can be obtained as:</p><formula xml:id="formula_10">label = arg max m (score)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The data used in this paper are part of the development phase of BioEye 2015 <ref type="bibr" target="#b21">[22]</ref> competition. Data recorded in three different sessions are available. First two sessions are separated by a time interval of 30 minutes containing recordings of 153 subjects (ages 18-43). A third session, conducted after 1 year, (37 subjects) is also available to evaluate the robustness against template aging. The database contains gaze sequences obtained using two distinct types of visual stimuli. In one set (RAN), a white dot moving in a dark background was used as the stimulus. The subjects were asked to follow the dot. Text excerpt shown on the screen was used as the stimulus in the other set (TEX). The samples were recorded with an EyeLink eye-tracker (with a reported spatial accuracy of 0.5 degrees) at 1000 Hz and down-sampled to 250 Hz with anti-aliasing filtering. The development dataset contains the ground truth about the identity of the persons. An additional evaluation set is also available without ground truth.</p><p>In each recording, visual angles in x and y direction, stimulus angle in x and y direction and information regarding the validity of the samples are available. Details about the stimulus types in BioEye2015 database are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Random dot stimulus (RAN 30min &amp; RAN 1year)</head><p>The stimulus used was a white dot appearing at random locations on a black computer screen. The position of the stimulus would change every second. The subjects were asked to follow the dot on the screen and recording was carried out for 100 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Text stimulus (TEX 30min &amp; TEX 1year)</head><p>The task, in this case, was reading text excerpts from the poem of Lewis Carroll "The Hunting of the Snark". The duration of this experiment was 60 seconds.</p><p>A comprehensive list of the datasets and parameters are shown in <ref type="table" target="#tab_6">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>The proposed algorithm has been evaluated in the labeled development set. Rank-1 accuracy and EER are used for evaluating the algorithm. Rank-1 (R1) accuracy is defined as the ratio of the total number of correct detections to the number of samples used. EER is the percentage at which False Acceptance Rate (FAR) and False Rejection Rate (FRR) are equal. Detection Error Trade-off (DET) curves are shown for all the datasets. Rank(n) accuracy is the number of correct detections in the top n candidates. Cumulative match characteristics (CMC) is the cumulative plot of rank(n) accuracy. CMC curves are also plotted for all the four datasets. The evaluation set in the BioEye2015 dataset is unlabeled. However, we report the R1 accuracy as obtained from the public results <ref type="bibr" target="#b21">[22]</ref> of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Performance in the development datasets</head><p>The model was trained using 50% of data in the development datasets. We have trained and tested the algorithm on  completely disjoint sessions to test its generalization ability. For example, in RAN 30min sequence there are 153 samples available for two different sessions. We have trained the Algorithm only on the first session (using a random 50% subset of the data). The evaluation was carried out on the session 2 data. We have not used the data from the same session for training and testing since it won't account for intersession variability.</p><p>The average R1 accuracy and EER were calculated from random 50% subsets of development datasets. This procedure was repeated 100 times and the average R1 accuracy and EER were obtained. The results obtained along with the standard deviations are given in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>The R1 accuracy in RAN 30min and TEX 30min databases are above 90% indicating the robustness of the proposed framework. The EER on RAN 30min database is found out to be 2.59%, comparable to the accuracy levels of fingerprint (2.07% EER) <ref type="bibr" target="#b33">[34]</ref>, voice recognition systems, and facial geometry (15% EER) <ref type="bibr" target="#b34">[35]</ref> biometrics. R1 accuracy <ref type="table" target="#tab_8">(Table 5</ref>) of the proposed algorithm obtained from the development set was compared with the baseline algorithm (CEM-B) <ref type="bibr" target="#b20">[21]</ref>. The average cumulative matching characteristics curves for the four datasets are shown in <ref type="figure">Fig.  5</ref> and <ref type="figure" target="#fig_2">Fig. 6</ref>.</p><p>The Detection Error Trade-off (DET) curves for the development datasets are shown in <ref type="figure" target="#fig_3">Fig. 3 and Fig. 4</ref>. In <ref type="figure" target="#fig_3">Fig.  3</ref> (a) and (b), FNR becomes very small as FPR increases indicating a good separation from impostors. The reduction in FNR may be because of the addition of scores of all the fixations and saccades in the score fusion stage. Impostor scores are considerably smaller than genuine scores in the proposed approach. The performance in 1-year sessions is poor compared to 30-minutes sessions indicating template aging effects.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance in the evaluation sets</head><p>The evaluation part of the database is unlabeled. However, the results of the competition are available on the website <ref type="bibr" target="#b21">[22]</ref>. The evaluation set of the dataset had only one unlabeled data for every labeled sample. We have used this one to one correspondence assumption in the final stage of the algorithm. Let there be n labeled and n unlabeled recordings. The task is to assign each unlabeled file to a labeled file. The scores obtained from RBF output stage were stored in a matrix D (with dimension nxn). D(i, j) denotes the normalized similarity score between i th labeled and j th unlabeled samples. We have selected the best match for each unlabeled recording using Algorithm 3. The use of this one to one assumption improved the results. However, this assumption may not be suitable for practical biometric identification/verification scenarios. The proposed method has been found to outperform all the other methods even without the one to one assumption indicating the robustness for biometric applications. The results with and without this assumption are shown in <ref type="table" target="#tab_9">Table 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational complexity</head><p>The algorithm has been implemented in an Intel Core i5 CPU, 3.33 GHz PC with 4 GB RAM. The average training time for the network without code optimization (singlethreaded) in MATLAB is about 400 seconds (with 153 samples). In the testing phase, for predicting one unlabeled recording, it takes on an average 0.21 seconds (in TEX 30min). The time taken for training and testing phase can be improved considerably by implementation in C, C++, using parallel processing platforms like Graphical Processing Units (GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Performance of the algorithm</head><p>The R1 accuracy of the proposed method is high in both TEX and RAN datasets, which indicates the possibility of developing a task-independent biometric system. The EER and R1 accuracy achieved show the robustness of the proposed score fusion approach. The selected features show good discrimination ability in both stimuli. The accuracy with 1-year datasets is comparatively lesser than that with the 30-minute datasets. This lower accuracy may be attributed to template aging effects. Some of the selected features may show variability over time <ref type="bibr" target="#b35">[36]</ref> [37]. The feature selection was carried out in 30-minute datasets due to the availability of a large number of subjects. Feature selection with 1-year datasets may lead to overfitting because of fewer subjects. This issue can be solved by using the feature selection in 1-year datasets with a larger number of subjects, which may identify features that are robust against template aging. However, the results show significant improvement compared to the state of the art methods. The proposed algorithm was ranked first in the BioEye 2015 <ref type="bibr" target="#b21">[22]</ref> competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Limitations</head><p>Controlled experimental setup was used to collect the data used in this work. The sampling rate and quality of data used in the present work were very high since it was collected in lab conditions using chinrest. Accurate estimation of the features in noisy, low sampling rates is necessary for the use in a practical biometric scenario. The nature of eye movements may be affected by the level of alertness, fatigue, emotions, cognitive loading, etc. Consumption of caffeine and alcohol by the subjects may affect the performance of the proposed algorithm. The features selected for biometrics should be invariant to such variations. Only two sessions of data were available for each subject. Intersession variability and template aging effects need to be studied further. Lack of publicly available databases containing a large number of samples (to account for template aging, uncontrolled environment, affective states, intersession variability) is another problem. Creation of a large database with such variability could provide more robust solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This work proposes a novel framework for biometric identification based on dynamic characteristics of eye movements. The raw eye movement data is classified into a sequence of fixations and saccades. We extract a large set of features from fixations and saccades to characterize each individual. The important features extracted from fixations and saccades are identified based on a backward selection framework. Two different Gaussian RBF networks are trained using features from fixations and saccades separately. In the detection phase, scores obtained from both RBF networks are used to get the subject's identity. The high accuracy obtained shows the robustness of the proposed algorithm. The proposed framework can be easily integrated into the existing iris recognition systems. A combination of the proposed approach with conventional iris recognition systems may give rise to a new counterfeit-resistant biometric system. The comparable accuracy in distinct types of stimuli indicates the possibility of developing a task-independent system for eye movement biometrics. The proposed method can also be used for continuous authentication in desktop environments. Robustness of the algorithm against lower sampling rates, calibration error and noise can be explored in future. The effect of duration of data on the level of accuracy is another topic to be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Schematic of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :Fig. 5 :</head><label>345</label><figDesc>DET curve for (a) RAN 30min and (b) TEX 30min DET curve for (a) RAN 1year and (b) TEX 1year CMC curve for (a) RAN 30min and (b) TEX 30min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>CMC curve for (a) RAN 1year and (b) TEX 1year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 3 :</head><label>3</label><figDesc>One to one matching Data: D (Score matrix) Result: Matches [n, n] = size(D); for i ← 1 to n do [row, col] = f ind(D == max(D(:))); D(row, :) = −∞; D(, : col) = −∞; pair=[row, col]; Matches.append(pair); end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 (</head><label>1</label><figDesc>Fixation features) and Table 2 (Saccade features). The features thus selected are used as inputs to the classification algorithm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>List of features extracted from fixations</figDesc><table><row><cell cols="2">Used in</cell><cell>Fixation Features</cell><cell>Description</cell></row><row><cell cols="2">TEX RAN</cell><cell></cell><cell></cell></row><row><cell>N</cell><cell>Y</cell><cell>Fixation duration</cell><cell>Obtained from I-VT result</cell></row><row><cell>N</cell><cell>N</cell><cell>Standard Deviation (X)</cell><cell>From the screen coordinates during fixation</cell></row><row><cell>Y</cell><cell>N</cell><cell>Standard Deviation (Y)</cell><cell>"</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Length of path traveled in screen</cell></row><row><cell>Y</cell><cell>Y</cell><cell>Path length</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>List of features extracted from saccades</figDesc><table><row><cell>Used in</cell><cell></cell><cell>Saccade Features</cell><cell>Description</cell></row><row><cell>TEX</cell><cell>RAN</cell><cell></cell><cell></cell></row><row><cell>N</cell><cell>N</cell><cell>Saccadic duration</cell><cell>Obtained from I-VT result</cell></row><row><cell>Y</cell><cell>Y</cell><cell>Dispersion</cell><cell>D = (max(X) − min(X)) + (max(Y ) − min(Y )), during saccade</cell></row><row><cell cols="3">NYYYYY NNNYYY M3S2K(Angular Velocity)</cell><cell>Features from angular velocity</cell></row><row><cell cols="2">YYYYYN YYYYYY</cell><cell>M3S2K(Angular Acceleration)</cell><cell>Features from angular acceleration</cell></row><row><cell>Y</cell><cell>Y</cell><cell>Standard Deviation(X)</cell><cell>Obtained from screen positions</cell></row><row><cell>Y</cell><cell>Y</cell><cell>Standard Deviation(Y)</cell><cell>"</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Distance traveled in screen,</cell></row><row><cell>Y</cell><cell>Y</cell><cell>Path length</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Details about the database</figDesc><table><row><cell>Dataset Name</cell><cell>RAN 30min</cell><cell>RAN 1year</cell><cell cols="2">TEX 30min TEX 1year</cell></row><row><cell>Number of subjects</cell><cell>153</cell><cell>37</cell><cell>153</cell><cell>37</cell></row><row><cell>Stimulus</cell><cell>White dot moving in a dark background</cell><cell>White dot moving in a dark background</cell><cell>Text excerpt</cell><cell>Text excerpt</cell></row><row><cell cols="2">Duration of experiment 100 seconds</cell><cell>100 seconds</cell><cell>60 seconds</cell><cell>60 seconds</cell></row><row><cell>Interval between</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>30 minutes</cell><cell>1 year</cell><cell>30 minutes</cell><cell>1 year</cell></row><row><cell>and testing data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Results in the development datasets</figDesc><table><row><cell></cell><cell>RAN 30</cell><cell>RAN 1yr</cell><cell>TEX 30</cell><cell>TEX 1yr</cell></row><row><cell>R1</cell><cell cols="4">90.10±2.76 79.31±6.86 92.38±2.56 83.41±6.98</cell></row><row><cell cols="2">EER 2.59±0.71</cell><cell cols="2">10.96±4.59 3.78±0.77</cell><cell>9.36±3.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison of R1 accuracy in the entire development dataset</figDesc><table><row><cell></cell><cell cols="4">RAN 30 RAN 1yr TEX 30 TEX 1yr</cell></row><row><cell cols="2">Our Method 89.54%</cell><cell>81.08%</cell><cell>85.62%</cell><cell>78.38%</cell></row><row><cell>Baseline</cell><cell>40.52%</cell><cell>16.22%</cell><cell>52.94%</cell><cell>40.54%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">Comparison of R1 accuracy with baseline method</cell></row><row><cell cols="2">in evaluation dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">RAN 30 RAN 1yr TEX 30 TEX 1yr</cell></row><row><cell>Our Method</cell><cell>93.46%</cell><cell>83.78%</cell><cell>89.54%</cell><cell>83.78%</cell></row><row><cell cols="2">Our Method* 98.69%</cell><cell>89.19%</cell><cell>98.04%</cell><cell>94.59%</cell></row><row><cell>Baseline</cell><cell>33.99%</cell><cell>40.54%</cell><cell>58.17%</cell><cell>48.65%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the organizers of BioEye 2015 competition for providing the data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="20" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>An introduction to biometric recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<title level="m">Behavioral Biometrics For Human Identification: Intelligent Applications</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person authentication using brainwaves (eeg) and maximum a posteriori model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D R</forename><surname>Millán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="752" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ecg biometric recognition without fiducial detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hatzinakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Consortium Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Independent testing of iris recognition technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Group</surname></persName>
		</author>
		<idno>NBCHC030114/0002</idno>
	</analytic>
	<monogr>
		<title level="j">Final Report</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Biometric attack vectors and defences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Issues for liveness detection in biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuckers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hornak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Derakhshani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasaradhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Biometric Consortium Conference</title>
		<meeting>Biometric Consortium Conference<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The neurology of eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Zee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford university press</publisher>
			<biblScope unit="volume">90</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards taskindependent person authentication using eye movement signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sedlak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bednarik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications</title>
		<meeting>the 2010 Symposium on Eye-Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eye movements in biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kasprowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Eyemovements as a biometric,&quot; in Image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bednarik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fränti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="780" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biometric identification via an oculomotor plant mathematical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayarathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Aragon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications</title>
		<meeting>the 2010 Symposium on Eye-Tracking Research &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biometric authentication via oculomotor plant characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aragon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (ICB), 2012 5th IAPR International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complex eye movement pattern biometrics: the effects of environment and stimulus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2115" to="2126" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Information Forensics and Security</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Biometric identification based on the eye movements and graph matching techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Economou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fotopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="786" to="792" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human eye movements as a trait for biometrical identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Economou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fotopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics: Theory, Applications and Systems (BTAS), 2012 IEEE Fifth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On biometric verification of a user by means of eye movement data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juhola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMMM 2012, The Second International Conference on Advances in Information Mining and Management</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gant: Gaze analysis technique for human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cantoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1027" to="1038" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Biometric identification via eye movement scanpaths in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex eye movement pattern biometrics: Analyzing fixations and saccades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (ICB), 2013 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bioeye2015,competition on biometrics via eye movements</title>
		<ptr target="http://bioeye.cs.txstate.edu/" />
		<imprint>
			<biblScope unit="page" from="2015" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Binocular coordination of human horizontal saccadic eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Collewijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Erkelens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Steinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="182" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instrument considerations in measuring fast eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hainl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the selection of optimum savitzky-golay filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Seelamantula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="380" to="391" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smoothing and differentiation of data by simplified least squares procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Golay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical chemistry</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1639" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Biometric verification via complex eye movements: The effects of environment and stimulus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics: Theory, Applications and Systems (BTAS), 2012 IEEE Fifth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying fixations and saccades in eye-tracking protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Salvucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 symposium on Eye tracking research &amp; applications</title>
		<meeting>the 2000 symposium on Eye tracking research &amp; applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimally straight and optimally curved saccades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Herman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="7455" to="7457" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human eye-head coordination in two dimensions under different sensorimotor conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Opstal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="560" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Radial basis functions, multivariable functional interpolation and adaptive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Three learning phases for radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Kestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="458" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fvc2004: Third fingerprint verification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Authentication</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="831" to="846" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>and ice 2006 large-scale experimental results</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Template aging in eye movement-driven biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">V</forename><surname>Komogortsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Defense+ Security. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The impact of temporal proximity between samples on eye movement biometric identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kasprowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Information Systems and Industrial Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
