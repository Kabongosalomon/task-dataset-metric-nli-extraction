<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
							<email>smajumdar@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
							<email>bginsburg@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: key word spotting</term>
					<term>speech commands recogni- tion</term>
					<term>deep neural networks</term>
					<term>depth-wise separable convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an MatchboxNet -an end-to-end neural network for speech command recognition. MatchboxNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. MatchboxNet reaches state-of-the art accuracy on the Google Speech Commands dataset while having significantly fewer parameters than similar models. The small footprint of Match-boxNet makes it an attractive candidate for devices with limited computational resources. The model is highly scalable, so model accuracy can be improved with modest additional memory and compute. Finally, we show how intensive data augmentation using an auxiliary noise dataset improves robustness in the presence of background noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We present MatchboxNet, a new compact, end-to-end neural network for keyword spotting (KWS) specifically designed for devices with low computational and memory resources. Match-boxNet builds on the QuartzNet architecture <ref type="bibr" target="#b0">[1]</ref>. It consists of a stack of blocks with residual connections <ref type="bibr" target="#b1">[2]</ref>. Each block is composed from 1D time-channel separable convolutions (these are similar to 2D depth-wise separable convolutions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>), batch normalization, ReLU and dropout layers. This paper makes the following contributions:</p><p>1. An end-to-end neural model for speech command recognition based on 1D time-channel separable convolutions 2. The model achieves state-of-the-art accuracy on Google Speech command datasets <ref type="bibr" target="#b4">[5]</ref> but requires significantly fewer parameters than models which achieve similar accuracy.</p><p>3. The model scales well with the number of parameters. <ref type="bibr" target="#b3">4</ref>. A methodology to improve the model's robustness to background speech and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural network (NN)-based systems for Automatic Speech Recognition (ASR) have a long history, spearheaded by Time Delay Neural Networks (TDNN) for isolated word recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. TDNN and Recurrent NNs (RNNs) were first used together with Hidden Markov Models (HMMs) in hybrid systems, where NN was used only for phonetic classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Rapid progress in deep learning for ASR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> triggered research in end-to-end NN-based models for KWS. In  small-footprint KWS <ref type="bibr" target="#b13">[14]</ref>. Their model was composed of two convolutional layers, max-pooling in the temporal dimension, linear, and soft-max layers. Following the success of ResNets <ref type="bibr" target="#b1">[2]</ref> in computer vision, Qian et al. <ref type="bibr" target="#b14">[15]</ref> applied ResNets for ASR. Arik et al. <ref type="bibr" target="#b15">[16]</ref> suggested Convolutional-RNN, which combined the strengths of convolutional layers and recurrent layers to exploit long-range context.</p><p>The introduction of the Google Speech Command dataset <ref type="bibr" target="#b4">[5]</ref> in 2018 accelerated research in KWS and resulted in variety of new NN-based models, including deep residual networks ( <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>), special RNN with weight sharing <ref type="bibr" target="#b18">[19]</ref>, an RNN-Transducer with attention <ref type="bibr" target="#b19">[20]</ref>, and CNN with dilated convolutions and gating mechanisms <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MatchboxNet Architecture</head><p>The MatchboxNet architecture is based on the QuartzNet endto-end convolutional NN for ASR <ref type="bibr" target="#b0">[1]</ref>. Similar to QuartzNet, MatchboxNet uses 1D time-channel separable convolutions to reduce model size versus regular 1D convolutions.</p><p>A MatchboxNet-BxRxC model has B residual blocks. Each block has R sub-blocks. All sub-blocks in a block have the same number of output channels C (see <ref type="figure" target="#fig_1">Fig. 1</ref>). A basic sub-block consists of a 1D-time-channel separable convolution, 1x1 pointwise convolutions, batch norm, ReLU, and dropout. The 1D-time-channel separable convolution has C filters with a kernel of the size k. All models have four additional subblocks: one prologue layer -'Conv1' before the first block, and three epilogue sub-blocks ('Conv2', 'Conv3', and 'Conv4') before the final soft-max layer -see <ref type="figure" target="#fig_1">Figure 1</ref>) for details.</p><p>For example, the complete architecture for MatchboxNet-3x2x64 (B=3 blocks, R=2 sub-block per block, C=64 channels) is shown in the <ref type="table" target="#tab_0">Table 1</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We train MatchboxNet on the Google Speech Commands Dataset <ref type="bibr" target="#b4">[5]</ref>. The dataset has two versions which we denote by v1 and v2. Version 1 has 65,000 utterances from various speakers, each utterance is 1 second long. Each of these utterances belongs to one of 30 classes corresponding to common words like Yes, No, "Go", "Stop", "Left", "Down", numerical digits, etc. Version 2 has 105,000 utterances, each 1 second long, belonging to one of 35 classes. We re-balanced both training datasets so all classes will have the same number of samples by duplication of random samples. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Methodology</head><p>First, the input audio wave is converted into sequence of 64 melfrequency cepstral coefficients (MFCC) calculated from 25ms windows with a 10ms overlap. We perform symmetric padding of the temporal dimension with zeros to fixed length of 128 feature vectors per sample.</p><p>Next, the input is augmented with time shift perturbations in the range of T = [−5, 5] milliseconds and white noise with magnitude [−90, −46] dB. In addition, we applied SpecAugment <ref type="bibr" target="#b21">[22]</ref> with 2 continuous time mask of size [0, 25] time steps, and 2 continuous frequency mask of size [0, 15] frequency bands. We also used SpecCutout <ref type="bibr" target="#b22">[23]</ref>, with 5 rectangular masks with time and frequency dimensions similar to used in SpecAugment.</p><p>All models are trained with the NovoGrad optimizer <ref type="bibr" target="#b23">[24]</ref>, with β1 = 0.95 and β2 = 0.5. We utilize the Warmup-Hold-Decay learning rate schedule as in <ref type="bibr" target="#b24">[25]</ref> with a warm-up ratio of 5%, a hold ratio of 45%, and a polynomial (2nd order) decay <ref type="bibr" target="#b0">1</ref> One can use cross-entropy loss with class based weighing instead of re-balancing.</p><p>for the remaining 50% of the schedule. We use a maximum learning rate of 0.05 and a minimum learning rate of 0.001. We also incorporate weight decay of 0.001. We train all models for 200 epochs using mixed precision <ref type="bibr" target="#b25">[26]</ref> on 2 V-100 GPUs with a batch size of 128 per GPU. All experiments were carried out using the NeMo toolkit <ref type="bibr" target="#b26">[27]</ref> and plan to make all code necessary to reproduce these results available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Comparing with other published results, MatchboxNet-3x1x64 and MatchboxNet-3x2x64 obtain state-of-the-art (SOTA) accuracy on the Google Speech Commands dataset v1 and close to the SOTA on dataset v2, while requiring significantly fewer parameters than other models (see <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>). For comparison we used the following models:</p><p>• DenseNet-BC: a variant of ResNets with dense connectivity in between layers of each block <ref type="bibr" target="#b27">[28]</ref>. An intermediate point-wise convolution layer applied prior to the convolution block acts as a "bottleneck (B)" layer to reduce number of parameters. The number of channels in the convolutional layer can be reduced via a "compression (C)" factor.</p><p>• EdgeSpeechNet: ResNet-like deep residual ConvNet optimized for edge devices <ref type="bibr" target="#b28">[29]</ref>.</p><p>• Harmonic Tensor 2D-CNN: triangular band-pass filters of the n-th harmonic of center frequencies, are extracted and concatenated into a Harmonic Tensor of dimensionality H × F × T (harmonic × frequency × time) which is then passed into a simple 2D-Convolutional NN <ref type="bibr" target="#b29">[30]</ref>.</p><p>• 'Embedding + Head': the acoustic embedding model with multiple heads is pre-trained to distinguish between various keyword groups on 200 million 2-second audio clips from YouTube. These heads are discarded after pre-training, and a single head is used to fine-tune the embedding model on the downstream task <ref type="bibr" target="#b30">[31]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Scaling</head><p>We study the model scalability on the Google Speech Commands dataset v2 using MatchboxNet-3x2x64 as baseline. We scale model up using two methods: increase the depth B × R or increase the number of channels C. We found that both methods work in a similar way -the accuracy increases with model size until we hit ≈ 97.6% <ref type="table" target="#tab_3">(Table. 4</ref>). 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Robustness to Noise</head><p>To improve the robustness of MatchboxNet in the presence of noise, we retrained the model with background noise designed to interfere with speech signal. We construct a background noise dataset using audio samples from the Freesound database <ref type="bibr" target="#b33">[34]</ref>. We partition each of these audio samples into segments of 1 second each, with no overlap between segments. Following this methodology, we obtain close to 55,000 noise samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training with Noise Augmentation</head><p>We train MatchboxNet-3x1x64 by augmenting all training samples with randomly sampled noise segments. We scale the signal to noise ratio (SNR) randomly between 0 to 50 dB. In cases where the noise segment has a shorter duration than the training sample, we randomly augment a sub-segment of the training sample. The model accuracy on clean data is similar to the baseline model trained with basic augmentation only <ref type="table" target="#tab_4">(Table. 5</ref>). In order to evaluate the model robustness to environmental noise and background speech, we test the model with different noise conditions with SNR from -10 dB to +50 dB. We evaluate each test sample with 10 different randomly sampled noise segments, and compute the average accuracy over the entire test set. The model trained with additional noise augmentation is significantly more robust to external noise, even when the noise signal is much higher in amplitude than the noise used during training <ref type="figure" target="#fig_2">(Fig. 2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Speech Commands Recognition with Background Speech and Noise Detection</head><p>To use a keyword spotting model in a continuous audio stream, it should be able to differentiate speech commands from the background speech or noise. For this, we added roughly 3500 samples for environmental noise and similar number of background speech samples from Freesound database to the training set. We re-trained a MatchboxNet-3x1x64 model to classify all original commands plus two additional classes -'background noise' and 'background voice'. The model accuracy on the expanded speech commands datasets is shown in <ref type="table" target="#tab_5">Table 6</ref>. Training with additional background speech and noise augmentation significantly improves the model robustness to noise <ref type="figure" target="#fig_3">(Fig. 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Robustness To Noise With Model Scaling</head><p>We further evaluate the relative robustness of larger Match-boxNet models to environmental noise and background speech. We train two models, MatchboxNet-3x1x64 and 6x2x64 with the exact same noise augmentation scheme as described above.</p><p>We then evaluate the models on the unseen test set, perturbed by 10 random noise samples per test sample and compute the average accuracy. While both models are highly robust to external noise, MatchboxNet-6x2x64 consistently outperforms the smaller MatchboxNet-3x1x64 (see <ref type="table" target="#tab_6">Table 7</ref> and <ref type="figure">Figure 4</ref>)  <ref type="figure">Figure 4</ref>: MatchboxNet-3×1×64 and MatchboxNet-6×2×64 trained with additional background speech and noise augmentation. Accuracy vs SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we present MatchboxNet, a new end-to-end deep neural network architecture for efficient recognition of speech commands on devices with limited computational and memory resources. MatchboxNet is a deep residual network composed from 1D time-channel separable convolution, batch-norm layers, ReLU and dropout layers. The model has state-of-the-art accuracy on the Google Speech Commands v1 dataset with significantly fewer parameters than models with similar accuracy. MatchboxNet is scalable, allowing it to be deployed on devices with different memory and compute capabilities. By using intensive data augmentation with auxiliary background noise during training, we have shown the model can be made very robust with respect to background noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2015 Sainath and Parada proposed a convolutional NN for a Preprint. Submitted to INTERSPEECH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>MatchboxNet BxRxC model: B -number of blocks, R -number of sub-blocks, C -the number of channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>MatchboxNet-3 × 1 × 64 trained with background noise augmentation, Speech Commands dataset v2. Accuracy vs SNR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>MatchboxNet-3 × 1 × 64 trained with additional background speech and noise augmentation, expanded Google Speech Commands dataset v2. Accuracy vs SNR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MatchboxNet-3x2x64 model has B=3 blocks, each black has R=2 time-channel separable convolutional subblocks with C=64 channels, plus 4 additional sub-blocks: prologue -Conv1, and epilogue -Conv2, Conv3, Conv4).</figDesc><table><row><cell>Block</cell><cell># Blocks</cell><cell># Sub Blocks</cell><cell># Output Channels</cell><cell>Kernel</cell></row><row><cell>Conv1</cell><cell>1</cell><cell>1</cell><cell>128</cell><cell>11</cell></row><row><cell>B1</cell><cell>1</cell><cell>2</cell><cell>64</cell><cell>13</cell></row><row><cell>B2</cell><cell>1</cell><cell>2</cell><cell>64</cell><cell>15</cell></row><row><cell>B3</cell><cell>1</cell><cell>2</cell><cell>64</cell><cell>17</cell></row><row><cell>Conv2</cell><cell>1</cell><cell>1</cell><cell>128</cell><cell>29, dilation=2</cell></row><row><cell>Conv3</cell><cell>1</cell><cell>1</cell><cell>128</cell><cell>1</cell></row><row><cell>Conv4</cell><cell>1</cell><cell>1</cell><cell># classes</cell><cell>1</cell></row><row><cell>Soft-max</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cross-entropy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MatchboxNet on Google Speech Commands dataset v1, the accuracy is averaged over 5 trials (95% Confidence Interval).</figDesc><table><row><cell>Model</cell><cell># Parameters, K</cell><cell cols="2">Accuracy, % Reference</cell></row><row><cell>ResNet-15</cell><cell>238</cell><cell>95.8 ± 0.351</cell><cell>[17]</cell></row><row><cell>DenseNet-BC-100</cell><cell>800</cell><cell>96.77</cell><cell>[32]</cell></row><row><cell>EdgeSpeechNet-A</cell><cell>107</cell><cell>96.80</cell><cell>[29]</cell></row><row><cell>MatchboxNet-3x1x64</cell><cell>77</cell><cell>97.21 ± 0.067</cell><cell></cell></row><row><cell>MatchboxNet-3x2x64</cell><cell>93</cell><cell>97.48 ± 0.107</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MatchboxNet on Google Speech Commands dataset v2, the accuracy is averaged over 5 trials (95% Confidence Interval).</figDesc><table><row><cell>Model</cell><cell># Parameters, K</cell><cell cols="2">Accuracy, % Reference</cell></row><row><cell>Attention RNN</cell><cell>202</cell><cell>94.30</cell><cell>[33]</cell></row><row><cell>Harmonic Tensor 2D-CNN</cell><cell>-</cell><cell>96.39</cell><cell>[30]</cell></row><row><cell>"Embedding + Head" Model</cell><cell>385</cell><cell>97.7</cell><cell>[31]</cell></row><row><cell>MatchboxNet-3x1x64</cell><cell>77</cell><cell>96.91 ± 0.101</cell><cell></cell></row><row><cell>MatchboxNet-3x2x64</cell><cell>93</cell><cell>97.21 ± 0.072</cell><cell></cell></row><row><cell>MatchboxNet-6x2x64</cell><cell>140</cell><cell>97.37 ± 0.110</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Scaling up MatchboxNet depth and number of channels, Speech Commands Dataset v2</figDesc><table><row><cell cols="2">B R</cell><cell>C</cell><cell cols="2"># Parameters, K Accuracy,%</cell></row><row><cell>3</cell><cell>2</cell><cell>64</cell><cell>93</cell><cell>97.21</cell></row><row><cell>3</cell><cell>3</cell><cell>64</cell><cell>109</cell><cell>97.36</cell></row><row><cell>3</cell><cell>4</cell><cell>64</cell><cell>125</cell><cell>97.17</cell></row><row><cell>3</cell><cell>5</cell><cell>64</cell><cell>149</cell><cell>97.37</cell></row><row><cell>4</cell><cell>2</cell><cell>64</cell><cell>109</cell><cell>97.20</cell></row><row><cell>5</cell><cell>2</cell><cell>64</cell><cell>124</cell><cell>97.31</cell></row><row><cell>6</cell><cell>2</cell><cell>64</cell><cell>140</cell><cell>97.55</cell></row><row><cell>3</cell><cell>2</cell><cell>80</cell><cell>118</cell><cell>97.44</cell></row><row><cell>3</cell><cell>2</cell><cell>96</cell><cell>145</cell><cell>97.41</cell></row><row><cell>3</cell><cell cols="2">2 112</cell><cell>177</cell><cell>97.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>MatchboxNet</figDesc><table><row><cell></cell><cell cols="2">-3x1x64 trained with additional back-</cell></row><row><cell cols="3">ground speech and noise augmentation, Google Speech Com-</cell></row><row><cell cols="3">mands dataset v2. Accuracy (%) is averaged over 5 trials (95%</cell></row><row><cell>confidence interval).</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Augmentation</cell><cell>Accuracy, %</cell></row><row><cell>MatchboxNet 3x1x64</cell><cell>basic</cell><cell>96.91 ± 0.101</cell></row><row><cell cols="3">MatchboxNet 3x1x64 + background speech and noise 97.05 ± 0.099</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>MatchboxNet-3 × 1 × 64 trained with additional background speech and noise augmentation, expanded Speech Commands dataset. Accuracy (%) is averaged over 5 trials (95% confidence interval).</figDesc><table><row><cell>Model</cell><cell cols="3">Dataset # Parameters Accuracy, %</cell></row><row><cell>MatchboxNet-3x1x64</cell><cell>v1</cell><cell>77K</cell><cell>96.88 ± 0.073</cell></row><row><cell>MatchboxNet-3x1x64</cell><cell>v2</cell><cell>77K</cell><cell>96.97 ± 0.071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>MatchboxNet-3 × 1 × 64 and MatchboxNet-6 × 2 × 64 trained with additional background speech and noise augmentation. Accuracy (%) is averaged over 10 trials with random noise. 87.21 94.53 96.40 96.89 97.05 97.09 6x2x64 71.02 88.81 95.04 96.74 97.16 97.29 97.33</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>SNR (in dB)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>-10</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>3x1x64 69.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We analyzed the remaining misclassified samples, and found that most of them are very hard to recognize, even for humans.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>We would like to thank NVIDIA AI Applications team for the help and valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">QuartzNet: deep automatic speech recognition with 1D time-channel separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Depthwise separable convolutions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A time-delay neural network architecture for isolated word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A time-delay neural network architecture for isolated word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global optimization of a neural network-hidden Markov model hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Flammia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">252259</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">IPA: improved phone modelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tandem connectionist feature extraction for conventional hmm systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Biologically plausible speech recognition with LSTM neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biologically Inspired Approaches to Advanced Information Technology. BioADIT</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05390</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Acoustic modeling with densely connected residual network for multichannel speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FastGRNN: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient keyword spotting using dilated convolutions and gating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic gradient methods with layerwise adaptive moments for training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">NeMo: a toolkit for building ai applications using neural modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09577</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">EdgeSpeechNets: Highly efficient deep neural networks for speech recognition on the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-driven harmonic filters for audio representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training keyword spotters with limited and synthesized speech data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01322</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11102</idno>
		<title level="m">On feature normalization and data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L D S</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Freesound technical demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="411" to="412" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
