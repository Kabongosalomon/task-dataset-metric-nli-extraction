<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide-and-conquer based Large-Scale Spectral Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiucai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Imakura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Sakurai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<postCode>305-8577</postCode>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Divide-and-conquer based Large-Scale Spectral Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Spectral Clustering Landmark selection Approximate Similarity Computation Large-scale clustering Large-scale datasets</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Spectral clustering is one of the most popular clustering methods. However, how to balance the efficiency and effectiveness of the large-scale spectral clustering with limited computing resources has not been properly solved for a long time. In this paper, we propose a divide-and-conquer based largescale spectral clustering method to strike a good balance between efficiency and effectiveness. In the proposed method, a divide-and-conquer based landmark selection algorithm and a novel approximate similarity matrix approach are designed to construct a sparse similarity matrix within low computational complexities. Then clustering results can be computed quickly through a bipartite graph partition process. The proposed method achieves the lower computational complexity than most existing large-scale spectral clustering methods. Experimental results on ten large-scale datasets have demonstrated the efficiency and effectiveness of the proposed methods. The MATLAB code of the proposed method and experimental datasets are available at https://github.com/Li-Hongmin/MyPaperWithCode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is one of the most fundamental problems in data mining and machine learning, aiming to categorize data points into clusters such that the data points in the same cluster are more similar while data points in different clusters are more different from each other <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref>. Spectral clustering has attracted increasing attention due to the promising ability to deal with nonlinearly separable datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. It has been successfully applied to various problem domains such as biology <ref type="bibr" target="#b17">[18]</ref>, image segmentation <ref type="bibr" target="#b30">[31]</ref>, and recommend systems <ref type="bibr">[33,</ref><ref type="bibr" target="#b27">28]</ref>. Although spectral clustering algorithm often provides better performances than traditional clustering algorithm likes -means especially for complex datasets, it is significantly limited to be applied to large-scale datasets due to its high computational complexity and space complexity <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The conventional spectral clustering algorithm mainly consists of two high-cost steps, i.e., similarity matrix construction and eigen-decomposition. For a dataset with objects, the two steps take computational complexities of ( 2 ) and ( 3 ), respectively. The computational consumption of these two steps is the main reason that hinders the application of spectral clustering algorithms on largescale data.</p><p>In recent years, there has been an increasing amount of literature on alleviating the computational complexity of spectral clustering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Previous research <ref type="bibr" target="#b6">[7]</ref> has established that the sparse similarity matrix construed by only remaining -nearest neighbors or -nearest neighbors can efficiently reduce the space complexity. As a result, some sparse eigensolvers can solve the eigen-decomposition problems within the lower computational complexity. The matrix specification strategy can avoid storing the dense sim-ilarity matrix to reduces the space complexity, but it still needs to compute the dense similarity matrix at first, which costs ( 2 ) computational complexity. Besides the matrix specification, another commonly used strategy is based on a cross-similarity matrix construction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>. Fowlkes et al. <ref type="bibr" target="#b9">[10]</ref> apply the Nyström method to reduce the high complexity of spectral clustering algorithm, which first randomly selects a small subset of samples as landmarks, then construct a similarity sub-matrix between these landmarks and remaining samples. Although the random landmark selection is very efficient, it is often unstable concerning the quality of the landmark set. Moreover, it has been shown that a larger is often favorable for better approximation. To address the potential instability of random selection, Cai et al. <ref type="bibr" target="#b3">[4]</ref> extend the Nyström method and propose the landmark-based spectral clustering method, which uses -means to obtain cluster centers as landmark points to construct the similarity sub-matrix. With the constructed × sub-matrix, they then convert it into sparse by preserving the -nearest landmarks of the data points and filling with zeros to others. By the -means based landmarks selection, the LSC algorithm shows better performance than Nyström. On this basis, some studies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13]</ref> on the landmark selection are further proposed to improve the instability of sub-matrix based large-scale spectral clustering. However, the computational complexity of the submatrix construction can still be a critical bottleneck when dealing with large-scale clustering tasks. Huang et al. <ref type="bibr" target="#b11">[12]</ref> propose a hybrid representative (landmark) selection method that initializes candidate samples randomly from the dataset and performs -means to obtain cluster centers as the representative points then computes the approximation ofnearest representatives. It does not compute the dense similarity sub-matrix but approximates a sparse sub-matrix, further reducing similarity construction costs. However, those sub-matrix based spectral clustering algorithms are typically restricted by an ( ) or ( 1 2 ) complexity bottleneck, which is still a critical hurdle for them to deal with large-scale datasets where a larger is often desired for achieving better approximation. Although some considerable studies have been proposed in recent years, it remains a highly challenging problem, i.e., how to make spectral clustering handle large-scale datasets efficiently and effectively within limited computing resources.</p><p>In this paper, to achieve a better balance between the effectiveness and efficiency of the spectral clustering for largescale datasets, we propose the divide-and-conquer spectral clustering (DnC-SC) method. In DnC-SC, a novel divideand-conquer based landmark selection method is proposed to generate high-quality landmarks, which reduces the computational complexity of -means based selection from ( ) to ( ), where is the selection rate parameter that determines the upper bound of computational complexity. Besides, a fast approximation method for -nearest landmarks is designed to efficiently build a sparse sub-matrix with ( ) computational complexity and ( ) space complexity. A cross similarity matrix is constructed between the data points and the landmarks, which can be interpreted as the edges matrix of a bipartite graph. The bipartite graph partition is then conducted to solve the spectrum with ( ( + ) + 3 ), where is the number of clusters. Finally, the -means method is used to obtain the clustering result on the spectrum with ( 2 ), where is the number of iterations during -means. As it generally holds that , , ≪ ≪ , the computational and space complexity of our DnC-SC algorithm are respectively dominated by ( ) and ( ). The experimental results on eight large-scale datasets (consisting of five real-word datasets and five synthetic datasets) show the priority performance of proposed methods on both efficiency and effectiveness.</p><p>The main contributions of the proposed method are summaries as follows:</p><p>• A divide-and-conquer-based landmark selection method is proposed to efficiently find centralized subset centers as landmarks in a recursive manner.</p><p>• A fast -nearest landmarks search method is designed, which uses centers' nature of landmarks to identify the most possible -nearest landmarks candidates.</p><p>• A large-scale spectral clustering algorithm termed DnC-SC is proposed, which efficiently constructs the similarity matrix and uses bipartite graph partition to obtain final clustering results. Its computational and space complexity is dominated by ( ) and ( ), which achieves the lower computational complexity than most existing large-scale spectral clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>This section reviews the literature related to spectral clustering and large-scale spectral clustering extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spectral Clustering</head><p>Spectral clustering aims to partition the data points into clusters using the spectrum of the graph Laplacians <ref type="bibr" target="#b22">[23]</ref>. Given a dataset = 1 , … , with data points, spectral clustering algorithm first constructs similarity matrix , where indicates the similarity between data points and via a similarity measure metric. Let = − , where is called graph Laplacian and is a diagonal matrix with = ∑</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. The objective function of spectral clustering can be formulated based on the graph Laplacian as follow:</p><formula xml:id="formula_0">max tr , s.t. = ,<label>(1)</label></formula><p>where tr(⋅) denotes the trace norm of a matrix. The rows of matrix are the low dimensional embedding of the original data points. Generally, spectral clustering computes as the bottom eigenvectors of , and finally applies -means on to obtain the clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Similarity Matrix for Large-scale Spectral Clustering</head><p>To capture the relationship between all data points in , an × similarity matrix is needed to be constructed in conventional spectral clustering <ref type="bibr" target="#b22">[23]</ref>, which costs ( 2 ) time and ( 2 ) memory and is not feasible for large-scale clustering tasks. Instead of a full similarity matrix, many accelerated spectral clustering methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> are using a similarity sub-matrix to represent each data points by the cross-similarity between data points and a set of representative data points (i.e., landmarks) via some similarity measures, as</p><formula xml:id="formula_1">= Φ( , ),<label>(2)</label></formula><p>where = { 1 , 2 , … , } ( ≪ ) is a set of landmarks with the same dimension to , Φ(⋅) indicate a similarity measure metric, and ∈ ℝ × is the similarity sub-matrix to represent the ∈ ℝ × with respect to the ∈ ℝ × . For large-scale spectral clustering using such similarity matrix, a symmetric similarity matrix can be designed as <ref type="bibr" target="#b28">[29]</ref> = .</p><p>(</p><p>The size of matrix is ( + )×( + ). Taking the advantage of the bipartite structure, some fast eigen-decomposition methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> can then be used to obtain the spectral embedding. Finally, -means is conducted on the embedding to obtain clustering results.</p><p>The clustering result is directly related to the quality of that consists of the similarities between data points and landmarks. Thus, the performance of landmark selection is crucial to the clustering result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Framework</head><p>To further reduce the complexity of spectral clustering, we propose the DnC-SC method that complies with the submatrix based formulation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> and aims to break through</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divide-and-conquer based</head><p>Landmark Selection Data <ref type="figure">Figure 1</ref>: An overview of proposed DnC-SC method. Given a dataset, the DnC-SC method first finds the landmarks via divideand-conquer based landmark selection, then approximately constructs the similarity matrix, finally conducts a bipartite graph partition to obtain final clustering results. Our main contributions focus on the first two phases (colored as orange), i.e., the landmark selection and similarity construction phases.</p><p>the efficiency bottleneck of previous algorithms. DnC-SC consists of three phases: (1) Divide-and-conquer based landmark selection: we consider landmark selection as an optimization problem and present a divide-and-conquer based landmark selection method to find the landmarks via solving the sub-optimization problems recursively. (2) Approximate similarity matrix construction: we design a novel strategy to efficiently approximate the -nearest landmarks for each data point and construct a sparse cross-similarity matrix between the data points and the landmarks. (3) Bipartite graph partitioning: we interpret the cross-similarity as a bipartite graph and conduct a bipartite graph partition to obtain the clustering result. We summary the proposed method in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Divide-and-conquer based Landmark Selection</head><p>Let = { 1 , 2 , ⋯ , } denote a set of landmarks, where ∈ ℝ has the same dimension as . Ideally, the landmark points 1 , 2 , ⋯ , would roughly represent the distribution of . Since the nearest landmark graph will be utilized in the similarity construction, would intuitively best represent the original data points when total distances between any ∈ and their nearest landmarks are minimum. Therefore, we measure how well represent the nearest samples via the residual sum of squares (RSS) as follows,</p><formula xml:id="formula_3">= ∑ =1 ∑ ∈ ‖ ‖ ‖ − ‖ ‖ ‖ 2 ,<label>(4)</label></formula><p>where denotes RSS and , 2 , … , indicate the subsets that are nearest to 1 , 2 , ⋯ , , respectively. From the clustering view, , 2 , … , can be considered as the clusters. Therefore, the objective function of landmarks can be written as follows:</p><formula xml:id="formula_4">= arg min 1 ,…, ∑ =1 ∑ ∈ ‖ ‖ ‖ − ‖ ‖ ‖ 2 .<label>(5)</label></formula><p>According to <ref type="bibr" target="#b4">(5)</ref>, can be solved by -means directly. Some previous studies <ref type="bibr" target="#b3">[4]</ref> show the effectiveness of -means based selection. However, directly conducting -means on largescale datasets faces a high time cost of ( ). Moreover, </p><formula xml:id="formula_5">1 st iteration 2 nd iteration 3 rd iteration ! (!) = 3 ! ($) = 3 $ ($) = 3 % ($) = 3 (2) (1) Dividing</formula><p>Dividing Centers Samples Dividing Dividing <ref type="figure">Figure 2</ref>: An illustration of the divide-and-conquer base landmark selection: (1) The dataset is initially divided into (1) 1 = 3 initial subsets; (2) Each subset is further divided into 3 smaller subsets and the total number of subsets reaches = 9. Finally, the centers of subsets are turned as landmarks.</p><p>-means often needs more iterations to converges on largescale datasets.</p><p>Thus, we propose a divide-and-conquer based landmark selection method, which aims to find a set of high-quality landmarks efficiently. Denote as a small number and ≪ ≪ . We define as selection rate parameter that is the upper boundary of desired subset number in each dividing process to limit the computational complexity. Instead of directly dividing data points into subsets for landmark selection, we first divide data points into subsets, then recursively divide each subset into smaller subsets ( ≤ ), until the total number of subsets reaches . <ref type="figure">Figure 2</ref> gives a simple example. The data points are recursively divided into subsets until the total number of subsets is , which avoids directly applying -means to obtain too many subsets at once. In the divide-and-conquer strategy, the number of desired subsets in each iteration is much smaller than , and the subsets are smaller and smaller during iterations. Suppose each dividing will divide subsets of the same size and the number of iteration of each dividing is , we can estimate the total computational complexity as follows:</p><formula xml:id="formula_6">(( + + 2 + 3 + … ) ) = ( ). (6)</formula><p>According to <ref type="formula">(6)</ref>, compared with -means based landmark selection, the divide-and-conquer strategy can naturally reduce the computational complexity from ( ) to ( ). Moreover, we design an efficient dividing algorithm, named light--means, to further accelerate the whole process into ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Divide-and-conquer Selection Strategy</head><p>Before starting landmark selection, we first review the optimization problem <ref type="bibr" target="#b3">(4)</ref>. The variables in (4) are the 1 , … , which are used to map the unique 1 , … , . By setting the subsets 1 , … , and landmark number as the variables, we can rewrite the optimization problem (4) into a function form as follows:</p><formula xml:id="formula_7">( , ) = arg min 1 ,…, ∑ =1 ∑ ∈ ‖ ‖ ‖ − ‖ ‖ ‖ 2 ,<label>(7)</label></formula><p>where (⋅) indicates a centralized clustering problem that divides into subsets and is the center of subset . For any dividing problem that divides into ℎ subsets, the function ( , ℎ) can be used to describe the dividing problem, and its computational complexity is (‖ ‖ℎ ), where ‖ ‖ is the total number of samples in . More importantly, function ( ⋅ ℎ) can be used to derive the recursive function as follows:</p><formula xml:id="formula_8">( , ℎ) = ⋃ =1 ( , ),<label>(8)</label></formula><formula xml:id="formula_9">{ 1 , … , } = ( , ),<label>(9)</label></formula><p>where is a subset of and = ⋃ =1 ; is the desired subset number of subset and ℎ = ∑ =1 ; &lt; ℎ. (8) can simply divide any optimization problem into subproblems, which builds a bridge between global problem ( , ℎ) and local problem ( , ). We can recursively apply <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_9">(9)</ref> to divide the optimization problem <ref type="formula" target="#formula_7">(7)</ref> into the sub-problems small enough and solve them locally and efficiently.</p><p>Denote as the total number of subsets during -th iteration. We will stop the recursive process in the -th iteration when reaches the desired total number of subsets . Initially, we assign all data points as one subset. As we only have one subset ( 1 = 1 &lt; ), the dividing process happens. Let ( ) be the desired number of subset for dividing process on -th subset during -th iteration. We naturally set the desired number (1) 1 = . However, directly apply ( , ) may be time-consuming. For &gt; , we force = to obtain subsets partially. As a result, we have the initial setting as follows:</p><formula xml:id="formula_10">(1) 1 = (1) 1 = (10)</formula><p>In the first iteration, we divide <ref type="bibr" target="#b0">(1)</ref> 1 into (1) 1 subsets in as follows:</p><formula xml:id="formula_11">{ (2) 1 , (2) 2 , ⋯ , (2) 2 } = ( (1) 1 , (1) 1 ),<label>(11)</label></formula><p>where <ref type="bibr" target="#b0">(1)</ref> indicates the -th subset during first iteration and</p><formula xml:id="formula_12">2 = ∑ 1 =1 ( ) 1 = (1) 1</formula><p>is the total number of subsets. From the second iteration, there are more and more subsets being obtained. Thus, we need a subset number allocation strategy to determine the desired number of subsets and guide iteration dynamically. We define the RSS of subset ( ) as</p><formula xml:id="formula_13">( ) = ∑ ∈ ( ) ‖ ‖ − ‖ ‖ 2 ,<label>(12)</label></formula><p>where is the center of the subset ( ) . Consider the global problem <ref type="formula" target="#formula_4">(5)</ref>, the desired number of subsets should be proportional to their RSS. We propose a dynamical allocation strategy as follows:</p><formula xml:id="formula_14">( ) = ⎧ ⎪ ⎨ ⎪ ⎩ ( ) ∑ ( ) , if ( ) ∑ ( ) &lt; , , otherwise,<label>(13)</label></formula><p>where ( ) is the allocated divide number for subset ( ) . Then, all ( ) are turned as integers and fix the +1 &lt;= , where +1 = ∑ ( ) . After obtaining ( ) 1 , … , ( ) ( &lt; ), we will divide each subset ( ) into ( ) smaller subsets via ( ( ) , ( ) ). We then collect all subsets as follows:</p><formula xml:id="formula_15">{ ( +1) 1 , ( +1)) 2 , ⋯ , ( +1) +1 } = ⋃ =1 ( ( ) , ( ) ). (14)</formula><p>We repeat the above process until subsets have been produced and set the subset centers as the landmarks. Take an example using <ref type="figure">Figure 2</ref>, where we set = 3 and = 9. In the first iteration, we assign all data points as one subset. Since the desired landmark number &gt; , we set (1) 1 = 3. Then we initially divide the dataset (1) 1 = into (1) 1 = 3 subsets. In the second iteration, there are three subsets <ref type="bibr" target="#b1">(2)</ref> 1 , (2) 2 and (2) 3 . According to <ref type="bibr" target="#b12">(13)</ref>, we compute</p><formula xml:id="formula_16">the (3) 1 , (3) 2 , (3) 2 . Suppose (3) 1 = (3) 2 = (3) 2 = 3, we then divide (2) 1 , (2) 2 , (2) 3 into (3) 1 , (3) 2 ,<label>(3)</label></formula><p>2 smaller subset respectively. In the third iteration, there are 9 subsets</p><formula xml:id="formula_17">(3) 1 , … , (3) 9 .</formula><p>Since the total number of subsets 3 = 9 reaches the desired landmark number = 9, we stop the recursive process in the third iteration. Finally, compute the subset centers 1 , 2 , … , of (3) 1 , … , (3) 9 and set them as the landmarks.</p><p>Note that the dividing process (⋅) can be directly solved by the -means method. However, directly apply -means on large data is time-consuming. To further reduce the complexity, we propose a modified -means method, named light--means. When dataset size is large, we conduct the dividing process via light--means. Otherwise, we use the traditional -means method. We summary the divide-andconquer based landmark selection method in Algorithm 1.  <ref type="bibr" target="#b15">16</ref> Collect the latest cluster centers as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Light--means Algorithm</head><p>We define ′ as an upper bound. When the size of ( ) is larger than ′ , we will use light--means to compute ( ( ) , ( ) ). The light--means is performed as the following steps:</p><p>1. Randomly select ′ representatives from ( ) and denote them in a set as and the complement of is .</p><p>2. Conduct -means to divide into ( ) subsets; 3. Find the nearest subset centers for the remained data points in ; 4. Assign the remained data points in to their the nearest subset (with the center nearest to these points). <ref type="figure" target="#fig_1">Figure 3</ref> shows a comparison between -means and light--means method, which are the implementation examples of (1) in <ref type="figure">Figure 2</ref>. Given a subset <ref type="bibr" target="#b0">(1)</ref> 1 , the light--means first randomly select ′ data points and denotes them as and the complement is</p><formula xml:id="formula_18">( (1) 1 = ∪ ).</formula><p>Then the -means is used to divide into (1) 1 subsets, i.e., 1 , 2 , 3 . For each data points in , find its nearest center and assign it to the subset, i.e., 1 , 2 , 3 , according to its nearest center. Finally, return the combined subsets 2 1 , 2 2 , 2 3 as the results of this dividing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Light--means</head><p>Input: Data ( ) , the number of cluster ( ) , number of samples ′ ; Output: ( ) subsets;</p><p>1 Randomly select ′ samples from ( ) and denote them as ; 2 Denote the complement of as ;</p><p>3 Apply -means to divide into ( ) subsets; <ref type="bibr" target="#b3">4</ref> Find the nearest center of samples in ; <ref type="bibr" target="#b4">5</ref> Assign the samples in to the subset according to their nearest centers.</p><p>Denote ( ) as the number of samples in ( ) . The computational complexity of light--means for the dividing process ( ( ) , ( ) ) should be ( ′ ( ) )+ (( ( ) − ′ ) ( ) )) = ( ( ) ( ) + ′ ( ) ( − 1)), where ( ( ) ( ) ) is the dominant term. While, -means costs ( ( ) ( ) for the same dividing process. Compared with -means, light--means significantly alleviates the computational complexity on iterative optimization. Empirically, the number of ′ is suggested to be several times larger than , e.g., ′ = 10 , to provide enough samples for the -means algorithm. Since our landmark selection focuses more on the local dividing, the light--means can effectively divide the large subsets into small ones and find more accurate subsets locally. Finally, we summarise the light--means method in Algorithm 2.  By introducing the divide-and-conquer based landmark selection, the complexity of landmark selection is reduced to ( ) from ( ) of -means based selection. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates that the proposed divide-and-conquer based landmark selection can better represent data distribution than the random selection and has similar performance -means based selection, but it has the lower complexity than -means based selection.</p><formula xml:id="formula_19">! " ! # ! " ! $ " ! % " ! &amp; " ! ' … " ! ' " … " ! # ! " ! # " ! $ " ! % " ! &amp; " ! ' … " ! ' " … (1) (2) " ! ' " " ! # = { " ! # ,…, " ! ' ,…, " ! ' " } ' ! = { " ! # ,…, " ! ' }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approximate Similarity Matrix Construction</head><p>After landmark selection, the next object is to construct a similarity matrix between entire data points and the landmarks. Instead of dense similarity matrix, we design a similarity matrix ∈ ℝ × according to -nearest neighbor as follows:</p><formula xml:id="formula_20">= exp( −‖ − ‖ 2 2 2 ), if ∈ ( ), 0, otherwise,<label>(15)</label></formula><p>where ( ) denotes the set of -nearest landmarks of and is the bandwidth of Gaussian kernel. Note that there are only non-zero entries in the sparse matrix .</p><p>To estimate ( ), we propose a new -nearest landmarks approximation method. The main idea is to use the subset centers' nature of landmarks to estimate the possible nearest candidates, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Formally, we denote as the subset that belongs to, and the landmark 1 as the center of . Since 1 is the subset center of , it essentially is the nearest landmark of according to <ref type="bibr" target="#b4">(5)</ref>. Take the advantage of this landmark nature, we search the -nearest landmarks of each data point ∈ according to following two steps:</p><p>Step 1: Find ′ possibles candidates. As (1) of <ref type="figure" target="#fig_3">Figure   5</ref> shows, we find the ′ -nearest ( ′ &gt; ) landmarks of 1 and denoted them as</p><formula xml:id="formula_21">′ ( 1 ) = 1 , … , ′ .</formula><p>Since the exact -nearest landmarks of are highly possible closed to 1 , we treat ′ ( 1 ) as possible candidates set. Empirically, the number of ′ is suggested to be several times larger than , e.g., ′ = 10 , to provide enough candidates to search ( ).</p><p>Step 2: Search the -nearest landmarks. As <ref type="formula" target="#formula_1">(2)</ref> of <ref type="figure" target="#fig_3">Figure 5</ref> shows, we search the -nearest landmarks of among ′ ( 1 ) and denote them as ( ). After the -nearest landmarks approximation, we compute the similarity matrix according to <ref type="bibr" target="#b14">(15)</ref>. For all data points, the complexity of step 1 is ( 2 ( + )) and step 2 is ( ( + )). The computational complexity of our similarity construction is ( 2 ( + ) + ( + )). As , ≪ ≪ , the dominate term in the complexity is ( ). Compared with the exact similarity construction of ( ) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>, our method is much faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bipartite Graph Partitioning</head><p>After obtaining the similarity matrix , we conduct graph partition on the graph Laplacian of the similarity matrix. The similarity matrix reflects the relationships between and . Therefore can be interpreted as the cross-similarity matrix of the bipartite graph:</p><formula xml:id="formula_22">= { , , },<label>(16)</label></formula><p>where ∪ is the node-set. As a result, the object is changed to a bipartite graph partition problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. The conventional spectral clustering finds a low dimensional embedding via the spectrum of graph Laplacian, which solves the generalized eigen-problem <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_23">= ,<label>(17)</label></formula><p>where = − is the graph Laplacian and is a diagonal matrix with = ∑</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. Substituting the bipartite similarity matrix <ref type="formula" target="#formula_2">(3)</ref> into <ref type="formula" target="#formula_0">(17)</ref>, we have</p><formula xml:id="formula_24">− − = 0 0 ,<label>(18)</label></formula><p>where ∈ ℝ × and ∈ ℝ × are the diagonal matrices whose entries are ( , ) = ∑ =1 and ( , ) = ∑ =1 , respectively. Thus, the eigenvector can be interpreted as two parts ∈ ℝ × and ∈ ℝ × . is the spectral embedding on side while is the spectral embedding on side. To find the low dimensional embedding for objects, we rewrite (18) as follows:</p><formula xml:id="formula_25">− = ,<label>(19)</label></formula><formula xml:id="formula_26">− ⊤ = .<label>(20)</label></formula><p>From <ref type="formula" target="#formula_0">(19)</ref>, we compute eigenvector as follows:</p><formula xml:id="formula_27">= 1 1 − −1 ,<label>(21)</label></formula><p>where −1 is also called the transition probability matrix <ref type="bibr" target="#b14">[15]</ref>. Substituting <ref type="bibr" target="#b20">(21)</ref> into <ref type="formula" target="#formula_1">(20)</ref>, we have</p><formula xml:id="formula_28">( − −1 ) = (2 − ) .<label>(22)</label></formula><p>According to <ref type="bibr" target="#b21">(22)</ref>, we design a smaller eigen-problem (23) as follows:</p><formula xml:id="formula_29">= ,<label>(23)</label></formula><p>where ∶= − −1 , ∶= (2 − ). The matrix is also the Laplacian of the graph = { , }. The eigen-problem (23) can be solve within a smaller time cost ( 3 ). Note that 0 ≤ &lt; 1 and 0 ≤ &lt; 1. The value of is unique and increases along with . Supposing 1 , … , are the bottom eigenvalues of the eigen-problem (23), the 1 , … , are also the bottom eigenvalues of the eigen-problem <ref type="bibr" target="#b16">(17)</ref>. Therefore the bottom eigenvalues can be obtained according to <ref type="bibr" target="#b20">(21)</ref>. Since the 1 1− is constant value, in practice, we compute the spectral embedding as follows:</p><formula xml:id="formula_30">= −1 .<label>(24)</label></formula><p>After solving the eigen-problem, the bottom eigenvectors in are stacked to form a low dimensional embedding ∈ ℝ × . In practice, we normalized by its 1-norm as , then apply -means clustering oñ to obtain the final clustering results <ref type="bibr" target="#b16">[17]</ref>. The -means clustering is then performed on this embedding to obtain the clusters as the final clustering result with ( 2 ) computational complexity. We summary the divided-and-conquer based large-scale spectral clustering in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computational Complexity Analysis</head><p>In this section, we summary the computational cost of the proposed method in each phase.</p><p>The divide-and-conquer based landmark selection takes   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method landmark selection</head><p>Similarity construction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eigendecomposition</head><formula xml:id="formula_31">Nyström / ( ) ( + 3 ) LSC-R / ( ) ( 2 + 3 ) LSC-K ( ) ( ) ( 2 + 3 ) U-SPEC ( 2 ) ( 1 2 ) ( ( + )+ 3 ) DnC-SC ( ) ( ) ( ( + )+ 3 )</formula><p>* The final -means is ( 2 ) for each method. + + 2 ) + 3 + 2 ( + )), where ( ( ) ) is the dominant term. <ref type="table" target="#tab_3">Table 1</ref> provides a comparison of computational complexity of our DnC-SC algorithm against several other large-scale spectral clustering algorithms. The space complexity of DnC-SC is ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relations with Other Methods</head><p>As a large-scale spectral clustering method, the proposed method is closely related to the methods in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. We compare the proposed method with the two methods to discuss the improvements of the proposed method.</p><p>Firstly, we compare them on the landmark selection methods. Both the two methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> directly or indirectly apply -means based landmark selection. LSC-K method <ref type="bibr" target="#b3">[4]</ref> directly conduct -means algorithm to select landmarks within a high computational complexity ( ). While the U-SPEC <ref type="bibr" target="#b11">[12]</ref> indirectly conduct -means algorithm on a random set of samples to select landmarks, which finds a bal-ance between -means and random selection within ( 2 ) time cost. Despite U-SPEC can efficiently find landmarks, it also has two limitations: 1) The quality of landmarks highly depends on how good the random set of samples is set up; 2) Since landmarks are not the centers for all data points, the center's nature of landmark can not be used to approximate the similarity matrix. The proposed method uses the divide-and-conquer based landmark selection, which can effectively produce high-quality landmarks. We design a objection function <ref type="formula" target="#formula_4">(5)</ref> is to find the landmarks that best represent all data points with minimum RSS. We then propose a divide-and-conquer strategy to divide (5) into local subproblems and use light--means to effectively solve them. Finally, we combine all sub-problems and obtain landmarks. The our landmark selection produces landmarks within ( ) computational time. Moreover, the our landmarks are essentially the centers of subsets for all data points, which can be used to approximate the similarity matrix next.</p><p>Secondly, we compare them on the similarity construction. LSC-K needs to cost ( ) to compute the dense similarity matrix at first to conduct the -nearest neighbor sparse. The U-SPEC method indirectly computes the sparse similarity sub-matrix in a coarse-to-fine mechanism to approximate the -nearest landmarks within ( 1 2 ) time cost. U-SPEC first cluster landmarks into 1 2 clusters and then compute the distances between data points and 1 2 cluster centers to find the possible range of nearest landmarks. For the proposed method, since the landmarks essentially are the cluster centers of data points, we can easily identify a highly possible range of -nearest landmarks according to the centers' nature of landmarks and find -nearest landmarks in this range. The proposed -nearest landmarks search method costs ( ) computational time. Overall, DnC-SC consists of divide-and-conquer based landmark selection, approximate similarity construction, and bipartite graph partition. It conducts spectral clustering tasks within ( ) computational complexity and ( ) space complexity, which is faster than most existing large-scale spectral clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct experiments on five real and five synthetic datasets to evaluate the performance of the proposed DnC-SC methods. The comparison experiments against several state-of-the-art spectral clustering methods show better performance on clustering quality and efficiency for DnC-SC methods. Besides that, the analysis of the parameters is performed. For each experiment, the test method is repeated 20 times, and the average performance is reported. All experiments are conducted in Matlab R2020a on a Mac Pro with 3 GHz 8-Core Intel Xeon E5 and 16 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Measures</head><p>Our experiments are conducted on eight large-scale datasets, varying from nine thousand to as large as twenty million data   <ref type="figure">Figure 6</ref> shows the synthetic datasets. The properties of the datasets are summarized in <ref type="table" target="#tab_4">Table 2</ref>. We adopt two widely used evaluation metrics, i.e., Normalized Mutual Information (NMI) <ref type="bibr" target="#b21">[22]</ref> and Accuracy (ACC) <ref type="bibr" target="#b25">[26]</ref>, to evaluate the clustering results. <ref type="figure">Let = [ 1 , 2 , ..</ref>., ] be the data matrix. For each data point , denote and as the cluster label of ground truth and obtained cluster label from clustering methods, respectively. The ACC is defined as:</p><formula xml:id="formula_32">(a) TS-60K (1%) (b) TM-1M (1%) (c) TC-6M (1%) (d) CG-10M (0.1%) (e) FL-20M (0.1%)</formula><formula xml:id="formula_33">ACC = ∑ =1 ( , map( )) ,<label>(25)</label></formula><p>where is the number of data and ( , ) is a function to check and are equal or not, returning 1 if equals otherwise returning 0. The map( ) is a best mapping function that maps each predicted label to the most possibly true cluster label by permuting operations <ref type="bibr" target="#b24">[25]</ref>. Let denote a set of clusters of ground truth and obtained from clustering methods. Mutual information (MI) is defined as</p><formula xml:id="formula_34">( , ) = ∑ ∈ , ∈ ( , )ln ( , ) ( ) ( ) ,<label>(26)</label></formula><p>where ( ) and ( ) are marginal probabilities that a sample happens to belong to cluster or while ( , ) is the joint probabilities that a sample happens to belong to cluster both and . The NMI is the normalization of MI by the joint entropy as follow:</p><formula xml:id="formula_35">( , ) = ∑ ∈ , ∈ ( , )ln ( , ) ( ) ( ) − ∑ ∈ , ∈ ( , )ln( ( , )) ,<label>(27)</label></formula><p>A better clustering result will provide a larger value of NMI/ACC. Both NMI and ACC are in the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline Methods and Experimental Settings</head><p>In this experiments, we compare the proposed method with two baseline clustering methods, which are -means clustering and spectral clustering (SC) <ref type="bibr" target="#b6">[7]</ref> , as well as six state-of-the-art large-scale spectral clustering methods. The compared spectral clustering methods are listed as follows:</p><p>1. SC <ref type="bibr" target="#b6">[7]</ref>: original spectral clustering 6 . 2. Nyström <ref type="bibr" target="#b9">[10]</ref>: Nyström spectral clustering 6 . 3. LSC-K <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using -means based landmark selection 7 . 4. LSC-R <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using random landmark selection 7 . 5. LSC-KH <ref type="bibr" target="#b26">[27]</ref>: Landmark-based spectral clustering using -means partition to find the hubs as the landmarks 8 . 6. LSC-RH <ref type="bibr" target="#b26">[27]</ref>: Landmark-based spectral clustering using random partition to find the hubs as the landmarks 8 . 7. U-SPEC <ref type="bibr" target="#b11">[12]</ref>: Ultra-Scalable Spectral Clustering 5 .</p><p>There are several common parameters among the methods mentioned above. We set these parameters as follow:</p><p>• We set the number of landmarks or landmarks as = 1000 for DnC-SC, U-SPEC, Nyström, LSC-K, and LSC-R methods. The parameter analysis on will be further conducted in Section 5.3.1.</p><p>• We set the = 5 for the number of nearest neighbor for DnC-SC, U-SPEC, LSC-K, and LSC-R. The parameter analysis on will be further conducted in Section 5.3.2. 6 http://alumni.cs.ucsb.edu/ wychen/sc.html 7 http://www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html 8 https://github.com/Li-Hongmin/MyPaperWithCode • The DnC-SC method has a unique parameter . In the experiments, = 200 is used for the datasets whose size is less than 100,000, otherwise = 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Large-scale Spectral Clustering Methods</head><p>In this section, we compare the proposed DnC-SC method with five state-of-the-art spectral clustering methods, as well as the -means clustering and original spectral clustering methods as the baseline methods.</p><p>We report the experimental results in <ref type="table" target="#tab_7">Tables 3, 4</ref> and 5, where we use N/A to denote the case when MATLAB reports the error of out of memory. Only two methods (proposed DnC-SC and U-SPEC) pass all datasets because they can approximately compute the similarity matrix within a limited memory. The proposed DnC-SC method achieves the best clustering performance of both ACC and NMI eight times on ten benchmark datasets according to <ref type="table" target="#tab_7">Table 3</ref> and 4. The proposed DnC-SC method achieves the best efficiency nine times on ten benchmark datasets according to <ref type="table" target="#tab_9">Table 5</ref>.</p><p>In addition, we report the average performance score and rank for each method in <ref type="table" target="#tab_7">Tables 3, 4</ref> and 5. The proposed DnC-SC method achieves the best average scores of both ACC and NMI. The DnC-SC method shows average ranks of 1.50 of ACC and 1.40 of NMI, which implies the best clustering quality in all spectral clustering methods. Moreover, the DnC-SC method costs much less average time than the other competitors and achieves a rank of 1.50, which implies the most efficient method in this experiment. Overall, the proposed DnC-SC method shows significant effectiveness and efficiency comparing with six state-of-the-art large-scale spectral clustering methods.</p><p>We conduct a series of parameters analysis experiments to demonstrate the performance of the proposed method varying different parameter settings. We select four dataset (Letters, MNIST, TS-60K, and TM-1M) as benchmark datasets to conduct the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Number of Landmarks</head><p>We first conduct parameter analysis to compare the largescale spectral clustering methods by varying the number of landmarks (also called landmarks) and report the experimental results in <ref type="table" target="#tab_5">Table 6</ref>. In general, we can see that a larger value of brings a better performance of ACC and NMI but cost more time. The proposed DnC-SC achieves the best ACC and NMI scores on all datasets except the MNIST. On MNIST dataset, the proposed DnC-SC method shows the second best ACC and NMI scores after the LSC-K method. In terms of time cost, the proposed DnC-SC method shows the best efficiency on all datasets. Overall, the proposed DnC-SC method shows significant effectiveness and efficiency in this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Number of Nearest Landmarks</head><p>We then conduct parameter analysis to compare the largescale spectral clustering methods by varying the number of nearest landmark and report the experimental results in   <ref type="table" target="#tab_11">Table 7</ref>. Note that the Nyström method does not have the parameter . Therefore, we do not show the results of the Nyström method in this experiment. According to <ref type="table" target="#tab_11">Table 7</ref>, the performance of most methods varies for different values. Overall, the proposed DnC-SC show the best effectiveness and efficiency on all dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Number of Nearest Landmarks and selection rate</head><p>To further demonstrate the proposed method, we evaluate the performances by varying parameters and and report the experimental results in <ref type="table" target="#tab_12">Table 8</ref>. For proposed DnC-SC methods, the selection rate parameter directly affects     the computational complexity of landmark selection, while the number of nearest landmarks affects similarity construction, respectively. As we can see, a larger or generally leads more time cost while not necessarily achieves better performance. Overall, the proposed method shows considerable robustness with various parameters on ACC and NMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Influence of Landmark Selection Strategies</head><p>In this section, we compare the performances between the divide-and-conquer based landmark selection and themeans base landmark selection. The experimental results are reported in <ref type="table">Table 9</ref>. As we mentioned, the divide-andconquer based landmark selection algorithm recursively solves the optimization problems 5, which -means methods can also solve. We have pointed out the lack of efficiency of  <ref type="table">Table 9</ref> Clustering performance (ACC(%), NMI(%), and time costs(s)) for DnC-SC using divide-and-conquer based landmark selection and -means based landmark selection. directly applying -means on large-scale datasets in Section 4.1. Note that the number of maximum iterations of -means in landmark selection is turned as 5, which is the same setting as LSC-K and U-SPEC implementation. In <ref type="table">Table 9</ref>,means based landmark selection algorithm generally shows better ACC and NMI on most datasets except TM-1M dataset, while the difference in performance is not significant. Compared to -means based selection, our divide-and-conquer based landmark selection algorithm strikes a balance between efficiency and effectiveness. It achieves significantly better efficiency than the -means based selection and yields com- <ref type="table" target="#tab_3">Table 10</ref> Clustering performance (ACC(%), NMI(%), and time costs(s)) for DnC-SC using approximate -nearest landmarks and exact -nearest landmarks. petitive clustering quality compared to the -means based selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Influence of Approximated -nearest Landmarks</head><p>In this section, we compare the approximated -nearest landmarks and exact -nearest landmarks. The experimental results are reported in <ref type="table" target="#tab_3">Table 10</ref>. The approximatednearest landmarks approach first finds the possible candidates according to the center's nature of landmarks and then searches the -nearest landmarks among them. The exact -nearest landmarks approach costs ( ) computational time, while the proposed approximation can reduce the time cost to ( ). As the Tables 10 shows, the exactnearest landmarks approach achieves slightly better ACC and NMI scores than the proposed approximation. However, the performances between the two methods are not significantly different. In term of time cost, the proposed approximation approach shows highly efficient performance compared with the exact -nearest landmarks. Note that exact -nearest landmarks approach can not be conducted on datasets whose sizes are more than one million due to the high computational cost. Overall, the proposed approximate -nearest landmark approach show the robustness and efficiency in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a large-scale clustering method, termed divide-and-conquer based spectral clustering (DnC-SC). In DnC-SC, a divide-and-conquer based landmark selection algorithm is designed to obtain the landmarks effectively. A new approximate similarity matrix construction approach is proposed to utilize the center's nature of the landmarks to fast construct the similarity matrix between data points and -nearest landmarks. Finally, the bipartite graph partition is conducted to obtain the final clustering results. The experimental results on synthetic and real-world datasets show that the proposed method outperforms other state-of-the-art large-scale spectral clustering methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>light--means (1) Randomly select ' representatives as (2) Apply -means on (3) Find the nearest centers of &amp; (4) Assign &amp; to the nearest subset Directly apply -means on ! (!) An comparison between -means and light--means. (a) -means directly divide all samples (1) 1 into 3 subsets. (b) In light--means, -means is applied on ′ representatives, which significantly reduce the complexity on large data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the landmarks produced by (a) random selection, (b) -means based selection, and (c) Divideand-conquer based selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An illustration of our -nearest landmarks approximation: (1) Find the ′ -nearest ( ′ &gt; ) landmarks of 1 , where ∈ and 1 is the subset center of ; (2) Find the -nearest landmarks among ′ -nearest landmarks of 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>( ) time. The similarity construction takes ( + 2 ( + )) time. The eigen-decomposition takes ( ( + ) + 3 ) time. The -means discretization takes ( 2 ) time. With consideration to , , ≪ ≪ , the overall computational complexity of DnC-SC is ( ( + 2 +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>*</head><label></label><figDesc>LSC-KH and LSC-RH cannot be conduct on the TM-60K and TM-1M dataset due to the memory bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(k-means based seletion)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 Comparison</head><label>1</label><figDesc></figDesc><table /><note>of the computational complexity of several large- scale spectral clustering methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Properties of the real and synthetic datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell>#Object</cell><cell cols="2">#Dimension #Class</cell></row><row><cell></cell><cell>USPS</cell><cell>9298</cell><cell>256</cell><cell>10</cell></row><row><cell></cell><cell>PenDigits</cell><cell>10,992</cell><cell>16</cell><cell>10</cell></row><row><cell></cell><cell>Letters</cell><cell>20,000</cell><cell>16</cell><cell>26</cell></row><row><cell>Real</cell><cell>MNIST</cell><cell>70,000</cell><cell>784</cell><cell>10</cell></row><row><cell></cell><cell>Covertype</cell><cell>581,012</cell><cell>54</cell><cell>7</cell></row><row><cell></cell><cell>TS-60K</cell><cell>600,000</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>TM-1M</cell><cell>1,000,000</cell><cell>2</cell><cell>2</cell></row><row><cell>Synthetic</cell><cell>TC-6M</cell><cell>6,000,000</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>CG-10M</cell><cell>10,000,000</cell><cell>2</cell><cell>11</cell></row><row><cell></cell><cell>FL-20M</cell><cell>20,000,000</cell><cell>2</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the five synthetic datasets. Note that only a 1% or 0.1% samples of each dataset is plotted.</figDesc><table /><note>points. Specifically, the five real datasets are USPS [6] 1 , PenDigits [1] 2 , Letters [11] 3 , MNIST [5] 1 , and Covertype [2] 4 . The five synthetic datasets are Two Spiral-60K (TS- 60K), Two Moons-1M (TM-1M), Three Circles-6M (TC-6M), Cirecles and Gaussians-10M (CG-10M) [12] 5 , Flower-20M (FL-20M) [12] 5 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>Clustering performance (ACC% ± std) for large-scale spectral clustering methods USPS 67.01 ±0.70 73.21 ±3.10 69.47 ±1.38 74.02 ±7.34 73.90 ±4.42 73.66 ±5.18 73.89 ±4.27 80.79 ±3.13 82.55 ±1.96 PenDigits 64.40 ±4.73 67.23 ±4.35 72.46 ±0.18 82.30 ±2.95 81.55 ±3.79 82.17 ±4.09 81.55 ±5.12 81.74 ±4.95 82.27 ±1.33 Letters 25.56 ±1.00 31.21 ±0.76 31.30 ±0.40 33.20 ±2.52 32.34 ±0.15 31.13 ±0.88 31.60 ±1.67 33.20 ±1.16 33.54 ±1.21</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>MINST</cell><cell>56.60 ±2.71</cell><cell>N/A</cell><cell>57.02 ±3.66</cell><cell>80.96 ±0.10</cell><cell>62.00 ±3.99</cell><cell cols="2">66.59 ±5.33 67.60 ±6.02</cell><cell>72.00 ±3.33</cell><cell>74.24 ±2.14</cell></row><row><cell>Covertype</cell><cell>24.04 ±0.22</cell><cell>N/A</cell><cell>21.65 ±1.30</cell><cell>24.71 ±1.45</cell><cell>23.62 ±1.10</cell><cell>N/A</cell><cell>N/A</cell><cell>24.40 ±2.20</cell><cell>23.48 ±1.86</cell></row><row><cell>TS-60K</cell><cell>56.96 ±0.00</cell><cell>N/A</cell><cell cols="3">55.94 ±10.17 70.37 ±4.57 62.91 ±13.74</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">65.78 ±13.63 81.00 ±9.29</cell></row><row><cell>TM-1M</cell><cell>75.21 ±0.00</cell><cell>N/A</cell><cell>64.63 ±8.40</cell><cell cols="2">51.76 ±0.54 66.41 ±26.68</cell><cell>N/A</cell><cell>N/A</cell><cell>99.96 ±0.01</cell><cell>99.96 ±0.01</cell></row><row><cell>TC-6M</cell><cell>33.34 ±0.00</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>99.86 ±0.03</cell><cell>99.87 ±0.02</cell></row><row><cell>CG-10M</cell><cell>60.47 ±2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">66.77 ±3.97 66.83 ±4.61</cell></row><row><cell>FL-20M</cell><cell>50.07 ±2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">80.17 ±3.97 81.90 ±5.61</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>70.45</cell><cell>72.59</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.80</cell><cell>5.10</cell><cell>2.80</cell><cell>3.90</cell><cell>4.70</cell><cell>4.5</cell><cell>2.30</cell><cell>1.50</cell></row></table><note>* N/A denotes the case when MATLAB reports the error of out of memory.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Clustering performance (NMI% ± std) for large-scale spectral clustering methods</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>USPS</cell><cell>61.28 ±0.42</cell><cell>77.90 ±0.55</cell><cell>65.07 ±1.23</cell><cell>81.37 ±1.92</cell><cell>76.22 ±0.76</cell><cell cols="2">76.41 ±1.74 76.24 ±1.00</cell><cell>81.86 ±1.95</cell><cell>82.86 ±0.21</cell></row><row><cell>PenDigits</cell><cell>67.65 ±1.18</cell><cell>71.70 ±1.21</cell><cell>65.48 ±0.21</cell><cell>80.78 ±0.55</cell><cell>79.15 ±1.74</cell><cell cols="2">80.78 ±0.55 79.15 ±1.74</cell><cell>81.68 ±2.33</cell><cell>82.01 ±1.08</cell></row><row><cell>Letters</cell><cell>34.95 ±0.54</cell><cell>34.96 ±0.63</cell><cell>40.07 ±0.41</cell><cell>44.68 ±1.56</cell><cell>42.36 ±0.86</cell><cell cols="2">42.31 ±0.75 42.20 ±1.30</cell><cell>45.11 ±0.54</cell><cell>45.37 ±0.85</cell></row><row><cell>MINST</cell><cell>50.90 ±1.10</cell><cell>N/A</cell><cell>49.05 ±1.55</cell><cell>76.81 ±0.18</cell><cell>62.53 ±1.87</cell><cell cols="2">65.08 ±2.16 65.14 ±2.47</cell><cell>69.15 ±0.76</cell><cell>72.00 ±0.51</cell></row><row><cell>Covertype</cell><cell>7.55 ±0.00</cell><cell>N/A</cell><cell>7.98 ±0.98</cell><cell>9.21 ±0.14</cell><cell>8.06 ±0.07</cell><cell>N/A</cell><cell>N/A</cell><cell>8.19 ±0.04</cell><cell>8.30 ±0.30</cell></row><row><cell>TS-60K</cell><cell>22.22 ±0.00</cell><cell>N/A</cell><cell cols="3">21.64 ±14.69 39.16 ±9.25 39.80 ±17.52</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">62.52 ±17.01 73.84 ±5.08</cell></row><row><cell>TM-1M</cell><cell>19.21 ±0.00</cell><cell>N/A</cell><cell>8.03 ±8.58</cell><cell>0.10 ±0.05</cell><cell>28.11 ±48.63</cell><cell>N/A</cell><cell>N/A</cell><cell>99.52 ±0.08</cell><cell>99.52 ±0.05</cell></row><row><cell>TC-6M</cell><cell>34.95 ±0.54</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>99.14 ±0.19</cell><cell>99.15 ±0.08</cell></row><row><cell>CG-10M</cell><cell>64.94 ±1.61</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>79.98 ±2.10</cell><cell>80.91 ±3.59</cell></row><row><cell>FL-20M</cell><cell>65.02 ±2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">86.77 ±3.97 87.67 ±3.18</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>71.39</cell><cell>72.39</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.40</cell><cell>5.30</cell><cell>3.10</cell><cell>4.20</cell><cell>4.50</cell><cell>4.70</cell><cell>2.00</cell><cell>1.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>Time costs(s) of large-scale spectral clustering methods.</figDesc><table><row><cell>Dataset</cell><cell>KM</cell><cell>SC</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>LSC-KH</cell><cell>LSC-RH</cell><cell>U-SPEC</cell><cell>DnC-SC</cell></row><row><cell>USPS</cell><cell>0.37 ±0.18</cell><cell>3.15 ±0.18</cell><cell>1.44 ±0.04</cell><cell>1.35 ±0.09</cell><cell>0.64 ±0.14</cell><cell>0.71 ±0.06</cell><cell>0.88 ±0.07</cell><cell>3.36 ±0.25</cell><cell>1.25 ±0.07</cell></row><row><cell>PenDigits</cell><cell>0.05 ±0.05</cell><cell>3.15 ±0.11</cell><cell>1.61 ±0.10</cell><cell>1.20 ±0.37</cell><cell>0.77 ±0.34</cell><cell>0.71 ±0.05</cell><cell>0.68 ±0.07</cell><cell>2.07 ±0.95</cell><cell>0.64 ±0.08</cell></row><row><cell>Letters</cell><cell>0.26 ±0.05</cell><cell>13.67 ±2.35</cell><cell>4.70 ±0.17</cell><cell>3.89 ±0.28</cell><cell>2.03 ±0.34</cell><cell>2.26 ±0.17</cell><cell>2.63 ±0.28</cell><cell>1.58 ±0.06</cell><cell>0.90 ±0.10</cell></row><row><cell>MINST</cell><cell>21.40 ±1.02</cell><cell>N/A</cell><cell>6.54 ±0.11</cell><cell>17.29 ±0.82</cell><cell>5.80 ±0.31</cell><cell cols="2">18.04 ±2.35 15.38 ±2.43</cell><cell>11.96 ±0.32</cell><cell>5.11 ±0.51</cell></row><row><cell>Covertype</cell><cell>14.02 ±4.39</cell><cell>N/A</cell><cell>571.69 ±144.60</cell><cell>354.74 ±90.80</cell><cell>41.00 ±12.38</cell><cell>N/A</cell><cell>N/A</cell><cell>15.96 ±1.44</cell><cell>13.15 ±3.00</cell></row><row><cell>TS-60K</cell><cell>1.39 ±0.18</cell><cell>N/A</cell><cell>1283.33 ±248.12</cell><cell>167.29 ±39.99</cell><cell>16.35 ±1.62</cell><cell>N/A</cell><cell>N/A</cell><cell>17.36 ±20.89</cell><cell>4.01 ±1.16</cell></row><row><cell>TM-1M</cell><cell>1.12 ±0.17</cell><cell>N/A</cell><cell cols="3">3401.61 ±410.03 3997.21 ±1436.73 591.02 ±127.86</cell><cell>N/A</cell><cell>N/A</cell><cell>7.85 ±0.21</cell><cell>6.46 ±1.13</cell></row><row><cell>TC-6M</cell><cell>35.23 ±1.72</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>30.46 ±1.52</cell><cell>25.05 ±3.04</cell></row><row><cell>CG-10M</cell><cell>134.42 ±9.28</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>381.72 ±72.24</cell><cell>281.05 ±77.04</cell></row><row><cell>FL-20M</cell><cell>311.94 ±2.91</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">1530.30 ±578.44 837.38 ±213.70</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>165.96</cell><cell>117.50</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.80</cell><cell>4.50</cell><cell>4.40</cell><cell>2.60</cell><cell>4.30</cell><cell>4.20</cell><cell>3.30</cell><cell>1.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by varying number of landmark .</figDesc><table><row><cell>Dataset</cell><cell cols="2">Letters</cell><cell>MNIST</cell><cell>TS-60K</cell><cell>TM-1M</cell></row><row><cell>ACC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>200</cell><cell>400</cell><cell>600</cell><cell>800 1000</cell><cell></cell></row><row><cell></cell><cell></cell><cell># of p</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by</cell><cell></cell></row><row><cell></cell><cell cols="2">varying number of nearest landmarks .</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Letters</cell><cell>MNIST</cell><cell>TS-60K</cell><cell>TM-1M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc>Clustering performance (ACC(%), NMI(%), and time costs(s)) for different methods by varying number of nearest landmark and selection rate .</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="5">Letters</cell><cell></cell><cell cols="5">MNIST</cell><cell></cell><cell cols="5">TS-60K</cell><cell></cell><cell>TM-1M</cell></row><row><cell>ACC</cell><cell>ACC(%)</cell><cell>0 20 40</cell><cell></cell><cell></cell><cell></cell><cell>ACC(%)</cell><cell cols="2">0 50</cell><cell></cell><cell></cell><cell></cell><cell>ACC(%)</cell><cell cols="2">0 50 100</cell><cell></cell><cell></cell><cell></cell><cell>ACC(%)</cell><cell cols="2">0 50 100</cell></row><row><cell></cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 3 # of K 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell></row><row><cell>NMI</cell><cell>NMI(%)</cell><cell>0 50</cell><cell></cell><cell></cell><cell></cell><cell>NMI(%)</cell><cell cols="2">0 50</cell><cell></cell><cell></cell><cell></cell><cell>NMI(%)</cell><cell cols="2">0 50 100</cell><cell></cell><cell></cell><cell></cell><cell>NMI(%)</cell><cell cols="2">0 50 100</cell></row><row><cell></cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 3 # of K 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>250 200 # of 150 100 50</cell></row><row><cell>Time cost</cell><cell>Time(s)</cell><cell>0 0.5 1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time(s)</cell><cell>0 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time(s)</cell><cell>0 5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time(s)</cell><cell>0 5 10</cell></row><row><cell></cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of 150 100</cell><cell></cell><cell>2 # of K 3 4</cell><cell>5</cell><cell>6</cell><cell>50</cell><cell>250 200 # of u 150 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>on Geoscience and Remote Sensing 46, 2126-2136. [32] Zhang, X., Zong, L., You, Q., Yong, X., 2016. Sampling for nyström extension-based spectral clustering: incremental perspective and novel analysis. ACM Transactions on Knowledge Discovery from Data (TKDD) 11, 1-25. [33] Zhang, Z., Kulkarni, S.R., 2014. Detection of shilling attacks in recommender systems via spectral clustering, in: 17th International Conference on Information Fusion (FUSION), IEEE. pp. 1-8. Hongmin Li is currently working toward a Ph.D. degree at the Department of Computer Science, University of Tsukuba, Japan. He received his MS degree in computer science from the University of Tsukuba, Japan. His current research interests include clustering, machine learning, and its application fields. Xiucai Ye received the Ph.D. degree in computer science from the University of Tsukuba, Tsukuba, Japan, in 2014. She is currently an Assistant Professor with the Department of Computer Science, and Center for Artificial Intelligence Research (C-AIR), University of Tsukuba. Her current research interests include feature selection, clustering, bioinformatics, machine learning and its application fields. She is a member of IEEE. Akira Imakura is an Associate Professor at Faculty of Engineering, Information and Systems, University of Tsukuba, Japan. He received Ph.D. (2011) from Nagoya University, Japan. He was appointed as Japan Society for the Promotion of Science Research Fellowship for Doctor Course Student (DC2) from 2010 to 2011, as a Research Fellow at Center for Computational Sciences, University of Tsukuba, Japan from 2011 to 2013, and also as a JST ACTI researcher from 2016 to 2019. His current research interests include developments and analysis of highly parallel algorithms for large matrix computations. Recently, he also investigates matrix factorization-based machine learning algorithms. He is a member of JSIAM, IPSJ and SIAM. Tetsuya Sakurai is a Professor of Department of Computer Science, and the Director of Center for Artificial Intelligence Research (C-AIR) at the University of Tsukuba. He is also a visiting professor at the Open University of Japan, and a visiting researcher of Advanced Institute of Computational Science at RIKEN. He received a Ph.D. in Computer Engineering from Nagoya University in 1992. His research interests include high performance algorithms for large-scale simulations, data and image analysis, and deep neural network computations. He is a member of the Japan Society for Industrial and Applied Mathematics (JSIAM), the Mathematical Society of Japan (MSJ), Information Processing Society of Japan (IPSJ), Society for Industrial and Applied Mathematics (SIAM).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Divide-and-conquer based Large-Scale Spectral Clustering</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This study was supported by in part by the New Energy and Industrial Technology Development Organization (NEDO) Grant (ID:18065620) and JST COI-NEXT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Blackard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and electronics in agriculture</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="131" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sampling with minimum sum of squared similarities for nystrom-based large scale spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Birol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering via landmarkbased sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1669" to="1680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speed up kernel discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transac</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-clustering documents and words using bipartite spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of kernel and spectral methods for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Camastra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spectral grouping using the nystrom method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Letter recognition using holland-style adaptive classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Slate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="161" to="182" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultrascalable spectral clustering and ensemble clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kwoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1212" to="1226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hubness-based sampling method for nyström spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Imakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale multi-view spectral clustering via bipartite graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Superpixel segmentation using linear spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1356" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding and enhancement of internal clustering validation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="982" to="994" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spectral clustering of biological sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pentney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Landmark selection for spectral clustering based on weighted pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rafailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="465" to="472" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Clustering methods, in: Data mining and knowledge discovery handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maimon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="321" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Agglomerative information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="617" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Document clustering based on nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast approximate spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering using sparse representation based on hubness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SmartWorld, Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People and Smart City Innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1731" to="1737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral clustering with adaptive similarity measure in kernel space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="751" to="765" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bipartite graph partitioning and data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved nyström lowrank approximation and error analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1232" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spectral clustering ensemble applied to sar image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
