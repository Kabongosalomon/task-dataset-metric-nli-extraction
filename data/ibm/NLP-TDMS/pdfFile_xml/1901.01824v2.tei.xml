<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<email>quanliu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
					</analytic>
					<monogr>
						<title level="m">The 28th ACM International Conference on Information and Knowledge Management (CIKM &apos;19)</title>
						<meeting> <address><addrLine>Beijing, China; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">5</biblScope>
							<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357384.3358140</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an interactive matching network (IMN) for the multi-turn response selection task. First, IMN constructs word representations from three aspects to address the challenge of out-of-vocabulary (OOV) words. Second, an attentive hierarchical recurrent encoder (AHRE), which is capable of encoding sentences hierarchically and generating more descriptive representations by aggregating with an attention mechanism, is designed. Finally, the bidirectional interactions between whole multi-turn contexts and response candidates are calculated to derive the matching information between them. Experiments on four public datasets show that IMN outperforms the baseline models on all metrics, achieving a new state-of-the-art performance and demonstrating compatibility across domains for multi-turn response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Retrieval models and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KEYWORDS</head><p>Interactive matching network, multi-turn response selection, retrievalbased chatbot ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Building a chatbot that can converse naturally with humans on open domain topics is a challenging yet intriguing problem in artificial intelligence <ref type="bibr" target="#b0">[1]</ref>. Response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation, is an important retrieval-based approach for chatbots <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The techniques of word embeddings and sentence embeddings are important to response selection as well as many other natural language processing (NLP) tasks. The context and the response Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '19, November 3-7, 2019, Beijing, China © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6976-3/19/11. . . $15.00 https://doi.org <ref type="bibr">/10.1145/3357384.3358140</ref> must be projected to a vector space appropriately to capture their relationships, which are essential for the subsequent procedures. Typically, word embeddings established on the task-specific training set and a single-layer recurrent neural network are employed for the response selection task. Another key technique to the response selection task lies in context-response matching. Chen et al. <ref type="bibr" target="#b1">[2]</ref> showed that interactions between pairs of sentences can provide useful information to help matching.</p><p>Wu et al. <ref type="bibr" target="#b8">[9]</ref> proposed the sequential matching network (SMN) to match the response with each utterance and then to accumulate matching information by an RNN. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> refined utterance and employed self-matching attention to route the vital information in each utterance based on the SMN. Zhou et al. <ref type="bibr" target="#b12">[13]</ref> proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked selfattention.</p><p>In this paper, we propose a novel neural network architecture, called the interactive matching network (IMN), for multi-turn response selection in retrieval-based chatbots. Our proposed IMN is similar to SMN but has three main differences: (1) constructing word representations from three aspects to enhance the representations at the word-level, (2) enhancing sentence representations through an attentive hierarchical recurrent encoder to enhance the representations at the sentence-level and (3) capturing interactions between contexts and responses by collecting matching information bidirectionally to enrich the representations. We test our model on Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b3">[4]</ref>, Ubuntu Dialogue Corpus V2 <ref type="bibr" target="#b4">[5]</ref>, Douban Conversation Corpus <ref type="bibr" target="#b8">[9]</ref> and Ecommerce Dialogue Corpus <ref type="bibr" target="#b11">[12]</ref>. The results show that our model can outperform the baseline models on all metrics, achieving new state-of-the-art performance and showing compatibility across domains for multi-turn response selection.</p><p>In summary, our contributions in this paper are threefold.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formalization</head><p>Given a dialogue dataset D, an example of the dataset can be represented as (c, r , y). Specifically, c = {u 1 , u 2 , ..., u n } represents a conversation context with {u k } n k =1 as the utterances. r is a response candidate, and y ∈ {0, 1} denotes a label. y = 1 indicates that r is a proper response for c; otherwise, y = 0. Our goal is to learn a matching model д(c, r ), which provides the matching degree between c and r by minimizing the sigmoid cross entropy from D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Representation Layer</head><p>One challenge of large dialogue corpora is the large number of OOV words. To address this issue, we propose to construct word representations with a combination of general pretrained word embedding, those estimated on the task-specific training set and character-level embeddings.</p><p>Formally, the embeddings of the k-th utterance in a conversation and a response candidate at this layer are denoted as U 0</p><formula xml:id="formula_0">k = {u 0 k,i } l u k i=1 and R 0 = {r 0 j } l r j=1</formula><p>. u 0 k,i and r 0 j ∈ R d are embeddings of a d-dimensional vector. l u k and l r are the numbers of words in U 0 k and R 0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Encoding Layer</head><p>Typically, the outputs of the top layer in a multi-layer RNNs are regarded as the final sentence representations, and the other layers are neglected. However, the lower layers can also provide useful sentence descriptions, such as part-of-speech tagging and syntaxrelated information. Motivated by the method of ELMo <ref type="bibr" target="#b5">[6]</ref>, we propose a new sentence encoder, called the attentive hierarchical recurrent encoder (AHRE) to make full use of the representations at all hidden layers.</p><p>BiLSTMs <ref type="bibr" target="#b2">[3]</ref> are employed as our basic building blocks. In an M-layer RNN, each m t h layer takes the output of the m − 1 th layer as its input.</p><p>Finally, we obtain a set of M representations {U 1 k , ..., U M k } and {R 1 , ..., R M } for the k-th utterance in a conversation and a response</p><formula xml:id="formula_1">candidate through the M-layer RNNs, where U m k = {u m k,i } l u k i=1 and R m = {r m j } l r j=1 , l ∈ {1, ..., M }.</formula><p>Here, we propose to combine the set of representations to obtain the enhanced representations u enc k,i and r enc j by learning the attention weights of all the layers. Mathematically, we have</p><formula xml:id="formula_2">u enc k,i = M m=1 w m u m k,i , r enc j = M m=1 w m r m j ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">U enc k = {u enc k,i } l u k i=1 , R enc = {r enc j } l r j=1</formula><p>and w l are the softmaxnormalized weights shared between utterances and responses, which need to be estimated during the training process. As a result, representations given by AHRE are expected to fuse multi-level characteristics of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Matching Layer</head><p>Unlike previous work, which matches responses with each utterance in a context separately in an utterance-response manner <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, IMN matches the response with the whole context in a global context-response way, i.e., considering the whole context as a single sequence. The global context-response matching can help select the most relevant parts of the whole context and neglect the irrelevant parts.</p><p>First, the context C enc = {c enc i } l c i=1 with l c = n k=1 l u k is formed by concatenating the set of utterance representations {U enc k } n k=1 . Then, an attention-based alignment is employed to collect information between two sequences by computing the attention weight between each tuple as e i j = (c enc i ) T · r enc j . For a word in the response, its response-to-context relevant representation is composed as</p><formula xml:id="formula_4">r enc j = l c i=1 exp(e i j ) l c k =1 exp(e k j ) c enc i , j ∈ {1, ..., l r },<label>(2)</label></formula><p>whereR</p><formula xml:id="formula_5">enc = {r enc j } l r j=1 ,r enc j is a weighted summation of {c enc i } l c i=1</formula><p>. The same calculation is performed for each word in a context to form context-to-response representationC enc = {c enc i } l c i=1 . To further enhance the collected information, the matching matrices are formed as</p><formula xml:id="formula_6">C mat = [C enc ;C enc ; C enc −C enc ; C enc ⊙C enc ],<label>(3)</label></formula><formula xml:id="formula_7">R mat = [R enc ;R enc ; R enc −R enc ; R enc ⊙R enc ].<label>(4)</label></formula><p>Finally, the concatenated context C mat need to be converted to separate utterances {U mat k } n k =1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ubuntu Corpus V1</head><p>Ubuntu Corpus V2 R 2 @1 R 10 @1 R 10 @2 R 10 @5 R 2 @1 R 10 @1 R 10 @2 R 10 @5 CompAgg <ref type="bibr" target="#b6">[7]</ref> 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Aggregation Layer</head><p>The aggregation layer converts the matching matrices of separated utterances and responses into a final matching vector. First, the set of utterance embeddings U aдr = {u aдr k } n k=1 and the response embeddings r aдr are obtained by composing the enhanced local matching information U mat k and R mat with a BiLSTM, and a combination of max pooling and last hidden state pooling.</p><p>Furthermore, the set of utterance inference vectors U aдr = {u aдr k } n k =1 is fed into another BiLSTM in chronological order of the utterances in the context, followed by another pooling operation to obtain the aggregated context embeddings c aдr .</p><p>The final matching feature vector is the concatenation of the context embeddings and the response embeddings as m = [c aдr ; r aдr ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Prediction Layer</head><p>We then input the matching feature vector m into a multi-layer perceptron (MLP) classifier. The MLP returns a score to denote the matching degree of a context-response pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Datasets</head><p>We tested IMN on Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b3">[4]</ref>, Ubuntu Dialogue Corpus V2 <ref type="bibr" target="#b4">[5]</ref>, Douban Conversation Corpus <ref type="bibr" target="#b8">[9]</ref> and E-commerce Dialogue Corpus <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We used the same evaluation metrics as those used in previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. We calculated the recall of the true positive replies among the k selected responses from n available candidates, denoted as R n @k. In addition, mean average precision (MAP), mean reciprocal rank (MRR) and precision-at-one (P@1), are especially considered for the Douban corpus, following the settings of previous work.  <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> present the evaluation results of IMN and previous methods. All the results except ours are from the existing literature. IMN outperforms other models on all metrics and datasets, which demonstrates its ability to select the best-matched response and its compatibility across domains (system troubleshooting, social network and e-commerce). The Douban Corpus includes multiple correct candidates for a context in its test set. Hence, MAP and MRR are recommended for reference. Our proposed model outperforms the present state-of-the-art methods on the respective datasets by a margin of 2.6% in terms of R 10 @1 on Ubuntu V1; 11.9% in terms of R 10 @1 on Ubuntu V2; 2.0% in terms of MAP and 1.4% in terms of MRR on Douban Corpus; and 12.0% in terms of R 10 @1 on E-commerce Corpus, achieving a new state-of-the-art performance on all datasets. Furthermore, we provide ensemble models built by averaging the outputs of four single models with identical architectures and different random initializations. Our code has been published at https://github.com/JasonForJoy/IMN to help replicate our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ABLATIONS AND ANALYSIS</head><p>To demonstrate the importance of each component in our proposed model, various parts of the architecture were ablated, as shown in <ref type="table" target="#tab_3">Table 3</ref>.  AHRE. The number of layers in the AHRE was set to 3. The AHRE can be considered as a generalized recurrent encoder that degenerates into a single-layer RNN when the number of layers in the AHRE is set to 1. The softmax-normalized weights of layers in the AHRE are listed in <ref type="table" target="#tab_4">Table 4</ref>, which indicates that each layer of the multi-layer RNNs contributes to the embeddings.</p><p>Char emb. The character embeddings in the word representation layer were ablated, which resulted in a performance decrease. Additionally, we found that the lowest layer of the RNN in the AHRE constituted the highest weight, as shown in <ref type="table" target="#tab_4">Table 4</ref>. These two results may be explained by the importance of morphology information to the response selection.</p><p>Match. The decreased performance indicates that interactions between contexts and responses are beneficial for matching. We conduct a case study and visualize the response-to-context weights used in Eq. 2 to demonstrate their ability to select relevant parts as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Some important words such as "connect", "router" and "ethernet" in the response can select their relevant words in the context, and some unimportant words such as "tried", "channels" and "the" in the context occupy small weights when forming representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an interactive matching network for the response selection task. An empirical study on four public datasets shows that our proposed model outperforms the baseline models on all metrics, achieving new state-of-the-art performance and showing compatibility across domains for multi-turn response selection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>The Adam method was employed for optimization, with a batch size of 96 for the two English datasets and 128 for the two Chinese datasets. The initial learning rate was 0.001 and was exponentially decayed by 0.96 every 5000 steps. Dropout with a rate of 0.2 was applied to the word embeddings and all hidden layers. The word embeddings for the English datasets were concatenations of the 300-dimensional GloVe embeddings, 100-dimensional embeddings estimated on the training set using the Word2Vec algorithm and 150-dimensional character-level embeddings with window sizes of {3, 4, and 5}, each consisting of 50 filters. The word embeddings for the Chinese datasets were concatenations of the 200-dimensional embeddings from and the 200-dimensional embeddings estimated on the training set using the Word2Vec algorithm. Character-level embeddings were not employed for the two Chinese datasets due to the large number of Chinese characters. The word embeddings were not updated during training.</p><p>All hidden states of the LSTM had 200 dimensions. The number of BiLSTM layers in the AHRE was 3. The MLP at the prediction layer had a hidden unit size of 256 with ReLU activation. The maximum word length was set to 18, the maximum utterance length was set to 50, and the maximum context length was set to 10. We padded with zeros if the number of utterances in a context was less than 10; otherwise, we kept the last 10 utterances. We used the development dataset to set the stop condition to select the best model for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Performance of Different Number of Layers in AHRE</head><p>For the Ubuntu V2 dataset, the number of layers in the AHRE was tuned on its validation set. Using three layers achieved the best performance as shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 )</head><label>1</label><figDesc>This paper proposes a new model, named IMN, for multi-turn response selection in retrieval-based chatbots. (2) The empirical results show that our proposed model outperforms the baseline models in terms of all metrics on four datasets, achieving new state-of-the-art performance for multi-turn response selection. (3) This paper presents detailed experiments and discussions on contributions of each part to context-response pair matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>An overview of our proposed IMN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Response-to-context attention weights for a sample. The darker units mean larger values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Performance of IMN for different numbers of layers in the AHRE on Ubuntu V2 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results of IMN and previous methods on Ubuntu Dialogue Corpus V1 and V2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of IMN and previous methods on the Douban Conversation Corpus and E-commerce Corpus. 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R</figDesc><table><row><cell></cell><cell cols="3">Douban Conversation Corpus</cell><cell></cell><cell cols="3">E-commerce Corpus</cell></row><row><cell></cell><cell cols="7">MAP MRR P@1 R 10 @5</cell></row><row><cell>SMN [9]</cell><cell>0.529 0.569 0.397</cell><cell>0.233</cell><cell>0.396</cell><cell>0.724</cell><cell>0.453</cell><cell>0.654</cell><cell>0.886</cell></row><row><cell>DUA [12]</cell><cell>0.551 0.599 0.421</cell><cell>0.243</cell><cell>0.421</cell><cell>0.780</cell><cell>0.501</cell><cell>0.700</cell><cell>0.921</cell></row><row><cell>DAM [13]</cell><cell>0.550 0.601 0.427</cell><cell>0.254</cell><cell>0.410</cell><cell>0.757</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IMN</cell><cell>0.570 0.615 0.433</cell><cell>0.262</cell><cell>0.452</cell><cell>0.789</cell><cell>0.621</cell><cell>0.797</cell><cell>0.964</cell></row><row><cell cols="2">IMN(Ensemble) 0.576 0.618 0.441</cell><cell>0.268</cell><cell>0.458</cell><cell>0.796</cell><cell>0.672</cell><cell>0.845</cell><cell>0.970</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation tests on Ubuntu V2 test set. R 2 @1 R 10 @1 R 10 @2 R 10 @5</figDesc><table><row><cell>IMN</cell><cell>0.945</cell><cell>0.771</cell><cell>0.886</cell><cell>0.979</cell></row><row><cell>-AHRE</cell><cell>0.940</cell><cell>0.758</cell><cell>0.874</cell><cell>0.974</cell></row><row><cell cols="2">-Char emb 0.941</cell><cell>0.762</cell><cell>0.878</cell><cell>0.976</cell></row><row><cell>-Match</cell><cell>0.904</cell><cell>0.613</cell><cell>0.792</cell><cell>0.958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Layer-wise weights of a three-layer AHRE.</figDesc><table><row><cell cols="3">Layer 1 Layer 2 Layer 3</cell></row><row><cell>Weights 0.4938</cell><cell>0.2181</cell><cell>0.2881</cell></row><row><cell cols="3">Have you tried using different channels __eou__ __eot__ ? No , how do I do that ? __eou__ __eot__ Can you connect to router via ethernet cable and check the settings ? __eou__ __eot__</cell></row><row><cell>I can connect to the router via ethernet , yes . What settings should I check ? __eou__ __eou__ help your for grateful am I . now go to have I response</cell><cell>context</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the datasets that our model is tested on.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="3">Train Valid Test</cell></row><row><cell></cell><cell>pairs</cell><cell>1M</cell><cell cols="2">0.5M 0.5M</cell></row><row><cell>Ubuntu V1</cell><cell cols="2">positive:negative 1: 1</cell><cell>1: 9</cell><cell>1: 9</cell></row><row><cell></cell><cell>positive/context</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>pairs</cell><cell>1M</cell><cell cols="2">195k 189k</cell></row><row><cell>Ubuntu V2</cell><cell cols="2">positive:negative 1: 1</cell><cell>1: 9</cell><cell>1: 9</cell></row><row><cell></cell><cell>positive/context</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>pairs</cell><cell>1M</cell><cell>50k</cell><cell>10k</cell></row><row><cell>Douban</cell><cell cols="2">positive:negative 1: 1</cell><cell>1: 1</cell><cell>1: 9</cell></row><row><cell></cell><cell>positive/context</cell><cell>1</cell><cell>1</cell><cell>1.18</cell></row><row><cell></cell><cell>pairs</cell><cell>1M</cell><cell>10k</cell><cell>10k</cell></row><row><cell>E-commerce</cell><cell cols="2">positive:negative 1: 1</cell><cell>1: 1</cell><cell>1: 9</cell></row><row><cell></cell><cell>positive/context</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">INTERACTIVE MATCHING NETWORKWe present here our proposed IMN model, which is composed of five layers.Figure 1shows an overview of the architecture.arXiv:1901.01824v2 [cs.CL] 11 Nov 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We tested IMN on two English public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1<ref type="bibr" target="#b3">[4]</ref> and Ubuntu Dialogue Corpus V2<ref type="bibr" target="#b4">[5]</ref>, and two Chinese datasets, Douban Conversation Corpus<ref type="bibr" target="#b8">[9]</ref> and E-commerce Dialogue Corpus<ref type="bibr" target="#b11">[12]</ref>. Ubuntu Dialogue Corpus V1 and V2 contain multi-turn dialogues about Ubuntu system troubleshooting in English. Here, we adopted the version of Ubuntu Dialogue Corpus V1 shared in Xu et al.<ref type="bibr" target="#b9">[10]</ref>, in which numbers, paths and URLs were replaced by placeholders. Compared with Ubuntu Dialogue Corpus V1, the training, validation and test dialogues in the V2 dataset were generated in different periods without overlap. Besides, the V2 dataset discriminates between the end of an utterance (_eou_) and the end of a turn (_eot_). In both of the Ubuntu corpora, the positive responses are true responses from humans, and the negative responses are randomly sampled. The Douban Conversation Corpus was crawled from a Chinese social network on open-domain topics. It was constructed in a similar way to the Ubuntu corpus. The Douban Conversation Corpus collected responses via a small inverted-index system, and labels were manually annotated. The E-commerce Dialogue Corpus collected real-world conversations between customers and customer service staff from the largest e-commerce platform in China. Some statistics of these datasets are provided inTable 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUPPLEMENTAL MATERIAL A.1 Detailed Dataset Descriptions</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Survey on Dialogue Systems: Recent Advances and New Frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<title level="m">Enhanced lstm for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training end-to-end dialogue systems with the ubuntu dialogue corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Thomas Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="31" to="65" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Compare-Aggregate Model for Matching Text Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1611.01747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongbo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling Multi-turn Conversation with Deep Utterance Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
