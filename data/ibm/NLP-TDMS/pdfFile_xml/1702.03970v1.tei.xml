<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Interpretation of the French Street Name Signs Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
							<email>chunhui@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar-Shyang</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjith</forename><surname>Unnikrishnan</surname></persName>
							<email>ranjith@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
							<email>julianibarz@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sacha</forename><surname>Arnoud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Ampthitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Interpretation of the French Street Name Signs Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Networks</term>
					<term>End-to-end Networks</term>
					<term>Image Dataset</term>
					<term>Multiview Dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the French Street Name Signs (FSNS) Dataset consisting of more than a million images of street name signs cropped from Google Street View images of France. Each image contains several views of the same street name sign. Every image has normalized, title case folded ground-truth text as it would appear on a map. We believe that the FSNS dataset is large and complex enough to train a deep network of significant complexity to solve the street name extraction problem "end-to-end" or to explore the design trade-offs between a single complex engineered network and multiple sub-networks designed and trained to solve sub-problems. We present such an "end-to-end" network/graph for Tensor Flow and its results on the FSNS dataset. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The detection and recognition of text from outdoor images is of increasing research interest to the fields of computer vision, machine learning and optical character recognition. The combination of perspective distortion, uncontrolled source text quality, and lack of significant structure to the text layout adds extra challenge to the still incompletely solved problem of accurately recognizing text from all the world's languages. Demonstrating the interest, several datasets related to the problem have become available: including ICDAR 2003 Robust Reading <ref type="bibr" target="#b10">[11]</ref>, SVHN <ref type="bibr" target="#b12">[13]</ref>, and, more recently, COCO-Text <ref type="bibr" target="#b15">[16]</ref>, with details of these and others shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>While these datasets each make a useful contribution to the field, the majority are very small compared to the size of a typical deep neural network. As the dataset size increases, it becomes increasingly difficult to maintain the accuracy of the ground-truth, as the task of annotating must be delegated to an increasingly large pool of workers less involved with the project. In the COCO-text <ref type="bibr" target="#b15">[16]</ref> dataset for instance, the authors performed an audit themselves of the accuracy of the ground truth, and found that the annotators had found legible text regions with a recall of 84%, and transcribed the text content with an accuracy of 87.5%. Even at an edit distance of 1, the text content accuracy was still only 92.5%, with missing punctuation being the largest remaining category of error.</p><p>Synthetic data has been shown <ref type="bibr" target="#b7">[8]</ref> to be a good solution to this problem and can work well provided the synthetic data generator includes the formatting/distortions that will be present in the target problem. Some real-world data however, by its very nature, can be hard to predict, so real data remains the first choice in many cases where available.</p><p>The difficulty remains therefore, in generating a sufficiently accurately annotated, large enough dataset of real images, to satisfy the needs of modern data-hungry deep network-based systems, which can learn as large a dataset as we can provide, without necessarily giving back the generalization that we would like. To this end, and to make OCR more like image captioning, we present the French Street Name Signs (FSNS) dataset, which we believe to be the first to offer multiple views of the same physical object, and thus the chance for a learning system to compensate for degradation in any individual view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basics of the FSNS Dataset</head><p>As its name suggests, the FSNS dataset is a set of signs, from the streets of France, that bear street names. Some example images are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Each image carries four tiles of 150 × 150 pixels laid out horizontally, each of which contains a pre-detected street name sign, or random noise in the case that less than four independent views are available of the same physical sign. The text detection problem is thus largely eliminated, although the signs are still of variable size and orientation within each tile image. Also each sign carries multiple text lines, with a maximum of 3 lines of significant text, with the possibility of other additional lines of irrelevant text. Each of the tiles within an image is intended to be a different view of the same physical sign, taken from a different position and/or at a different time. Different physical signs of the same street name, from elsewhere on the same street, are included as separate images. There are over 1 million different physical signs. The different views are of different quality, possibly taken from an acute angle, or blurred by motion, distance from the camera, or by unintentional privacy filtering. Occasionally some of the tiles may be views of a different sign altogether, which can happen when two signs are attached to the same post. Some examples of these problems are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The multiple views can reduce some of the usual problems of outdoor images, such as occlusion by foreground objects, image truncation caused by the target object being at the edge of the frame, and varied lighting. Other problems cannot be solved by multiple views, such as bent, corroded or faded signs.</p><p>The task of the system then is to obtain the best possible canonical text result by combining information from the multiple views, either by processing each tile independently and combining the results, or by combining information deep within the recognition system (most likely deep network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">How the FSNS Dataset Was Created</head><p>The following process was used to create the FSNS dataset: 1. A street-name-sign detector was applied to all Google Street View images from France. The detector returns an image rectangle around each street name sign, together with its geographic location (latitude and longitude). 2. Multiple images of the same geographic location were gathered together (spatially clustered). 3. Text from the signs was transcribed using a combination of reCAPTCHA <ref type="bibr" target="#b2">[3]</ref>, OCR and human operators. 4. Transcribed text was presented to human operators to verify the accuracy of the transcription.</p><p>Incorrect samples were re-routed for human transcription (back to step 3) or discarded if already the result of a human transcription. 5. Images were bucketized geographically (by latitude/longitude) so that the train, validation, test, and private test sets come from disjoint geographic locations, with 100 m wide strips of "wall" in between that are not used, to ensure that the same physical sign can't be viewed from different sets. 6. Since roads are long entities that may pass between the disjoint geographic sections, there may be multiple signs of the same street name at multiple locations in different subsets. Therefore as each subset is generated, any images with truth strings that match a truth string in any previously generated subset are discarded. Each subset thereby has a disjoint set of truth strings. 7. All images for which the truth string included a character outside of the chosen encoding set, or for which the encoded label length exceeded the maximum of 37, were discarded. The character set to be handled is thus carefully controlled.</p><p>Note that the transcription was systematically Title Case folded from the original transcription, in order to make it represent the way that the street name would appear on a map. This process includes removal of text that is not relevant, including data such as the district or building numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Normalized Truth Text</head><p>The FSNS dataset is made more interesting by the fact that the truth text is a normalized representation of the name of the street, as it should be written on the map, instead of a simple direct transcription of the text on the sign. The main normalization is Title Case transformation of the text, which is often written on the sign in all upper case. Title Case is specified as follows: The words: au, aux, de, des, du, et, la, le, les, sous, sur always appear in lower-case. The prefixes: d', l' always appear in lower-case. All other words, including suffixes after d' and l', always appear with the initial letter capitalized and the rest in lower-case.</p><p>The other main normalization is that some text on the sign, which is not part of the name of the street, is discarded. Although this seems a rather vague instruction, for a human, even without knowledge of French, it becomes easy after reading a few signs, as the actual street names fit into a reasonably obvious pattern, and the extraneous text is usually in a smaller size.</p><p>Some examples of some of these normalizations of the text between the sign and the truth text are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The task of transcribing the signs is thus not a basic OCR problem, but perhaps somewhat more like image captioning <ref type="bibr" target="#b16">[17]</ref>, by requiring an interpretation of what the sign means, not just its literal content. A researcher working with the FSNS dataset is hereby provided with a variety of design options between adding text post-processing to the output of an OCR engine and training a single network to learn the entire problem "end-to-end".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Details of the FSNS Dataset</head><p>The location of the FSNS dataset is documented in the README.md file. <ref type="bibr" target="#b1">2</ref> There are 3 disjoint subsets, Train, Validation and Test 3 . Each contains images of fixed size, 600 × 150 pixels, containing 4 tiles of 150 × 150 laid out horizontally, and padded with random noise where less than 4 views are available.</p><p>The size and location of each subset are shown in <ref type="table" target="#tab_1">Table 2</ref>, and some basic analysis of the word content of each subset is shown in <ref type="table" target="#tab_2">Table 3</ref>. The analysis in <ref type="table" target="#tab_2">Table 3</ref> excludes frequent words with frequency in the Train set &gt;100, and the words listed in Section 4 as lower-case. As might be expected, given the process by which the subsets have been made disjoint, the fraction of words in each subset that are out of vocabulary with respect to the Train subset is reasonably high at around 30%. Such a rate of out-of-vocabulary words will also make it difficult for a system to learn the full vocabulary from the Train set.   Each subset is stored as multiple TFRecords files of tf.train.Example protocol buffers, which makes them ready-made for input to TensorFlow <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b0">[1]</ref>. The Example protocol buffer is very flexible, so the full details of the content of each example are laid out in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Note that the ultimate goal of a machine learning system is to produce the UTF-8 string in "image/text." That may be achieved simply by learning the byte sequences in the text field. Alternatively, there is also a pre-encoded mapping to integer class-ids provided in "image/class" and "image/unpadded class". The mapping between these class-ids and the UTF-8 text is provided in a separate file entitled charset size=134.txt. Each line in that file lists a class-id, a tab character, and the UTF-8 string that is represented by the class-id. Class-id 0 represents a space, and the last class-id, 133, represents the "null" character, as used by the Connectionist Temporal Classification (CTC) alignment algorithm <ref type="bibr" target="#b4">[5]</ref> typically used with an LSTM network. Note that some class-ids map to multiple UTF-8 strings, as some normalization has been applied, such as folding all the different shapes of double quote to the same class.</p><p>The ground truth text in the FSNS dataset uses a subset of these characters. In addition to all digits, upper and lower-case A-Z, there are the following accented characters:àÀâÂäç ÇéÉèÈ eÊëËîÎïôÔ oeùÙûÛüÿ and these punctuation symbols: &lt;= -, ; ! ? / . ' " ( ) ] &amp; + a total of 109, including space.</p><p>For systems that process the multiple views separately, it is possible to avoid processing the noise padding. The number of real, non-noise views of a sign is given by the value of the field "image/orig width" divided by 150.</p><p>No sample in any of the subsets has a text field that encodes to more than 37 class-ids. 37 is not a completely arbitrary choice. When padded with nulls in between each label for CTC, (2×37+1 = 75) the classid sequences are no longer than half the width (150/2 = 75) of a single input view, which allows for some shrinkage of the data width in the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Challenge</head><p>The FSNS dataset provides a rich and interesting challenge in machine learning, due to the variety of tasks that are required. Here is a summary of the different processes that a model needs to learn to discover the right solution:</p><p>• Locating the lines of text within the sign within each image.</p><p>• Recognizing the text content within each line.</p><p>• Discarding irrelevant text.</p><p>• Title Case normalization.</p><p>• Combining data from multiple signs, ignoring data from blurred or inconsistent signs.</p><p>None of the above is an explicit goal of the challenge. The current trend in machine learning is to build and train a single large/deep network to solve all of a problem without additional algorithmic pieces on one end or another, or to glue trained components together <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b5">[6]</ref>. We believe that the FSNS data set is large enough to train a single deep network to learn all of the above tasks, and we provide an example in Section 7. We therefore propose that a competition based on the FSNS dataset should measure:</p><p>• Word recall: Fraction of space-delimited words in the truth that are present in the OCR output.</p><p>• Word precision: Fraction of space-delimited words in the OCR output that are present in the truth.</p><p>• Sequence error: the fraction of truth text strings that are not produced exactly by the network, after folding multiple spaces to single space.</p><p>Word recall and precision are almost universally used, and need no introduction. We add sequence error here because the strings are short enough that we can expect a significant number of them to be completely correct. Using only these metrics allows for end-to-end systems to compete directly against systems built from smaller components that are designed for specific sub-problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">An End-to-End Solution</head><p>We now describe a Tensor Flow graph that has been designed specifically to address the Challenge, end-to-end, using just the graph, with no algorithmic components. This means that the text line finding and handling of multiple views, including where there are less than four, is entirely learned and dealt with inside the network. Instead of using the orig width field in the dataset, the images are input as fixed size and the random padding informs the network of the lack of useful content. The network is based on the design that has been shown to work well for many languages in Tesseract <ref type="bibr" target="#b13">[14]</ref>, with some extensions to handle the multi-line, multi-tile FSNS dataset. The design is named Street-name Tensor-flow Recurrent End-to-End Transcriber (STREET). To perform the tasks listed above, the graph design has a high-level structure with purpose, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Conventional convolutional layers process the images to extract features. Since each view may contain up to three lines of text, the next step is intended to allow the network to find upto three text lines and recognize the text in each separately. The text may appear in different positions within each image, so some character position normalization is also required. Only then can the individual outputs be combined to produce a single target string. These components of the end-to-end system are described in detail below. Tensor Flow code for the STREET model described in this paper is available at the Tensor Flow Github repository. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Convolutional Feature Extraction</head><p>The input image, being 600 × 150, is de-tiled to make the input a batch of 4 images of size 150 × 150. This is achieved by a generic reshape, which is a combination of TensorFlow reshape and transpose operations that split one dimension of the input tensor and map the split parts to other dimensions. Two convolutional layers are then used with max pooling, with the expectation that they will find edges, and combine them into features, as well as reduce the size of the image down to 25 × 25. <ref type="figure" target="#fig_4">Figure 5</ref> shows the detail of the convolutions. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">150,</ref><ref type="bibr">150,</ref><ref type="bibr" target="#b2">3]</ref> [  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Textline Finding and Reading</head><p>Vertically summarizing Long Short-Term Memory (LSTM) <ref type="bibr" target="#b6">[7]</ref> cells are used to find text lines. Summarizing with an LSTM, inspired by the LSTM used for sequence to sequence translation <ref type="bibr" target="#b14">[15]</ref>, involves ignoring the outputs of all timesteps except the last. A vertically summarizing LSTM is a summarizing LSTM that scans the input vertically. It is thus expected to compute a vertical summary of its input, which will be taken from the last vertical timestep. Each x-position is treated independently. Three different vertical summarizations are used:</p><p>1. Upward to find the top textline. 2. Separate upward and downward LSTMs, with depth-concatenated outputs, to find the middle textline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Downward to find the bottom textline.</head><p>Although each vertically summarizing LSTM sees the same input, and could theoretically summarize the entirety of what it sees, they are organized this way so that they only have to produce a summary of the most recently seen information. Since the middle line is harder to find, that gets two LSTMs working in opposite directions. Each receives a copy of the output from the convolutional layers and passes its output to a separate bi-directional horizontal LSTM to recognize the text. Bidirectional LSTMs have been shown to be able to read text with high accuracy <ref type="bibr" target="#b1">[2]</ref>. The outputs of the bidirectional LSTMs are concatenated in the x-dimension, to string the text lines out in reading order. <ref type="figure" target="#fig_5">Figure 6</ref> shows the details. x concat, puts the textlines in sequence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Character Position Normalization</head><p>Assuming that each network component so far has achieved what it was designed to do, we now have a batch of four sets of one to three lines of text, spread spatially across the x-dimension. Each of the four sign images in a batch may have the text positioned differently, due to different perspective within each sign image. It is therefore useful to give the network some ability to reshuffle the data along the x-dimension. To that end we provide two more LSTM layers, one scanning left-to-right across the x-dimension, and the other right-to-left, as shown in <ref type="figure">Figure 7</ref>. Instead of a bidirectional configuration, they operate in two distinct layers. This allows state information to be passed to the right or left in the x-dimension, allowing the characters in each of the four views to be aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Combination of Individual View Outputs</head><p>After giving the STREET network chance to normalize the position of the characters along the xdimension, a generic reshape is used to move the batch of 4 views into the depth dimension, which then becomes the input to a single unidirectional LSTM and the final softmax layer, in <ref type="figure">Figure 8</ref>.</p><p>The main purpose of this last LSTM is to combine the four views fo each sign to produce the most accurate result. If none of the layers that went before have done anything towards the Title Case normalization, this final LSTM layer is perfectly capable of learning to do that well. The only regularization used is a 50% dropout layer between the reshape that combines the four signs and the last LSTM layer. Details of each component of the STREET graph can be found in <ref type="table" target="#tab_7">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments and Results</head><p>As a baseline, Tesseract <ref type="bibr" target="#b13">[14]</ref> was tested, but the FSNS dataset is extremely difficult for it. The best results were obtained from the LSTM-based engine in version 4.00, with the addition of pre-processing to locate the rectangle of the sign, and invert the projective transformation, plus post-processing to Title Case the output to match the truth text, as well as combination of the highest confidence results from the four views. Even with this help, Tesseract only achieves word recall of 20-25%. See <ref type="table" target="#tab_8">Table 6</ref>. The majority of failure cases revolve around the textline finder, which includes noise connected components, drops characters, or merges textlines. The main cause of these difficulties appears to be the tight line spacing, compressed characters, and tight border that appears on most signs. The STREET model was trained using the CTC <ref type="bibr" target="#b4">[5]</ref> loss function, with the Adam optimizer <ref type="bibr" target="#b9">[10]</ref> in Tensor Flow, with a learning rate of 2 × 10 −5 , and 40 parallel training workers. The error metrics outlined in Section 6 were used. The results are also shown in <ref type="table" target="#tab_8">Table 6</ref>. The results show that the model is somewhat over-trained, yet the results for validation, test and private test are very close, which suggests that these subsets are large enough to be a good reflection of the model's true performance.  Some examples of error cases are shown in <ref type="figure" target="#fig_6">Figure 9</ref>. In the first example, the model can be confused by obstructions. On the second line, the model drops a small word, perhaps as not relevant. On the third line, a less frequent prefix is replaced by a more frequent one. In the final example, an accent is dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>The FSNS dataset provides an interesting machine learning challenge. We have shown that it is possible to obtain reasonable results for the entire task with a single end-to-end network, and the STREET network could easily be improved by application of common regularization approaches and/or changing the network structure. Alternatively there are many other possible approaches that involve applying algorithmic or learned solutions to parts of the problem. Here are a few:</p><p>• Detecting the position/orientation of the sign by image processing or even structure from motion methods, correcting the perspective, and applying a simple OCR engine.</p><p>• Text line finding followed by OCR on individual text lines.</p><p>• Detecting the worst sign(s) and discarding them, by blur detection, obstruction detection, contrast, or even determining that there is more than one physical sign in the image. A comparison of these approaches against the end-to-end approach would be very interesting and provide useful information for the direction of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Some examples of FSNS images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of blurring, obstruction, and incorrect spatial clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of images with their normalized truth text</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>High-level structure of the network graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Convolutional Feature Extraction and size reduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Text Line Finding and Reading</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Some examples of error cases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets of outdoor images containing text, including larger than single character ground truth. Information obtained mostly from the iapr-tc11.org website</figDesc><table><row><cell>Name</cell><cell>Content</cell><cell>Size</cell></row><row><cell>ICDAR2003 [11]</cell><cell>Images with word and character bounding boxes</cell><cell>Train: 258 Images, 1,157 words Test: 251</cell></row><row><cell></cell><cell></cell><cell>Images, 1,111 words</cell></row><row><cell>SVHN [13]</cell><cell>Images of numbers and single digits from Google</cell><cell>Train: 73,257 digits Test: 26,032 Additional:</cell></row><row><cell></cell><cell>Street View with boxes</cell><cell>531,131</cell></row><row><cell>COCO-text [16]</cell><cell>Images from the MS COCO dataset that contain text</cell><cell>63,686 images with 173,589 text regions</cell></row><row><cell>KAIST [9] Scene</cell><cell>Images with word and character boxes of Korean and</cell><cell>3,000 images</cell></row><row><cell>Text</cell><cell>English</cell><cell></cell></row><row><cell>NEOCR [12]</cell><cell>Images with text field boxes and perspective</cell><cell>659 images with 5,238 text fields</cell></row><row><cell></cell><cell>quadrangles.</cell><cell></cell></row><row><cell>SVT [18]</cell><cell>Images from Google Street View, with names of</cell><cell>Train: 100 images, 211 words Test: 250</cell></row><row><cell></cell><cell>businesses in them</cell><cell>images, 514 words</cell></row><row><cell>Synthetic Word</cell><cell>Synthetic images of real-world-like words</cell><cell>9 million images, 90k distinct words</cell></row><row><cell>[8]</cell><cell></cell><cell></cell></row><row><cell>FSNS</cell><cell>Images of French street name signs</cell><cell>&gt;1,000,000 images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Location and size of each subset of the FSNS dataset</figDesc><table><row><cell>Subset</cell><cell>Location</cell><cell cols="2">Number of Images Number of Words</cell></row><row><cell>Train</cell><cell>train/train@512</cell><cell>1044868</cell><cell>3189576</cell></row><row><cell>Validation</cell><cell>validation/validation@64</cell><cell>16150</cell><cell>50218</cell></row><row><cell>Test</cell><cell>test/test@64</cell><cell>20404</cell><cell>62650</cell></row><row><cell>Private Test</cell><cell>n/a</cell><cell>21054</cell><cell>65366</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Word counts excluding 'stop' words, (being the prefixes with a frequency &gt;100, and the lower-cased words) in each subset and number out of vocabulary (OOV) with respect to (wrt) words in the Train subset. Non-stop Words Unique Words Unique Words OOV wrt Train Total OOV words Percent OOV words</figDesc><table><row><cell>Subset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell>1336341</cell><cell>93482</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Validation</cell><cell>22250</cell><cell>7425</cell><cell>3482</cell><cell>7272</cell><cell>32.7</cell></row><row><cell>Test</cell><cell>28587</cell><cell>8675</cell><cell>4081</cell><cell>8526</cell><cell>29.8</cell></row><row><cell>Private Test</cell><cell>28752</cell><cell>8870</cell><cell>4265</cell><cell>9375</cell><cell>32.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The content of each Example proto in the TFRecords files</figDesc><table><row><cell>Key name</cell><cell>Type</cell><cell>Length</cell><cell>Content</cell></row><row><cell>image/format</cell><cell>bytes(string)</cell><cell>1</cell><cell>"PNG"</cell></row><row><cell>image/encoded</cell><cell>bytes(string)</cell><cell>1</cell><cell>Image encoded as PNG.</cell></row><row><cell>image/class</cell><cell>int64</cell><cell>37</cell><cell>Truth class-ids padded with nulls.</cell></row><row><cell>image/unpadded class</cell><cell>int64</cell><cell>Variable</cell><cell>Truth class-ids unpadded.</cell></row><row><cell>image/width</cell><cell>int64</cell><cell>1</cell><cell>Width of the image in pixels.</cell></row><row><cell>image/orig width</cell><cell>int64</cell><cell>1</cell><cell>Pre-padding width in pixels.</cell></row><row><cell>image/height</cell><cell>int64</cell><cell>1</cell><cell>Height of the image in pixels.</cell></row><row><cell>image/text</cell><cell>bytes(string)</cell><cell>1</cell><cell>Truth string in UTF-8.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Size and computational complexity of the layers in the graph</figDesc><table><row><cell>Name</cell><cell>Input</cell><cell>Output</cell><cell>Weights</cell><cell>Mult-add</cell></row><row><cell>Reshape0</cell><cell>1x150x600x3</cell><cell>4x150x150x3</cell><cell></cell><cell></cell></row><row><cell>Conv0 (5x5x16)</cell><cell>4x150x150x3</cell><cell>4x150x150x16</cell><cell>1216</cell><cell>109M</cell></row><row><cell>Maxpool0 2x2</cell><cell>4x150x150x16</cell><cell>4x75x75x16</cell><cell></cell><cell></cell></row><row><cell>Conv1 (5x5x64)</cell><cell>4x75x75x16</cell><cell>4x75x75x64</cell><cell>25664</cell><cell>577M</cell></row><row><cell>Maxpool1 3x3</cell><cell>4x75x75x64</cell><cell>4x25x25x64</cell><cell></cell><cell></cell></row><row><cell>V-SumLSTMs (4x)</cell><cell>4x25x25x64</cell><cell>4x1x25x128x4</cell><cell>33024x4</cell><cell>330M</cell></row><row><cell>DepthConcat</cell><cell>4x1x25x128x2</cell><cell>4x1x25x256</cell><cell></cell><cell></cell></row><row><cell>BidiLSTMs (3x)</cell><cell cols="3">4x1x25x128x2 + 4x1x25x256 4x1x25x256x3 263168x2 + 394240</cell><cell>92M</cell></row><row><cell>XConcat</cell><cell>4x1x25x256x3</cell><cell>4x1x75x256</cell><cell></cell><cell></cell></row><row><cell>LTRLSTM</cell><cell>4x1x75x256</cell><cell>4x1x75x128</cell><cell>197120</cell><cell>59M</cell></row><row><cell>RTLLSTM</cell><cell>4x1x75x128</cell><cell>4x1x75x128</cell><cell>131584</cell><cell>39M</cell></row><row><cell>Reshape1</cell><cell>4x1x75x128</cell><cell>1x1x75x512</cell><cell></cell><cell></cell></row><row><cell>LTRLSTM</cell><cell>1x1x75x512</cell><cell>1x1x75x256</cell><cell>787456</cell><cell>59M</cell></row><row><cell>Softmax</cell><cell>1x1x75x256</cell><cell>1x1x75x134</cell><cell>34438</cell><cell>2.6M</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>2.2M</cell><cell>1.3B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Error rate results</figDesc><table><row><cell>System</cell><cell>Test Set</cell><cell cols="3">Word Recall Word Precision Sequence Error</cell></row><row><cell>Tesseract</cell><cell>Validation</cell><cell>22.73</cell><cell>20.21</cell><cell>95.81</cell></row><row><cell>Tesseract</cell><cell>Test</cell><cell>23.58</cell><cell>20.49</cell><cell>98.91</cell></row><row><cell cols="2">Tesseract Private Test</cell><cell>23.93</cell><cell>21.05</cell><cell>95.93</cell></row><row><cell>STREET</cell><cell>Train</cell><cell>94.90</cell><cell>95.40</cell><cell>13.14</cell></row><row><cell>STREET</cell><cell>Validation</cell><cell>89.46</cell><cell>90.28</cell><cell>26.63</cell></row><row><cell>STREET</cell><cell>Test</cell><cell>88.81</cell><cell>89.71</cell><cell>27.54</cell></row><row><cell cols="2">STREET Private Test</cell><cell>89.48</cell><cell>90.32</cell><cell>26.64</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorflow/models/tree/master/street/README.md<ref type="bibr" target="#b2">3</ref> An additional private test set will be kept back for the purposes of organizing competitions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/tensorflow/models/tree/master/street</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-performance ocr for printed english and fraktur using lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ul-Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Azawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">:</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recaptcha</surname></persName>
		</author>
		<ptr target="https://www.google.com/recaptcha/intro/index.html" />
		<imprint>
			<biblScope unit="page" from="2016" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">:</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<biblScope unit="page" from="2016" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Touch tt: Scene text extractor using touchscreen interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="88" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icdar 2003 robust reading competitions: entries, results, and future directions</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neocr: A configurable dataset for natural image text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meyer-Wegener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Camera-Based Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="150" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<meeting><address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
	<note>4. Granada</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://github.com/tesseract-ocr/docs/tree/master/das_tutorial2016" />
		<title level="m">Tesseract blends old and new ocr technology</title>
		<imprint>
			<biblScope unit="page" from="2016" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
