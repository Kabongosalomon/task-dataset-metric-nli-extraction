<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PixelLink: Detecting Scene Text via Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CVTE Research 4 Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<email>dcai@zju.edu.cnxuelongli@opt.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba-Zhejiang University Joint Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PixelLink: Detecting Scene Text via Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art scene text detection algorithms are deep learning based methods that depend on bounding box regression and perform at least two kinds of predictions: text/nontext classification and location regression. Regression plays a key role in the acquisition of bounding boxes in these methods, but it is not indispensable because text/non-text prediction can also be considered as a kind of semantic segmentation that contains full location information in itself. However, text instances in scene images often lie very close to each other, making them very difficult to separate via semantic segmentation. Therefore, instance segmentation is needed to address this problem. In this paper, PixelLink, a novel scene text detection algorithm based on instance segmentation, is proposed. Text instances are first segmented out by linking pixels within the same instance together. Text bounding boxes are then extracted directly from the segmentation result without location regression. Experiments show that, compared with regression-based methods, PixelLink can achieve better or comparable performance on several benchmarks, while requiring many fewer training iterations and less training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading text in the wild, or robust reading has drawn great interest for a long time <ref type="bibr" target="#b24">(Ye and Doermann 2015)</ref>. It is usually divided into two steps or sub-tasks: text detection and text recognition.</p><p>The detection task, also called localization, takes an image as input and outputs the locations of text within it. Along with the advances in deep learning and general object detection, more and more accurate as well as efficient scene text detection algorithms have been proposed, e.g., CTPN <ref type="bibr" target="#b19">(Tian et al. 2016)</ref>, TextBoxes <ref type="bibr" target="#b12">(Liao et al. 2017)</ref>, SegLink <ref type="bibr" target="#b16">(Shi, Bai, and Belongie 2017)</ref> and EAST <ref type="bibr" target="#b26">(Zhou et al. 2017</ref>). Most of these state-of-the-art methods are built on Fully Convolutional Networks <ref type="bibr" target="#b14">(Long, Shelhamer, and Darrell 2015)</ref>, and perform at least two kinds of predictions: 1. Text/non-text classification. Such predictions can be taken as probabilities of pixels being within text bounding boxes <ref type="bibr" target="#b25">(Zhang et al. 2016)</ref>. But they are more frequently used as confidences on regression results (e.g., TextBoxes, SegLink, EAST). 2. Location regression. Locations of text instances, or their segments/slices, are predicted as offsets from reference boxes (e.g., TextBoxes, SegLink, CTPN), or absolute locations of bounding boxes (e.g., EAST). In methods like SegLink, linkages between segments are also predicted. After these predictions, post-processing that mainly includes joining segments together (e.g., SegLink, CTPN) or Non-Maximum Suppression (e.g., TextBoxes, EAST), is applied to obtain bounding boxes as the final output.</p><p>Location regression has long been used in object detection, as well as in text detection, and has proven to be effective. It plays a key role in the formulation of text bounding boxes in state-of-the-art methods. However, as mentioned above, text/non-text predictions can not only be used as the confidences on regression results, but also as a segmentation score map, which contains location information in itself and can be used to obtain bounding boxes directly. Therefore, regression is not indispensable.</p><p>However, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, text instances in scene images usually lie very close to each other. In such cases, they are very difficult, and are sometimes even impossible to separate via semantic segmentation (i.e., text/non-text prediction) only; therefore, segmentation at the instance level is further required.</p><p>To solve this problem, a novel scene text detection algorithm, PixelLink, is proposed in this paper. It extracts text locations directly from an instance segmentation result, instead of from bounding box regression. In PixelLink, a Deep Neural Network (DNN) is trained to do two kinds of pixelwise predictions, text/non-text prediction, and link prediction. Pixels within text instances are labeled as positive (i.e., text pixels), and otherwise are labeled as negative (i.e., nontext pixels). The concept of link here is inspired by the link design in SegLink, but with significant difference. Every pixel has 8 neighbors. For a given pixel and one of its neighbors, if they lie within the same instance, the link between them is labeled as positive, and otherwise negative. Predicted positive pixels are joined together into Connected Components (CC) by predicted positive links. Instance segmentation is achieved in this way, with each CC representing a detected text. Methods like minAreaRect in OpenCV (Its 2014) can be applied to obtain the bounding boxes of CCs as the final detection result.</p><p>Our experiments demonstrate the advantages of PixelLink over state-of-the-art methods based on regression. Specifically, trained from scratch, PixelLink models can achieve comparable or better performance on several benchmarks while requiring fewer training iterations and less training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic&amp;Instance Segmentation</head><p>The segmentation task is to assigning pixel-wise labels to an image. When only object category is considered, it is called semantic segmentation. Dominating methods for this task usually adopts the approach of Fully Convolution Networks (FCN) <ref type="bibr" target="#b14">(Long, Shelhamer, and Darrell 2015)</ref>. Instance segmentation is more challenging than semantic segmentation because it requires not only object category for each pixel but also a differentiation of instances. It's more relevant to general object detection than semantic segmentation, for being aware of object instances. Recent methods in this field make heavy use of object detection systems. FCIS ) extends the idea of position-sensitive prediction in R-FCN . Mask R-CNN <ref type="bibr" target="#b5">(He et al. 2017a)</ref> changes the RoIPooling in Faster R- <ref type="bibr">CNN (Ren et al. 2015)</ref> to RoIAlign. They both do detection and segmentation in a same deep model, and highly depend their segmentation results on detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmentation-based Text Detection</head><p>Segmentation has been adopted in text detection for a long time.  cast the detection task as a semantic segmentation problem, by predicting three kinds of score maps: text/non-text, character classes, and character linking orientations. They are then grouped into words or lines. In <ref type="bibr" target="#b25">(Zhang et al. 2016)</ref>, TextBlocks are found from a saliency map predicted by FCN, and character candidates are extracted using MSER <ref type="bibr" target="#b1">(Donoser and Bischof 2006)</ref>. Lines or words are formed using hand-crafted rules at last. In CCTN ), a coarse network is used to detect text regions roughly by generating a text region heat-map, and then the detected regions are refined into text lines by a fine text network, which outputs a central line area heat-map and a text line area heat-map. These methods often suffer from time-consuming post-processing steps and unsatisfying performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Regression-based Text Detection</head><p>Most methods in this category take advantage of the development in general object detection. CTPN <ref type="bibr" target="#b19">(Tian et al. 2016</ref>) extends the anchor idea in object detection to predict text slices, which are then connected through heuristic rules. TextBoxes <ref type="bibr" target="#b12">(Liao et al. 2017)</ref>, a text-specific SSD , adopts anchors of large aspect ratio and kernels of irregular shape, to fit for the large-aspect-ratio feature of scene text. RRPN <ref type="bibr" target="#b15">(Ma et al. 2017</ref>) adds rotation to both anchors and RoIPooling in Faster R-CNN, to deal with the orientation of scene text. SegLink <ref type="bibr" target="#b16">(Shi, Bai, and Belongie 2017)</ref> adopts SSD to predict text segments, which are linked into complete instances using the linkage prediction. EAST <ref type="bibr" target="#b26">(Zhou et al. 2017</ref>) performs very dense predictions that are processed using locality-aware NMS. All these regression-based text detection algorithms have predictions for both confidences and locations at the same time.</p><p>In this paper, state-of-the-art mainly refers to published methods that perform best on IC13 <ref type="bibr" target="#b8">(Karatzas et al. 2013)</ref> or IC15 <ref type="bibr" target="#b9">(Karatzas et al. 2015)</ref>, including TextBoxes, CTPN, SegLink, and EAST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detecting Text via Instance Segmentation</head><p>As shown in <ref type="figure">Fig. 2</ref>, PixelLink detects text via instance segmentation, where predicted positive pixels are joined together into text instances by predicted positive links. Bounding boxes are then directly extracted from this segmentation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Following SSD and SegLink, VGG16 (Simonyan and Zisserman 2014) is used as the feature extractor, with fully connected layers, i.e., fc6 and fc7, being converted into convolutional layers. The fashion of feature fusion and pixelwise prediction inherits from <ref type="bibr" target="#b14">(Long, Shelhamer, and Darrell 2015)</ref>. As shown in <ref type="figure">Fig. 3</ref>, the whole model has two separate headers, one for text/non-text prediction, and the other for link prediction. Softmax is used in both, so their outputs have 1*2=2 and 8*2=16 channels, respectively.</p><p>Two settings of feature fusion layers are implemented: {conv2 2, conv3 3, conv4 3, conv5 3, fc 7}, and {conv3 3, conv4 3, conv5 3, fc 7}, denoted as PixelLink+VGG16 2s, and PixelLink+VGG16 4s, respectively. The resolution of 2s predictions is a half of the original image, and 4s is a quarter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linking Pixels Together</head><p>Given predictions on pixels and links, two different thresholds can be applied on them separately. Positive pixels are then grouped together using positive links, resulting in a collection of CCs, each representing a detected text instance. Thus instance segmentation is achieved. It is worth noting that, given two neighboring positive pixels, their link are predicted by both of them, and they should be connected when one or both of the two link predictions are positive. This linking process can be implemented using disjoint-set data structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extraction of Bounding Boxes</head><p>Actually, the detection task is completed after instance segmentation. However, bounding boxes are required as detection results in challenges like IC13 <ref type="bibr" target="#b8">(Karatzas et al. 2013</ref>  <ref type="figure">Figure 2</ref>: Architecture of PixelLink. A CNN model is trained to perform two kinds of pixel-wise predictions: text/non-text prediction and link prediction. After being thresholded, positive pixels are joined together by positive links, achieving instance segmentation. minAreaRect is then applied to extract bounding boxes directly from the segmentation result. Noise predictions can be efficiently removed using post-filtering. An input sample is shown for better illustration. The eight heat-maps in the dashed box stand for the link predictions in eight directions. Although some words are difficult to separate in text/non-text prediction, they are separable through link predictions.</p><p>IC15 <ref type="bibr" target="#b9">(Karatzas et al. 2015)</ref>, and COCO-Text <ref type="bibr" target="#b20">(Veit et al. 2016)</ref>. Therefore, bounding boxes of CCs are then extracted through methods like minAreaRect in OpenCV (Its 2014). The output of minAreaRect is an oriented rectangle, which can be easily converted into quadrangles for IC15, or rectangles for IC13. It is worth mentioning that in PixelLink, there is no restriction on the orientation of scene text.</p><p>This step leads to the key difference between PixelLink and regression-based methods, i.e., bounding boxes are obtained directly from instance segmentation other than location regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Post Filtering after Segmentation</head><p>Since PixelLink attempts to group pixels together via links, it is inevitable to have some noise predictions, so a postfiltering step is necessary. A straightforward yet efficient solution is to filter via simple geometry features of detected boxes, e.g., width, height, area and aspect ratio, etc. For example, in the IC15 experiments in Sec. 5.3, a detected box is abandoned if its shorter side is less than 10 pixels or if its area is smaller than 300. The 10 and 300 are statistical results on the training data of IC15. Specifically, for a chosen filtering criteria, the corresponding 99-th percentile calculated on TRAINING set is chosen as the threshold value. For example, again, 10 is chosen as the threshold on shorter side length because about 99% text instances in IC15-train have a shorter side ≥ 10 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ground Truth Calculation</head><p>Following the formulation in TextBlocks <ref type="bibr" target="#b25">(Zhang et al. 2016)</ref>, pixels inside text bounding boxes are labeled as positive. If overlapping exists, only un-overlapped pixels are positive. Otherwise negative.</p><p>For a given pixel and one of its eight neighbors, if they belong to the same instance, the link between them is positive. Otherwise negative.</p><p>Note that ground truth calculation is carried out on input images resized to the shape of prediction layer, i.e., conv3 3 for 4s and conv2 2 for 2s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>The training loss is a weighted sum of loss on pixels and loss on links:</p><formula xml:id="formula_0">L = λL pixel + L link .<label>(1)</label></formula><p>Since L link is calculated on positive pixels only, the classification task of pixel is more important than that of link, and λ is set to 2.0 in all experiments.</p><p>Loss on Pixels Sizes of text instances might vary a lot. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>  <ref type="figure">Figure 3</ref>: Structure of PixelLink+VGG16 2s. fc6 and fc7 are converted into convolutional layers. The upsampling operation is done through bilinear interpolation directly. Feature maps from different stages are fused through a cascade of upsampling and add operations. All pooling layers except pool5 take a stride of 2, and pool5 takes 1. Therefore, the size of fc7 is the same as conv5 3, and no upsampling is needed when adding scores from these two layers. 'conv 1 × 1,2(16)' stands for a 1×1 convolutional layer with 2 or 16 kernels, for text/non-text prediction or link prediction individually.</p><p>put the same weight on all positive pixels, it's unfair to instances with small areas, and may hurt the performance. To deal with this problem, a novel weighted loss for segmentation, Instance-Balanced Cross-Entropy Loss, is proposed. In detail, for a given image with N text instances, all instances are treated equally by giving a same weight to everyone of them, denoted as B i in Equ. 2. For the i-th instance with area = S i , every positive pixels within it have a weight of</p><formula xml:id="formula_1">w i = Bi Si . B i = S N , S = N i S i , ∀i ∈ {1, . . . , N }<label>(2)</label></formula><p>Online Hard Example Mining (OHEM) <ref type="bibr" target="#b17">(Shrivastava, Gupta, and Girshick 2016)</ref> is applied to select negative pixels. More specifically, r * S negative pixels with the highest losses are selected, by setting their weights to ones. r is the negative-positive ratio and is set to 3 as a common practice.</p><p>The above two mechanisms result in a weight matrix, denoted by W , for all positive pixels and selected negative ones. The loss on pixel classification task is:</p><formula xml:id="formula_2">L pixel = 1 (1 + r)S W L pixel CE ,<label>(3)</label></formula><p>where L pixel CE is the matrix of Cross-Entropy loss on text/non-text prediction. As a result, pixels in small instances have a higher weight, and pixels in large instances have a smaller weight. However, every instance contributes equally to the loss.</p><p>Loss on Links Losses for positive and negative links are calculated separately and on positive pixels only:</p><formula xml:id="formula_3">L link pos = W pos link L link CE , L link neg = W neg link L link CE ,</formula><p>where L link CE is the Cross-Entropy loss matrix on link prediction. W pos link and W neg link are the weights of positive and negative links respectively. They are calculated from the W in Equ. 3. In detail, for the k-th neighbor of pixel (i, j):</p><formula xml:id="formula_4">W pos link (i, j, k) = W (i, j) * (Y link (i, j, k) == 1), W neg link (i, j, k) = W (i, j) * (Y link (i, j, k) == 0), where Y link is the label matrix of links.</formula><p>The loss on link prediction is a kind of class-balanced cross-entropy loss:</p><formula xml:id="formula_5">L link = L link pos rsum(W pos link ) + L link neg rsum(W neg link ) ,<label>(4)</label></formula><p>where rsum denotes reduce sum, which sums all elements of a tensor into scalar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation</head><p>Data augmentation is done in a similar way to SSD with an additional random rotation step. Input images are firstly rotated at a probability of 0.2, by a random angle of 0, π/2, π, or 3π/2, the same with <ref type="bibr" target="#b6">(He et al. 2017b</ref>). Then randomly crop them with areas ranging from 0.1 to 1, and aspect ratios ranging from 0.5 to 2. At last, resize them uniformly to 512 × 512. After augmentation, text instances with a shorter side less than 10 pixels are ignored. Text instances remaining less than 20% are also ignored. Weights for ignored instances are set to zero during loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>PixelLink models are trained and evaluated on several benchmarks, achieving on par or better results than stateof-the-art methods, showing that the text localization task can be well solved without bounding box regression. Some detection results are shown in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Datasets</head><p>ICDAR2015(IC15) Challenge 4 of IC15 <ref type="bibr" target="#b9">(Karatzas et al. 2015)</ref> is the most commonly used benchmark for detecting scene text in arbitrary directions. It consists of two sets: train and test, containing 1, 000 and 500 images respectively. Unlike previous ICDAR challenges, images are acquired using Google Glass without taking care of viewpoint, positioning or frame quality. Only readable Latin scripts longer than 3 characters are cared and annotated as word quadrilaterals. 'do not care' scripts are also annotated, but ignored in evaluation.</p><p>ICDAR2013(IC13) IC13 <ref type="bibr" target="#b8">(Karatzas et al. 2013</ref>) is another widely used benchmark for scene text detection, containing 229 images for training, and 233 for testing. Text instances in this dataset are mostly horizontal and annotated as rectangles in words.</p><p>MSRA-TD500(TD500) Texts in TD500 <ref type="bibr" target="#b22">(Yao et al. 2012)</ref> are also arbitrarily oriented, but much longer than those in IC15 because they are annotated in lines. TD500 contains 500 images in total, 300 for training and 200 for testing. Both English and Chinese scripts exist.</p><p>Corresponding standard evaluation protocols are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>PixelLink models are optimized by SGD with momentum = 0.9 and weight decay = 5 × 10 −4 . Instead of fine-tuning from an ImageNet-pretrained model, the VGG net is randomly initialized via the xavier method <ref type="bibr" target="#b2">(Glorot and Bengio 2010)</ref>. Learning rate is set to 10 −3 for the first 100 iterations, and fixed at 10 −2 for the rest. Details will be described in each experiment individually. The whole algorithm is implemented in Tensorflow 1.1.0 and pure Python, with the code of join operation described in Sec. 3.2 compiled with Cython. When trained with a batch size of 24 on 3 GPUs(GTX Titan X), it takes about 0.65s per iteration, and the whole training processing takes about 7∼8 hours. 128G RAM and two Intel Xeon CPUs(2.20GHz) are available on the machine where experiments are conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detecting Oriented Text in IC15</head><p>The training starts from a randomly initialized VGG16 model, on IC15-train only. Model of 4s requires about 40K iterations of training, and 2s longer, about 60K iterations. The best performance of PixelLink on IC15 is better than the existing best single-scale method (EAST+PVA2x) by 5.5% in F-score. To further check the generalization ability of PixelLink, the same 2s model is also tested on COCO-Text evaluation set without any fine-tuning, achieving a performance of <ref type="bibr">35.4, 54.0, 42.4 for recall, precision and Fscore, exceeding the corresponding results 32.4, 50.4, 39.5 of EAST (Zhou et al. 2017)</ref>.</p><p>For the differences on implementation details and running environment, it's not a easy task to conduct an objective and fair speed comparison. Speeds reported here are only intended to show that PixelLink is not slower when compared with the regression-based state-of-the-art methods, if   75.3 76.5 75.9 the same deep model, i.e., VGG16, is used as the base network. Note it that although the EAST+PVANET2x runs the fastest, its accuracy is much lower than PixelLink models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Detecting Long Text in TD500</head><p>Since lines are detected in TD500, the final model on IC15 is not used for fine-tuning. Instead, the model is pretrained on IC15-train for about 15K iterations and fine-tuned on TD500-train + HUST-TR400 <ref type="bibr" target="#b21">(Yao, Bai, and Liu 2014)</ref> for about 25K iterations. Images are resized to 768 × 768 for testing. Thresholds of pixel and link are set to (0.8, 0.7). The min shorter side in post-filtering is 15 and min area 600, as the corresponding 99th-percentiles of the training set. Results in Tab. 2 show that among all VGG16-based models, EAST behaves the worst for its highest demand on large receptive fields. Both SegLink and PixelLink don't need a deeper network to detect long texts, for their smaller demands on receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Detecting Horizontal Text in IC13</head><p>The final models for IC15 are fine-tuned on IC13-train, TD500-train and TR400, for about 10K iterations. In single scale testing, all images are resized to 512 × 512. Multiscales includes (384, 384), (512, 512), (768, 384), <ref type="bibr">(384,</ref><ref type="bibr">768)</ref>, <ref type="bibr">(768,</ref><ref type="bibr">768)</ref>, and a maximum longer side of 1600. Thresholds on pixel and link are (0.7, 0.5) for single-scale testing, and (0.6, 0.5) for multi-scale testing. The 99-th percentiles are 10, 300, for shorter side and area respectively, for post-filtering.</p><p>Different from regression-based methods like EAST and TextBoxes, PixelLink has no direct output as confidence on each detected bounding box, so its multi-scale testing scheme is specially designed. Specifically, prediction maps of different scales are uniformly resized to the largest height and the largest width among all maps. Then, fuse them by taking the average. The rest steps are identical to single-scale testing.</p><p>Results in Tab.3 show that multi-scale leads to an improvement of about 4 to 5 points in F-score, similar to the observation in TextBoxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Advantages of PixelLink</head><p>A further analysis of experiment results on IC15 shows that PixelLink, as a segmentation based method, has several ad- vantages over regression based methods. As listed in Tab. 4, among all methods using VGGNet, PixelLink can be trained much faster with less data, and behaves much better than the others. Specifically, after about only 25K iterations of training (less than a half of those needed by EAST or SegLink), PixelLink can achieve a performance on par with SegLink or EAST. Keep in mind that, PixelLink is trained from scratch, while the others need to be fine-tuned from an ImageNetpretrained model. When also trained from scratch on IC15train, SegLink can only achieve a F-score of 67.8 1 .</p><p>The question arises that, why PixelLink can achieve a better performance with many fewer training iterations and less training data? We humans are good at learning how to solve problems. The easier a problem is, the faster we can learn, the less teaching materials we will need, and the better we are able to behave. It may also hold for deep-learning models. So two factors may contribute.</p><p>Requirement on receptive fields When both adopting VGG16 as the backbone, SegLink behaves much better than EAST in long text detection, as shown in Tab. 2. This gap should be caused by their different requirements on receptive fields. Prediction neurons of EAST are trained to observe the whole image, in order to predict texts of any length. SegLink, although regression based too, its deep model only tries to predict segments of texts, resulting in a less requirement on receptive fields than EAST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficulty of tasks</head><p>In regression-based methods, bounding boxes are formulated as quadrangle or rotated rectangle. Every prediction neuron has to learn to predict their locations as precise numerical values, i.e. coordinates of four vertices, or center point, width, height and rotation angle. It's possible and effective, as has long been proven by algorithms like Faster R-CNN, SSD, YOLO, etc. However, such kind of predictions is far from intuitive and simple. Neurons have to study a lot and study hard to be competent for their assignments.</p><p>When it comes to PixelLink, neurons on the prediction layer only have to observe the status of itself and its neighboring pixels on the feature map. In another word, PixelLink has the least requirement on receptive fields and the easiest task to learn for neurons among listed methods. A useful guide for the design of deep models can be picked up from the assumption above: simplify the task for deep models if possible, because it might make them get trained faster and less data-hungry.</p><p>The success in training PixelLink from scratch with a very limited amount of data also indicates that text detection is much simpler than general object detection. Text detection may rely more on low-level texture feature and less on highlevel semantic feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Analysis</head><p>As shown in Tab. 5, ablation experiments have been done to analyze PixelLink models. Although PixelLink+VGG16 2s models have better performances, they are much slower and less practical than the 4s models, and therefore, Exp.1 of the best 4s model is used as the basic setting for comparison. Link is very important. In Exp.2, link is disabled by setting the threshold on link to zero, resulting in a huge drop on both recall and precision. The link design is important because it converts semantic segmentation into instance segmentation, indispensable for the separation of nearby texts in PixelLink.</p><p>Instance-Balance contributes to a better model. In Exp.3, Instance-Balance (IB) is not used and the weights of all positive pixels are set to the same during loss calculation. Even without IB, PixelLink can achieve a F-score of 81.2, outperforming state-of-the-art. When IB is used, a slightly better model can be obtained. Continuing the experiments on IC13, the performance gap is more obvious(shown in Tab. 6). Training image size matters. In Exp.4, images are resized to 384 × 384 for training, resulting in an obvious decline on both recall and precision. This phenomenon is in accordance with SSD.</p><p>Post-filtering is essential. In Exp.5, post-filtering is removed, leading to a slight improvement on recall, but a significant drop on precision.</p><p>Predicting at higher resolution is more accurate but slower. In Exp.6, the predictions are conducted on conv2 2, and the performance is improved w.r.t. both recall and precision, however, at a cost of speed. As shown in Tab. 1, the speed of 2s is less than a half of 4s, demonstrating a tradeoff between performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>PixelLink, a novel text detection algorithm is proposed in this paper. The detection task is achieved through instance segmentation by linking pixels within the same text instance together. Bounding boxes of detected text are directly extracted from the segmentation result, without performing location regression. Since smaller receptive fields are required and easier tasks are to be learned, PixelLink can be trained from scratch with less data in fewer iterations, while achieving on par or better performance on several benchmarks than state-of-the-art methods based on location regression. VGG16 is chosen as the backbone for convenient comparisons in the paper. Some other deep models will be investigated for better performance and higher speed.</p><p>Different from current prevalent instance segmentation methods   <ref type="bibr" target="#b5">(He et al. 2017a</ref>), PixelLink does not rely its segmentation result on detection performance. Applications of PixelLink will be explored on some other tasks that require instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Text instances often lie close to each other, making them hard to separate via semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Examples of detection results. From left to right in columns: IC15, IC13, and MSRA-TD500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, the area of 'Manchester' is greater than the sum of all the other words. When calculating loss, if we</figDesc><table><row><cell>Input Image</cell><cell></cell><cell>conv 1x1, 16 Link Prediction</cell></row><row><cell>pool1, /2 conv stage 1</cell><cell cols="2">conv 1x1, 2 Text/nontext Prediction</cell></row><row><cell>conv stage 2</cell><cell></cell></row><row><cell></cell><cell>conv 1x1, 2(16)</cell><cell>+</cell></row><row><cell>pool2, /2 conv stage 3</cell><cell></cell><cell>upsample</cell></row><row><cell></cell><cell>conv 1x1, 2(16)</cell><cell>+</cell></row><row><cell>pool3, /2 conv stage 4</cell><cell></cell><cell>upsample</cell></row><row><cell></cell><cell>conv 1x1, 2(16)</cell><cell>+</cell></row><row><cell>pool4, /2 conv stage 5</cell><cell></cell><cell>upsample</cell></row><row><cell></cell><cell>conv 1x1, 2(16)</cell><cell>+</cell></row><row><cell>pool5, /1</cell><cell></cell></row><row><cell>fc6 fc7</cell><cell>conv 1x1, 2(16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on IC15. 'R, P, F' stand for Recall, Precision and F-score. All the listed methods are tested at 720P or similar 1280 × 768. '-' means unreported. 'MS' stands for Multi-Scale. PixelLink+VGG16 2s and 4s predict at conv2 2 and conv3 3 respectively. PVANET2x is a modified version of PVANET<ref type="bibr" target="#b10">(Kim et al. 2016)</ref>, by doubling the channels.</figDesc><table><row><cell>All listed results for comparison are quoted from the corre-sponding original papers. Model R P F FPS</cell></row><row><cell>PixelLink+VGG16 2s PixelLink+VGG16 4s EAST+PVANET2x MS 78.3 83.3 81.0 82.0 85.5 83.7 3.0 81.7 82.9 82.3 7.3 -EAST+PVANET2x 73.5 83.6 78.2 13.2 EAST+VGG16 72.8 80.5 76.4 6.5 SegLink+VGG16 76.8 73.1 75.0 -CTPN+VGG16 51.6 74.2 60.9 7.1</cell></row><row><cell>Minimal shorter side length and area are used for post-filtering and set to 10 and 300 respectively, the correspond-ing 99th-percentiles of IC15-train by ignoring 'do-not-care' instances. Thresholds on pixel and link are found by grid search and set to (0.8, 0.8). Input images are resized to 1280 × 768 in testing. Results are shown in Tab. 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on MSRA-TD500. All listed results for comparison are quoted from the corresponding original papers.</figDesc><table><row><cell>Method</cell><cell cols="3">Recall Precision F-score</cell></row><row><cell>PixelLink + VGG16 2s PixelLink + VGG16 4s EAST + PVANET2x EAST + VGG16 SegLink + VGG16</cell><cell>73.2 73.0 67.4 61.6 70.0</cell><cell>83.0 81.1 87.3 81.7 86.0</cell><cell>77.8 76.8 76.1 70.2 77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on IC13, in DetEval. 'MS' stands for multiscale testing. 'MS?' means it is not reported whether MS is used. All listed results for comparison are quoted from the corresponding original papers.</figDesc><table><row><cell>Method</cell><cell cols="2">Recall Precision</cell><cell>F</cell></row><row><cell>PixelLink+VGG16 2s PixelLink+VGG16 4s PixelLink+VGG16 2s MS PixelLink+VGG16 4s MS TextBoxes+VGG16 TextBoxes+VGG16 MS EAST+PVANET2x MS? SegLink+VGG16 CTPN + VGG16</cell><cell>83.6 82.3 87.5 86.5 74 83 82.7 83.0 83.0</cell><cell>86.4 84.4 88.6 88.6 88 89 92.6 87.7 93.0</cell><cell>84.5 83.3 88.1 87.5 81 86 87.4 85.3 87.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of training speed and training data for IC15. SynthText<ref type="bibr" target="#b3">(Gupta, Vedaldi, and Zisserman 2016)</ref> is a synthetic dataset containing more than 0.8M images. All listed results for comparison are quoted from the corresponding original papers.</figDesc><table><row><cell>Method</cell><cell cols="2">ImageNet Pretrain Optimizer</cell><cell>Training Data</cell><cell cols="2">Iterations F-score</cell></row><row><cell>PixelLink+VGG16 4s at Iter-25K PixelLink+VGG16 4s at Iter-40K SegLink + VGG16 EAST+VGG16</cell><cell>No No Yes Yes</cell><cell>SGD SGD SGD ADAM</cell><cell>IC15-train IC15-train SynthText,IC15-train IC15-train,IC13-train</cell><cell>25K 40K 100K &gt;55K</cell><cell>79.7 82.3 75.0 76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Investigation of PixelLink with different settings on IC15. 'R': Recall, 'P': Precision; 'F': F-score. # Configurations R P F</cell></row><row><cell>1 2 3 Without Instance Balance 80.2 82.3 81.2 The best 4s model 81.7 82.9 82.3 Without link mechanism 58.0 71.4 64.0 4 Training on 384x384 79.6 81.2 80.4 5 No Post-Filtering 82.3 52.7 64.3 6 Predicting at 2s resolution 82.0 85.5 83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of Instance-Balance on IC13. The only difference on the two settings is the use of Instance-Balance. Testing is done without multi-scale.</figDesc><table><row><cell cols="4">IB Recall Precision F-score</cell></row><row><cell>Yes No</cell><cell>82.3 79.4</cell><cell>84.4 84.2</cell><cell>83.3 81.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This experiment is repeated 3 times using the open source code of SegLink. Its best performance is 63.6, 72.7, 67.8 for Recall, Precision and F-score respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegement</head><p>This work was supported by National Natural Science Foundation of China under Grant 61379071. Special thanks to Dr.</p><p>Yanwu Xu in CVTE Research for all his kindness and great help to us.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient maximally stable extremal region (mser) tracking. In Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="553" to="560" />
			<date type="published" when="2006" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<title level="m">Accurate text localization in natural image with cascaded convolutional text network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The OpenCV Reference Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itseez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>2.4.9.1 edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Document Analysis and Recognition (ICDAR), 2015 13th International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Icdar 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08021</idno>
		<title level="m">Pvanet: Deep but lightweight neural networks for realtime object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07709</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Arbitrary-oriented scene text detection via rotation proposals</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03155</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
