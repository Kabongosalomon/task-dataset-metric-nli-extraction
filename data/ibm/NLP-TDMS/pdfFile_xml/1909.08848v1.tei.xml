<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjith</forename><surname>George</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohreh</forename><surname>Mostaani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Geissenbuhler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olegs</forename><surname>Nikisins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Anjos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Marcel</surname></persName>
						</author>
						<title level="a" type="main">Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TIFS.2019.2916652</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Presentation attack detection</term>
					<term>Convolu- tional neural network</term>
					<term>Biometrics</term>
					<term>Face recognition</term>
					<term>Anti- spoofing</term>
					<term>Multi-channel sensors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition is a mainstream biometric authentication method. However, vulnerability to presentation attacks (a.k.a spoofing) limits its usability in unsupervised applications. Even though there are many methods available for tackling presentation attacks (PA), most of them fail to detect sophisticated attacks such as silicone masks. As the quality of presentation attack instruments improves over time, achieving reliable PA detection with visual spectra alone remains very challenging. We argue that analysis in multiple channels might help to address this issue. In this context, we propose a multi-channel Convolutional Neural Network based approach for presentation attack detection (PAD). We also introduce the new Wide Multi-Channel presentation Attack (WMCA) database for face PAD which contains a wide variety of 2D and 3D presentation attacks for both impersonation and obfuscation attacks. Data from different channels such as color, depth, near-infrared and thermal are available to advance the research in face PAD. The proposed method was compared with feature-based approaches and found to outperform the baselines achieving an ACER of 0.3% on the introduced dataset. The database and the software to reproduce the results are made available publicly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>B IOMETRICS offers a secure and convenient way for access control. Face biometrics is one of the most convenient modalities for biometric authentication due to its non-intrusive nature. Even though face recognition systems are reaching human performance in identifying persons in many challenging datasets <ref type="bibr" target="#b0">[1]</ref>, most face recognition systems are still vulnerable to <ref type="figure">Fig. 1: Figure showing</ref> bonafide, print and replay attacks from different PAD databases, Replay-Attack <ref type="bibr" target="#b4">[5]</ref> (first row), Replay-Mobile <ref type="bibr" target="#b5">[6]</ref> (second row), and MSU-MFSD <ref type="bibr" target="#b6">[7]</ref> (third row). presentation attacks (PA), also known as spoofing 1 attacks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Merely presenting a printed photo to an unprotected face recognition system could be enough to fool it <ref type="bibr" target="#b3">[4]</ref>. Vulnerability to presentation attacks limits the reliable deployment of such systems for applications in unsupervised conditions.</p><p>As per the ISO standard <ref type="bibr" target="#b2">[3]</ref>, presentation attack is defined as "a presentation to the biometric data capture subsystem with the goal of interfering with the operation of the biometric system". Presentation attacks include both 'impersonation' as well as 'obfuscation' of identity. The 'impersonation' refers to attacks in which the attacker wants to be recognized as a different person, whereas in 'obfuscation' attacks, the objective is to hide the identity of the attacker. The biometric characteristic or object used in a presentation attack is known as presentation attack instrument (PAI). Different kinds of PAIs can be used to attack face recognition systems. The presentation of a printed photo, or replaying a video of a subject, are common examples of 2D PAI which have been extensively explored in the available literature. Examples of bonafide and 2D PAs from publicly available databases are shown in <ref type="figure">Fig. 1</ref>. More sophisticated attacks could involve manufacturing custom 3D masks which correspond to a target identity for impersonation or to evade identification. For reliable usage of face recognition technology, it is necessary to develop presentation attack detection (PAD) systems to detect such PAs automatically.</p><p>The majority of available research deals with the detection of print and replay attacks using visible spectral data. Most of the methods relies on the limitations of PAIs and quality degradation of the recaptured sample. Features such as color, texture <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, motion <ref type="bibr" target="#b3">[4]</ref>, and physiological cues <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> are often leveraged for PAD in images from visible spectrum.</p><p>While it is useful to have visible spectrum image based PAD algorithms for legacy face recognition systems, we argue that using only visual spectral information may not be enough for the detection of sophisticated attacks and generalization to new unseen PAIs. The quality of presentation attack instruments (PAI) evolves together with advances in cameras, display devices, and manufacturing methods. Tricking a multi-channel system is harder than a visual spectral one. An attacker would have to mimic real facial features across different representations. The PAD approaches which work in existing PAD databases may not work in real-world conditions when encountered with realistic attacks. Complementary information from multiple channels could improve the accuracy of PAD systems.</p><p>The objective of this work is to develop a PAD framework which can detect a variety of 2D and 3D attacks in obfuscation or impersonation settings. To this end, we propose the new Multi-Channel Convolutional Neural Network (MC-CNN) architecture, efficiently combining multi-channel information for robust detection of presentation attacks. The proposed network uses a pre-trained LightCNN model as the base network, which obviates the requirement to train the framework from scratch. In the proposed MC-CNN only low-level LightCNN features across multiple channels are re-trained, while highlevel layers of pre-trained LightCNN remain unchanged.</p><p>Databases containing a wide variety of challenging PAIs are essential for developing and benchmarking PAD algorithms. In this context, we introduce a Wide Multi-Channel presentation Attack (WMCA) dataset, which contains a broad variety of 2D and 3D attacks. The data split and evaluation protocols are predefined and publicly available. The algorithms, baselines, and the results are reproducible. The software and models to reproduce the results are available publicly <ref type="bibr" target="#b1">2</ref> .</p><p>The main contributions from this paper are listed below.</p><p>• We propose a novel framework for face presentation attack detection based on multi-channel CNN (MC-CNN). MC-CNN uses a face recognition subnetwork, namely LightCNN, making the framework reusable for both PAD and face recognition. The source codes for the network and instructions to train the model are made publicly available allowing to reproduce the findings. We benchmark the proposed method against selected reproducible baseline available in recent publications on the topic <ref type="bibr" target="#b11">[12]</ref>, as well as reimplementations of recent literature in multi-channel PAD <ref type="bibr" target="#b12">[13]</ref>. We demonstrate that the multi-channel approach is beneficial in both proposed and baseline systems. • The new WMCA database is introduced: the subjects in the database are captured using multiple capturing devices/channels, and the MC data is spatially and temporally aligned. The channels present are color, depth, thermal and infrared. The database contains a wide variety of 2D and 3D presentation attacks, specifically, 2D print and replay attacks, mannequins, paper masks, silicone masks, rigid masks, transparent masks, and non-medical eyeglasses. The rest of the paper is organized as follows. Section 2 revisits available literature related to face presentation attack detection. Section 3 presents the proposed approach. The details about the sensors and the dataset are described in section 4. Experimental procedure followed, and the baseline systems are described in Section 5. Extensive testing and evaluations of the proposed approach, along with comparisons with the baselines, discussions, and limitations of the proposed approach are presented in Section 6. Conclusions and future directions are described in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Most of the work related to face presentation attack detection addresses detection of 2D attacks, specifically print and 2D replay attacks. A brief review of recent PAD methods is given in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature based approaches for face PAD</head><p>For PAD using visible spectrum images, several methods such as detecting motion patterns <ref type="bibr" target="#b3">[4]</ref>, color texture, and histogram based methods in different color spaces, and variants of Local Binary Patterns (LBP) in grayscale  <ref type="bibr" target="#b7">[8]</ref> and color images <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, have shown good performance. Image quality based features <ref type="bibr" target="#b20">[21]</ref> is one of the successful feature based methods available in prevailing literature. Methods identifying moiré patterns <ref type="bibr" target="#b21">[22]</ref>, and image distortion analysis <ref type="bibr" target="#b6">[7]</ref>, use the alteration of the images due to the replay artifacts. Most of these methods treat PAD as a binary classification problem which may not generalize well for unseen attacks <ref type="bibr" target="#b11">[12]</ref>. Chingovska et al. <ref type="bibr" target="#b22">[23]</ref> studied the amount of clientspecific information present in features used for PAD. They used this information to build client-specific PAD methods. Their method showed a 50% relative improvement and better performance in unseen attack scenarios.</p><p>Arashloo et al. <ref type="bibr" target="#b23">[24]</ref> proposed a new evaluation scheme for unseen attacks. Authors have tested several combinations of binary classifiers and one class classifiers. The performance of one class classifiers was better than binary classifiers in the unseen attack scenario. BSIF-TOP was found successful in both one class and two class scenarios. However, in cross-dataset evaluations, image quality features were more useful. Nikisins et al. <ref type="bibr" target="#b11">[12]</ref> proposed a similar one class classification framework using one class Gaussian Mixture Models (GMM). In the feature extraction stage, they used a combination of Image Quality Measures (IQM). The experimental part involved an aggregated database consisting of replay attack <ref type="bibr" target="#b4">[5]</ref>, replay mobile <ref type="bibr" target="#b5">[6]</ref>, and MSU-MFSD <ref type="bibr" target="#b6">[7]</ref> datasets.</p><p>Heusch and Marcel <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b10">[11]</ref> recently proposed a method for using features derived from remote photoplethysmography (rPPG). They used the long term spectral statistics (LTSS) of pulse signals obtained from available methods for rPPG extraction. The LTSS features were combined with SVM for PA detection. Their approach obtained better performance than state of the art methods using rPPG in four publicly available databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN based approaches for face PAD</head><p>Recently, several authors have reported good performance in PAD using convolutional neural networks (CNN). Gan et al. <ref type="bibr" target="#b25">[26]</ref> proposed a 3D CNN based approach, which utilized the spatial and temporal features of the video. The proposed approach achieved good results in the case of 2D attacks, prints, and videos. Yang et al. <ref type="bibr" target="#b26">[27]</ref> proposed a deep CNN architecture for PAD. A preprocessing stage including face detection and face landmark detection is used before feeding the images to the CNN. Once the CNN is trained, the feature representation obtained from CNN is used to train an SVM classifier and used for final PAD task. Boulkenafet et al. <ref type="bibr" target="#b27">[28]</ref> summarized the performance of the competition on mobile face PAD. The objective was to evaluate the performance of the algorithms under realworld conditions such as unseen sensors, different illumination, and presentation attack instruments. In most of the cases, texture features extracted from color channels performed the best. Li et al. <ref type="bibr" target="#b29">[29]</ref> proposed a 3D CNN architecture, which utilizes both spatial and temporal nature of videos. The network was first trained after data augmentation with a cross-entropy loss, and then with a specially designed generalization loss, which acts as a regularization factor. The Maximum Mean Discrepancy (MMD) distance among different domains is minimized to improve the generalization property.</p><p>There are several works involving various auxiliary information in the CNN training process, mostly focusing on the detection of 2D attacks. Authors use either 2D or 3D CNN. The main problem of CNN based approaches mentioned above is the lack of training data, which is usually required to train a network from scratch. One commonly used solution is fine-tuning, rather than a complete training, of the networks trained for facerecognition, or image classification tasks. Another issue is the poor generalization in cross-database and unseen attacks tests. To circumvent these issues, some researchers have proposed methods to train CNN using auxiliary tasks, which is shown to improve generalization properties. These approaches are discussed below.</p><p>Liu et al. <ref type="bibr" target="#b30">[30]</ref> presented a novel method for PAD with auxiliary supervision. Instead of training a network end-to-end directly for PAD task, they used CNN-RNN model to estimate the depth with pixel-wise supervision and estimate remote photoplethysmography (rPPG) with sequence-wise supervision. The estimated rPPG and depth were used for PAD task. The addition of the auxiliary task improved the generalization capability.</p><p>Atoum et al. <ref type="bibr" target="#b31">[31]</ref> proposed a two-stream CNN for 2D presentation attack detection by combining a patch-based model and holistic depth maps. For the patch-based model, an end-to-end CNN was trained. In the depth estimation, a fully convolutional network was trained using the entire face image. The generated depth map was converted to a feature vector by finding the mean values in the N × N grid. The final PAD score was obtained by fusing the scores from the patch and depth CNNs.</p><p>Shao et al. <ref type="bibr" target="#b33">[32]</ref> proposed a deep convolutional network-based architecture for 3D mask PAD. They tried to capture the subtle differences in facial dynamics using the CNN. Feature maps obtained from the convolutional layer of a pre-trained VGG <ref type="bibr" target="#b34">[33]</ref> network was used to extract features in each channel. Optical flow was estimated using the motion constraint equation in each channel. Further, the dynamic texture was learned using the data from different channels. The proposed approach achieved an AUC (Area Under Curve) score of 99.99% in the 3DMAD dataset.</p><p>Lucena et al. <ref type="bibr" target="#b12">[13]</ref> presented an approach for face PAD using transfer learning from pre-trained models (FASNet). The VGG16 <ref type="bibr" target="#b34">[33]</ref> architecture which was pretrained on ImageNet <ref type="bibr" target="#b35">[34]</ref> dataset was used as the base network as an extractor, and they modified the final fully connected layers. The newly added fully connected layers in the network were fine-tuned for PAD task. They obtained HTERs of 0% and 1.20% in 3DMAD and Replay-Attack dataset respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-channel based approaches and datasets for face PAD</head><p>In general, most of the visible spectrum based PAD methods try to detect the subtle differences in image quality when it is recaptured. However, this method could fail as the quality of capturing devices and printers improves. For 3D attacks, the problem is even more severe. As the technology to make detailed masks is available, it becomes very hard to distinguish between bonafide and presentation attacks by just using visible spectrum imaging. Several researchers have suggested using multi-spectral and extended range imaging to solve this issue <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref>.</p><p>Akhtar et al. <ref type="bibr" target="#b38">[37]</ref> outlines the major challenges and open issues in biometrics concerning presentation attacks. Specifically, in case of face PAD, they discuss a wide variety of possible attacks and possible solutions. They pointed out that sensor-based solutions which are robust against spoofing attempts and which works even in 'in the wild' conditions require specific attention.</p><p>Hadid et al. <ref type="bibr" target="#b39">[38]</ref> presented the results from a large scale study on the effect of spoofing on different biomet-rics traits. They have shown that most of the biometrics systems are vulnerable to spoofing. One class classifiers were suggested as a possible way to deal with unseen attacks. Interestingly, countermeasures combining both hardware (new sensors) and software were recommended as a robust PAD method which could work against a wide variety of attacks.</p><p>Raghavendra et al. <ref type="bibr" target="#b36">[35]</ref> presented an approach using multiple spectral bands for face PAD. The main idea is to use complementary information from different bands. To combine multiple bands, they observed a wavelet-based feature level fusion and a score fusion methodology. They experimented with detecting print attacks prepared using different kinds of printers. They obtained better performance with score level fusion as compared to the feature fusion strategy.</p><p>Erdogmus and Marcel <ref type="bibr" target="#b40">[39]</ref> evaluated the performance of a number of face PAD approaches against 3D masks using 3DMAD dataset. This work demonstrated that 3D masks could fool PAD systems easily. They achieved HTER of 0.95% and 1.27% using simple LBP features extracted from color and depth images captured with Kinect.</p><p>Steiner et al. <ref type="bibr" target="#b37">[36]</ref> presented an approach using multispectral SWIR imaging for face PAD. They considered four wavelengths -935nm, 1060nm, 1300nm and 1550nm. In their approach, they trained an SVM for classifying each pixel as a skin pixel or not. They defined a Region Of Interest (ROI) where the skin is likely to be present, and skin classification results in the ROI is used for classifying PAs. The approach obtained 99.28 % accuracy in per-pixel skin classification.</p><p>Dhamecha et al. <ref type="bibr" target="#b41">[40]</ref> proposed an approach for PAD by combining the visible and thermal image patches for spoofing detection. They classified each patch as either bonafide or attack and used the bonafide patches for subsequent face recognition pipeline.</p><p>Agarwal et al. <ref type="bibr" target="#b19">[20]</ref> proposed a framework for the detection of latex mask attacks from multi-channel data, which comprised of visible, thermal and infrared data. The dataset was collected independently for different channels and hence lacks temporal and spatial alignment between the channels. They have performed experiments using handcrafted features independently in the multiple channels. For PAD, the best performing system was based on redundant discrete wavelet transform (RDWT) and Haralick <ref type="bibr" target="#b42">[41]</ref> features. They computed the features from RDWT decompositions of each patch in a 4 × 4 grid and concatenated them to obtain the final feature vector. The computed feature vectors were used with SVM for the PAD task. From the experiments, it was shown that the thermal channel was more informative as compared to other channels obtaining 15.4% EER in the frame-based evaluation. However, experiments with fusion could not be performed since the channels were recorded independently.</p><p>In <ref type="bibr" target="#b43">[42]</ref> Bhattacharjee et al. showed that it is possible to spoof commercial face recognition systems with custom silicone masks. They also proposed to use the mean temperature of the face region for PAD.</p><p>Bhattacharjee et al. <ref type="bibr" target="#b44">[43]</ref> presented a preliminary study of using multi-channel information for PAD. In addition to visible spectrum images, they considered thermal, near infrared, and depth channels. They showed that detecting 3D masks and 2D attacks are simple in thermal and depth channels respectively. Most of the attacks can be detected with a similar approach with combinations of different channels, where the features and combinations of channels to use are found using a learning-based approach.</p><p>Several multi-channel datasets have been introduced in the past few years for face PAD. Some of the recent ones are shown in <ref type="table" target="#tab_0">Table I</ref>. From the table it can be seen that the variety of PAIs is limited in most of the available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>In general, presentation attack detection in a realworld scenario is challenging. Most of the PAD methods available in prevailing literature try to solve the problem for a limited number of presentation attack instruments. Though some success has been achieved in addressing 2D presentation attacks, the performance of the algorithms in realistic 3D masks and other kinds of attacks is poor.</p><p>As the quality of attack instruments evolves, it becomes increasingly difficult to discriminate between bonafide and PAs in the visible spectrum alone. In addition, more sophisticated attacks, like 3D silicone masks, make PAD in visual spectra challenging. These issues motivate the use of multiple channels, making PAD systems harder to by-pass.</p><p>We argue that the accuracy of the PAD methods can get better with a multi-channel acquisition system. Multi-channel acquisition from consumer-grade devices can improve the performance significantly. Hybrid methods, combining both extended hardware and software could help in achieving good PAD performance in realworld scenarios. We extend the idea of a hybrid PAD framework and develop a multi-channel framework for presentation attack detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>A Multi-Channel Convolutional Neural Network (MC-CNN) based approach is proposed for PAD. The main idea is to use the joint representation from multiple channels for PAD, using transfer learning from a pretrained face recognition network. Different stages of the framework are described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>Face detection is performed in the color channel using the MTCNN algorithm <ref type="bibr" target="#b45">[44]</ref>. Once the face bounding box is obtained, face landmark detection is performed in the detected face bounding box using Supervised Descent Method (SDM) <ref type="bibr" target="#b46">[45]</ref>. Alignment is accomplished by transforming image, such that the eye centers and mouth center are aligned to predefined coordinates. The aligned face images are converted to grayscale, and resized, to the resolution of 128 × 128 pixels.</p><p>The preprocessing stage for non-RGB channels requires the images from different channels to be aligned both spatially and temporally with the color channel. For these channels, the facial landmarks detected in the color channel are reused, and a similar alignment procedure is performed. A normalization using Mean Absolute Deviation (MAD) <ref type="bibr" target="#b47">[46]</ref> is performed to convert the range of non-RGB facial images to 8-bit format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network architecture</head><p>Many of previous work in face presentation attack detection utilize transfer learning from pre-trained networks. This is required since the data available for PAD task is often of a very limited size, being insufficient to train a deep architecture from scratch. This problem becomes more aggravated when multiple channels of data are involved. We propose a simpler way to leverage a pretrained face recognition model for multi-channel PAD task, adapting a minimal number of parameters.</p><p>The features learned in the low level of CNN networks are usually similar to Gabor filter masks, edges and blobs <ref type="bibr" target="#b48">[47]</ref>. Deep CNNs compute more discriminant features as the depth increases <ref type="bibr" target="#b49">[48]</ref>. It has been observed in different studies <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b50">[49]</ref>, that features, which are closer to the input are more general, while features in the higher levels contain task specific information. Hence, most of the literature in the transfer learning attempts to adapt the higher level features for the new tasks.</p><p>Recently, Freitas Pereira et al. <ref type="bibr" target="#b51">[50]</ref> showed that the high-level features in deep convolutional neural networks, trained in visual spectra, are domain independent, and they can be used to encode face images collected from different image sensing domains. Their idea was to use the shared high-level features for heterogeneous face recognition task, retraining only the lower layers. In their method they split the parameters of the CNN architecture into two, the higher level features are shared among the different channels, and the lower level features (known as Domain Specific Units (DSU)) are adapted separately for different modalities. The objective was to learn the same face encoding for different channels, by adapting the DSUs only. The network was trained using contrastive loss (with Siamese architecture) or triplet loss. Retraining of only low-level features has the advantage of modifying a minimal set of parameters.</p><p>We extend the idea of domain-specific units (DSU) for multi-channel PAD task. Instead of forcing the representation from different channels to be the same, we leverage the complementary information from a joint representation obtained from multiple channels. We hypothesize that the joint representation contains discriminatory information for PAD task. By concatenating the representation from different channels, and using fully connected layers, a decision boundary for the appearance of bonafide and attack presentations can be learned via back-propagation. The lower layer features, as well as the higher level fully connected layers, are adapted in the training phase.</p><p>The main idea used from <ref type="bibr" target="#b51">[50]</ref> is the adaptation of lower layers of CNN, instead of adapting the whole network when limited amount of target data is available. The network in <ref type="bibr" target="#b51">[50]</ref> only has one forward path, whereas in MC-CNN the network architecture itself is extended to accommodate multi-channel data. The main advantage of the proposed framework is the adaptation of a minimal amount of network weights when the training data is limited, which is usually the case with available PAD datasets. The proposed framework introduces a new way to deal with multi-channel PAD problem, reusing a large amount of face recognition data available when a limited amount of data is available for training PAD systems.</p><p>In this work, we utilize a LightCNN model <ref type="bibr" target="#b52">[51]</ref>, which was pre-trained on a large number of face images for face recognition. The LightCNN network is especially interesting as the number of parameters is much smaller than in other networks used for face recognition.</p><p>LightCNN achieves a reduced set of parameters using a Max-Feature Map (MFM) operation as an alternative to Rectified Linear Units (ReLU), which suppresses low activation neurons in each layer.</p><p>The block diagram of the proposed framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The pre-trained LightCNN model produces a 256-dimensional embedding, which can be used as face representation. The LightCNN model is extended to accept four channels. The 256-dimensional representation from all channels are concatenated, and two fully connected layers are added at the end for PAD task. The first fully connected layer has ten nodes, and the second one has one node. Sigmoidal activation functions are used in each fully connected layer. The higher level features are more related to the task to be solved. Hence, the fully connected layers added on top of the concatenated representations are tuned exclusively for PAD task. Reusing the weights from a network pretrained for face recognition on a large set of data, we avoid plausible over-fitting, which can occur due to a limited amount of training data.</p><p>Binary Cross Entropy (BCE) is used as the loss function to train the model using the ground truth information for PAD task.</p><p>The equation for BCE is shown below.</p><formula xml:id="formula_0">L = −(y log(p) + (1 − y) log(1 − p))<label>(1)</label></formula><p>where y is the ground truth, (y = 0 for attack and y = 1 for bonafide) and p is predicted probability. Several experiments were carried out by adapting the different groups of layers, starting from the low-level features. The final fully connected layers are adapted for PAD task in all the experiments.</p><p>While doing the adaptation, the weights are always initialized from the weights of the pre-trained layers. Apart from the layers adapted, the parameters for the rest of the network remain shared.</p><p>The layers corresponding to the color channel are not adapted since the representation from the color channel can be reused for face recognition, hence making the framework suitable for simultaneous face recognition and presentation attack detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE WIDE MULTI-CHANNEL PRESENTATION ATTACK DATABASE</head><p>The Wide Multi-Channel presentation Attack (WMCA) database consists of short video recordings for both bonafide and presentation attacks from 72 different identities. In this section, we provide the details on the data collection process, and statistics of the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Camera set up for data collection</head><p>For acquisition of face data, different sensors were selected to provide a sufficient range of high-quality information in both visual and infrared spectra. In addition, 3D information was provided by one sensor adding a depth map channel to the video stream. Overall, the data stream is composed of a standard RGB video stream, a depth stream (called RGB-D when considered together with the color video), a Near-Infrared (NIR) stream, and a thermal stream. While the RGB-D and NIR data are provided by an Intel RealSense SR300 sensor, the thermal data is provided by a Seek Thermal Compact PRO camera, both being relatively cheap devices aimed at the consumer market. The hardware specifications of these devices are described below.</p><p>1) Intel RealSense SR300 sensor: The Intel Re-alSense SR300 camera is a consumer grade RGB-D sensor aimed at gesture recognition and 3D scanning ( <ref type="figure" target="#fig_1">Fig. 3(c)</ref>). It features a full-HD RGB camera, capable of capturing resolution of 1920 × 1080 pixels in full-HD mode at 30 frames-per-second (fps) or 1260×720 pixels in HD mode at 60 fps.</p><p>2) Seek Thermal Compact PRO sensor: The Seek Thermal Compact PRO sensor is a commercial thermal camera aimed at the consumer market ( <ref type="figure" target="#fig_1">Fig. 3(b)</ref>). It provides a QVGA resolution of 320 × 240. This camera range is primarily intended to be mounted on smartphone devices. The sensor is capable of capturing at approximately 15 fps, with a non-continuous operation due to an electro-mechanical shutter used to calibrate the sensor at regular intervals (approx. 2s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Camera integration and calibration</head><p>For multi-sensor capture, it is essential that all sensors are firmly attached to a single mounting frame to maintain alignment and minimize vibrations. The setup was built using standard optical mounting posts, giving an excellent strong and modular mounting frame with the ability to precisely control the orientation of the devices, <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>To calibrate the cameras and to provide relative alignment of the sensors to the software architecture, we used a checkerboard pattern made from materials with different thermal characteristics. The data from this checkerboard was captured simultaneously from all the channels. For the pattern to be visible on the thermal channel, the target was illuminated by high power halogen lamps. Custom software was then implemented to automatically extract marker points allowing precise alignment of the different video streams. Sample images from all the four channels after alignment is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data collection procedure</head><p>The data was acquired during seven sessions over an interval of five months. The sessions were different ( <ref type="figure">Fig.  5</ref>) in their environmental conditions such as background (uniform and complex) and illumination (ceiling office light, side LED lamps, and day-light illumination) (Table II). At each session, 10 seconds of data both for bonafide and at least two presentation attacks performed by the study participant was captured. Session four was dedicated to presentation attacks only. Participants were asked to sit in front of the custom acquisition system and look towards the sensors with a neutral facial expression. If the subjects wore prescription glasses, their bonafide data was captured twice, with and without the medical glasses. The masks and mannequins were heated using a blower prior to capture to make the attack more challenging. The distance between the subject and the cameras was approximately 40cm for both bonafide and presentation attacks. The acquisition operator adjusted the cameras so that the subject's face was frontal and located within the field of view of all the sensors at the desired distance. Then they launched the capturing program which recorded data from the sensors for 10 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Presentation attacks</head><p>The presentation attacks were captured under the same conditions as bonafide. More than eighty different presentation attack instruments (PAIs) were used for the attacks most of which were presented both by a study participant and on a fixed support.</p><p>The PAIs presented in this database can be grouped into seven categories. Some examples can be seen in <ref type="figure" target="#fig_3">Fig.  6</ref>.</p><p>• glasses: Different models of disguise glasses with fake eyes (funny eyes glasses) and paper glasses. These attacks constitute partial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5: Examples of bonafide data in 6 different sessions. Top left is session one and bottom right is session seven.</head><p>There is no bonafide data for session four.</p><p>• fake head: Several models of mannequin heads were used, some of the mannequins were heated with a blower prior to capture. • print: Printed face images on A4 matte and glossy papers using professional quality Ink-Jet printer (Epson XP-860) and typical office laser printer (CX c224e). The images were captured by the rear camera of an "iPhone S6" and re-sized so that the size of the printed face is human like. • replay: Electronic photos and videos. An "iPad pro 12.9in" was used for the presentations. The videos were captured in HD at 30 fps by the front camera of an "iPhone S6" and in full-HD at 30 fps by the rear camera of the "iPad pro 12.9in". Some of the videos were re-sized so that the size of the face presented on the display is human like. • rigid mask: Custom made realistic rigid masks and several designs of decorative plastic masks. • flexible mask: Custom made realistic soft silicone masks. • paper mask: Custom made paper masks based on real identities. The masks were printed on the matte paper using both printers mentioned in the print category.</p><p>The total number of presentations in the database is 1679, which contains 347 bonafide and 1332 attacks. More detailed information can be found in <ref type="table" target="#tab_0">Table III 3</ref> .</p><p>Each file in the dataset contains data recorded at 30 fps for 10 seconds amounting to 300 frames per channel, except for thermal channel, which contains approximately 150 frames captured at 15 fps. All the  V. EXPERIMENTS This section describes the experiments performed on the WMCA dataset. All four channels of data obtained from Intel RealSense SR300 and Seek Thermal Compact PRO are used in the experiments. Various experiments were done to evaluate the performance of the system in "seen" and "unseen" attack scenarios. In the "seen" attack protocol, all types of PAIs are present in the train, development and testing subsets (with disjoint client ids in each fold). This protocol is intended to test the performance of the algorithm in the cases where the attack categories are known a priori. The "unseen" attack protocols try to evaluate the performance of the system on PAIs which were not present in the training and development subsets. The "unseen" attack protocols thus emulate the realistic scenario of encountering an attack which was not present in the training set. The evaluation protocols are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Protocols</head><p>As the consecutive frames are correlated, we select only 50 frames from each video which are uniformly sampled in the temporal domain. Individual frames from a video are considered as independent samples producing one score per frame. A biometric sample consists of frames from all four channels which are spatially and temporally aligned.</p><p>Two different sets of protocols were created for the WMCA dataset.</p><p>• grandtest protocol : The WMCA dataset is divided into three partitions: train, dev, and eval. The data split is done ensuring almost equal distribution of PA categories and disjoint set of client identifiers in each set. Each of the PAIs had different client id. The split is done in such a way that a specific PA instrument will appear in only one set. The "grandtest" protocol emulates the "seen" attack scenario as the PA categories are distributed uniformly in the three splits. • unseen attack protocols: The unseen attack protocols defined in the WMCA dataset contains three splits similar to the grandest protocol. Seven unseen attack sub-protocols were created for conducting unseen attack experiments using leave one out (LOO) technique. In each of the unseen attack protocols, one attack is left out in the train and dev sets. The eval set contains the bonafide and the samples from the attack which was left out in training. For example, in "LOO fakehead" protocol, the fake head attacks are not present in both train and dev sets. In the test set, only bonafide and fake head attacks were present. The training and tuning are done on data which doesn't contain the attack of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We train the framework using the data in the train set, and the decision threshold is found from the dev set by minimizing a given criteria (here, we used a BPCER = 1% for obtaining the thresholds). We also report the standardized ISO/IEC 30107-3 metrics <ref type="bibr" target="#b53">[52]</ref>, Attack Presentation Classification Error Rate (APCER), and Bonafide Presentation Classification Error Rate (BPCER) in the test set at the previously optimized threshold.</p><p>To summarize the performance in a single number, the Average Classification Error Rate (ACER) is used, which is an average of APCER and BPCER. The ACER is reported for both dev and test sets.</p><p>Apart from the error metrics, ROC curves are also shown for the baseline and MCCNN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline experiment setup</head><p>Since a new database is proposed in this paper, baseline experiments are performed for each channel first. The selected baselines are reproducible and have either open-source implementation <ref type="bibr" target="#b11">[12]</ref> or re-implementation. Three sets of baselines are described in this section.</p><p>1) IQM-LBP-LR baseline: This baseline consists of Image Quality Measures (IQM) <ref type="bibr" target="#b20">[21]</ref> for the RGB channel, and different variants of Local Binary Patterns (LBP) for non-RGB channels. The detailed description of the baseline systems is given below.</p><p>An individual PAD algorithm is implemented for every data stream from the camera. The structure of all PAD algorithms can be split into three blocks: preprocessor, a feature extractor, and classifier. The final unit of the PAD system is a fusion block, combining the outputs of channel-specific PAD algorithms, and producing a final decision.</p><p>The preprocessing part is exactly similar to the description in Section III-A, except for color channel. For the color channel, all three RGB channels are retained in the baseline experiments.</p><p>The feature extraction step aims to build a discriminative feature representation. For the color channel, the feature vector is composed of 139 IQMs <ref type="bibr" target="#b11">[12]</ref>. Spatially enhanced histograms of LBPs are selected as features for infrared, depth, and thermal channels <ref type="bibr" target="#b11">[12]</ref>. Optimal LBP parameters have been selected experimentally using grid search for each channel independently.</p><p>For classification, Logistic Regression (LR) is used as a classifier for color, infrared, depth, and thermal channels <ref type="bibr" target="#b3">4</ref> . The features are normalized to zero mean and unity standard deviation before the training. The normalization parameters are computed using samples of bonafide class only. In the prediction stage, a probability of a sample being a bonafide class is computed given trained LR model.</p><p>Scores from all PAD algorithms are normalized to [0, 1] range, and a mean fusion is performed to obtain the final PA score.</p><p>2) RDWT-Haralick-SVM baseline: In this baseline we used the re-implementation of the algorithm in <ref type="bibr" target="#b19">[20]</ref> for individual channels. We applied similar preprocessing strategy as discussed in the previous section in all channels. Haralick <ref type="bibr" target="#b42">[41]</ref> features computed from the RDWT decompositions in a 4 × 4 grid are concatenated and fed to a Linear SVM for obtaining the final scores. Apart from implementing the pipeline independently for each channel, we additionally performed a mean fusion of all channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) FASNet baseline:</head><p>We also compare our system to a deep learning based FASNet <ref type="bibr" target="#b12">[13]</ref> baseline. The FASNet uses the aligned color images as input for PAD task. We reimplemented the approach in PyTorch <ref type="bibr" target="#b54">[53]</ref> which is made available publicly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment setup with the proposed MC-CNN approach</head><p>The architecture shown in <ref type="figure" target="#fig_0">Fig. 2 was used</ref> for the experiments. The base network used is LightCNN with 29 layers. Further, to accommodate four channels of information, the same network is extended. The embedding layers from different channels are concatenated, and two fully connected layers are added at the end. We performed different experiments by re-training different sets of low-level layers. The layers which are not retrained are shared across the channels.</p><p>The dataset contained an unequal number of samples for bonafide and attacks in the training set. The effect of this class imbalance is handled by using a weighted BCE loss function. The weights for the loss function is computed in every mini-batch dynamically based on the number of occurrences of classes in the mini-batch. To compensate for the small amount of training data, we used data augmentation by randomly flipping the images horizontally. All channels are flipped simultaneously to preserve any cross-channel dependencies. A probability of 0.5 was used in this data augmentation. The network was trained using Binary Cross Entropy (BCE) loss using Adam Optimizer <ref type="bibr" target="#b55">[54]</ref> with a learning rate of 1 × 10 −4 . The network was trained for 25 epochs on a GPU grid with a mini-batch size of 32. Implementation was done in PyTorch <ref type="bibr" target="#b54">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>A. Experiments with grandtest protocol 1) Baseline results: The performance of the baselines in different individual channels and results from fusion are shown in <ref type="table" target="#tab_0">Table IV. From Table IV</ref>, it can be seen that for individual channels, thermal and infrared provides more discriminative information. RDWT-Haralick-SVM in infrared channel provides the best accuracy among individual channels. It is observed that the fusion of multiple channels improves accuracy. Score fusion improves the accuracy of both feature-based baselines. Deep learning-based baseline FASNet achieves better accuracy as compared to IQM and RDWT-Haralick features in color channel. The FASNet architecture was designed exclusively for three channel color images since it uses normalization parameters and weights from a pretrained model trained on ImageNet <ref type="bibr" target="#b35">[34]</ref> dataset. The training stage in FASNet is performed by fine-tuning the last fully connected layers. The usage of three channel images and finetuning of only last fully connected layers limits the straight forward extension of this architecture to other channels. In the baseline experiments, score fusion of individual channels achieved the best performance and is used as the baselines in the subsequent experiments. From this set of experiments, it is clear that the addition of multiple channels helps in boosting the performance of PAD systems. However, the performance achieved with the best baseline systems is not adequate for deployment in critical scenarios. The lower accuracy in the fusion baselines points to the necessity to have methods which utilize multi-channel information more efficiently.</p><p>2) Results with MC-CNN: The results with the proposed approach in the grandest protocol are shown in <ref type="table" target="#tab_0">Table VI</ref>. The corresponding ROCs are shown in <ref type="figure" target="#fig_5">Fig.  7</ref>  <ref type="bibr" target="#b4">5</ref> . <ref type="table" target="#tab_0">From Table VI</ref>, it can be seen that the proposed MC-CNN approach outperforms the selected baselines by a big margin. From the baseline results, it can be seen that having multiple channels alone doesn't solve the PAD problem. Efficient utilization of information from the various channels is required for achieving good PAD performance. The proposed framework utilizes complementary information from multiple channels with the joint representation. Transfer learning from the pretrained face recognition model proves to be effective for learning deep models for multi-channel PAD task while avoiding overfitting by adapting only a minimal set of DSU's. Overall, the proposed MC-CNN framework uses the information from multiple channels effectively boosting the performance of the system.</p><p>The performance breakdown per PAI for BPCER threshold of 1% is shown in <ref type="table" target="#tab_4">Table V</ref>. <ref type="table" target="#tab_4">From Table V</ref> it can be seen that the system achieves perfect accuracy in classifying attacks except for "glasses". A discussion about the performance degradation in the "glasses" attack is presented in Subsection VI-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization to unseen attacks</head><p>In this section, we evaluate the performance of the system under unseen attacks. Experiments are done with the different sub-protocols, which exclude one attack systematically in training. The algorithms are trained with samples from each protocol and evaluated on the test set which contains only the bonafide and the attack which was left out in training. The performance of MC-CNN, as well as the baseline system, are tabulated in <ref type="table" target="#tab_0">Table VII</ref>.</p><p>From this table, it can be seen that the MC-CNN algorithm performs well in most of the unseen attacks.   The baseline methods also achieve reasonable performance in this protocol. However, MC-CNN achieves much better performance as compared to the fusion baselines, indicating the effectiveness of the approach. The performance in the case of "glasses" is very poor for both the baseline and the MC-CNN approach. From <ref type="figure" target="#fig_6">Figure 8</ref>, it can be seen that the appearance of the glass attacks are very similar to bonafide wearing medical glasses in most of the channels. Since the "glasses" attacks were not present in the training set, they get classified as bonafide and reduce the performance of the system. The issue mentioned above is especially crucial for partial attacks in face regions with more variability. For example, partial attacks in eye regions would be harder to detect as there is a lot of variabilities introduced by bonafide samples wearing prescription glasses. Similarly, attacks in lower chin could be harder to detect due to variability introduced by bonafide samples with facial hair and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of MC-CNN framework</head><p>Here we study the performance of the proposed MC-CNN framework with two different sets of  experiments-one with re-training different sets of layers and another with different combinations of channels. 1) Experiments with adapting different layers: Here we try to estimate how adapting a different number of low-level layers affect the performance. The features for the grayscale channel were frozen, and different experiments were done by adapting different groups of lowlevel layers of the MC-CNN from first layer onwards. In all the experiments, the weights were initialized from the ones trained in the grayscale channel.</p><p>Different groups of parameters were re-trained and are notated as follows. The grouping follows the implementation of LightCNN from the authors in the opensource implementation <ref type="bibr" target="#b56">[55]</ref>. The name of the layers is the same as given in 29 layer network described in <ref type="bibr" target="#b52">[51]</ref>. The notations used for the combination of layers are listed below.</p><p>• FFC : Only two final fully connected (FFC) layers are adapted. Here F F C denotes the two final fully connected layers, and the rest of the names are for different blocks corresponding to the opensource implementation of LightCNN from <ref type="bibr" target="#b52">[51]</ref>.</p><p>The results obtained with re-training different layers are shown in <ref type="table" target="#tab_0">Table VIII</ref>. It can be seen that the performance improves greatly when we adapt the lower layers; however, as we adapt more layers, the performance starts to degrade. The performance becomes worse when all layers are adapted. This can be attributed to over-fitting as the number of parameters to learn is very large. The number of layers to be adapted is selected empirically. The criteria used in this selection is good performance while adapting a minimal number of parameters. For instance, the ACER obtained is 0.3% for 1-2+FFC and 1-4+FFC, the optimal number of layers to be adapted is selected as "2" (C1-B1-FFC) since it achieved the best performance adapting a minimal set of parameters. This combination selected as the best system and is used in all the other experiments.</p><p>2) Experiments with different combinations of channels: Here the objective is to evaluate the performance of the algorithm with different combinations of channels. This analysis could be useful in selecting promising channels which are useful for the PAD task. Additionally, the performance of individual channels is also tested in this set of experiments to identify the contribution from individual channels. It is to be noted that color, depth, and infrared channels are available from the Intel RealSense SR300 device, and the thermal channel is obtained from the Seek Thermal Compact PRO camera. It could be useful to find performance when data from only one sensor is available. We have done experiments with six different combinations for this task. The combinations used are listed below. 1) G+D+I+T : All channels, i.e., Grayscale, Depth, Infrared, and Thermal are used. 2) G+D+I : Grayscale, Depth and Infrared channels are used (All channels from Intel RealSense).   3) G : Only Grayscale channel is used. 4) D : Only Depth channel is used. 5) I : Only Infrared channel is used. 6) T : Only Thermal channel is used.</p><p>The architecture of the network remains similar to the one shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, where only the layers corresponding to the selected channels are present. Experiments were done using the different combinations of channels with the proposed framework. While training the model, the embeddings from the channels used in a specific experiment are used in the final fully connected layers. The training and testing are performed similarly as compared to experiments conducted in the grandtest protocol.</p><p>The results with different combinations of channels are compiled in <ref type="table" target="#tab_0">Table IX</ref>. It can be seen that the system with all four channels performs the best with respect to ACER (0.3%). The combination "CDI" achieves an ACER of 1.04% which is also interesting as all the three channels used is coming from the same device (Intel Re-alSense). This analysis can be helpful in cases where all the channels are not available for deployment. The performance of the system with missing or with a subset of channels can be computed apriori, and models trained on the available channels can be deployed quickly. Among the individual channels, thermal channel achieves the best performance with an ACER of 1.85%. However, it is to be noted that the analysis with individual channels is an ablation study of the framework, and the network is not optimized for individual channels. While doing the experiments with individual channels, the architecture is not MC-CNN anymore. The performance boost in the proposed framework is achieved with the use of multiple channels. From the experiments with different channels, it can be seen that the performance of the system with all four channels was the best. We have also tested the same system in a cross-database setting. The data used in this testing was part of the Government Controlled Test (GCT) in the IARPA ODIN [56] project. In the GCT data, it was observed that the system which uses all four channels was performing the best. The addition of complementary information makes the classifier more accurate. The combination of channels makes the framework more robust in general.</p><p>The experiments in the unseen attack scenario show some interesting results. Even though the framework is trained as a binary classifier, it is able to generalize well for most of the attacks when the properties of the unseen attacks can be learned from other types of presentations. This can be explained as follows, the 2D PAIs prints and replays can be characterized from depth channel alone. Having one of them in the training set is enough for the correct classification of the other class. The same idea can be extended to other attacks which need information from multiple channels for PAD. For example, if we have silicone masks in the training set; then classifying mannequins as an attack is rather easy. A PAI is relatively easy to detect when it is distinctive from bonafide in at least one of the channels. PAD becomes harder as the representations across channels become similar to that of bonafide. This makes the detection of partial attacks such as glasses which occlude a small portion of the face more complex. From the above discussion, it can be seen that, if we have a wide variety of sophisticated attacks in the training set, then the accuracy in detecting simpler unseen attacks seems to be better. This observation is interesting as this could help to tackle the unseen attack scenario, i.e., if we train the system using sufficient varieties of complex PAIs, then the resulting model can perform reasonably well on simpler "unseen" attacks. Further, the representation obtained from the penultimate layer of MC-CNN can be used to train one class classifiers/anomaly detectors which could be used to detect unseen attacks.</p><p>2) Limitations: One of the main limitations of the proposed framework is the requirement of spatially and temporally aligned channels. Spatial alignment can be achieved by careful calibration of the cameras. Achieving temporal alignment requires the sensors to be triggered in a supervised fashion. However, the proposed framework can handle small temporal misalignments and does not have very stringent requirement on absolute synchronization between channels, as long as there is no significant movement between the frames from different channels. Data from different channels recorded in multiple sessions, as in <ref type="bibr" target="#b19">[20]</ref> cannot be used in the proposed framework. In deployment scenarios, the time spent for data capture should be small from the usability point of view; synchronized capture between multiple sensors is suitable for this scenario since it reduces the overall time for data capture. Further, if the multiple channels are not synchronized, face detection in the additional channels is not trivial. Having spatial and temporal alignment obviates the requirement of face detection for all channels since the face location can be shared among different channels. Data capture can be done synchronously as long as the illumination requirements for one sensor is not interfering another sensor and there are no cross sensor interferences. More stringent timing control will be required if there are cross sensor incompatibilities.</p><p>From <ref type="table" target="#tab_0">Table IX</ref>, it is clear that having multiple channels improves performance significantly. However, it may not be feasible to deploy all the sensors in deployment scenarios. In the absence of certain channels, the proposed framework can be retrained to work with available channels (but with reduced performance). Further, it is possible to extend the proposed framework to work with a different set of additional channels by adding more channels to the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>As the quality of PAIs gets better and better, identifying presentation attacks using visible spectra alone is becoming harder. Secure use of biometrics requires more reliable ways to detect spoofing attempts. Presentation attack detection is especially challenging while presented with realistic 3D attacks and partial attacks. Using multiple channels of information for PAD makes the systems much harder to spoof. In this work, a Multi-channel CNN framework is proposed, which achieves superior performance as compared to baseline methods. We also introduce a new multi-channel dataset containing various 2D and 3D attacks tackling identity concealment and impersonation. The proposed database includes a variety of attacks including 2D prints, video and photo replays, mannequin heads, paper, silicone, and rigid masks among others. From the experiments, it becomes clear that the performance of algorithms is poor when only the color channel is used. Addition of multiple channels improves the results greatly. Furthermore, the unseen attack protocols and evaluations indicate the performance of the system in the real-world scenarios, where the system encounters attacks which were not present in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Part of this research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2017-17020200005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Block diagram of the proposed approach. The gray color blocks in the CNN part represent layers which are not retrained, and other colored blocks represent re-trained/adapted layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The integrated setup used for WMCA data collection; a) rendering of the integrated system, b) Seek Thermal Compact PRO sensor , c) Intel RealSense SR300 sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Sample images of a) Bonafide and b) Silicone mask attack from the database for all channels after alignment. The images from all channels are aligned with the calibration parameters and normalized to eight bit for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Examples of presentation attacks with different PAIs. (a): glasses (paper glasses), (b): glasses (funny eyes glasses), (c): print, (d): replay, (e): fake head, (f): rigid mask (Obama plastic Halloween mask), (g): rigid mask (transparent plastic mask), (h): rigid mask (custom made realistic), (i): flexible mask (custom made realistic), and (j): paper mask.channels are recorded in uncompressed format, and the total size of the database is 5.1 TB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>ROC for MC-CNN and the baseline methods in WMCA grandtest protocol eval set 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Preprocessed data from four channels for bonafide with glasses (first row) and funny eyes glasses attack (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>• C1-FFC (1+FFC) : First convolutional layer including MFM, and FFC are adapted. • C1-B1-FFC (1-2+FFC) : Adapting ResNet blocks along with the layers adapted in the previous set. • C1-B1-G1-FFC (1-3+FFC) : Adapts group1 along with the layers adapted in the previous set. • 1-N+FFC : Adapts layers from 1 to N along with FFC. • ALL (1-10 +FFC) : All layers are adapted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Recent multi-channel face PAD datasets</figDesc><table><row><cell>Database</cell><cell>Year</cell><cell>Samples</cell><cell>Attacks</cell><cell>Channels</cell><cell>Synchronous Capture</cell></row><row><cell>3DMAD [14]</cell><cell>2013</cell><cell>17 subjects</cell><cell>3D: Mask attacks</cell><cell>Color and depth</cell><cell></cell></row><row><cell>I 2 BVSD [15]</cell><cell>2013</cell><cell>75 subjects</cell><cell>3D: Facial disguises</cell><cell>Color and thermal</cell><cell>-</cell></row><row><cell>GUC-LiFFAD [16]</cell><cell>2015</cell><cell>80 subjects</cell><cell>2D: Print and replay</cell><cell>Light-field imagery</cell><cell></cell></row><row><cell>MS-Spoof [17]</cell><cell>2016</cell><cell>21 subjects</cell><cell>2D: Print</cell><cell>Color and NIR (800nm)</cell><cell></cell></row><row><cell>BRSU [18]</cell><cell>2016</cell><cell>50+ subjects</cell><cell>3D: Masks, facial disguise</cell><cell>Color &amp; 4 SWIR bands</cell><cell></cell></row><row><cell>EMSPAD [19]</cell><cell>2017</cell><cell>50 subjects</cell><cell>2D: Print(laser &amp;Inkjet)</cell><cell>7-band multi-spectral data</cell><cell></cell></row><row><cell>MLFP [20]</cell><cell>2017</cell><cell>10 subjects</cell><cell>3D: Obfuscation with la-</cell><cell>Visible, NIR and thermal</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>tex masks</cell><cell>bands</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Session description for WMCA data collection</figDesc><table><row><cell cols="2">Session Background</cell><cell>Illumination</cell></row><row><cell>1</cell><cell>uniform</cell><cell>ceiling office light</cell></row><row><cell>2</cell><cell>uniform</cell><cell>day-light illumination</cell></row><row><cell>3</cell><cell>complex</cell><cell>day-light illumination</cell></row><row><cell>4</cell><cell>uniform</cell><cell>ceiling office light</cell></row><row><cell>5</cell><cell>uniform</cell><cell>ceiling office light</cell></row><row><cell>6</cell><cell>uniform</cell><cell>side illumination with LED lamps</cell></row><row><cell>7</cell><cell>complex</cell><cell>ceiling office light</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Statistics for WMCA database.</figDesc><table><row><cell>Type</cell><cell>#Presentations</cell></row><row><cell>bonafide</cell><cell>347</cell></row><row><cell>glasses</cell><cell>75</cell></row><row><cell>fake head</cell><cell>122</cell></row><row><cell>print</cell><cell>200</cell></row><row><cell>replay</cell><cell>348</cell></row><row><cell>rigid mask</cell><cell>137</cell></row><row><cell>flexible mask</cell><cell>379</cell></row><row><cell>paper mask</cell><cell>71</cell></row><row><cell>TOTAL</cell><cell>1679</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of the baseline systems and the components in grandtest protocol of WMCA dataset. The values reported are obtained with a threshold computed for BPCER 1% in dev set.</figDesc><table><row><cell>Method</cell><cell cols="2">dev (%)</cell><cell></cell><cell>test (%)</cell><cell></cell></row><row><cell></cell><cell>APCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>Color (IQM-LR)</cell><cell>76.58</cell><cell>38.79</cell><cell>87.49</cell><cell>0</cell><cell>43.74</cell></row><row><cell>Depth (LBP-LR)</cell><cell>57.71</cell><cell>29.35</cell><cell>65.45</cell><cell>0.03</cell><cell>32.74</cell></row><row><cell>Infrared (LBP-LR)</cell><cell>32.79</cell><cell>16.9</cell><cell>29.39</cell><cell>1.18</cell><cell>15.28</cell></row><row><cell>Thermal (LBP-LR)</cell><cell>11.79</cell><cell>6.4</cell><cell>16.43</cell><cell>0.5</cell><cell>8.47</cell></row><row><cell>Score fusion (IQM-LBP-LR Mean fusion)</cell><cell>10.52</cell><cell>5.76</cell><cell>13.92</cell><cell>1.17</cell><cell>7.54</cell></row><row><cell>Color (RDWT-Haralick-SVM)</cell><cell>36.02</cell><cell>18.51</cell><cell>35.34</cell><cell>1.67</cell><cell>18.5</cell></row><row><cell>Depth (RDWT-Haralick-SVM)</cell><cell>34.71</cell><cell>17.85</cell><cell>43.07</cell><cell>0.57</cell><cell>21.82</cell></row><row><cell>Infrared (RDWT-Haralick-SVM)</cell><cell>14.03</cell><cell>7.51</cell><cell>12.47</cell><cell>0.05</cell><cell>6.26</cell></row><row><cell>Thermal (RDWT-Haralick-SVM)</cell><cell>21.51</cell><cell>11.26</cell><cell>24.11</cell><cell>0.85</cell><cell>12.48</cell></row><row><cell>Score fusion (RDWT-Haralick-SVM Mean fusion)</cell><cell>6.2</cell><cell>3.6</cell><cell>6.39</cell><cell>0.49</cell><cell>3.44</cell></row><row><cell>FASNet</cell><cell>18.89</cell><cell>9.94</cell><cell>17.22</cell><cell>5.65</cell><cell>11.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="2">: Performance breakdown across different</cell></row><row><cell cols="2">PAIs. Accuracy for each type of attack at at BPCER</cell></row><row><cell>1% is reported.</cell><cell></cell></row><row><cell cols="2">ATTACK TYPE MC-CNN @ BPCER : 1%</cell></row><row><cell>glasses</cell><cell>90.82</cell></row><row><cell>fake head</cell><cell>100.0</cell></row><row><cell>print</cell><cell>100.0</cell></row><row><cell>replay</cell><cell>100.0</cell></row><row><cell>rigid mask</cell><cell>100.0</cell></row><row><cell>flexible mask</cell><cell>100.0</cell></row><row><cell>paper mask</cell><cell>100.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Performance of the proposed system as compared to the best baseline method on the dev and test set of the grandtest protocol of WMCA dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">dev (%)</cell><cell></cell><cell>test (%)</cell><cell></cell></row><row><cell></cell><cell>APCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>MC-CNN</cell><cell>0.68</cell><cell>0.84</cell><cell>0.6</cell><cell>0</cell><cell>0.3</cell></row><row><cell>RDWT+Haralick Score fusion</cell><cell>6.2</cell><cell>3.6</cell><cell>6.39</cell><cell>0.49</cell><cell>3.44</cell></row><row><cell>IQM+LBP Score fusion</cell><cell>10.52</cell><cell>5.76</cell><cell>13.92</cell><cell>1.17</cell><cell>7.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Performance of the baseline and the MC-CNN system with unseen attack protocols. The values reported are obtained with a threshold computed for BPCER 1% in dev set.</figDesc><table><row><cell>Protocol</cell><cell cols="3">RDWT+Haralick Score fusion test (%)</cell><cell></cell><cell>IQM+LBP Score fusion test (%)</cell><cell></cell><cell></cell><cell>MC-CNN test (%)</cell><cell></cell></row><row><cell></cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>LOO fakehead</cell><cell>4.82</cell><cell>1.5</cell><cell>3.16</cell><cell>4.12</cell><cell>0.64</cell><cell>2.38</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>LOO flexiblemask</cell><cell>28.06</cell><cell>0.03</cell><cell>14.05</cell><cell>56.36</cell><cell>0.8</cell><cell>28.58</cell><cell>5.04</cell><cell>0</cell><cell>2.52</cell></row><row><cell>LOO glasses</cell><cell>97.09</cell><cell>0.61</cell><cell>48.85</cell><cell>100</cell><cell>1.72</cell><cell>50.86</cell><cell>84.27</cell><cell>0</cell><cell>42.14</cell></row><row><cell>LOO papermask</cell><cell>4.01</cell><cell>0.49</cell><cell>2.25</cell><cell>31.51</cell><cell>1.17</cell><cell>16.34</cell><cell>0</cell><cell>0.7</cell><cell>0.35</cell></row><row><cell>LOO prints</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3.52</cell><cell>1.08</cell><cell>2.3</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>LOO replay</cell><cell>10.03</cell><cell>1.51</cell><cell>5.77</cell><cell>0.15</cell><cell>1.53</cell><cell>0.84</cell><cell>0</cell><cell>0.24</cell><cell>0.12</cell></row><row><cell>LOO rigidmask</cell><cell>15.3</cell><cell>0</cell><cell>7.65</cell><cell>27.47</cell><cell>1.08</cell><cell>14.27</cell><cell>0.63</cell><cell>0.87</cell><cell>0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Performance of the MC-CNN when different combinations of layers were adapted.</figDesc><table><row><cell>Method</cell><cell cols="2">dev (%)</cell><cell></cell><cell>test (%)</cell><cell></cell></row><row><cell></cell><cell>APCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>FFC (FFC)</cell><cell>1.51</cell><cell>1.26</cell><cell>2.88</cell><cell>0</cell><cell>1.44</cell></row><row><cell>C1-FFC (1+FFC)</cell><cell>1.77</cell><cell>1.38</cell><cell>2.44</cell><cell>0</cell><cell>1.22</cell></row><row><cell>C1-B1-FFC(1-2+FFC)</cell><cell>0.68</cell><cell>0.84</cell><cell>0.6</cell><cell>0</cell><cell>0.3</cell></row><row><cell>C1-B1-G1-FFC(1-3+FFC)</cell><cell>1.1</cell><cell>1.05</cell><cell>1.11</cell><cell>0.05</cell><cell>0.58</cell></row><row><cell>C1-B1-G1-B2-FFC(1-4+FFC)</cell><cell>0.23</cell><cell>0.61</cell><cell>0.58</cell><cell>0.02</cell><cell>0.3</cell></row><row><cell>(1-5+FFC)</cell><cell>1.14</cell><cell>0.57</cell><cell>0.99</cell><cell>0.56</cell><cell>0.77</cell></row><row><cell>(1-6+FFC)</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>0</cell><cell>50</cell></row><row><cell>(1-7+FFC)</cell><cell>97.56</cell><cell>48.78</cell><cell>96.88</cell><cell>0</cell><cell>48.44</cell></row><row><cell>(1-8+FFC)</cell><cell>99.99</cell><cell>49.99</cell><cell>100</cell><cell>0</cell><cell>50</cell></row><row><cell>(1-9+FFC)</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>0</cell><cell>50</cell></row><row><cell>ALL(1-10+FFC)</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>0</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Performance of the MC-CNN with various combinations of channels. From the experiments in the previous subsections, it can be seen that the performance of the proposed algorithm surpasses the selected featurebased baselines. Transfer learning from face recognition network proves to be effective in training deep multichannel CNN's with a limited amount of training data.</figDesc><table><row><cell>System</cell><cell cols="2">dev (%)</cell><cell></cell><cell>test (%)</cell><cell></cell></row><row><cell></cell><cell>APCER</cell><cell>ACER</cell><cell>APCER</cell><cell>BPCER</cell><cell>ACER</cell></row><row><cell>G+D+I+T</cell><cell>0.68</cell><cell>0.84</cell><cell>0.6</cell><cell>0</cell><cell>0.3</cell></row><row><cell>G+D+I</cell><cell>0.78</cell><cell>0.89</cell><cell>2.07</cell><cell>0</cell><cell>1.04</cell></row><row><cell>G</cell><cell>41.14</cell><cell>21.07</cell><cell>65.65</cell><cell>0</cell><cell>32.82</cell></row><row><cell>D</cell><cell>10.3</cell><cell>5.65</cell><cell>11.77</cell><cell>0.31</cell><cell>6.04</cell></row><row><cell>I</cell><cell>3.5</cell><cell>2.25</cell><cell>5.03</cell><cell>0</cell><cell>2.51</cell></row><row><cell>T</cell><cell>4.19</cell><cell>2.59</cell><cell>3.14</cell><cell>0.56</cell><cell>1.85</cell></row><row><cell cols="2">D. Discussions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1) Performance:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The term spoofing should be deprecated in favour of presentation attacks to comply with the ISO standards.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Source code available at: https://gitlab.idiap.ch/bob/bob.paper. mccnn.tifs2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For downloading the dataset, visit https://www.idiap.ch/dataset/ wmca</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We are pointing out that we investigated other classifiers such as SVM but as no performance improvement was noticed we decided to keep a simple method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The score distributions from the CNN is bimodal (with low variance in each mode) with most of the values concentrated near zero and one, which explains the lack of points in the lower APCER values in the ROC plots.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in face detection and facial image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Handbook of biometric antispoofing-trusted biometrics under spoofing attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Information technology Biometric presentation attack detection Part 1: Framework</title>
		<imprint>
			<date type="published" when="2016-01" />
			<pubPlace>Standard</pubPlace>
		</imprint>
	</monogr>
	<note>International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counter-measures to photo attacks in face recognition: a public database and a baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 international joint conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<idno>no. EPFL-CONF-192369</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference of the Biometrics Special Interest Group</title>
		<meeting>the 11th International Conference of the Biometrics Special Interest Group</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The replay-mobile face presentation-attack database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Costa-Pazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vazquez-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics Special Interest Group (BIOSIG), 2016 International Conference of the</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="761" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face antispoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2636" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face spoofing detection from single images using micro-texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Määttä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 international joint conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Presentation attack detection methods for face recognition systems: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Remote blood pulse analysis for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Biometric Anti-Spoofing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On effectiveness of anomaly detection approaches against unseen presentation attacks in face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nikisins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<idno>no. EPFL- CONF-233583</idno>
	</analytic>
	<monogr>
		<title level="m">The 11th IAPR International Conference on Biometrics (ICB 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transfer learning using convolutional neural networks for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lucena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Moia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spoofing in 2d face recognition with 3d masks and anti-spoofing with kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing disguised faces: Human and machine evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">99212</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Presentation attack detection for face recognition using light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1060" to="1075" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition systems under spoofing attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition Across the Imaging Spectrum</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design of an active multispectral swir camera system for skin detection and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sporrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sensors</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the vulnerability of extended multispectral face recognition systems towards presentation attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Identity, Security and Behavior Analysis (ISBA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face presentation attack with latex masks in multispectral videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="275" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality assessment for fake biometric detection: Application to iris, fingerprint, and face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="710" to="724" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Live face video vs. spoof face video: Use of moiré patterns to detect replay video attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the use of client identity information for face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dos Anjos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="787" to="796" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An anomaly detection approach to face spoofing detection: A new formulation and evaluation protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="13" to="868" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pulse-based features for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9th International Conference on</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Biometrics Theory, Applications and Systems</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d convolutional neural network based on face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learn convolutional neural network for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5601</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benlamoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Bekhouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ouafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taleb</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A competition on generalized softwarebased face presentation attack detection in mobile scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning generalized deep feature representation for face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2639" to="2652" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep models for face anti-spoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face anti-spoofing using patch and depth-based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">IEEE International Joint Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep convolutional dynamic texture learning with adaptive channel-discriminability for 3d mask face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="748" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extended multispectral face presentation attack detection: An approach based on fusing information from individual spectral bands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Fusion (Fusion), 2017 20th International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reliable face anti-spoofing using multispectral swir imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Biometric liveness detection: Challenges and research opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security &amp; Privacy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Biometrics systems under spoofing attack: an evaluation methodology and lessons learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spoofing face recognition with 3d masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1084" to="1097" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Disguise detection and face recognition in visible and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (ICB), 2013 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical and structural approaches to texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spoofing deep face recognition with custom silicone masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9th International Conference on</title>
		<imprint>
			<publisher>Applications and Systems</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>BTAS)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What you can&apos;t see can help you-extended-range imaging for 3d-mask presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</title>
		<meeting>the 16th International Conference on Biometrics Special Interest Group., no. EPFL-CONF-231840</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Gesellschaft fuer Informatik eV (GI</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Licata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="764" to="766" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150203</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Lcnn: Low-level feature embedded cnn for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03928</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using domain specific units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Freitas Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Information technology International Organization for Standardization</title>
	</analytic>
	<monogr>
		<title level="j">International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="https://www.iarpa.gov/index.php/research-programs/odin" />
	</analytic>
	<monogr>
		<title level="j">IARPA ODIN</title>
		<imprint>
			<biblScope unit="volume">accessed</biblScope>
			<date type="published" when="2018-10-20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
