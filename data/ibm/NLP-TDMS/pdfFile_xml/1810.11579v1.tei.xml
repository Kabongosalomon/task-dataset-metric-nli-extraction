<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A 2 -Nets: Double Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
							<email>chenyunpeng@u.nus.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
							<email>yannisk@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
							<email>jianshu@u.nus.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">AI Institute National University of Singapore</orgName>
								<address>
									<postCode>360</postCode>
									<settlement>Qihoo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A 2 -Nets: Double Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the "double attention block", a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works. * Part of the work is done during internship at Facebook Research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Convolutional Neural Networks (CNNs) have been successfully applied in image and video understanding during the past few years. Many new network topologies have been developed to alleviate optimization difficulties <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and increase the learning capacities <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>, which benefit recognition performance for both images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref> and videos <ref type="bibr" target="#b22">[23]</ref> significantly.</p><p>However, CNNs are inherently limited by their convolution operators which are dedicated to capturing local features and relations, e.g. from a 7 × 7 region, and are inefficient in modeling long-range interdependencies. Though stacking multiple convolution operators can enlarge the receptive field, it also comes with a number of unfavorable issues in practice. First, stacking multiple operators makes the model unnecessarily deep and large, resulting in higher computation and memory cost as well as increased over-fitting risks. Second, features far away from a specific location have to pass through a stack of layers before affecting the location for both forward propagation and backward propagation, increasing the optimization difficulties during the training. Third, the features visible to a distant location are actually "delayed" ones from several layers behind, causing inefficient reasoning. Though some recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> can partially alleviate the above issues, they are either non-flexible <ref type="bibr" target="#b10">[11]</ref> or computationally expensive <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work, we aim to overcome these limitations by introducing a new network component that enables a convolution layer to sense the entire spatio-temporal space 2 from its adjacent layer immediately. The core idea is to first gather key features from the entire space into a compact set and then distribute them to each location adaptively, so that the subsequent convolution layers can sense features from the entire space even without a large receptive filed. We develop a generic function for such purpose and implement it with an efficient double attention mechanism. The first second-order attention pooling operation selectively gathers key features from the entire space, while the second adopts another attention mechanism to adaptively distribute a subset of key features that are helpful to complement each spatio-temporal location for high-level tasks. We denote our proposed double-attention block as A 2 -block and its resultant network as A 2 -Net.</p><p>The double-attention block is related to a number of recent works, including the Squeeze-and-Excitation Networks <ref type="bibr" target="#b10">[11]</ref>, covariance pooling <ref type="bibr" target="#b13">[14]</ref>, the Non-local Neural Networks <ref type="bibr" target="#b24">[25]</ref> and the Transformer architecture of <ref type="bibr" target="#b23">[24]</ref>. However, compared with these existing works, it enjoys several unique advantages: Its first attention operation implicitly computes second-order statistics of pooled features and can capture complex appearance and motion correlations that cannot be captured by the global average pooling used in SENet <ref type="bibr" target="#b10">[11]</ref>. Its second attention operation adaptively allocates features from a compact bag, which is more efficient than exhaustively correlating the features from all the locations with every specific location as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>. Extensive experiments on image and video recognition tasks clearly validate the above advantages of our proposed method.</p><p>We summarize our contributions as follows:</p><p>• We propose a generic formulation for capturing long-range feature interdependencies via universal gathering and distribution functions.</p><p>• We propose the double attention block for gathering and distributing long-range features, an efficient architecture that captures second-order feature statistics and makes adaptive feature assignment. The block can model long-range interdependencies with a low computational and memory footprint and at the same time boost image/video recognition performance significantly.</p><p>• We investigate the effect of our proposed A 2 -Net with extensive ablation studies and prove its superior performance through comparison with the state-of-the-arts on a number of public benchmarks for both image recognition and video action recognition tasks, including ImageNet-1k, Kinetics and UCF-101.</p><p>The rest of the paper is organized as follows. We first motivate and present our approach in Section 2, where we also discuss the relation of our approach to recent works. We then evaluate and report results in Section 3 and conclude the paper with Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Convolutional operators are designed to focus on local neighborhoods and therefore fail to "sense" the entire spatial and/or temporal space, e.g. the entire input frame or one location across multiple frames. A CNN model thus usually employs multiple convolution layers (or recurrent units <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>) in order to capture global aspects of the input. Meanwhile, self-attentive and correlation operators like second-order pooling have been recently shown to work well in a wide range of tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. In this section we present a component capable of gathering and distributing global features to each spatial-temporal location of the input, helping subsequent convolution layers sense the entire space immediately and capture complex relations. We first formally describe this desired component by providing a generic formulation and then introduce our double attention block, a highly efficient instantiation of such a component. We finally discuss the relation of our approach to other recent related approaches. Let X ∈ R c×d×h×w denote the input tensor for a spatio-temporal (3D) convolutional layer, where c denotes the number of channels, d denotes the temporal dimension <ref type="bibr" target="#b2">3</ref> and h, w are the spatial dimensions of the input frames. For every spatio-temporal input location i = 1, . . . , dhw with local feature v i , let us define</p><formula xml:id="formula_0">z i = F distr (G gather (X), v i ) ,<label>(1)</label></formula><p>to be the output of an operator that first gathers features in the entire space and then distributes them back to each input location i, taking into account the local feature v i of that location. Specifically, G gather adaptively aggregates features from the entire input space, and F distr distributes the gathered information to each location i, conditioned on the local feature vector v i .</p><p>The idea of gathering and distributing information is motivated by the squeeze-and-excitation network (SENet) <ref type="bibr" target="#b10">[11]</ref>. Eqn. (1), however, presents it in a more general form that leads to some interesting insights and optimizations. In <ref type="bibr" target="#b10">[11]</ref>, global average pooling is used in the gathering process, while the resulted single global feature is distributed to all locations, ignoring different needs across locations. Seeing these shortcomings, we introduce this genetic formulation and propose the Double Attention block, where global information is first gathered by second-order attention pooling (instead of firstorder average pooling), and the gathered global features are adaptively distributed conditioned on the need of current local feature v i , by a second attention mechanism. In this way, more complex global relations can be captured by a compact set of features and each location can receive its customized global information that is complementary to the exiting local features, facilitating learning more complex relations. The proposed component is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a). At below, we first describe its architecture in details and then discuss some instantiations and its connections to other recent related approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The First Attention Step: Feature Gathering</head><p>A recent work <ref type="bibr" target="#b14">[15]</ref> used bilinear pooling to capture second-order statistics of features and generate global representations. Compared with the conventional average and max pooling which only compute first-order statistics, bilinear pooling can capture and preserve complex relations better. Concretely, bilinear pooling gives a sum pooling of second-order features from the outer product of all the feature vector pairs (a i , b i ) within two input feature maps A and B:</p><formula xml:id="formula_1">G bilinear (A, B) = AB = ∀i a i b i ,<label>(2)</label></formula><p>where A = [a 1 , · · · , a dhw ] ∈ R m×dhw and B = [b 1 , · · · , b dhw ] ∈ R n×dhw . In CNNs, A and B can be the feature maps from the same layer, i.e. A = B, or from two different layers, i.e. A = φ(X; W φ ) and B = θ(X; W θ ), with parameters W φ and W θ .</p><p>By introducing the output variable G = [g 1 , · · · , g n ] ∈ R m×n of the bilinear pooling and rewriting the second feature B as B = [b 1 ; · · · ;b n ] where eachb i is a dhw-dimensional row vector, we can reformulate Eqn. <ref type="formula" target="#formula_1">(2)</ref> as</p><formula xml:id="formula_2">g i = Ab i = ∀jb ij a j .<label>(3)</label></formula><p>Eqn. <ref type="formula" target="#formula_2">(3)</ref> gives a new perspective on the bilinear pooling result: instead of just computing secondorder statistics, the output of bilinear pooling G is actually a bag of visual primitives, where each primitive g i is calculated by gathering local features weighted byb i . This inspires us to develop a new attention-based feature gathering operation. We further apply a softmax onto B to ensure jb ij = 1, i.e. a valid attention weighting vector, which gives following second-order attention pooling process:</p><formula xml:id="formula_3">g i = A softmax(b i ) .<label>(4)</label></formula><p>The first row in <ref type="figure" target="#fig_0">Figure 1 (b)</ref> shows the second-order attention pooling that corresponds to Eqn. (4), where both A and B are outputs of two different convolution layers transforming the input X. In implementation, we let A = φ(X; W φ ) and B = softmax (θ(X; W θ )). The second-order attention pooling offers an effective way to gather key features: it captures the global features, e.g. texture and lighting, whenb i is densely attended on all locations; and it captures the existence of specific semantic, e.g. an object and parts, whenb i is sparsely attended on a specific region. We note that similar understandings were presented in <ref type="bibr" target="#b6">[7]</ref>, in which they proposed a rank-1 approximation of a bilinear pooling operation associated with a fully connected classifier. However, in our work, we propose to apply attention pooling to gather visual primitives at different locations into a bag of global descriptors using softmax attention map and do not apply any low-rank constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Second Attention Step: Feature Distribution</head><p>The next step after gathering features from the entire space is to distribute them to each location of the input, such that the subsequent convolution layer can sense the global information even with a small convolutional kernel.</p><p>Instead of distributing the same summarized global features to all locations like SENet <ref type="bibr" target="#b10">[11]</ref>, we propose to get more flexibility by distributing an adaptive bag of visual primitives based on the need of feature v i at each location. In this way, each location can select features that are complementary to the current feature which can make the training easier and help capture more complex relations. This is achieved by selecting a subset of feature vectors from G gather (X) with soft attention:</p><formula xml:id="formula_4">z i = ∀j v ij g j = G gather (X)v i , where ∀j v ij = 1.<label>(5)</label></formula><p>Eqn. (5) formulates the proposed soft attention for feature selection. In our implementation, we apply the softmax function to normalize v i into the one with unit sum, which is found to give better convergence. The second row in <ref type="figure" target="#fig_0">Figure 1 (b)</ref> shows the above feature selection step. Similar to the way we generate the attention map, the set of attention weight vectors is also generated by a convolution layer follow by a softmax normalizer, i.e. V = softmax (ρ(X; W ρ )) where W ρ contains parameters for this layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Double Attention Block</head><p>We combine the above two attention steps to form our proposed double-attention block, with its computation graph in deep neural networks is given in <ref type="figure">Figure 2</ref>. To formulate the double attention operation, we substitute Eqn. (4) and Eqn. (5) into Eqn. (1) and obtain  <ref type="figure">Figure 2</ref>: The computational graph of the proposed double attention block. All convolution kernel size is 1 × 1 × 1. We insert this double attention block to existing convolutional neural network, e.g. residual networks <ref type="bibr" target="#b8">[9]</ref>, to form the A 2 -Net. <ref type="figure" target="#fig_0">Figure 1 (b)</ref> shows the combined double attention operation and <ref type="figure">Figure 2</ref> shows the corresponding computational graph, where the feature arrays A, B and V are generated by three different convolution layers operating on the input feature array X followed by softmax normalization if necessary. The output result Z is given by conducting two matrix multiplications with necessary reshape and transpose operations. Here, an additional convolution layer is added at the end to expand the number of channels for the output Z, such that it can be encoded back to the input X via element-wise addition. During the training process, gradient of the loss function can be easily computed using auto-gradient <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> with the chain rule.</p><formula xml:id="formula_5">Z = F distr (G gather (X), V ) = G gather (X)softmax (ρ(X; W ρ )) = φ(X; W φ )softmax (θ(X; W θ )) softmax (ρ(X; W ρ )) .<label>(6)</label></formula><p>There are two different ways to implement the computational graph of Eqn. <ref type="bibr" target="#b5">(6)</ref>. One is to use the left association as given in Eqn. <ref type="bibr" target="#b5">(6)</ref> with computation graph is shown in <ref type="figure">Figure 2</ref>. The other is to conduct the right association, as formulate below:</p><formula xml:id="formula_6">Z = φ(X; W φ ) softmax (θ(X; W θ )) softmax (ρ(X; W ρ )) .<label>(7)</label></formula><p>We note these two different associations are mathematically equivalent and thus will produce the same output. However, they have different computational cost and memory consumption. The computational complexity of the second matrix multiplication in "left association" in Eqn. <ref type="formula" target="#formula_5">(6)</ref> is O(mndhw), while "right association" in Eqn. <ref type="bibr" target="#b6">(7)</ref> has complexity of O(m(dhw) 2 ). As for the memory cost <ref type="bibr" target="#b3">4</ref> , storing the output of the results of the first matrix multiplication costs mn/2 18 MB and (dhw) 2 /2 18 MB for the left and right associations respectively. In practice, an input data array X with 32 28 × 28 frames and 512 channel size can easily cost more than 2GB memory when adopting the right association, much more expensive than 1MB cost of the left association. In this case, left association is also more computationally efficient than the right one. Therefore, for common cases where (dhw) 2 &gt; nm, we suggest implementation in Eqn. (6) with left association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discussion</head><p>It is interesting to observe that the implementation in Eqn. <ref type="bibr" target="#b6">(7)</ref> with right association can be further explained by the recent NL-Net <ref type="bibr" target="#b24">[25]</ref>, where the first multiplication captures pair-wise relations between local features and gives an output relation matrix in R dhw×dhw . The resulted relation matrix is then applied to linearly combine the transformed features φ(X) into the output feature Z. The difference is apparent in the design of the pair-wise relation function, where we propose a new relation function, i.e. softmax (θ(X)) softmax (ρ(X)) rather than using the Embedded Gaussian formulation <ref type="bibr" target="#b23">[24]</ref> to capture the pair-wise relations. Meanwhile, as discussed above, any such a method practically suffers from high computational and memory costs, and relies on the some subsampling tricks to reduce the cost which may potentially hurts the accuracy. Since NL-Net is the current state-of-the-art for video recognition tasks and also closely related, we directly compare and extensively discuss performance between the two in the Experiments section. The results clearly show that our proposed method not only outperforms NL-Net, but does so with higher efficiency and accuracy. As the Embedded Gaussian NL-Net formulation that we compare in the experiments is mathematically equivalent to the self-attention formulation of <ref type="bibr" target="#b23">[24]</ref>, conclusions/comparisons to NL-Net extend to the transformer networks as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first conduct extensive ablation studies to evaluate the proposed A 2 -Nets on the Kinetics <ref type="bibr" target="#b11">[12]</ref> video recognition dataset and compare it with the state-of-the-art NL-Net <ref type="bibr" target="#b24">[25]</ref>. Then we conduct more experiments using deeper and wider neural networks on both image recognition and video recognition tasks and compare it with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Backbone CNN We use the residual network <ref type="bibr" target="#b9">[10]</ref> as our backbone CNN for all experiments. <ref type="table" target="#tab_0">Table 1</ref> shows architecture details of the backbone CNNs for video recognition tasks, where we use ResNet-26 for all ablation studies and ResNet-29 as one of the baseline methods. The computational cost is measured by FLOPs, i.e. floating-point multiplication-adds, and the model complexity is measured by #Params, i.e. total number of trained parameters. The ResNet-50 is almost 2× deeper and wider than the ResNet-26 and thus only used for last several experiments when comparing with the state-of-the-art methods. For the image recognition task, we use the same ResNet-50 but without the temporal dimension for both the input/output data and convolution kernels.</p><p>Training and Testing Settings We use MXNet <ref type="bibr" target="#b2">[3]</ref> to experiment on the image classification task, and PyTorch <ref type="bibr" target="#b17">[18]</ref> on video classification tasks. For image classification, we report standard single model single 224 × 224 center crop validation accuracy, following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. For experiments on video datasets, we report both single clip accuracy and video accuracy. All experiments are conducted using a distributed K80 GPU cluster and the networks are optimized by synchronized SGD. Code and trained models will be released on GitHub soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>For the ablation studies on Kinetics <ref type="bibr" target="#b0">[1]</ref>, we use 32 GPUs per experiment with a total batch size of 512 training from scratch. All networks take 16 frames with resolution 112 × 112 as input. The base learning rate is set to 0.2 and is reduced with a factor of 0.1 at the 20k-th, 30k-th iterations, and terminated at the 37k-th iteration. We set the number of output channels for three convolution layers θ(·), φ(·) and ρ(·) to be 1/4 of the number of input channels. Note that sub-sampling trick is not adopted for all methods for fair comparison.</p><p>Single Block <ref type="table" target="#tab_1">Table 2</ref> shows the results when only one extra block is added to the backbone network. The block is placed after the second residual unit of a certain stage. As can be seen from the last three rows, our proposed A 2 -block constantly improves the performance compared with both the  Notably the extra cost is very little. We also find that the performance gain from placing A 2 -block on top layers is more significant than placing it at lower layers. This may be because the top layers give more semantically abstract representations that are suitable for extracting global visual primitives. Comparatively, the Nonlocal Network <ref type="bibr" target="#b24">[25]</ref> shows less accuracy gain and more computational cost than ours. Since the computational cost for Nonlocal Network is increased quadratically on bottom stage, we are even unable to finish the training when the block is placed at Conv2. <ref type="table" target="#tab_2">Table 3</ref> shows the performance gain when multiple blocks are added to the backbone networks. As can be seen from the results, our proposed A 2 -Net monotonically improves the accuracy when more blocks are added and costs less #FLOPs compared with its competitor. We also find that adding blocks to different stages can lead to more significant accuracy gain than adding all blocks to the same stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Blocks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments on Image Recognition</head><p>We evaluate the proposed A 2 -Net on ImageNet-1k <ref type="bibr" target="#b12">[13]</ref> image classification dataset, which contains more than 1.2 million high resolution images in 1, 000 categories. Our implementation is based on the code released by <ref type="bibr" target="#b4">[5]</ref> using 64 GPUs with a batch size of 2, 048. The base learning rate is set to √ 0.1 and decreases with a factor of 0.1 when training accuracy is saturated.   As can be seen from <ref type="table" target="#tab_3">Table 4</ref>, a ResNet-50 equipped with 5 extra A 2 -blocks at Conv3 and Conv4 outperforms a much larger ResNet-152 architecture. We note that the A 2 -blocks embedded ResNet-50 is also over 40% more efficient than ResNet-152 and only costs 6.5 GFLOPs and 33.0 M parameters. Compared with the SENet <ref type="bibr" target="#b10">[11]</ref>, the A 2 -Net also achieves better accuracy which proves the effectiveness of the proposed double attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiment Results on Video Recognition</head><p>In this subsection, we evaluate the proposed method on learning video representations. We consider the scenario where static image features are pretrained but motion features are learned from scratch by training a model on the large-scale Kinetics <ref type="bibr" target="#b0">[1]</ref> dataset, and the scenario where well-trained motion features are transfered to small-scale UCF-101 <ref type="bibr" target="#b19">[20]</ref> dataset.</p><p>Learning Motion from Scratch on Kinetics We use ResNet-50 pretrained on ImageNet and add 5 randomly initialized A 2 -blocks to build the 3D convolutional network. The corresponding backbone is shown in <ref type="table" target="#tab_0">Table 1</ref>. The network takes 8 frames (sampling stride: 8) as input and is trained for 32k iterations with a total batch size of 512 using 64 GPUs. The initial learning rate is set to 0.04 and decreased in a stepwise manner when training accuracy is saturated. The final result is shown in <ref type="table" target="#tab_4">Table 5</ref>. Compared with the state-of-the-art I3D <ref type="bibr" target="#b0">[1]</ref> and R(2+1)D <ref type="bibr" target="#b22">[23]</ref>, our proposed model shows higher accuracy even with a less number of sampled frames, which once again confirms the superiority of the proposed double-attention mechanism.</p><p>Transfer the Learned Feature to UCF-101 The UCF-101 contains about 13, 320 videos from 101 action categories and has three train/test splits. The training set of UCF-101 is several times smaller than the Kinetics dataset and we use it to evaluate the generality and robustness of the features learned by our model pre-trained on Kinetics. The network is trained with a base learning rate of 0.01 which is decreased for three times with a factor 0.1, using 8 GPUs with a batch size of 104 clips and tested with 224 × 224 input resolution on single scale. <ref type="table" target="#tab_5">Table 6</ref> shows results of our proposed model and comparison with state-of-the-arts. Consistent with above results, the A 2 -Net achieves leading performance with significantly lower computational cost. This shows that the features learned by A 2 -Net are robust and can be effectively transfered to new dataset in very low cost compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we proposed a double attention mechanism for deep CNNs to overcome the limitation of local convolution operations. The proposed double attention method effectively captures the global information and distributes it to every location in a two-step attention manner. We well formulated the proposed method and instantiated it as an light-weight block that can be easily inserted into to existing CNNs with little computational overhead. Extensive ablation studies and experiments on a number of benchmark datasets, including ImageNet-1k, Kinetics and UCF-101, confirmed the effectiveness of the proposed A 2 -Net on both 2D image recognition tasks and 3D video recognition tasks. In the future, we want to explore integrating the double attention in recent compact network architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>, to leverage the expressiveness of the proposed method for smaller, mobile-friendly models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the double-attention mechanism. (a) An example on a single frame input for explaining the idea of our double attention method, where the set of global featues is computed only once and then shared by all locations. Meanwhile, each location i will generate its own attention vector based on the need of its local feature v i to select a desired subset of global features that is helpful to complement current location and form the featurez i . (b) The double attention operation on a three dimensional input array A. The first attention step is shown on the top and produces a set of global features. At location i, the second attention step generates the new local feature z i , as shown at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Three backbone Residual Networks for the video tasks. The input size for ResNet-26 and ResNet-29 are 16×112×112, while the input size for ResNet-50 is 8×224×224. We follow<ref type="bibr" target="#b24">[25]</ref> and set k = [3, 1, 3], [3, 1, 3, 1, 3, 1],<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref> for ResNet-50 in last three stages and decrease the temporal size to reduce computational cost.</figDesc><table><row><cell>stage</cell><cell></cell><cell>ResNet-26</cell><cell></cell><cell></cell><cell>ResNet-29</cell><cell></cell><cell>output</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>output</cell></row><row><cell>conv1</cell><cell cols="3">3×5×5, 16, stride (1,2,2)</cell><cell cols="3">3×5×5, 16, stride (1,2,2)</cell><cell>16×56×56</cell><cell cols="3">3×5×5, 32, stride (1,2,2) max pooling, stride (1,2,2)</cell><cell>8×56×56</cell></row><row><cell></cell><cell></cell><cell>1×1×1, 32</cell><cell></cell><cell></cell><cell>1×1×1, 32</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 64</cell><cell></cell><cell></cell></row><row><cell>conv2</cell><cell></cell><cell>3×3×3, 32</cell><cell> × 2</cell><cell></cell><cell>3×3×3, 32</cell><cell> × 2</cell><cell>8×56×56</cell><cell></cell><cell>3×3×3, 64</cell><cell> × 3</cell><cell>8×56×56</cell></row><row><cell></cell><cell></cell><cell>1×1×1, 128</cell><cell></cell><cell></cell><cell>1×1×1, 128</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1×1×1, 64</cell><cell></cell><cell></cell><cell>1×1×1, 64</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 128</cell><cell></cell><cell></cell></row><row><cell>conv3</cell><cell></cell><cell>3×3×3, 64</cell><cell> × 2</cell><cell></cell><cell>3×3×3, 64</cell><cell> × 2</cell><cell>8×28×28</cell><cell></cell><cell>k×3×3, 128</cell><cell> × 4</cell><cell>4×28×28</cell></row><row><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 512</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1×1×1, 128</cell><cell></cell><cell></cell><cell>1×1×1, 128</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell></row><row><cell>conv4</cell><cell></cell><cell>3×3×3, 128</cell><cell> × 2</cell><cell></cell><cell>3×3×3, 128</cell><cell> × 3</cell><cell>8×14×14</cell><cell></cell><cell>k×3×3, 256</cell><cell> × 6</cell><cell>4×14×14</cell></row><row><cell></cell><cell></cell><cell>1×1×1, 512</cell><cell></cell><cell></cell><cell>1×1×1, 512</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 1024</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell><cell>1×1×1, 256</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 512</cell><cell></cell><cell></cell></row><row><cell>conv5</cell><cell></cell><cell>3×3×3, 256</cell><cell> × 2</cell><cell></cell><cell>3×3×3, 256</cell><cell> × 2</cell><cell>8×7×7</cell><cell></cell><cell>k×3×3, 512</cell><cell> × 3</cell><cell>4×7×7</cell></row><row><cell></cell><cell></cell><cell>1×1×1, 1024</cell><cell></cell><cell></cell><cell>1×1×1, 1024</cell><cell></cell><cell></cell><cell></cell><cell>1×1×1, 2048</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">global average pool, fc, softmax global average pool, fc, softmax</cell><cell>1×1×1</cell><cell cols="3">global average pool, fc, softmax</cell><cell>1×1×1</cell></row><row><cell>(#Params, FLOPs)</cell><cell></cell><cell cols="2">(7.0 M, 8.3 G)</cell><cell></cell><cell cols="2">(7.6 M, 9.2 G)</cell><cell></cell><cell></cell><cell cols="2">(33.4 M, 31.3 G)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between single nonlocal block<ref type="bibr" target="#b24">[25]</ref> and single double attention block on the Kinetics dataset. The performance of vanilla residual networks without extra block is shown in the top row.</figDesc><table><row><cell>Model</cell><cell>+ 1 Block</cell><cell cols="6">#Params FLOPs ∆ FLOPs Clip @1 ∆ Clip@1 Video@1</cell></row><row><cell>ResNet-26</cell><cell>None</cell><cell>7.043 M</cell><cell>8.3 G</cell><cell>-</cell><cell>50.4 %</cell><cell>-</cell><cell>60.7 %</cell></row><row><cell>ResNet-29</cell><cell>None</cell><cell>7.620 M</cell><cell>9.2 G</cell><cell>900 M</cell><cell>50.8 %</cell><cell>+0.5 %</cell><cell>61.6 %</cell></row><row><cell></cell><cell>@ Conv2</cell><cell cols="2">7.061 M 49.0 G</cell><cell>40.69 G</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-26 + NL [25]</cell><cell>@ Conv3</cell><cell cols="2">7.112 M 13.7 G</cell><cell>5.45 G</cell><cell>51.5 %</cell><cell>+1.1 %</cell><cell>62.0 %</cell></row><row><cell></cell><cell>@ Conv4</cell><cell>7.312 M</cell><cell>9.3 G</cell><cell>1.04 G</cell><cell>51.7 %</cell><cell>+1.3 %</cell><cell>62.3 %</cell></row><row><cell></cell><cell>@ Conv2</cell><cell>7.061 M</cell><cell>8.7 G</cell><cell>463 M</cell><cell>51.2 %</cell><cell>+0.8 %</cell><cell>61.8 %</cell></row><row><cell>ResNet-26 + A 2</cell><cell>@ Conv3</cell><cell>7.112 M</cell><cell>8.7 G</cell><cell>463 M</cell><cell>51.9 %</cell><cell>+1.5 %</cell><cell>62.0 %</cell></row><row><cell></cell><cell>@ Conv4</cell><cell>7.312 M</cell><cell>8.7 G</cell><cell>463 M</cell><cell>52.3 %</cell><cell>+1.9 %</cell><cell>62.6 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between performance from multiple nonlocal blocks<ref type="bibr" target="#b24">[25]</ref> and multiple double attention blocks on Kinetics dataset. We report both top-1 clips accuracy and top-1 video accuracy for all the methods. The vanilla residual networks without extra blocks are shown in the top row.</figDesc><table><row><cell>Model</cell><cell>+N Blocks</cell><cell cols="6">#Params FLOPs ∆ FLOPs Clip @1 ∆ Clip@1 Video @1</cell></row><row><cell>ResNet-26</cell><cell>None</cell><cell>7.043 M</cell><cell>8.3 G</cell><cell>-</cell><cell>50.4 %</cell><cell>-</cell><cell>60.7 %</cell></row><row><cell>ResNet-29</cell><cell>None</cell><cell>7.620 M</cell><cell>9.2 G</cell><cell>900 M</cell><cell>50.8 %</cell><cell>+0.5 %</cell><cell>61.6 %</cell></row><row><cell></cell><cell>1 @ Conv4</cell><cell>7.312 M</cell><cell>9.3 G</cell><cell>1.04 G</cell><cell>51.7 %</cell><cell>+1.3 %</cell><cell>62.3 %</cell></row><row><cell>ResNet-26 + NL [25]</cell><cell>2 @ Conv4</cell><cell cols="2">7.581 M 10.4 G</cell><cell>2.08 G</cell><cell>52.0 %</cell><cell>+1.6 %</cell><cell>62.9 %</cell></row><row><cell></cell><cell cols="3">4 @ Conv3&amp;4 7.719 M 21.3 G</cell><cell>12.97 G</cell><cell>52.4 %</cell><cell>+2.0 %</cell><cell>62.8 %</cell></row><row><cell></cell><cell>1 @ Conv4</cell><cell>7.312 M</cell><cell>8.7 G</cell><cell>463 M</cell><cell>52.3 %</cell><cell>+1.9 %</cell><cell>62.6 %</cell></row><row><cell>ResNet-26 + A 2</cell><cell>2 @ Conv4</cell><cell>7.581 M</cell><cell>9.2 G</cell><cell>925 M</cell><cell>52.5 %</cell><cell>+2.1 %</cell><cell>63.1 %</cell></row><row><cell></cell><cell cols="3">4 @ Conv3&amp;4 7.719 M 10.1 G</cell><cell>1.85 G</cell><cell>53.0 %</cell><cell>+2.6 %</cell><cell>63.5 %</cell></row></table><note>baseline ResNet-26 and the deeper ResNet-29.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-thearts on ImageNet-1k.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>ResNet [9]</cell><cell cols="3">ResNet-50 75.3 % 92.2 % ResNet-152 77.0 % 93.3 %</cell></row><row><cell cols="4">SENet [11] ResNet-50 76.7 % 93.4 %</cell></row><row><cell>A 2 -Net</cell><cell cols="3">ResNet-50 77.0 % 93.5 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons with state-of-the-arts results on Kinetics. Only RGB information is used for input.</figDesc><table><row><cell>Model</cell><cell cols="4">#Frames FLOPs Video @1 Video @5</cell></row><row><cell>ConvNet+LSTM [1]</cell><cell>-</cell><cell>-</cell><cell>63.3 %</cell><cell>-</cell></row><row><cell>I3D [1]</cell><cell>64</cell><cell>107.9 G</cell><cell>71.1 %</cell><cell>89.3 %</cell></row><row><cell>R(2+1)D [23]</cell><cell>32</cell><cell>152.4 G</cell><cell>72.0 %</cell><cell>90.0 %</cell></row><row><cell>A 2 -Net</cell><cell>8</cell><cell>40.8 G</cell><cell>74.6 %</cell><cell>91.5 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparisons with state-of-the-arts results on UCF-101. The averaged Top-1 video accuracy on three train/test splits is reported.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>FLOPs</cell><cell>Video @1</cell></row><row><cell>C3D [21]</cell><cell>VGG</cell><cell>38.5 G</cell><cell>82.3 %</cell></row><row><cell>Res3D [22]</cell><cell>ResNet-18</cell><cell>19.3 G</cell><cell>85.8 %</cell></row><row><cell>I3D-RGB [1]</cell><cell>Inception</cell><cell>107.9 G</cell><cell>95.6 %</cell></row><row><cell>R(2+1)D-RGB [23]</cell><cell>ResNet-34</cell><cell>152.4 G</cell><cell>96.8 %</cell></row><row><cell>A 2 -Net</cell><cell>ResNet-50</cell><cell>41.6 G</cell><cell>96.4 %</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here by "space" we mean the entire feature maps of an input frame and the complete spatio-temporal features from a video sequence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For a spatial (2D) convolution, i.e. when the input is an image, d = 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">All values are stored in 32-bit float.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4470" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Is second-order information helpful for large-scale visual recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08050</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11164</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
