<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoFIS: Automatic Feature Inter-action Selection in Factorization Models for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 23-27, 2020. August 23-27, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<email>liubinbh@126.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
							<email>zhuchenxv@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
							<email>liguilin2@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jincai</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<email>tangruiming@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Noah&amp;apos;s Ark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lab</forename><forename type="middle">‡</forename><surname>Shanghai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jincai</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Yu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoFIS: Automatic Feature Inter-action Selection in Factorization Models for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<title level="j" type="main">KDD</title>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">August 23-27, 2020. August 23-27, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403314</idno>
					<note>Architecture Search * Co-first authors with equal contributions. Bin Liu is now affiliated with ByteDance. Weinan Zhang and Ruiming Tang are the corresponding authors. This work is spon-sored by Huawei Innovation Research Program and NSFC (61702327,61772333). ACM ISBN 978-1-4503-7998-4/20/08. . . $15.00 Event, USA. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Recommender systems</term>
					<term>KEYWORDS Recommendation</term>
					<term>Factorization Machine</term>
					<term>Feature Selection</term>
					<term>Neural</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning feature interactions is crucial for click-through rate (CTR) prediction in recommender systems. In most existing deep learning models, feature interactions are either manually designed or simply enumerated. However, enumerating all feature interactions brings large memory and computation cost. Even worse, useless interactions may introduce noise and complicate the training process. In this work, we propose a two-stage algorithm called Automatic Feature Interaction Selection (AutoFIS). AutoFIS can automatically identify important feature interactions for factorization models with computational cost just equivalent to training the target model to convergence. In the search stage, instead of searching over a discrete set of candidate feature interactions, we relax the choices to be continuous by introducing the architecture parameters. By implementing a regularized optimizer over the architecture parameters, the model can automatically identify and remove the redundant feature interactions during the training process of the model. In the re-train stage, we keep the architecture parameters serving as an attention unit to further boost the performance. Offline experiments on three large-scale datasets (two public benchmarks, one private) demonstrate that AutoFIS can significantly improve various FM based models. AutoFIS has been deployed onto the training platform of Huawei App Store recommendation service, where a 10-day online A/B test demonstrated that AutoFIS improved the DeepFM model by 20.3% and 20.1% in terms of CTR and CVR respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-through rate (CTR) prediction is crucial in recommender systems, where the task is to predict the probability of the user clicking on the recommended items (e.g., movie, advertisement) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>. Many recommendation decisions can then be made based on the predicted CTR. The core of these recommender systems is to extract significant low-order and high-order feature interactions.</p><p>Explicit feature interactions can significantly improve the performance of CTR models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. Early collaborative filtering recommendation algorithms, such as matrix factorization (MF) <ref type="bibr" target="#b15">[16]</ref> and factorization machine (FM) <ref type="bibr" target="#b26">[27]</ref>, extract second-order information with a bi-linear learning model. However, not all interactions are conducive to performance. Some tree-based methods have been proposed to find useful intersections automatically. Gradient boosting decision tree (GBDT) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> tries to find the interactions with higher gradients of the loss function. AutoCross <ref type="bibr" target="#b20">[21]</ref> searches effective interactions in a treestructured space. But tree models can only explore a small fraction of all possible feature interactions in recommender systems with multi-field categorical data <ref type="bibr" target="#b24">[25]</ref>, so their exploration ability is restricted.</p><p>In the meantime, Deep Neural Network (DNN) models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> are proposed. Their representational ability is stronger and they could explore most of the feature interactions according to the universal approximation property <ref type="bibr" target="#b11">[12]</ref>. However, there is no guarantee that a DNN naturally converges to any expected functions using gradientbased optimization. A recent work proves the insensitive gradient issue of DNN when the target is a large collection of uncorrelated functions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. Simple DNN models may not find the proper feature interactions. Therefore, various complicated architectures have been proposed, such as Deep Interest Network (DIN) <ref type="bibr" target="#b31">[32]</ref>, Deep Factorization Machine (DeepFM) <ref type="bibr" target="#b7">[8]</ref>, Product-based Neural Network (PNN) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, and Wide &amp; Deep <ref type="bibr" target="#b5">[6]</ref>. Factorization Models (specified in Definition 1), such as FM, DeepFM, PNN, Attention Factorization Machine (AFM) <ref type="bibr" target="#b29">[30]</ref>, Neural Factorization Machine (NFM) <ref type="bibr" target="#b9">[10]</ref>, have been proposed to adopt a feature extractor to explore explicit feature interactions.</p><p>However, all these models are simply either enumerating all feature interactions or requiring human efforts to identify important feature interactions. The former always brings large memory and computation cost to the model and is difficult to be extended into high-order interactions. Besides, useless interactions may bring unnecessary noise and complicate the training process <ref type="bibr" target="#b27">[28]</ref>. The latter, such as identifying important interactions manually in Wide &amp; Deep <ref type="bibr" target="#b5">[6]</ref>, is of high labor cost and risks missing some counterintuitive (but important) interactions.</p><p>If useful feature interactions can be identified beforehand in these factorization models, the models can focus on learning over them without having to deal with useless feature interactions. Through removing the useless or even harmful interactions, we would expect the model to perform better with reduced computation cost.</p><p>To automatically learn which feature interactions are essential, we introduce a gate (in open or closed status) for each feature interaction to control whether its output should be passed to the next layer. In previous works, the status of the gates is either specified beforehand by expert knowledge <ref type="bibr" target="#b5">[6]</ref> or set as all open <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. From a data-driven point of view, whether open or closed a gate should depend on the contribution of each feature interaction to the final prediction. Apparently, those contributing little should be closed to prevent introducing extra noise to model learning. However, it is an NP-Hard problem to find the optimal set of open gates for model performance, as we face an incredibly huge (2 C 2 m , with m the number of feature fields, if we only consider 2 nd -order feature interactions) space to search.</p><p>Inspired by the recent work DARTS <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> for neural architecture search, we propose a two-stage method AutoFIS for automatic selecting low-order and high-order feature interactions in factorization models. In the search stage, instead of searching over a discrete set of candidate feature interactions, we relax the choices to be continuous by introducing a set of architecture parameters (one for each feature interaction) so that the relative importance of each feature interaction can be learned by gradient descent. The architecture parameters are jointly optimized with neural network weights by GRDA optimizer <ref type="bibr" target="#b2">[3]</ref> (an optimizer which is easy to produce a sparse solution) so that the training process can automatically abandon unimportant feature interactions (with zero values as the architecture parameters) and keep those important ones. After that, in the re-train stage, we select the feature interactions with non-zero values of the architecture parameters and re-train the model with the selected interactions while keeping the architecture parameters as attention units instead of indicators of interaction importance.</p><p>Extensive experiments are conducted on three large-scale datasets (two are public benchmarks, and the other is private). Experimental results demonstrate that AutoFIS can significantly improve the CTR prediction performance of factorization models on all datasets. As AutoFIS can remove about 50%-80% 2 nd -order feature interactions, original models can always achieve improvement on efficiency. We also apply AutoFIS for 3 r d -order interaction selection by learning the importance of each 3 r d -order feature interaction. Experimental results show that with about 1%-10% of the 3 r d -order interactions selected, the AUC of factorization models can be improved by 0.1%-0.2% without introducing much computation cost. The results show a promising direction of using AutoFIS for automatic high-order feature interaction selection. Experiments also demonstrate that important 2 nd -and 3 r d -order feature interactions, identified by AutoFIS in factorization machine, can also greatly boost the performance of current state-of-the-art models, which means we can use a simple model for interaction selection and apply the selection results to other models. Besides, we analyze the effectiveness of feature interactions selected by our model on real data and synthetic data. Furthermore, a ten-day online A/B test is performed in a Huawei App Store recommendation service, where AutoFIS yielding recommendation model achieves improvement of CTR by 20.3% and CVR by 20.1% over DeepFM, which contributes a significant business revenue growth.</p><p>To summarize, the main contributions of this paper can be highlighted as follows:</p><p>(1) We empirically verify that removing the redundant feature interactions is beneficial when training factorization models. (2) We propose a two-stage algorithm AutoFIS to automatically select important low-order and high-order feature interactions in factorization models. In the search stage, AutoFIS can learn the relative importance of each feature interaction via architecture parameters within one full training process. In the re-train stage, with the unimportant interactions removed, we re-train the resulting neural network while keeping architecture parameters as attention units to help the learning of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>CTR prediction is generally formulated as a binary classification problem <ref type="bibr" target="#b18">[19]</ref>. In this section we briefly review factorization models for CTR prediction and AutoML models for recommender systems. Factorization machine (FM) <ref type="bibr" target="#b26">[27]</ref> projects each feature into a low-dimensional vector and models feature interactions by inner product, which works well for sparse data. Field-aware factorization machine (FFM) <ref type="bibr" target="#b13">[14]</ref> further enables each feature to have multiple vector representations to interact with features from other fields.</p><p>Recently, deep learning models have achieved state-of-the-art performance on some public benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>. Several models use MLP to improve FM, such as Attention FM <ref type="bibr" target="#b29">[30]</ref>, Neural FM <ref type="bibr" target="#b9">[10]</ref>. Wide &amp; Deep <ref type="bibr" target="#b5">[6]</ref> jointly trains a wide model for artificial features and a deep model for raw features. DeepFM <ref type="bibr" target="#b7">[8]</ref> uses an FM layer to replace the wide component in Wide &amp; Deep. PNN <ref type="bibr" target="#b23">[24]</ref> uses MLP to model the interaction of FM layer and feature embeddings while PIN <ref type="bibr" target="#b24">[25]</ref> introduces a network-in-network architecture to model pairwise feature interactions with sub-networks rather than simple inner product operations in PNN and DeepFM. Note that all existing factorization models simply enumerate all 2 nd -order feature interactions which contain many useless and noisy interactions.</p><p>Gradient boosting decision tree (GBDT) <ref type="bibr" target="#b4">[5]</ref> is a method to do feature engineering and search interactions by decision tree algorithm. Then the transformed feature interactions can be fed into to logistic regression <ref type="bibr" target="#b10">[11]</ref> or FFM <ref type="bibr" target="#b14">[15]</ref>. In practice, tree models are more suitable for continuous data but not for high-dimensional categorical data in recommender system because of the low usage rate of categorical features <ref type="bibr" target="#b24">[25]</ref>.</p><p>In the meantime, there exist some works using AutoML techniques to deal with the problems in recommender system. Au-toCross <ref type="bibr" target="#b20">[21]</ref> is proposed to search over many subsets of candidate features to identify effective interactions. This requires training the whole model to evaluate the selected feature interactions, but the candidate sets are incredibly many: i.e., there are 2 C 2 m candidate sets for a dataset with m fields for just 2 nd -order feature interactions. Thus AutoCross accelerates by two aspects of approximation: (i) it greedily constructs local-optimal feature sets via beam search in a tree structure, and (ii) it evaluates the newly generated feature sets via field-aware logistic regression. Due to such two approximations, the high-order feature interactions extracted from AutoCross may not be useful for deep models. Compared with AutoCross, our proposed AutoFIS only needs to perform the search stage once to evaluate the importance of all feature interactions, which is much more efficient. Moreover, the learned useful interactions will improve the deep model as they are learned and evaluated in this deep model directly.</p><p>Recently, one-shot architecture search methods, such as DARTS <ref type="bibr" target="#b19">[20]</ref>, have become the most popular neural architecture search (NAS) algorithms to efficiently search network architectures <ref type="bibr" target="#b0">[1]</ref>. In recommender systems, such methods are utilized to search proper interaction functions for collaborative filtering models <ref type="bibr" target="#b25">[26]</ref>. The model in <ref type="bibr" target="#b25">[26]</ref> focuses on identifying proper interaction functions for feature interactions while our model focuses on searching and keeping important feature interactions. Inspired by the recent work DARTS for neural architecture search, we formulate the problem of searching the effective feature interactions as a continuous searching problem by incorporating architecture parameters. Different from DARTS using two-level optimization to optimize the architecture parameters and the model weights alternatively and iteratively with the training set and the validation set, we use one-level optimization to train these two types of parameters jointly with all data as the training set. We analyze their difference theoretically in Section 3.2, and compare their performance in the Experiments Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we describe the proposed AutoFIS, an algorithm to select important feature interactions in factorization models automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Factorization Model (Base Model)</head><p>First, we define factorization models:</p><formula xml:id="formula_1">Definition 3.1.</formula><p>Factorization models are the models where the interaction of several embeddings from different features is modeled into a real number by some operation such as inner product or neural network.</p><p>We take FM, DeepFM, and IPNN as instances to formulate our algorithm and explore the performance on various datasets. <ref type="figure" target="#fig_0">Figure 1</ref> presents the architectures of FM, DeepFM and IPNN models. FM consists of a feature embedding layer and a feature interaction layer. Besides these two layers, DeepFM and IPNN model include an extra layer: MLP layer. The difference between DeepFM and IPNN is that feature interaction layer and MLP layer work in parallel in DeepFM, while ordered in sequence in IPNN.</p><p>In the subsequent subsections, we will brief the feature embedding layer and feature interaction layer in FM. To work with DeepFM and IPNN model, the MLP layer and output layer are also formalized. Then the detail of how our proposed AutoFIS working on the feature interaction layers is elaborated, i.e., selecting important feature interactions based on architecture parameters.</p><p>Feature Embedding Layer. In most CTR prediction tasks, data is collected in a multi-field categorical form 1 . A typical data preprocess is to transform each data instance into a high-dimensional sparse vector via one-hot or multi-hot encoding. A field is represented as a multi-hot encoding vector only when it is multivariate. A data instance can be represented as</p><formula xml:id="formula_2">x = [x 1 , x 2 , · · · , x m ],</formula><p>where m is the number of fields and x i is the one-hot or multi-hot encoding vector of the i-th field. A feature embedding layer is used to transform an encoding vector into a low-dimensional vector as</p><formula xml:id="formula_3">e i = V i x i .<label>(1)</label></formula><p>where V i ∈ R d ×n i is the a matrix, n i is the number of feature values in the i-th field and d is the dimension of low-dimensional vectors.</p><formula xml:id="formula_4">• If x i is a one-hot vector with j-th element x i [j] = 1, then the representation of x i is V j i . • If x i is a multi-hot vector with x i [j] = 1 for j = i 1 , i 2 , · · · , i k</formula><p>and the embeddings of these elements are {V i1 i , V i2 i , · · · , V ik i }, then the representation of x i is the sum or average of these embeddings <ref type="bibr" target="#b6">[7]</ref>. The output of the feature embedding layer is then the concatenation of multiple embedding vectors as</p><formula xml:id="formula_5">E = [e 1 , e 2 , ..., e m ].</formula><p>Feature Interaction Layer. After transforming the features to low-dimensional space, the feature interactions can be modeled in such a space with the feature interaction layer. First, the inner product of the pairwise feature interactions is calculated:</p><formula xml:id="formula_6">[⟨e 1 , e 2 ⟩, ⟨e 1 , e 3 ⟩, ..., ⟨e m−1 , e m ⟩],<label>(2)</label></formula><p>where e i is the feature embedding of i-th field, ⟨·, ·⟩ is the inner product of two vectors. The number of pair-wise feature interactions in this layer is C 2 m . In FM and DeepFM models, the output of the feature interaction layer is:</p><formula xml:id="formula_7">l f m = ⟨w, x⟩ + m i=1 m j &gt;i ⟨e i , e j ⟩.<label>(3)</label></formula><p>Here, all the feature interactions are passed to the next layer with equal contribution. As pointed in Section 1 and will be verified in Section 4, not all the feature interactions are equally predictive and useless interactions may even degrade the performance. Therefore, we propose the AutoFIS algorithm to select important feature interactions efficiently.</p><p>To study whether our methods can be used to identify important high-order interactions, we define the feature interaction layer with 3 r d -order interactions (i.e., combination of three fields) as:</p><formula xml:id="formula_8">l 3r d f m = ⟨w, x⟩ + m i=1 m j &gt;i ⟨e i , e j ⟩ + m i=1 m j &gt;i m t &gt;j ⟨e i , e j , e t ⟩. (4)</formula><p>MLP Layer. MLP Layer consists of several fully connected layers with activation functions, which learns the relationship and combination of features. The output of one such layer is</p><formula xml:id="formula_9">a (l +1) = relu(W (l ) a (l ) + b (l ) ),<label>(5)</label></formula><p>where a (l ) ,W (l ) , b (l ) are the input, model weight, and bias of the l-th layer. Activation relu(z) = max(0, z). a (0) is the input layer and MLP(a (0) ) = a (h) , where h is the depth of MLP layer MLP.</p><p>Output Layer. FM model has no MLP layer and connects the feature interaction layer with prediction layer directly:</p><formula xml:id="formula_10">y FM = sigmoid(l f m ) = 1 1 + exp(−l f m ) ,<label>(6)</label></formula><p>whereŷ FM is the predicted CTR. DeepFM combines feature interaction layer and MLP layers in parallel asŷ</p><formula xml:id="formula_11">DeepFM = sigmoid(l f m + MLP(E)).<label>(7)</label></formula><p>While in IPNN, MLP layer is sequential to feature interaction layer asŷ</p><formula xml:id="formula_12">IPNN = sigmoid(MLP([E, l f m ])).<label>(8)</label></formula><p>Note that the MLP layer of IPNN can also serve as a re-weighting of the different feature interactions, to capture their relative importance. This is also the reason that IPNN has a higher capacity than FM and DeepFM. However, with the IPNN formulation, one cannot retrieve the exact value corresponding to the relative contribution of each feature interaction. Therefore, the useless feature interactions in IPNN can be neither identified nor dropped, which brings extra noise and computation cost to the model. We would show in the following subsections and Section 4 that how the proposed method AutoFIS could improve IPNN. Architecture Parameters <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> ( −1, ) <ref type="figure">Figure 2</ref>: Overview of AutoFIS Objective Function. FM, DeepFM, and IPNN share the same objective function, i.e., to minimize the cross-entropy of predicted values and the labels as</p><formula xml:id="formula_13">L(y,ŷ M ) = −ylogŷ M − (1 − y)log(1 −ŷ M ),<label>(9)</label></formula><p>where y ∈ {0, 1} is the label andŷ M ∈ [0, 1] is the predicted probability of y = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AutoFIS</head><p>AutoFIS automatically selects useful feature interactions, which can be applied to the feature interaction layer of any factorization model. In this section, we elaborate on how it works. AutoFIS can be split into two stages: search stage and re-train stage. In the search stage, AutoFIS detects useful feature interactions; while in the re-train stage, the model with selected feature interactions is re-trained. Search Stage. To facilitate the presentation of the algorithm, we introduce the gate operation to control whether to select a feature interaction: an open gate corresponds to selecting a feature interaction, while a closed gate results in a dropped interaction. The total number of gates corresponding to all the 2 nd -order feature interactions is C 2 m . It is very challenging to find the optimal set of open gates in a brute-force way, as we face an incredibly huge (2 C 2 m ) space to search. In this work, we approach the problem from a different viewpoint: instead of searching over a discrete set of open gates, we relax the choices to be continuous by introducing architecture parameters α , so that the relative importance of each feature interaction can be learned by gradient descent. The overview of the proposed AutoFIS is illustrated in <ref type="figure">Figure 2</ref>.</p><p>This architecture selection scheme by gradient learning is inspired by DARTS <ref type="bibr" target="#b19">[20]</ref>, where the objective is to select one operation from a set of candidate operations in convolutional neural network (CNN) architecture.</p><p>To be specific, we reformulate the interaction layer in factorization models (shown in Equation 3) as</p><formula xml:id="formula_14">l AutoFIS = ⟨w, x⟩ + m i=1 m j &gt;i α (i, j) ⟨e i , e j ⟩,<label>(10)</label></formula><p>where α = {α <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> , · · · , α (m−1,m) } are the architecture parameters.</p><p>In the search stage of AutoFIS, α (i, j) values are learned in such a way that α (i, j) can represent the relative contribution of each feature interaction to the final prediction. Then, we can decide the gate status of each feature interaction by setting those unimportant ones (i.e., with zero α (i, j) values) closed. Batch Normalization. From the viewpoint of the overall neural network, the contribution of a feature interaction is measured by α (i, j) · ⟨e i , e j ⟩ (in <ref type="figure" target="#fig_0">Equation 10</ref>). Exactly the same contribution can be achieved by re-scaling this term as (</p><formula xml:id="formula_15">α (i, j)</formula><p>Since the value of ⟨e i , e j ⟩ is jointly learned with α (i, j) , the coupling of their scale would lead to unstable estimation of α (i, j) , such that α (i, j) can no longer represent the relative importance of ⟨e i , e j ⟩.</p><p>To solve this problem, we apply Batch Normalization (BN) <ref type="bibr" target="#b12">[13]</ref> on ⟨e i , e j ⟩ to eliminate its scale issue. BN has been adopted by training deep neural networks as a standard approach to achieve fast convergence and better performance. The way that BN normalizes values gives an efficient yet effective way to solve the coupling problem of α (i, j) and ⟨e i , e j ⟩.</p><p>The original BN normalizes the activated output with statistics information of a mini-batch. Specifically,</p><formula xml:id="formula_16">z = z in − µ B σ 2 B + ϵ and z out = θ ·ẑ + β,<label>(11)</label></formula><p>where z in ,ẑ and z out are input, normalized and output values of BN; µ B and σ B are the mean and standard deviation values of z in over a mini-batch B; θ and β are trainable scale and shift parameters of BN; ϵ is a constant for numerical stability.</p><p>To get stable estimation of α (i, j) , we set the scale and shift parameters to be 1 and 0 respectively. The BN operation on each feature interaction ⟨e i , e j ⟩ is calculated as</p><formula xml:id="formula_17">⟨e i , e j ⟩ B N = ⟨e i , e j ⟩ − µ B (⟨e i , e j ⟩) σ 2 B (⟨e i , e j ⟩) + ϵ ,<label>(12)</label></formula><p>where µ B and σ B are the mean and standard deviation of ⟨e i , e j ⟩ in mini-batch B. GRDA Optimizer. Generalized regularized dual averaging (GRDA) optimizer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> is aimed to get a sparse deep neural network. To update α at each gradient step t with data Z t we use the following equation:</p><formula xml:id="formula_18">α t +1 = arg min α {α T (−α 0 + γ t i =0 ∇L(α t ; Z i +1 ) + д(t, γ ) ∥α ∥ 1 + 1/2 ∥α ∥ 2 2 } (13)</formula><p>where д(t, γ ) = cγ 1/2 (tγ ) u , and γ is the learning rate, c and µ are adjustable hyper-parameters to trade-off between accuracy and sparsity.</p><p>In the search stage, we use GRDA optimizer to learn the architecture parameters α and get a sparse solution. Those unimportant feature interactions (i.e., with zero α (i, j) values) will be thrown away automatically. Other parameters are learned by Adam optimizer, as normal.</p><p>One-level Optimization. To learn the architecture parameters α (i, j) in the search stage of AutoFIS, we propose to optimize α jointly with all the other network weights v (such as w in Equation 3 and W (l ) , b (l ) in <ref type="figure" target="#fig_4">Equation 5</ref>). This is different from DARTS. DARTS treats α as higher-level decision variables and the network weights as lower-level variables, then optimizes them with a bi-level optimization algorithm. In DARTS, it is assumed that the model can select the operation only when the network weights are properly learned so that α can "make its proper decision". In the context of AutoFIS formulation, this means that we can decide whether a gate should be open or closed after the network weights are properly trained, which leads us back to the problem of fully training 2 C 2 m models to make the decision. To avoid this issue, DARTS proposes to approximate the optimal value of the network weights with only one gradient descent step and train α and v iteratively.</p><p>We argue that the inaccuracy of this approximation might downgrade the performance. Therefore, instead of using bi-level optimization, we propose to optimize α and v jointly with one-level optimization. Specifically, the parameters α and v are updated together with gradient descent using the training set by descending on α and v based on</p><formula xml:id="formula_19">∂ v L t r ain (v t −1 , α t −1 ) and ∂ α L t r ain (v t −1 , α t −1 ).<label>(14)</label></formula><p>In this setting, α and v can explore their design space freely until convergence, and α is learned to serve as the contribution of individual feature interactions. In Section 4, we would show the superiority of one-level optimization over two-level optimization.</p><p>Re-train Stage. After the training of the search stage, some unimportant interactions are thrown away automatically according to the architecture parameters α * in search stage. We use G (i, j) to represent the gate status of feature interaction ⟨e i , e j ⟩ and set G (i, j) as 0 when α * (i, j) = 0; otherwise, we set G (i, j) as 1. In the re-train stage, the gate status of these unimportant feature interactions are fixed to be closed permanently.</p><p>After removing these unimportant interactions, we re-train the new model with α kept in the model. Specifically, we replace the feature interaction layer in <ref type="bibr">Equation 3</ref> with</p><formula xml:id="formula_20">l r e f m = ⟨w, x⟩ + m i=1 m j &gt;i α (i, j) G (i, j) ⟨e i , e j ⟩.<label>(15)</label></formula><p>Note here α (i, j) no longer serves as an indicator for deciding whether an interaction should be included in the model (as in search stage). Instead, it serves as an attention unit for the architecture to learn the relative importance of the kept feature interaction. In this stage, we do not need to select the feature interactions. Therefore, all parameters are learned by Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive offline experiments 2 on two benchmark public datasets and a private dataset, as well as online A/B test, to answer the following questions:</p><p>• RQ1: Could we boost the performance of factorization models with the selected interactions by AutoFIS? • RQ2: Could interactions selected from simple models be transferred to the state-of-the-art models to reduce their inference time and improve prediction accuracy? • RQ3: Are the interactions selected by AutoFIS really important and useful? • RQ4: Can AutoFIS improve the performance of existing models in a live recommender system? • RQ5: How do different components of our AutoFIS (e.g., <ref type="bibr">BN)</ref> contribute to the performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Experiments are conducted for the following two public datasets (Avazu and Criteo) and one private dataset: Avazu 3 : Avazu was released in the CTR prediction contest on Kaggle. 80% of randomly shuffled data is allotted to training and <ref type="table">Table 1</ref>: Benchmark performance: "time" is the inference time for 2 million samples. "top" represents the percentage of feature interactions kept for 2 nd / 3 r d order interaction. "cost" contain the GPU time of the search and re-train stage. "Rel. Impr." is the relative AUC improvement over FM model. Note: FFM has a lower time and cost due to its smaller embedding size limited by GPU memory constraint. validation with 20% for testing. Categories with less than 20 times of appearance are removed for dimensionality reduction. Criteo 4 : Criteo contains one month of click logs with billions of data samples. We select "data 6-12" as training and validation set while selecting "day-13" for evaluation. To counter label imbalance, negative down-sampling is applied to keep the positive ratio roughly at 50%. 13 numerical fields are converted into one-hot features through bucketing, where the features in a certain field appearing less than 20 times are set as a dummy feature "other".</p><p>Private: Private dataset is collected from a game recommendation scenario in Huawei App Store. The dataset contains app features (e.g., ID, category), user features (e.g., user's behavior history) and context features. Statistics of all the datasets are summarized in <ref type="table" target="#tab_2">Table 2</ref>.  <ref type="bibr" target="#b26">[27]</ref> and DeepFM <ref type="bibr" target="#b7">[8]</ref> models to show its effectiveness (denoted as Aut-oFM and AutoDeepFM, respectively). We compare it with GBDTbased methods (GBDT+LR <ref type="bibr" target="#b10">[11]</ref>, GBDT+FFM <ref type="bibr" target="#b14">[15]</ref>) and Factorization Machine models (AFM <ref type="bibr" target="#b29">[30]</ref>, FwFM <ref type="bibr" target="#b22">[23]</ref>, FFM <ref type="bibr" target="#b13">[14]</ref>, IPNN <ref type="bibr" target="#b24">[25]</ref>). Due to its huge computational costs and the unavailability of the source code, we do not compare our models with AutoCross <ref type="bibr" target="#b20">[21]</ref>. The common evaluation metrics for CTR prediction are AUC (Area Under ROC) and Log loss (cross-entropy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Parameter Settings.</head><p>To enable any one to reproduce the experimental results, we have attached all the hyper-parameters for each model in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Implementation Details.</head><p>Selecting 2 nd -order feature interactions for AutoFM and AutoDeepFM, in the search stage, we first train the model with α and v jointly on all the training data. Then we remove those useless interactions and re-train our model.</p><p>To implement AutoFM and AutoDeepFM for 3 r d -order feature interaction selection, we reuse the selected 2 nd -order interactions in Equation <ref type="bibr" target="#b14">15</ref> and enumerate all the 3 r d -order feature interactions 4 http://labs.criteo.com/downloads/download-terabyte-click-logs/ in the search stage to learn their importance. Finally, we re-train our model with the selected 2 nd -and 3 r d -order interactions.</p><p>Note that in the search stage, the architecture parameters α are optimized by GRDA optimizer and other parameters v are optimized by Adam optimizer. In the re-train stage, all parameters are optimized by Adam optimizer.  <ref type="table">Table 1</ref> summarizes the performance of AutoFM and AutoDeepFM by automatically selecting 2 nd -and 3 r d -order important interactions on Avazu and Criteo datasets and <ref type="table" target="#tab_3">Table 3</ref> reports their performance on Private dataset. We can observe: (1) For Avazu dataset, 71% of the 2 nd -order interactions can be removed for FM and 76% for DeepFM. Removing those useless interactions can not only make the model faster at inference time: the inference time of AutoFM(2nd) and AutoDeepFM(2nd) is apparently less than FM and DeepFM; but also significantly increase the prediction accuracy: the relative performance improvement of AutoFM(2nd) over FM is 0.49% and that of Au-toDeepFM(2nd) over DeepFM is 0.20% in terms of AUC. Similar improvement can also be drawn from the other datasets. (2) For high-order feature interaction selection, only 2% -10% of all the 3 r d -order feature interactions need to be included in the model. The inference time of AutoFM(3rd) and AutoDeepFM(3rd) is much less than that of FM(3rd) and DeepFM(3rd) (which is comparable to FM and DeepFM). Meanwhile, the accuracy is significantly improved by removing unimportant 3 r d -order feature interactions, i.e., the relative performance improvement of AutoFM(3rd) over FM(3rd) is 0.22% and that of AutoDeepFM(3rd) over DeepFM(3rd) is 0.20% in terms of AUC on Avazu. Observations on Criteo are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Interaction Selection by AutoFIS (RQ1)</head><p>(3) All such performance boost could be achieved with marginal time cost (for example, it takes 24 minutes and 128 minutes for AutoDeepFM(3rd) to search important 2 nd -and 3 r d -order feature interactions in Avazu and Criteo with a single GPU card). The same result might take the human engineers many hours or days to achieve by identifying such important feature interactions manually.</p><p>Note that directly enumerating the 3 r d -order feature interactions in FM and DeepFM enlarges the inference time about 7 to 12 times, which is unacceptable in industrial applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transferability of the Selected Feature</head><p>Interactions (RQ2) In this subsection, we investigate whether the feature interactions learned by AutoFM (which is a simple model) could be transferred to the state-of-the-art models such as IPNN to boost their performance. As shown in <ref type="table" target="#tab_4">Table 4</ref>, using 2 nd -order feature interactions selected by AutoFM (namely AutoIPNN(2nd)) achieves comparable performance to IPNN, with around 30% and 50% of all the interactions in Avazu and Criteo. Moreover, the performance is significantly improved by using both 2 nd -and 3 r d -order feature interactions (namely AutoIPNN(3rd)) selected by AutoFM. Both evidences verify the transferability of the selected feature interactions in AutoFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Effectiveness of Feature Interaction</head><p>Selected by AutoFIS (RQ3)</p><p>In this subsection, we will discuss the effectiveness of feature interaction selected by AutoFIS. We conduct experiments on real data and synthetic data to analyze it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">The Effectiveness of selected feature interaction on Real Data.</head><p>We define statistics_AUC to represent the importance of a feature interaction to the final prediction. For a given interaction, we construct a predictor only considering this interaction where the prediction of a test instance is the statistical CTR (#downloads/#impressions) of specified feature interaction in the training set. Then the AUC of this predictor is statistics_AUC with respect to this given feature interaction. Higher statistics_AUC indicates a more important role of this feature interaction in prediction. Then we visualize the relationship between statistics_AUC and α value. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we can find that most of the feature interactions selected by our model (with high absolute α value) have high statistics_AUC, but not all feature interactions with high statistics_AUC are selected. That is because the information in these interactions may also exist in other interactions which are selected by our model.  To evaluate the effectiveness of the selected interactions by our model, we also select the top-N (N is the number of second-order feature interactions selected by our model) interactions based on statistics_AUC and re-train the model with these interactions. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the performance of our model is much better than the model with selected interactions by statistics_AUC with same computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.2</head><p>The Effectiveness of selected feature interaction on Synthetic Data. In this section, we conduct a synthetic experiment to validate the effectiveness of selected feature interaction.</p><p>This synthetic dataset is generated from an incomplete poly-2 function, where the bi-linear terms are analogous to interactions between categories. Based on this dataset, we investigate (i) whether our model could find the important interactions (ii) the performance of our model compared with other factorization machine models.</p><p>The input x of this dataset is randomly sampled from N categories of m fields. The output y is binary labeled depending on the sum of linear terms and parts of bi-linear terms.</p><formula xml:id="formula_21">y = δ ( m i=1 w i x i + i, j ∈C v i, j x i x j + b + ϵ)<label>(16)</label></formula><formula xml:id="formula_22">δ (z) = 1, i f z ≥ threshold 0, otherwise<label>(17)</label></formula><p>The data distribution p(x), selected bi-linear term sets C and w, v, b are randomly sampled and fixed. The data pairs are i.i.d. sampled to build the training and test datasets. We also add a small random noise ϵ to the sampled data. We use FM and our model to fit the synthetic data. We use AUC to evaluate these models on the test dataset.</p><p>We choose m = 6, N = 60 to test the effectiveness of our model. Selected bi-linear term sets C is randomly initialized as C = {(x 0 , x 1 ), (x 2 , x 5 ), (x 3 , x 4 )}. <ref type="figure" target="#fig_3">Figure 4</ref> presents the performance comparison between our model and FM, which demonstrates the superiority of our model. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, our model could extract the important interactions precisely. The interactions in C have the highest α and some unimportant interactions (with α value 0) have been removed. AutoFM as a better model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Deployment &amp; Online Experiments (RQ4)</head><p>Online experiments were conducted in the recommender system of Huawei App Store to verify the superior performance of Au-toDeepFM. Huawei App Store has hundreds of millions of daily active users which generates hundreds of billions of user log events everyday in the form of implicit feedback such as browsing, clicking and downloading apps. In online serving system, hundreds of candidate apps that are most likely to be downloaded by the users are selected by a model from the universal app pool. These candidate apps are then ranked by a fine-tuned ranking model (such as DeepFM, AutoDeepFM) before presenting to users. To guarantee user experience, the overall latency of the above-mentioned candidate selection and ranking is required to be within a few milliseconds. To deploy AutoDeepFM, we utilize a three-node cluster, where each node is with 48 core Intel Xeon CPU E5-2670 (2.30GHZ), 400GB RAM and as well as 2 NVIDIA TESLA V100 GPU cards. Specifically, a ten-day AB test is conducted in a game recommendation scenario in the App Store. Our baseline in online experiments is DeepFM, which is a strong baseline due to its extraordinary accuracy and high efficiency which has been deployed in the commercial system for a long time.</p><p>For the control group, 5% of users are randomly selected and presented with recommendation generated by DeepFM. DeepFM is chosen as a strong baseline due to its extraordinary accuracy and high efficiency, which has been deployed in our commercial system for a long time. For the experimental group, 5% of users are presented with recommendation generated by AutoDeepFM. <ref type="figure">Figure 6</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> show the improvement of the experimental group over the control group with CTR (#downloads/#impressions) and CVR (#downloads/#users) respectively. We can see that the system is rather stable where both CTR and CVR fluctuated within 8% during the A/A testing. Our AutoDeepFM model is launched to the live system on Day 8. From Day 8, we observe a significant <ref type="figure">Figure 6</ref>: Online experimental results of CTR. improvement over the baseline model with respect to both CTR and CVR. The average improvement of CTR is 20.3% and the average improvement of CVR is 20.1% over the ten days of A/B test. These results demonstrate the magnificent effectiveness of our proposed model. From Day 18, we conduct again A/A test to replace our AutoDeepFM model with the baseline model in the experimental group. We observe a sharp drop in the performance of the experimental group, which once more verifies that the improvement of online performance in the experimental group is indeed introduced by our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A/A test A/A test A/B test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A/A test A/A test A/B test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation Study (RQ5)</head><p>4.7.1 Stability of α estimation across different seeds. In this part, we conduct experiments to check whether the trained value of α is stable across different random initializations. A stable estimation of α means that the model's decision on which interaction is important is not affected by the random seed. We run the search stage of AutoFM with different seeds on Avazu. The Pearson correlation of α estimated from different seeds is around 0.86, this validates that the estimation of α is stable. Without the use of BN for the feature interaction (which is essentially FwFM model), this Pearson correlation drop to around 0.65.   <ref type="table" target="#tab_7">Table 6</ref>. Recall that AutoFIS has two stages: search stage and re-train stage. To verify the effectiveness of the search stage of AutoFIS, we compare it with "Random" strategy, which selects feature interactions randomly. Similarly, in the re-train stage, we validate the advantages of BN and α. The relationship between different components in the two stages is presented in <ref type="table" target="#tab_7">Table 6</ref>. The performance of such variants presented in <ref type="table" target="#tab_8">Table 7</ref>. Note that for "Random" strategy, we choose the same number of interactions with AutoFM, and we try ten different "Random" strategies and average the results. We can get several conclusions:</p><p>(1) Comparing AutoFM-BN-α with Random+FM, we can see that selection by AutoFIS can always achieve better performance than Random selection with same number of interactions. It demonstrates that important interactions are identified by Aut-oFIS in the search stage.  <ref type="table">Table 8</ref>. The performance gap between Aut-oFM and Bi-AutoFM (and between AutoDeepFM and Bi-AutoDeepFM) demonstrates the superiority of one-level optimization over bi-level, with the reason stated in "One-level Optimization" section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we proposed AutoFIS to automatically select important 2 nd -and 3 r d -order feature interactions. The proposed methods are generally applicable to all the factorization models and the selected important interactions can be transferred to other deep learning models for CTR prediction. The proposed AutoFIS is easy to implement with marginal search costs, and the performance improvement is significant in two benchmark datasets and one private dataset. The proposed methods have been deployed onto the training platform of Huawei App Store recommendation service, with significant economic profit demonstrated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architectures of FM, DeepFM and IPNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Relationship between statistics_AUC and α value for each second-order interaction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training results of the synthetic experiments to verify</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Training results of the synthetic experiments to verifyAutoFM could find more important interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Online experimental results of CVR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 2 ) 3</head><label>23</label><figDesc>The performance gap between Random+FM and FM in Criteo dataset indicates that random selection on feature interactions may outperform the model keeping all the feature interactions under some circumstances, which supports our statement: removing some useless feature interactions could improve the performance.(3) The comparison between AutoFM and AutoFM-BN validates the effectiveness of BN in the re-train stage, where the reason is stated in "AutoFIS" section. (4) The performance gap between AutoFM-BN and AutoFM-BNα shows that α improve the performance, as it differentiates the contribution of different feature interactions in the re-train stage.Table 8: Comparison of one-level and bi-level optimization One-level V.S. bi-level optimization. In this section, we compare the one-level and bi-level optimization on AutoFM and the results are presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>28% * denotes statistically significant improvement (measured by t-test with p-value&lt;0.005) over baselines with same order. AutoFM compares with FM and AutoDeepFM compares with all baselines.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avazu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Criteo</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>AUC</cell><cell>log loss</cell><cell>top</cell><cell>time (s)</cell><cell>search + re-train cost (min)</cell><cell>Rel. Impr.</cell><cell>AUC</cell><cell>log loss</cell><cell>top</cell><cell>time (s)</cell><cell>search + re-train cost (min)</cell><cell>Rel. Impr.</cell></row><row><cell>FM</cell><cell>0.7793</cell><cell>0.3805</cell><cell>100%</cell><cell>0.51</cell><cell>0 + 3</cell><cell>0</cell><cell>0.7909</cell><cell>0.5500</cell><cell>100%</cell><cell>0.74</cell><cell>0 + 11</cell><cell>0</cell></row><row><cell>FwFM</cell><cell>0.7822</cell><cell>0.3784</cell><cell>100%</cell><cell>0.52</cell><cell>0 + 4</cell><cell>0.37%</cell><cell>0.7948</cell><cell>0.5475</cell><cell>100%</cell><cell>0.76</cell><cell>0 + 12</cell><cell>0.49%</cell></row><row><cell>AFM</cell><cell>0.7806</cell><cell>0.3794</cell><cell>100%</cell><cell>1.92</cell><cell>0 + 14</cell><cell>0.17%</cell><cell>0.7913</cell><cell>0.5517</cell><cell>100%</cell><cell>1.43</cell><cell>0 + 20</cell><cell>0.05%</cell></row><row><cell>FFM</cell><cell>0.7831</cell><cell>0.3781</cell><cell>100%</cell><cell>0.24</cell><cell>0 + 6</cell><cell>0.49%</cell><cell>0.7980</cell><cell>0.5438</cell><cell>100%</cell><cell>0.49</cell><cell>0 + 39</cell><cell>0.90%</cell></row><row><cell>DeepFM</cell><cell>0.7836</cell><cell>0.3776</cell><cell>100%</cell><cell>0.76</cell><cell>0 + 6</cell><cell>0.55%</cell><cell>0.7991</cell><cell>0.5423</cell><cell>100%</cell><cell>1.17</cell><cell>0 + 16</cell><cell>1.04%</cell></row><row><cell>GBDT+LR</cell><cell>0.7721</cell><cell>0.3841</cell><cell>100%</cell><cell>0.45</cell><cell>8 + 3</cell><cell>-0.92%</cell><cell>0.7871</cell><cell>0.5556</cell><cell>100%</cell><cell>0.62</cell><cell>40 + 10</cell><cell>-0.48%</cell></row><row><cell>GBDT+FFM</cell><cell>0.7835</cell><cell>0.3777</cell><cell>100%</cell><cell>2.66</cell><cell>6 + 21</cell><cell>0.54%</cell><cell>0.7988</cell><cell>0.5430</cell><cell>100%</cell><cell>1.68</cell><cell>9 + 57</cell><cell>1.00%</cell></row><row><cell>AutoFM(2nd)</cell><cell>0.7831*</cell><cell>0.3778*</cell><cell>29%</cell><cell>0.23</cell><cell>4 + 2</cell><cell>0.49%</cell><cell>0.7974*</cell><cell>0.5446*</cell><cell>51%</cell><cell>0.48</cell><cell>14 + 9</cell><cell>0.82%</cell></row><row><cell>AutoDeepFM(2nd)</cell><cell>0.7852*</cell><cell>0.3765*</cell><cell>24%</cell><cell>0.48</cell><cell>7 + 4</cell><cell>0.76%</cell><cell>0.8009*</cell><cell>0.5404*</cell><cell>28%</cell><cell>0.69</cell><cell>22 + 11</cell><cell>1.26%</cell></row><row><cell>FM(3rd)</cell><cell>0.7843</cell><cell>0.3772</cell><cell>100%</cell><cell>5.70</cell><cell>0 + 21</cell><cell>0.64%</cell><cell>0.7965</cell><cell>0.5457</cell><cell>100%</cell><cell>8.21</cell><cell>0 + 72</cell><cell>0.71%</cell></row><row><cell>DeepFM(3rd)</cell><cell>0.7854</cell><cell>0.3765</cell><cell>100%</cell><cell>5.97</cell><cell>0 + 23</cell><cell>0.78%</cell><cell>0.7999</cell><cell>0.5418</cell><cell>100%</cell><cell>13.07</cell><cell>0 + 125</cell><cell>1.14%</cell></row><row><cell>AutoFM(3rd)</cell><cell>0.7860*</cell><cell>0.3762*</cell><cell>25% / 2%</cell><cell>0.33</cell><cell>22 + 5</cell><cell>0.86%</cell><cell>0.7983*</cell><cell>0.5436*</cell><cell>35% / 1%</cell><cell>0.63</cell><cell>75 + 15</cell><cell>0.94%</cell></row><row><cell>AutoDeepFM(3rd)</cell><cell>0.7870*</cell><cell>0.3756*</cell><cell>21% / 10%</cell><cell>0.94</cell><cell>24 + 10</cell><cell>0.99%</cell><cell>0.8010*</cell><cell>0.5404*</cell><cell>13% / 2%</cell><cell>0.86</cell><cell>128 + 17</cell><cell>1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell>#instances</cell><cell>#dimension</cell><cell>#fields</cell><cell>pos ratio</cell></row><row><cell>Avazu</cell><cell>4 × 10 7</cell><cell>6 × 10 5</cell><cell>24</cell><cell>0.17</cell></row><row><cell>Criteo</cell><cell>1 × 10 8</cell><cell>1 × 10 6</cell><cell>39</cell><cell>0.50</cell></row><row><cell>Private</cell><cell>2 × 10 8</cell><cell>3 × 10 5</cell><cell>29</cell><cell>0.02</cell></row><row><cell cols="3">4.2 Experimental Settings</cell><cell></cell><cell></cell></row></table><note>4.2.1 Baselines and Evaluation Metrics. We apply AutoFIS to FM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance in Private Dataset. "Rel. Impr." is the relative AUC improvement over FM model.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>log loss</cell><cell>top</cell><cell>ReI. Impr</cell></row><row><cell>FM</cell><cell>0.8880</cell><cell>0.08881</cell><cell>100%</cell><cell>0</cell></row><row><cell>FwFM</cell><cell>0.8897</cell><cell>0.08826</cell><cell>100%</cell><cell>0.19%</cell></row><row><cell>AFM</cell><cell>0.8915</cell><cell>0.08772</cell><cell>100%</cell><cell>0.39%</cell></row><row><cell>FFM</cell><cell>0.8921</cell><cell>0.08816</cell><cell>100%</cell><cell>0.46%</cell></row><row><cell>DeepFM</cell><cell>0.8948</cell><cell>0.08735</cell><cell>100%</cell><cell>0.77%</cell></row><row><cell>AutoFM(2nd)</cell><cell>0.8944*</cell><cell>0.08665*</cell><cell>37%</cell><cell>0.72%</cell></row><row><cell cols="4">AutoDeepFM(2nd) 0.8979* 0.08560* 15%</cell><cell>1.11%</cell></row><row><cell cols="5">*  denotes statistically significant improvement (measured by t-test with p-value&lt;0.005).</cell></row></table><note>AutoFM compares with FM and AutoDeepFM compares with all baselines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of transferring interactions selected by Aut-oFM to IPNN. AutoIPNN(2nd) indicates IPNN with 2 nd -order interactions selected by AutoFM(2nd) and AutoIPNN(3rd) indicates IPNN with 2 nd -and 3 r d -order interactions selected by AutoFM(3rd).</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell cols="3">Avazu log loss time(s) AUC</cell><cell cols="2">Criteo log loss time(s)</cell></row><row><cell>IPNN</cell><cell>0.7868</cell><cell>0.3756</cell><cell>0.91</cell><cell>0.8013</cell><cell>0.5401</cell><cell>1.26</cell></row><row><cell cols="2">AutoIPNN(2nd) 0.7869</cell><cell>0.3755</cell><cell>0.58</cell><cell>0.8015</cell><cell>0.5399</cell><cell>0.76</cell></row><row><cell cols="4">AutoIPNN(3rd) 0.7885* 0.3746* 0.71</cell><cell cols="3">0.8019* 0.5392* 0.86</cell></row><row><cell cols="7">*  denotes statistically significant improvement (measured by t-test with p-value&lt;0.005).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison between the model with interactions selected by our model and by statistics_AUC on Avazu Dataset</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>log loss</cell></row><row><cell cols="2">Selected by statistics_AUC 0.7804</cell><cell>0.3794</cell></row><row><cell>Selected by AutoFM</cell><cell>0.7831</cell><cell>0.3778</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Different Variants.</figDesc><table><row><cell>Variants AutoFM AutoFM-BN AutoFM-BN-α Random+FM</cell><cell>search stage AutoFIS Random BN re-train stage α √ √ √ × √ × √ × √ × × × × √ × ×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison of different feature interaction selection strategies. : with fewer interactions, FM may have better performance. Effectiveness of components in AutoFIS. To validate the effectiveness of individual components in AutoFIS, we propose several variants, which are enumerated in</figDesc><table><row><cell>Model</cell><cell cols="2">Avazu AUC log loss</cell><cell cols="2">Criteo AUC log loss</cell></row><row><cell>FM</cell><cell>0.7793</cell><cell>0.3805</cell><cell>0.7909</cell><cell>0.5500</cell></row><row><cell>AutoFM</cell><cell cols="2">0.7831 0.3778</cell><cell>0.7974</cell><cell>0.5446</cell></row><row><cell>AutoFM-BN</cell><cell>0.7824</cell><cell>0.3783</cell><cell>0.7971</cell><cell>0.5450</cell></row><row><cell>AutoFM-BN-α</cell><cell>0.7811</cell><cell>0.3793</cell><cell>0.7946</cell><cell>0.5481</cell></row><row><cell>Random+FM</cell><cell>0.7781</cell><cell>0.3809</cell><cell>0.7940  *</cell><cell>0.5486</cell></row><row><cell>4.7.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Features in numerical form are usually transformed into categorical form by bucketing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">η ) · (η · ⟨e i , e j ⟩), where η is a real number.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Repeatable experiment code: https://github.com/zhuchenxv/AutoFIS 3 http://www.kaggle.com/c/avazu-ctr-prediction</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PARAMETER SETTINGS</head><p>For Avazu and Criteo datasets, the parameters of baseline models are set following <ref type="bibr" target="#b24">[25]</ref>. For AutoFM and AutoDeepFM we use the same hyper-parameters as the base models (i.e., FM and DeepFM accordingly) except for extra ones in AutoFM and AutoDeepFM. c=0.0005 mu=0.8 * Note: bs=batch size, opt=optimizer, lr=learning rate, k=embedding size, wt_init = initial value for α, wt_l1 = l 1 regularization on α, wt_l2 = l 2 regularization on α, t=Softmax Temperature, l2_a= L2 Regularization on Attention Network, net=MLP structure, LN=layer normalisation, BN=batch normaliation, c and mu are parameters in GRDA Optimizer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Cross: Making Use of Context in Recurrent Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A generalization of regularized dual averaging and its dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Kang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Cheng</surname></persName>
		</author>
		<idno>CoRR. abs/1909.10072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Kang Chao</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09358</idno>
		<title level="m">Directional Pruning of Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">XGBoost:A Scalable Tree Boosting System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengtze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar Deepak</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In DLRS@RecSys</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haishan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Practical Lessons from Predicting Clicks on Ads at Facebook. In ADKDD@KDD</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fieldaware Factorization Machines for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">3 IdiotsâĂŹ Approach for Display Advertising Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<ptr target="https://www.csie.ntu.edu.tw/r01922136/kaggle-2014-criteo.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11926</idno>
		<title level="m">StacNAS: Towards stable and consistent optimization for differentiable Neural Architecture Search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Zhang</surname></persName>
		</author>
		<editor>WWW. ACM</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">DARTS: Differentiable Architecture Search. In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1936" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Field-weighted factorization machines for click-through rate prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><forename type="middle">Lobos</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Product-Based Neural Networks for User Response Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Neural Interaction Function Search for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Kwok Yong Li Cho-Jui Hsieh Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Failures of Gradient-Based Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Shammah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3067" to="3075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
		<idno>ADKDD@KDD. 12</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3119" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
