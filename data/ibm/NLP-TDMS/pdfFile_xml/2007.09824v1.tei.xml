<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Gated and Bifurcated Stacked U-Net Module for Document Image Dewarping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hmrishav</forename><surname>Bandyopadhyay</surname></persName>
							<email>hmrishavbandyopadhyay@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Dasgupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibaran</forename><surname>Das</surname></persName>
							<email>†nibaran.das@jadavpuruniversity.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mita</forename><surname>Nasipuri</surname></persName>
							<email>mitanasipuri@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electronics and Telecomm. Engg</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Engg</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<addrLine>West Bengal</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Gated and Bifurcated Stacked U-Net Module for Document Image Dewarping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Document image dewarping</term>
					<term>warped document image rectification</term>
					<term>dense grid prediction</term>
					<term>stacked u-net</term>
					<term>gated networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Capturing images of documents is one of the easiest and most used methods of recording them. These images however, being captured with the help of handheld devices, often lead to undesirable distortions that are hard to remove. We propose a supervised Gated and Bifurcated Stacked U-Net module to predict a dewarping grid and create a distortion free image from the input. While the network is trained on synthetically warped document images, results are calculated on the basis of real world images. The novelty in our methods exists not only in a bifurcation of the U-Net to help eliminate the intermingling of the grid coordinates, but also in the use of a gated network which adds boundary and other minute line level details to the model. The end-to-end pipeline proposed by us achieves state-of-the-art performance on the DocUNet dataset after being trained on just 8 percent of the data used in previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the rising popularity of smartphones and portable cameras, digitized documents have become a part and parcel of the common man's life. Document images, although captured easily with the help of these cameras, often lose the ability to represent the actual document due to inconsistencies in structure, illumination, and camera angles. The high variance of these document images from a flatbed-scanned version of the same make them unsuitable for further information extraction and analysis.</p><p>Previous methods of dewarping with classical image processing techniques performed well when faced with standard curves or spherical distortions, but failed when dealing with sharp folds and corner-type distortions. To rectify document images distorted with simultaneous folds and curves, various deep learning architectures have been proposed <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>We propose a document image dewarping model which outstrips the previous models in both speed and dewarp quality. The specific contributions of our work can be summarized as: 1) Our network takes in 256x256 images as inputs to produce an unwarping grid which can be interpolated to reconstruct images at their original resolution. The parameters are learned efficiently and as such the model learns in just 8 thousand images, less than one-tenth of the dataset sizes in previous end-to-end methods. 2) We propose a bifurcated U-Net as the secondary U-Net of our stacked U-Net system to help in channel level segregation while predicting dense grid unwarps. 3) A gated branch of the primary U-Net is proposed following <ref type="bibr" target="#b2">[3]</ref> that enables the secondary U-Net to recognize lines and boundaries in the warped document image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Previous literature has studied the problem of document unwarping through both single and multi image tasks. Vision systems have been designed that make use of well calibrated stereo cameras or structured light projectors <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[7]</ref> to get insight into distortion factors present within the document. Specifically, Meng et. al. <ref type="bibr" target="#b7">[7]</ref> set up a platform with structured laser beams for acquiring curls in the warped document images. Although these systems give high quality results, their application is severely bound due to the limitation posed by additional hardware.</p><p>Tsoi and Brown <ref type="bibr" target="#b8">[8]</ref> came up with a way to reduce hardware by utilizing the boundary information from multi-view images and generated the rectified image. Multi view systems like these which require more than one image for reconstructing 3-D shapes can do without additional hardware, but would always require more than one image, which essentially serves as another limitation.</p><p>Dewarping methods involving single images although free from these limitations, pose a bigger problem as the two dimensional image is now used to get a three dimensional perspective of the document. Here, we find the use of not only classical image processing and machine learning methods, but also deep learning models.</p><p>Techniques involving classical image processing have been able to sufficiently dewarp linearly warped images of documents [9]- <ref type="bibr" target="#b11">[11]</ref>, but have been forced into a corner when non-linearly warped or folded-document images have been tested on them <ref type="bibr" target="#b12">[12]</ref>. While some of them have passed with considerably good values in one evaluation metric, they have been severely penalized in another. Ezaki et. al. <ref type="bibr" target="#b13">[13]</ref> proposed a global optimization based dewarping technique concentrated on converting warped lines to parallel ones and thus, rectifying non-linearly warped document images. Similar work involving curled text-line detection based techniques were demonstrated in <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>. Boundary fitting techniques like <ref type="bibr" target="#b16">[16]</ref>- <ref type="bibr" target="#b21">[21]</ref> , however, proved to be considerably more effective in solving this problem when compared with techniques involving detection and rectification on a line level.</p><p>An approach involving segmentation of text lines for detection of warps was proposed by Gatos et. al. <ref type="bibr" target="#b22">[22]</ref>. Similar works involved the warped text-line being represented as texture restricted by two smooth curved lines on the top and bottom and were described in <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. These methods however, involve techniques which rely far too much on the text to image ratio of the document. Typically, such segmentation methods were found to fail in documents with a larger proportion of images.</p><p>We find better text-line detection in <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> which can handle fairly complex page layouts. They take up iterative checks for alignment of text lines in images and make repairs wherever needed. These techniques, although being highly successful in evaluation metrics, are iterative and thus inherently slow. Moreover, their heavy reliance on the availability of clear boundaries make them impractical in real world scenarios.</p><p>CNN based methods for document dewarping were employed by Das et. al. in <ref type="bibr" target="#b27">[27]</ref> where the application was limited to detection of paper creases for rectification. The first end to end deep learning model for dewarping of document images was proposed by Ma et. al. in <ref type="bibr" target="#b0">[1]</ref>. Along with the development of an end to end model, <ref type="bibr" target="#b0">[1]</ref> also made available for the first time, a method of generating huge warped document datasets. Recently, Das et. al. came up with a way of generating more realistic document images with illumination effects in <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET</head><p>The recent availability of synthetic datasets has resulted in a sudden spur in the use of deep learning in this domain. We generate a synthetic dataset as proposed by <ref type="bibr" target="#b0">[1]</ref>. The task of generation of the dataset proceeds by first generating a perturbed mesh. This mesh then provides the sparse deformation field needed to build a dense warping map. This warping map when applied to the original image can generate a distorted image of the document. With the help of random warping maps , we generate 8K images consisting of folds and curves. To incorporate realistic effects on the generated data, we make use of texture images available in Describable Texture Dataset (DTD) <ref type="bibr" target="#b28">[28]</ref> and apply background textures to generated warped images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>Our Network as in <ref type="figure" target="#fig_1">Fig. 2</ref> makes significance changes to a stacked U-Net backbone in order to help recognize warpings in document images and to dewarp them. Specifically, we propose the use of a gated network for identification of lines and boundaries and pass them onto the second U-Net concatenated with data received from the first U-Net. We also make use of a bifurcation in the last decoder to get us fairly independent channel values for the dense grid prediction.</p><p>The network takes deformed document images as input I ∈ R h×w×3 and generates a dense grid prediction G ∈ R hxwx2 . With this grid, an unwarp of the input image can be performed to get the distortion free document. Broadly, the network can be divided into the primary and secondary stacks of the U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Primary U-Net</head><p>The primary U-Net as in <ref type="figure" target="#fig_2">Fig. 3</ref> or the first of the stacked U-Net series contains a set of up-sampling and down-sampling layers. The main deviation from a traditional U-Net can be seen in the presence of a gated network.</p><p>The Gated Network [ <ref type="figure">Fig. 4</ref>] extracts data from layers before the 2nd, 4th and 5th poolings. It primarily consists of Gated Convolutional Layers (GCLs) as proposed in <ref type="bibr" target="#b2">[3]</ref>. The presence of gates in the layers helps the model reject information The skip connections of a traditional U-Net in the U-Net modules are replaced by a CNN path with ReLU non-linearity to help in data extraction at different spatial levels before being fed to the concurrent decoders.</p><p>The primary U-Net module thus takes in the image of the deformed document I ∈ R 256×256×3 and passes it through a series of encoders to get the bottleneck of B ∈ R 1024×8×8 . The layers L 2 ∈ R 64×128×128 , L 4 ∈ R 256×32×32 , L 5 ∈ R 512×16×16 are extracted and go through the gated module G. Finally, the decoded outputs O ∈ R 2×256×256 , gated network outputs G o ∈ R, 16×256×256 and the initial convolution outputs X ∈ R 32×256×256 are concatenated to produce the output of the primary U-Net U 1 ∈ R 50×256×256 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Secondary Bifurcated U-Net</head><p>Convolutional networks fail spectacularly when dealing with coordinate data as shown by <ref type="bibr" target="#b29">[29]</ref>.To solve this while making grid predictions, <ref type="bibr" target="#b1">[2]</ref> uses the modules suggested in <ref type="bibr" target="#b29">[29]</ref>. However, we find that a more task specific network can be designed which can enhance the ability of CNNs while working with a dense grid prediction.</p><p>The general CNN works by summing computed data across all input channels for specific window sizes. Ultimately the number of channels in the output is the number of filters that the convolutional block contains. We came to the conclusion that using a single decoder in the final U-Net block would mean that although information is extracted in all blocks, only the last two convolutional filters would decode the grid values into their respective channels for the final output. To get round this issue, we came up with the usage of multiple decoder blocks for the single secondary U-Net encoder.</p><p>From our experiments, we found a spike in the results as soon as we shifted from a single decoder to multiple decoders with shared weights. This further strengthened our case as the decoder with shared weights, although restricted by weight sharing, had the advantage of being decoded separately right from the bottleneck layer. Finally, we found the best results when we lifted the restriction of weight sharing and made the decoders independent of each other. In our network, the encoder output is split into two and is sent separately to the two decoders. The output from these decoders is concatenated and normalized with the Tanh activation function to provide us the output of the network.</p><p>The output of the primary U-Net U 1 ∈ R 50×256×256 is fed as input to the secondary U-Net. The Bottleneck B ∈ R 1024×8×8 is split into B 1 ∈ R 512×8×8 and B 2 ∈ R 512×8×8 . These blocks go through the decoders to give outputs O 1 ∈ R 256×256×1 and O 2 ∈ R 256×256×1 , which are concatenated and normalized by a Tanh activation function to get the final grid g ∈ R 256×256×2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LOSS FUNCTIONS</head><p>The loss function that helps train our network can be expressed as a summation of an edge loss and a grid loss, weighted with an appropriate λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge Loss</head><p>The edge loss can be expressed as a binary classification loss as we try to predict if a pixel belongs to the 'edge class' or not. We make use of the Binary Cross Entropy loss function for this. Comparison of the output of the gated network is done by taking an edge detection function as the ground truth. The edge loss can be expressed in mathematical terms as:</p><formula xml:id="formula_0">L e = − 1 N N i=0 y i .log(ŷ i ) + (1 − y i ).log(1 −ŷ i )</formula><p>Whereŷ i represents pixel wise value of the predicted output and y i gives the ground truth measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grid Loss</head><p>The grid loss trains not only the secondary U-Net but also the primary one as the loss is propagated throughout the model. Although previous methods as <ref type="bibr" target="#b0">[1]</ref> suggest the use of Mean Absolute Error (L1) to train the network against the ground truth grid, from our experiments we find the Least Squares method (L2) to be more robust. In fact, using Least Squares loss also helps our training algorithm to converge to a minima in a smoother way as compared to using L1 loss. Mathematically, we express grid loss as:</p><formula xml:id="formula_1">L g = 1 N N i=0 (g i −ĝ i ) 2</formula><p>Expressing the combined loss function, we have</p><formula xml:id="formula_2">L = 1 N N i=0 (g i −ĝ i ) 2 −λ. 1 N N i=0 y i .log(ŷ i )+(1−y i ).log(1−ŷ i )</formula><p>We take the value of λ as 0.9 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION SCHEME</head><p>For the evaluation of our network and comparison with images of different sizes, previous methods suggest use of evaluation metrics like image similarity and Optical Character Recognition (OCR). OCR, however is highly dependent on the image to text ratio in the document and fails when the document comprises primarily of images. The limitation of application of this metric in all images is why we choose image similarity as the evaluation metric for our methods.</p><p>In order to compare the similarity of output images and the scanned ground truth, we make a comparative study of results from image similarity metrics like MS-SSIM (Multi-Scale Structural Similarity Index), LD (Local Distortion), and SSIM (Structural Similarity Index) at various pyramid levels of the images.Although MS-SSIM is essentially a weighted average of SSIM applied across levels, we take note of SSIM values in order to realize how the unwarping quality varies along different scales of the image. VII. RESULTS</p><p>The MS-SSIM evaluations are performed on the DocUNet Image dataset <ref type="bibr" target="#b0">[1]</ref>. A 5-level-pyramid is used for calculating MS-SSIM where the weight for each level is set at 0.0448, 0.2856, 0.3001, 0.2363, 0.1333 respectively following the original implementation <ref type="bibr" target="#b31">[30]</ref>.</p><p>The official implementation for the network discussed in here has been made open source and can be accessed at 'https://github.com/DVLP-CMATERJU/RectiNet' Although comparison has been made across DocUNet <ref type="bibr" target="#b0">[1]</ref>, DewarpNet <ref type="bibr" target="#b1">[2]</ref>, and our method, DewarpNet uses the Doc3D <ref type="figure">Fig. 7</ref>. Rows from top to bottom: Cropped Images; DewarpNet Results; Our Results; Ground Truth dataset while we use the DocUNet dataset for training. As due to unforeseen circumstances the Doc3D dataset has accessibility issues, we have been forced to restrict our experiments to the DocUNet dataset only. The fact that the Doc3D dataset is much better and more realistic than the DocUNet dataset, as seen in their tests, must be taken into consideration while comparing the results of both methods.</p><p>It should also be noted that the results in <ref type="bibr" target="#b1">[2]</ref> are on specific 880x680 area based resizes while we compare results on full resolution scanned images. This might account for any differences in our results from their tabulated data. Furthermore, the results in DocUNet <ref type="bibr" target="#b0">[1]</ref> are results we obtained from their tabulations as their code was not made publicly available. As such, their results still represent values in 880x680 area patches and are for reference only.</p><p>We observe that our model achieves state-of-the art results in terms of SSIM values. It dominates the SSIM board till before the 4th down-sample, after which DewarpNet takes over. We would like to draw attention to the fact that the fourth down-sample would mean images have been reduced to 1 256 of their original area and as such have been blurred almost completely. In-spite of leading in SSIM for 3 levels, we find our MS-SSIM metric lower than that of DewarpNet as DewarpNet's SSIM values for the 4th and 5th down-sample  are much higher than ours in the previous three levels. Our model performs considerably better when compared with <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b32">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ABLATION EXPERIMENTS</head><p>In order to test the effectiveness of our methods, we strip the model of the gated network and set up shared weights in the bifurcated U-Net . Aligning with our line of thought, we observe that the MS-SSIM values show a dip by 6.40% and 3.8 % when we remove the gated network and add the shared weights respectively. The exact values can be seen in <ref type="table" target="#tab_0">Table  III</ref>. It should be noted that these values are lower than the reported values from <ref type="bibr" target="#b0">[1]</ref> although they have similar backbone because of the difference in our training dataset lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION AND FUTURE WORK</head><p>In this paper, we presented a model capable of predicting dense grid mapping for dewarping 2D document images. We demonstrated the effectiveness of a bifurcated U-Net for dense grid predictions when compared to a traditional U-Net. We further enhanced our methods by the addition of a gated network for incorporating edge and line level details. The effectiveness of our model was exhibited when we reached DewarpNet level results while training our network on only 8K images.</p><p>We find our results limited in certain respects. Comparing DewarpNet and our method, we observe that our method leaves out boundaries from the original document image while DewarpNet inculcates extra regions in the output. These strongly signify the need of a document localization module that can correctly identify and mark document boundaries.</p><p>Also in our experiments, we see that MS-SSIM as a metric does not provide as much attention to line level detail as it does to overall image structure, texture etc. The area dependency of MS-SSIM and LD also causes them to give highly varied results for the same distortion level in images of different areas. Thus, future work on an area independent standardized metric is highly necessary for proper evaluation of results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Rectified Images on the basis of Inputs supplied to the end-to-end network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Complete Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CFig. 3 .</head><label>3</label><figDesc>Primary U-Net Fig. 4. Gated Convolutional Network not related to edges or boundaries. As a result of this, we have layers that deal only with edge data. The GCL works by forming attention maps through the layers extracted and formulates convolutions based on them. Both attention map computations and GCLs being differentiable, backpropagation can be performed end-to-end. The edge level detail passed by the gated network serves to 'inform' the CNN of boundaries and orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Secondary U-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Input Image (b) Results from Das et. al. (c) Image edges from gated network (d) Results from end-to-end model Comparison with Das et. al. [27]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison with DewarpNet Results. Our Model can be seen to have a higher SSIM till the 3rd level, after which, DewarpNet gets the upper hand. Level here refers to Gaussian Pyramid levels. The first level is the original image while the nth level has been down-sampled n-1 times .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SSIM</head><label>I</label><figDesc>VALUES ON VARYING LEVELS</figDesc><table><row><cell>Level</cell><cell>Our Method</cell><cell>DewarpNet [2]</cell></row><row><cell>1</cell><cell>0.548915</cell><cell>0.493146</cell></row><row><cell>2</cell><cell>0.467136</cell><cell>0.433653</cell></row><row><cell>3</cell><cell>0.39162</cell><cell>0.387747</cell></row><row><cell>4</cell><cell>0.332977</cell><cell>0.369569</cell></row><row><cell>5</cell><cell>0.302610</cell><cell>0.464170</cell></row><row><cell>6</cell><cell>0.387984</cell><cell>0.575128</cell></row><row><cell>7</cell><cell>0.504144</cell><cell>0.607561</cell></row><row><cell>8</cell><cell>0.560574</cell><cell>0.586102</cell></row><row><cell>9</cell><cell>0.541162</cell><cell>0.546075</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MS</head><label>II</label><figDesc>-SSIM AND LD VALUES ON COMPARISON WITH OTHER METHODS</figDesc><table><row><cell>Method</cell><cell>MS-SSIM ↑</cell><cell>LD↓</cell></row><row><cell>Tian et. al. [31]</cell><cell>0.13*</cell><cell>33.69</cell></row><row><cell>DocUNet [1]</cell><cell>0.410*</cell><cell>14.08</cell></row><row><cell>Our Method</cell><cell>0.415</cell><cell>13.2</cell></row><row><cell>DewarpNet [2]</cell><cell>0.437</cell><cell>8.98</cell></row><row><cell cols="3">*: Results obtained from research reports where images have been scaled</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III MS</head><label>III</label><figDesc>-SSIM VALUES ON DIFFERENT FEATURES OF NETWORK</figDesc><table><row><cell>Feature</cell><cell>MS-SSIM</cell></row><row><cell>Shared Weights accross Decoder</cell><cell>0.399</cell></row><row><cell>Gated Network removal</cell><cell>0.388</cell></row><row><cell>Actual Method</cell><cell>0.415</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Docunet: Document image unwarping via a stacked u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B J W D S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dewarpnet: Single-image document unwarping with stacked 3d and 2d regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilkrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.05740" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shape reconstruction and image restoration for non-flat surfaces of documents with a stereo vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kawarago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="482" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Document capture using stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM Symposium on Document Engineering, ser. DocEng 04</title>
		<meeting>the 2004 ACM Symposium on Document Engineering, ser. DocEng 04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/1030397.1030434</idno>
		<ptr target="https://doi.org/10.1145/1030397.1030434" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document restoration using 3d shape: a general deskewing algorithm for arbitrarily warped documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Seales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active flattening of curved document images via two structured beams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3890" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view document rectification using boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document image de-warping for text/graphics recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="348" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document flattening through grid modeling and regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="971" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast skew correction technique for camera captured business card images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasipuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 Annual IEEE India Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document image dewarping contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Int. Workshop on Camera-Based Document Analysis and Recognition</title>
		<meeting><address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dewarping of document image by global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Document Analysis and Recognition (ICDAR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="302" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document image dewarping using robust estimation of curled text lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Document Analysis and Recognition (ICDAR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1001" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document image dewarping based on line estimation for visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kakumanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourbakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="625" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model based book dewarping method to handle 2d images captured by a digital camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="158" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Model-based dewarping method and apparatus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Heaney</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Rapelje</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="page">604</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 7,330</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Usage of continuous skeletal image representation for document images de-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Masalovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mestetskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Camera-Based Document Analysis and Recognition</title>
		<meeting>International Workshop on Camera-Based Document Analysis and Recognition<address><addrLine>Curitiba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A book dewarping system by boundary-based 3d surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="403" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Method and apparatus providing perspective correction and/or image dewarping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huggett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kirsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="page">998</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 8,411</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Indexing of historical document images: Ad hoc dewarping technique for handwritten text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bolelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Italian Research Conference on Digital Libraries</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation based recovery of arbitrarily warped document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ntirogiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="989" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image dewarping and text extraction from mobile captured distinct documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chethan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="330" to="337" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Method, apparatus, and computer-readable recording medium for converting document image captured by using camera to dewarped document image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Seo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page">211</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 9</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel word spotting method based on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text-line detection in camera-captured document images using the state estimation of connected components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5358" to="5368" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The common fold: utilizing the four-fold to dewarp printed documents from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sudharshana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilkrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Symposium on Document Engineering</title>
		<meeting>the 2017 ACM Symposium on Document Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>abs/1807.03247</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<ptr target="http://arxiv.org/abs/1807.03247" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectification and 3d reconstruction of curved document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
