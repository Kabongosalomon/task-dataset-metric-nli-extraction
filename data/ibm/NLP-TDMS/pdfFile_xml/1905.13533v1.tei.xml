<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autonomous Human Activity Classification from Ego-vision Camera and Accelerometer Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-28">28 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Eng. and Computer Science</orgName>
								<orgName type="institution">Syracuse University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senem</forename><surname>Velipasalar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical Eng. and Computer Science</orgName>
								<orgName type="institution">Syracuse University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Autonomous Human Activity Classification from Ego-vision Camera and Accelerometer Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-28">28 May 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been significant amount of research work on human activity classification relying either on Inertial Measurement Unit (IMU) data or data from static cameras providing a third-person view. Using only IMU data limits the variety and complexity of the activities that can be detected. For instance, the sitting activity can be detected by IMU data, but it cannot be determined whether the subject has sat on a chair or a sofa, or where the subject is. To perform fine-grained activity classification from egocentric videos, and to distinguish between activities that cannot be differentiated by only IMU data, we present an autonomous and robust method using data from both egovision cameras and IMUs. In contrast to convolutional neural network-based approaches, we propose to employ capsule networks to obtain features from egocentric video data. Moreover, Convolutional Long Short Term Memory framework is employed both on egocentric videos and IMU data to capture temporal aspect of actions. We also propose a genetic algorithm-based approach to autonomously and systematically set various network parameters, rather than using manual settings. Experiments have been performed to perform 9-and 26-label activity classification, and the proposed method, using autonomously set network parameters, has provided very promising results, achieving overall accuracies of 86.6% and 77.2%, respectively. The proposed approach combining both modalities also provides increased accuracy compared to using only egovision data and only IMU data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Proposed Method</head><p>We present a new model architecture to process firstperson, also known as egocentric, images and IMU data. The proposed architecture can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is composed of our proposed recurrent CapsNet (for processing images), an LSTM network (for processing IMU data), and fully connected layers. In addition, we also propose and apply a Genetic Algorithm (GA)-based approach to autonomously and simultaneously optimize multiple parameters of our network architecture. These parameters are shown in parentheses with red color in <ref type="figure" target="#fig_0">Fig. 1</ref>. For instance, the parameters for the fully connected layers, and the primary capsules are examples of the parameters autonomously set by our proposed approach.</p><p>Sabour et al. <ref type="bibr" target="#b1">[2]</ref> introduced the Capsule Networks (Cap-sNets) to explore spatial relationships between features, and reported state-of-the-art performance on the MNIST database. CapsNets <ref type="bibr" target="#b1">[2]</ref> were used for image classification on individual images, whereas our goal is to perform finegrained activity classification from video data. Thus, in this paper, instead of using a single image with the original CapsNet, we propose a Recurrent CapsNet (RecCapsNet), which takes a sequence of images as input. We implement a 2D Convolutional LSTM (convLSTM) <ref type="bibr" target="#b2">[3]</ref> layer to extract features and capture the temporal aspect. For robustness, we use multiple digit/class layers instead of using only a single digit layer as was done in <ref type="bibr" target="#b1">[2]</ref>. In order to prevent gradient vanishing, we remove the squash function for digit layers and implement ReLu activation function instead.</p><p>As seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, 16 consecutive images are passed through a 2D convolutional layer separately. The size of each input image is 36×36. Then, the output for each image is sent to multiple primary capsules, the number of which is determined by our GA. When 16 consecutive images are formed, 50% overlap is used throughout the video. The number of convolutional units for each primary capsule is also determined by the GA. The output from the primary capsule layer is then sent through two digit/class layers, whose parameters are set by the GA. We then apply a Convolutional LSTM layer, followed by a fully connected (FC) layer, for the analysis of the egocentric video data.</p><p>For the decoder part, we apply 16 sub-decoders to each image frame. Each sub-decoder has the same structure with the decoder of the original CapsNet except the sigmoid output is 1296 (36 × 36).</p><p>As for the IMU, data from 16 consecutive time frames is used. Each of the 16 IMU data vectors has 36 components obtained by concatenating data from the four IMU sensors. Each IMU sensor contributes nine entries from accelerometer, gyroscope and magnetometer measurements. The time stamps are provided for camera and IMU data in the CMU-MMAC dataset. To align the camera and IMU data, for a given camera image, the IMU time stamp that is closest to the camera time stamp is found. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Autonomously and Simultaneously Refining the Network Parameters</head><p>The overall structure of the proposed method to refine the network parameters is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this approach, a Genetic Algorithm (GA) is used to make a decision from a set of discrete choices. The complete set of network parameters refined by the GA together with the discrete set of values that they can take are shown in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Initial Population and Evaluation</head><p>The first generation of the networks, N 1 = {N 1 , N 2 , ..., N nm }, where n m is the number of models, is generated by randomly choosing the values of parameters from the possible choices. The value of n m was set to be 10 in our experiments. Each generated network model N i is evaluated by the fitness function f (N i ), which is a measure of the accuracy of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Selection</head><p>t-many top-ranked models are selected first, and then rmany models are selected randomly from the rest of the network models. Then, d-many models are dropped to prevent over-fitting and getting stuck at a local optimum. The remaining selected models are the parent models (P ), which will be used to create new models for the next generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Crossover and Mutation</head><p>Crossover is applied to generate n m -many child network models from the parents. As opposed to always choosing two parents randomly from the parent pool, we associate a counter C P with each parent P , and initialize it to zero. This counter is incremented by one each time a parent is used for crossover. First, two parents are selected randomly from the t + r − d many parents. A new 'child' network is generated from the parents via crossover, and the counters of the parents are incremented by one. Then, two parents, whose counter is still zero, are selected randomly from the parent pool. Another network is generated from them via crossover, and the counters of the parents are incremented. If there is only one network model left with counter equal to zero, and the number of children is still less than n m , then this model is chosen as one of the parents, and the other parent is chosen randomly from the rest of the models who have a counter value of one. If there are no more parents left with counter equal to zero, and the number of children is still less than n m , then two parents, whose counter is one, are picked randomly, and their counter is incremented to two after crossover. This process is repeated until the number of children models reaches n m .</p><p>The crossover between parent models a and b is performed, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, by using a single-point crossover. After all the n m -many child networks are obtained, mutation is performed. The values of the parameters corresponding to randomly chosen k-many indices are randomly changed to one of the possible choices shown in Table 1. The value of k was chosen to be 3 in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experimental Results</head><p>We have used CMU Multi-Modal Activity (CMU-MMAC) database <ref type="bibr" target="#b0">[1]</ref>, which contains data from multimodal sensors monitoring human subjects preparing food. 25 subjects were recorded cooking five different recipes. The sensor modalities used for data collection include three high resolution static cameras, two low-resolution static cameras, one wearable camera, five microphones and IMUs. In our experiments, we used the egocentric camera data and the wired IMU data. We resized the image frames from camera to 36 × 36 pixels. We down-sampled the IMU data to make the measuring frequency the same with the egocentric camera (30 Hz). Then, we synchronized/aligned the IMU data with camera data.</p><p>We performed two sets of experiments by using 9 and 26 different activity classes. The name of the activities for each case can be seen in the confusion matrices in Figures 4 and 5. Example images are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. As can be seen, especially for the 26-class case, the activities involved are very close in the 'activity space', and this fine-grain classification is a very challenging problem.</p><p>A total of 10 videos from five subjects (2 videos per subject) have been used for training and testing. Videos from each subject were randomly divided so that 70%, 20%, 10% of the samples were allocated for training, validation and testing, respectively.</p><p>We first performed classification with manually preset network parameters, and then with the parameters determined autonomously by our GA-based approach described above. The overall accuracies from these experiments are summarized in <ref type="table" target="#tab_1">Table 2</ref>, wherein the accuracy is the ratio of all correctly classified instances to the total number of instances. When we use our proposed GA-based approach to autonomously set the various parameters of the network, this provides higher accuracy for both 9-class and 26-class labeling. Thus, the remainder of the results are presented for when the parameters are set with our GA-based approach. The confusion matrices for the 9-and 26-class activity classification are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> and 5, respectively. When subjects interact with larger objects, and movements are faster, a higher accuracy is achieved compared to slower  movements and interacting with smaller objects. For instance, it is harder to detect 'twisting cap on' and 'twisting cap off' actions, since the cap is always occluded by hand. As another example, actions such as cracking egg are harder to classify, since the egg is much smaller than the bowl.</p><p>In addition, as expected, higher overall precision and recall rates are achieved for 9-class labeling, since activities are much closer to each other and harder to differentiate for the 26-class labeling case. In <ref type="figure" target="#fig_6">Fig. 7</ref>, we show example images for the activities that are confused with each other in the 26-class labeling case (based on the confusion matrix in <ref type="figure" target="#fig_4">Fig. 5</ref>). These images illustrate once more the difficulty of performing very fine-grained activity classification. The first row shows taking a small cup vs. big cup. The second row shows walking to the fridge vs. closing the fridge, and the third row shows pouring into pan vs. putting the pan into the oven. As can be seen, these are very similar looking activities, and the proposed approach still provides very promising results for the 26-class labeling.</p><p>After setting the various network parameters by our GAbased approach, we performed a comparison of our proposed Rec-CapsNets method with using VGG16 features.  For this comparison, instead of employing the proposed RecCapsNet, we extracted image features from 16 consecutive image frames by using the convolutional layers of the CNN-based VGG16 <ref type="bibr" target="#b3">[4]</ref> without the top layers. We also used CapsNet on individual frames. The results are summarized in <ref type="table" target="#tab_2">Table 3</ref> for 9-label classification. As can be seen, using our proposed RecCapsNet provides a higher accuracy then using the VGG16 features. Moreover, to show the improvement provided by using multiple sensor modalities, we also obtained results by using each sensor modality by itself, namely by using only IMU data and only cam-era data. As can be seen in <ref type="table" target="#tab_2">Table 3</ref>, the proposed approach provides 29.07%, 20.29% and 19.16% increase in accuracy compared to using only IMU data, only egocentric camera data with VGG16 features and only egocentric camera data with CapsNet features, respectively. We also compared our results with two other works <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b4">[5]</ref>, which use the same MMAC dataset. In general, a direct comparison would not be commensurate, since they either employ different sets of sensors and handcrafted features, or the annotations are different. The reported accuracy in <ref type="bibr" target="#b4">[5]</ref>, from egocentric camera data, is 37.92% for 28 classes. When data from wearable camera as well as the multiple static cameras, watching the subjects, are used, the reported average accuracy is 54.62% <ref type="bibr" target="#b4">[5]</ref>. Spriggs et al. <ref type="bibr" target="#b5">[6]</ref> report an accuracy of 57.8% from 29 classes. Overall, our proposed method provides a significant improvement without relying on the static cameras watching the targets, which could also be important to alleviate privacy concerns. Moreover, using the proposed GA-based approach not only provides a way to systematically set the network parameters, but also improves the performance further compared to using the manually set parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusion</head><p>We have presented an autonomous method for fine-grain activity classification by using data from both egovision cameras and IMUs. We have employed capsule networks to obtain features from egocentric videos. We have also proposed a genetic algorithm-based approach to autonomously set various network parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Details of the proposed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The structure of the proposed Genetic Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Crossover process for the GA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Confusion matrix for 9-class scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Confusion matrix for 26-class scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Example images from the CMU-MMAC dataset. Rows: (1) using fridge, (2) taking eggs, (3) pouring into big bowl, (4) pouring into a measuring cup, (5) twisting cap (on or off).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Example challenging cases causing confusion. Columns: (1) taking a small cup (top) vs. big cup (bottom), (2) walking to fridge (top) vs. closing fridge (bottom), (3) pouring into pan (top) vs. putting pan into oven (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameters Autonomously Chosen by the GA</figDesc><table><row><cell>optimizers</cell><cell>{"adam", "rmsprop", "adagrad", "adadelta"}</cell></row><row><cell>activation functions</cell><cell>{"relu", "leaky relu", "sigmoid", "tanh"}</cell></row><row><cell>batch normalization</cell><cell>{True, False}</cell></row><row><cell>dropout</cell><cell>{True, False}</cell></row><row><cell>max pooling</cell><cell>{True, False}</cell></row><row><cell>kernel size</cell><cell>{3, 6, 9}</cell></row><row><cell>kernel stride</cell><cell>{1, 2, 3}</cell></row><row><cell>number of conv filters</cell><cell>{32, 64, 128 ... 512}</cell></row><row><cell>number of dense neurons</cell><cell>{32, 64, 128, 256}</cell></row><row><cell>number of lstm units</cell><cell>{16, 32, 64 ... 256}</cell></row><row><cell>dimension of capsules</cell><cell>{2, 4, 8, 16}</cell></row><row><cell cols="2">number of primary channels {16, 32, 64}</cell></row><row><cell>number of conv layers</cell><cell>{3,6}</cell></row><row><cell>number of dense layers</cell><cell>{1,3}</cell></row><row><cell>number of LSTM layers</cell><cell>{1,3}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overall accuracies for the 9-and 26-class labeling with and without using the proposed GA-based parameter setting</figDesc><table><row><cell></cell><cell>9-class</cell><cell></cell><cell></cell><cell>26-class</cell></row><row><cell></cell><cell cols="4">Preset prm. GA-based prm. Preset prm. GA-based prm.</cell></row><row><cell>Acc.</cell><cell>84.2%</cell><cell>86.6%</cell><cell>75.7%</cell><cell>77.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Accuracy rates from different modalities and approaches for 9-label classification</figDesc><table><row><cell>Sensor Modality</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>IMU only</cell><cell>LSTM</cell><cell>57.57%</cell></row><row><cell>Camera only</cell><cell>VGG16 CapsNet</cell><cell>66.35% 67.48%</cell></row><row><cell>Camera and IMU</cell><cell cols="2">VGG16 &amp; LSTM RecCapsNet &amp; LSTM (Proposed) 86.64% 82.97%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Guide to the carnegie mellon university multimodal activity (cmu-mmac) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Bargteil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Macey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beltran</surname></persName>
		</author>
		<idno>CMU-RI-TR-08-22</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conf. on NIPS</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and A new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conf. on NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5622" to="5632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition in the presence of one egocentric and multiple static cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recogn. Workshops. IEEE Computer Society Conf. on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
