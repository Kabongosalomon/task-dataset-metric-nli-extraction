<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Domain Few-Shot Learning with Meta Fine-Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cai</surname></persName>
							<email>jjcai@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><forename type="middle">Sheng</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pensees Pte Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Domain Few-Shot Learning with Meta Fine-Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we tackle the new Cross-Domain Few-Shot Learning benchmark proposed by the CVPR 2020 Challenge. To this end, we build upon state-of-the-art methods in domain adaptation and few-shot learning to create a system that can be trained to perform both tasks. Inspired by the need to create models designed to be fine-tuned, we explore the integration of transfer-learning (fine-tuning) with metalearning algorithms, to train a network that has specific layers that are designed to be adapted at a later fine-tuning stage. To do so, we modify the episodic training process to include a first-order MAML-based meta-learning algorithm, and use a Graph Neural Network model as the subsequent meta-learning module to compare the feature vectors. We find that our proposed method helps to boost accuracy significantly, especially when coupled with data augmentation. In our final results, we combine the novel method with the baseline method in a simple ensemble, and achieve an average accuracy of 73.78% on the benchmark. This is a 6.51% improvement over existing benchmarks that were trained solely on miniImagenet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decade, vast improvements to visual recognition systems have been achieved by training deep neural networks on ever-expanding training data-sets. Often, the ability of these neural network models to generalize directly depends on the size and variance of the training data-set. Unfortunately, acquiring a large training data-set is costly due to the need for human annotation. Furthermore, when dealing with rare examples in medical images (e.g. rare diseases) or satellite images (e.g. oil spills), the ability to obtain labelled samples is limited. Moreover, there is vast scope for improvement, as the human visual system is far less data-hungry than current deep learning methods.</p><p>To address the limitations of traditional deep learning, few-shot learning methods have emerged to train models that predict new classes by seeing only a few labelled images per class. These methods have shown promising im-provements over the last 5 years.</p><p>However, existing few-shot learning methods have been developed with the assumption that the training and test data-set arise from the same distribution. Domain shift would thus be an additional problem as it may prevent the robust transfer of features.</p><p>Hence, the CVPR 2020 challenge has introduced a new benchmark that aims to test for generalization ability across a range of vastly different domains, with domains from natural and medical images, domains without perspective, and domains without color <ref type="bibr" target="#b5">[6]</ref>. This robust evaluation framework allows one to truly test for how few-shot learning models do when faced with sharp domain shifts. This is in contrast to previous attempts at considering domainadaptation <ref type="bibr" target="#b1">[2]</ref>, as the training and test domains previously used were still fairly close to each other.</p><p>The main contribution of this paper is the integration of fine-tuning into the episodic training process by exploiting a first-order MAML-based meta-learning algorithm (henceforth "Meta Fine-Tuning"). This is done so that the network learns a set of initial weights that be easily fine-tuned on the support-set of the test domain.</p><p>Second, this paper integrates the above Meta Fine-Tuning algorithm into a Graph Neural Network that exploits the non-Euclidean structure of the relation between the support set and the query samples.</p><p>Third, as the baseline code-base only implements data augmentation for the training process, we implement data augmentation on the support set during fine-tuning, and achieve a further improvement in accuracy.</p><p>Finally, we combine the above method with a modified fine-tuning baseline method, and combine them into an ensemble to jointly make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relevant Work</head><p>The key method that this paper will build on is Graph Neural Networks. Graph-based convolutions can create more flexible representations of data beyond a simple Euclidean space <ref type="bibr" target="#b0">[1]</ref>. For the context of few-shot learning, after obtaining image features through deep learning networks, the problem can be reconstrued as a belief propaga-tion problem. Under this framework, labels from labelled support examples are propagated to unlabelled query examples. Hence, by representing the set of support examples as a densely-connected undirected graph, we can add each query example in, and learn edge weights <ref type="bibr" target="#b4">[5]</ref>.</p><p>Model-Agnostic Meta-Learning (MAML) was developed to train a network to learn a set of internal representations that can easily be adapted <ref type="bibr" target="#b3">[4]</ref>. It has been shown that the first-order approximations of the MAML algorithms, such as Reptile <ref type="bibr" target="#b6">[7]</ref>, that ignore second-order derivatives perform as well on established benchmarks.</p><p>A key method for domain adaptation is to fix earlier feature layers, and fine-tune later feature layers on the support examples <ref type="bibr" target="#b5">[6]</ref>. This could help transfer high-level features, while retraining domain-specific features.</p><p>Averaging the prediction scores of different models to reduce variance has been well-documented to achieve higher accuracy <ref type="bibr" target="#b2">[3]</ref>, which is why this paper explores the combination of models with different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Neural Networks</head><p>The Meta-Learning module we use is the Graph Neural Network, which has been used for cross-domain few-shot learning <ref type="bibr" target="#b7">[8]</ref>. To begin, I use a linear layer to project the feature vectors of dimensional F onto a lower-dimensional space d k . Then, the GNN takes in the input signal S ∈ R Ns+1 where N s is the number of support samples, with 1 vertex for the query sample. Then, a graph convolution layer GC(.) is performed with linear operations on local signals. This produces an output X (k+1) = GC(X (k) ), with the GC layer having d k × d k+1 parameters. To learn edge features, a MLP takes in the absolute difference between the the output vectors of vertices in the graph. For more details, refer to the GNN few-shot learning paper <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta Fine-Tuning</head><p>The core idea of meta-fine tuning is that instead of finetuning a pre-trained model that was not trained explicitly for fine-tuning, we can use meta-learning to find a set of weight initializations that are intended to be fine-tuned. To this end, we apply and adapt the first-order MAML algorithm <ref type="bibr" target="#b6">[7]</ref> and simulate the episodic training process. A first-order MAML algorithm can achieve comparable results with the secondorder algorithm at a lower computational cost.</p><p>The algorithm is model-agnostic and can be used with any existing metric-learning module. However, typically a metric-learning module with weights should be used, as a completely non-parametric module like Prototypical Networks, which uses nearest centroid, may not be able to compare subsequent fine-tuned image features in a robust way. In this paper, the GNN described above is used because it , g m for all model parameters using Adam Update initial parameters using learning rate θ:</p><formula xml:id="formula_0">φ f (L−k) = φ f (L−k) − θg f (L−k) φ f (k) = φ f (k) − θg f (k) φ m = φ m − θg m end</formula><p>is a highly trainable and flexible meta-learning module, and hence it can learn how to compare features that have been fine-tuned when trained on the fine-tuning process.</p><p>The method can also be applied to a model of any backbone depth, and you can freeze up to any number of layers. For this paper, we freeze the last ResNet block in ResNet10.</p><p>For further visualization, we include the figure below.  95.76% ± 0.65% 87.67% ± 0.44% 59.95% ± 0.45% 31.63% ± 0.49% 50 97.87% ± 0.48% 90.93% ± 0.45% 65.04% ± 0.47% 37.03% ± 0.50% <ref type="table">Table 2</ref>. Previous Benchmark's Best Model "Ft-Last1" trained on MiniImagenet from <ref type="bibr" target="#b5">[6]</ref> ing the episodic training loss. At prediction stage on the test domain, all layers in the ResNet10 are frozen in step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation</head><p>For data augmentation during training, we stick to the default parameters in the codebase. For data augmentation during testing, we sample 17 additional images from the support images (which we know the labels of), and perform jitter, random crops, and horizontal flips (if applicable) on a randomized basis. In the fine-tuning process, we weight the original images more by exposing the model to the original images more frequently. At the final prediction stage, only base images (which are center-crops) are used for both support and query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Combining Scores in the Ensemble</head><p>The baseline fine-tuning model used in the ensemble was modified so that we only fine-tune then last ResNet Block, and use an Adam optimizer with weight decay over 20 epochs rather than the default Stochastic Gradient Descent.</p><p>For our final submission results, we combine the predictions from the modified baseline fine-tuning model and the meta fine-tuning GNN model by normalizing the scores using a softmax function so that the scores from each model sum to 1 and are between 0 and 1, which ensures that each model is given equal weight in the prediction. Then we add them together and take argmax. I also implement transduction, as it is allowed in Track 1 of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Memory Requirements of GNN on 50-shot</head><p>The GNN builds a densely-connected graph between all the support samples and each new query sample. Hence, the space requirement for the 50-shot is tremendous, as the memory requirements scale up at O(n 2 ). Thus, in order to fit the model onto a 16GB Tesla V100, we average every 2 support samples' feature vectors into 1, so that we obtain 25 nodes for the GNN. This approximation scheme can be extended for higher number of shots (N s &gt; 50) as well. <ref type="table">(Table 1)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submission Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>The experimental setup involves training on miniImagenet and testing on CropDisease, EuroSAT, ISIC and ChestX. Models are trained for 400 epochs and then meta fine-tuned for 200 epochs. During training, we augment the training dataset using image transformations, following the protocol in <ref type="bibr" target="#b1">[2]</ref>. We modify the evaluation code in <ref type="bibr" target="#b5">[6]</ref> to ensure that the same test images are used and add checks to ensure that the same base images are used perepisode. The code can be found at https://github. com/johncai117/Meta-Fine-Tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposed Model</head><p>As shown on <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref>, the final proposed model vastly outperforms the previous benchmark introduced by <ref type="bibr" target="#b5">[6]</ref>. The average accuracy across all 12 tasks in the proposed model <ref type="table">(Table 1)</ref> is 73.78% while the average accuracy in the previous benchmark <ref type="table">(Table 2)</ref>, which is at 67.27%. This is a 6.51% improvement in the benchmark model that was trained solely on the miniImagenet dataset.</p><p>Even if we compare the accuracy with the performance of the benchmark model that was trained on multiple datasets (while the proposed model is not, due to the requirements of the challenge), the proposed model still has a vast improvement, as the previous IMS-f model achieved an average accuracy of 68.69% <ref type="bibr" target="#b5">[6]</ref>.</p><p>We can also further observe that the improvement in accuracy is most pronounced at the 5-shot level, with a 8.48% improvement in accuracy over the baseline Ft-Last1 model. This is followed by a 6.38% improvement at the 50-shot level and a 4.68% improvement at the 20-shot level. The non-linear improvement in the model may be attributed to the effect of data augmentation versus meta fine-tuning: data augmentation likely has the most effect when the number of support examples is very low, while fine-tuning has the most effect when the number of support examples is very high. This is supported by subsequent analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. of Shots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CropDisease</head><p>EuroSAT ISIC ChestX 5 96.14% ± 0.43% 87.13% ± 0.58% 53.00% ± 0.45% 26.76% ± 0.45% 20</p><p>98.66% ± 0.43% 95.01% ± 0.33% 62.72% ± 0.73% 32.83% ± 0.45% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Further Analysis of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single Model Study</head><p>For brevity, we only show results for 5 and 20 shot for the individual models. We see a clear pattern that meta finetuning is contributing more to improve accuracies on domains that are close to the training domain. We also see that baseline fine-tuning + DA is most effective at domains that are more distant from the training domain such as ChestX. We also find that the improvement going from 5 to 20 shots is less pronounced for baseline fine-tuning + DA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study: GNN and Simple Fine-Tuning</head><p>In this ablation study, we investigate which parts of the system are delivering superior performance. Simple finetuning refers to taking an existing GNN model and simply fine-tuning the last ResNet block. For brevity, we present results for EuroSAT and ISIC below for 20-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Method</head><p>EuroSAT ISIC No FT 86.57% ± 0.63% 52.32% ± 0.64% Simp FT 90.30% ± 0.47% 61.04% ± 0.64% Simp FT + DA 94.60% ± 0.37% 63.34% ± 0.72% Meta FT + DA 95.01% ± 0.33% 62.72% ± 0.73% <ref type="table">Table 5</ref>. 20-Shot: Further experiments with fine-tuning of GNN. DA refers to data augmentation From above, we see that Meta Fine-Tuning and Simple Fine-Tuning achieve comparable results when paired with domain adaptation. It can be seen that in domains that are more similar to miniImagenet, Meta Fine-Tuning performs slightly better, while for domains further away from the training set, Simple Fine-Tuning performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have developed a model that outperforms the benchmark by 6.51%. This was done by extending first-order MAML algorithms to Meta Fine-Tuning, and combining this with GNN, data augmentation and ensemble methods . Further analysis suggests that Meta Fine-Tuning does especially well at domains close to the source domain while simple fine-tuning with data augmentation works better on domains that are further away. One reason is that learning how to fine-tune on miniImagenet may have made the model less optimized for fine-tuning on distant domains.</p><p>Still, most practical applications for cross-domain fewshot learning would not involve such a radical domain-shift, given that there exists a set of diverse datasets that may be closer to each domain than miniImagenet. Hence, Meta Fine-Tuning would still have significant utility. Further research can be done to train a meta fine-tuning model that is more domain-agnostic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Meta Fine-Tuning Algorithm Initialize weights φ f for feature extractor and φ m for metric-learning module; for each episode do Sample N s support samples and N q query samples Freeze first L − k layers of feature extractor for step = 1,2,...,S do Sample batch b from the N s support samples Compute loss L s on these support samples using linear classifier Update the last k layers of feature extractor using SGD or Adam end Obtainφ f (k) = U S b (φ), the updated weights for last k layers Combine φ f (L−k) andφ f (k) to obtain new feature extractorφ f Feed images through feature extractor and then through the metric-learning module Compute the loss L(φ f , φ m ) on the N q query samples and compute update g f (L−k) , g f (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Meta-learning with fine-tuning. Green are trainable, brown are frozen. At test time, all will be frozen in step 2.During step 1 (Meta Fine-Tuning), only support examples are used, and the first 8-layers are frozen. A linear classifier on the ResNet10 features is used to predict the support labels, and the last 2-layers are updated accordingly using CE Loss for 5 epochs. At step 2, all layers are updated us-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>27% ± 0.40% 89.83% ± 0.46% 61.71% ± 0.44% 28.44% ± 0.43% 20 98.91% ± 0.19% 93.90% ± 0.34% 65.29% ± 0.56% 35.62% ± 0.47% 50 99.48% ± 0.13% 96.08% ± 0.25% 75.13% ± 0.56% 44.70% ± 0.64%</figDesc><table><row><cell>No. of Shots</cell><cell>CropDisease</cell><cell>EuroSAT</cell><cell>ISIC</cell><cell>ChestX</cell></row><row><cell cols="5">5 96.Table 1. Final Proposed Model: Meta Fine-Tuning GNN + Modified Baseline Fine-Tuning + Data Augmentation</cell></row><row><cell>No. of Shots</cell><cell>CropDisease</cell><cell>EuroSAT</cell><cell>ISIC</cell><cell>ChestX</cell></row><row><cell>5</cell><cell cols="4">88.72% ± 0.53% 80.45% ± 0.54% 47.20% ± 0.45% 25.96% ± 0.46%</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Single Model Study: Meta Fine-Tuning GNN + Data Augmentation</figDesc><table><row><cell>No. of Shots</cell><cell>CropDisease</cell><cell>EuroSAT</cell><cell>ISIC</cell><cell>ChestX</cell></row><row><cell>5</cell><cell cols="4">92.23% ± 0.46% 82.67% ± 0.50% 61.76% ± 0.50% 31.60% ± 0.41%</cell></row><row><cell>20</cell><cell cols="4">95.95% ± 0.30% 87.84% ± 0.46% 60.32% ± 0.59% 35.91% ± 0.42%</cell></row><row><cell cols="5">Table 4. Single Model Study: Modified Baseline Fine-Tuning + Data Augmentation</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3723" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A new benchmark for evaluation of cross-domain few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07200</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cross-domain few-shot classification via learned feature-wise transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08735</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
