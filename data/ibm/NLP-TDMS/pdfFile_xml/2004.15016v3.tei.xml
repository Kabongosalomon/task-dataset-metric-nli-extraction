<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-27">27 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Breit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Semantic Web Company</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Revenko</surname></persName>
							<email>artem.revenko@semantic-web.com</email>
							<affiliation key="aff0">
								<orgName type="department">Semantic Web Company</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiamehr</forename><surname>Rezaee</surname></persName>
							<email>2krezaee@comp.iust.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="institution">Iran University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tehran Institute for Advanced Studies</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
							<email>4camachocolladosj@cardiff.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-27">27 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-ofdomain settings. WiC-TSV data is available at https://competitions.codalab. org/competitions/23683</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a longstanding task in Natural Language Processing and Artificial Intelligence. While progress has been made in recent years, mainly thanks to the surge of transformer-based language models such as BERT <ref type="bibr" target="#b10">(Loureiro and Jorge, 2019;</ref><ref type="bibr" target="#b19">Vial et al., 2019;</ref><ref type="bibr" target="#b6">Huang et al., 2019)</ref>, the evaluation of WSD models has been limited to a set of (mostly SemEval-based) standard WSD datasets <ref type="bibr" target="#b15">(Raganato et al., 2017)</ref>. These datasets usually come in one of the two forms: lexical sample, in which a target word is placed in various contexts, triggering different senses, and all-words, in which all the content words in a given text are to be disambiguated. Both settings, however, come with a major restriction: word senses in the datasets are linked to external sense inventories such as WordNet <ref type="bibr">(Fellbaum, 1998)</ref>. Therefore, existing benchmarks are limited to only those WSD systems in which sense distinctions are defined according to an underlying sense inventory. This not only gives restrictions to the model's flexibility, but also enforces the assumption of the availability of complete data. However, as general sense inventories are complex to maintain they often lag behind in being up-to-date 1 , yielding to the absence of novel terms and term usages. Furthermore, the coverage of domain-specific terms and named entities in general sense inventories is quite limited, while domain-specific sense inventories are rare and in most cases incomplete.</p><p>As a motivating example, let us assume Technology as the target domain and the collection of information on the current technology landscape as a goal. Therefore, the following context needs to be disambiguated in order to evaluate its relevance:</p><p>From 1970 to 2007, Apple's chief executive was former Beatles road manager Neil Aspinall.</p><p>Even when incorporating a general sense inventory (which would include senses for the fruit and the tree) and a technology-specific sense inventory (which would include the sense for Apple Inc. the technology company), the actual target sense of this context (i.e., Apple Corps Limited, a multimedia corporation founded by the Beatles) may still be missing, which makes the annotation of the correct sense impossible. For these reasons, the current WSD task formulation and existing benchmarks are not fully able to evaluate the suitability of disambiguation systems in realistic domainspecific and/or enterprise settings.</p><p>In this paper, we try to fill this gap by proposing a re-formulation of the existing WSD task as well as a new benchmark for evaluating WSD systems under this paradigm.</p><p>Target Sense Verification (TSV) formulates the disambiguation of a word as a binary classification task where the equivalence of the intended sense of a word in context and a single given sense is evaluated. For instance, in the example above, the system would need to decide whether the sentence refers to Apple Inc. the technology company or not, by being provided with a sense indicator for solely Apple Inc. (e.g., the hypernym technology company or the definition).</p><p>A system able to efficiently solve the TSV task could be effectively used in the scenario of collecting and tagging large amounts of textual data; e.g., from social media, news agencies, blogs and for downstream tasks such as information retrieval, sentiment analysis or relation extraction. Furthermore, such a system could be a good candidate for entity linking (EL) as the task statement of TSV resembles the usage of enterprise knowledge graphs <ref type="bibr" target="#b4">(Galkin et al., 2017)</ref> for EL: typically, small domain-specific enterprise knowledge graphs only contain entities from the domain of interest, partially or completely missing the general purpose senses of the contained labels.</p><p>In order to train and evaluate models for TSV we constructed WiC-TSV (Word in Context -Target Sense Verification) a multi-domain dataset and evaluated standard unsupervised and supervised approaches (including language models). While WiC-TSV's training and development set consist of general purpose instances, the test set contains domain-specific instances from three different domains. Therefore, this dataset aims at evaluating the ability of a model to (1) disambiguate the word in context without an external sense inventory, (2) deal with unseen instances and incomplete data, and (3) transfer the intrinsic knowledge (gained on general domain data) into a specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Word Sense Disambiguation. The task of WSD consists of associating a word in context with its most appropriate entry in a given sense inventory <ref type="bibr" target="#b12">(Navigli, 2009</ref>), e.g., WordNet. For WSD there are many associated datasets <ref type="bibr" target="#b15">(Raganato et al., 2017;</ref><ref type="bibr" target="#b18">Vial et al., 2018;</ref><ref type="bibr" target="#b16">RÃ¶der et al., 2018;</ref><ref type="bibr" target="#b8">Ling et al., 2015)</ref>, including domainspecific ones <ref type="bibr" target="#b0">(Agirre et al., 2009;</ref><ref type="bibr" target="#b2">Faralli and Navigli, 2012)</ref>. The main difference between WSD and its re-formulation TSV is that for TSV the availability of a sense inventory is not required. Instead of associating a word in context with its most appropriate sense, the usage of a single given sense in the provided context is to be verified. Systems that aim to solve the proposed task are therefore not required to model all senses of the target word, but only a single sense instead.</p><p>This facilitates the development of systems for specific domains or settings, as no general-domain knowledge resource is required to perform this task. For instance, an Indonesian company may want to retrieve all sentences referring to the Java island and not other unrelated senses. This framing of the task is frequent in business and data mining settings where domain-specific knowledge resources or inventories may be available, without the need for modeling instances from other domains.</p><p>WiC. The task closest to the proposed WiC-TSV is probably Word-in-Context (Pilehvar and Camacho-Collados, 2019, WiC), which our dataset is based on. WiC is a binary classification dataset where a target word is presented within two different contexts. The task consists of deciding whether the word is associated with the same sense in the two contexts or not. WiC is also one of the tasks included in the general language understanding framework SuperGLUE <ref type="bibr" target="#b20">(Wang et al., 2019)</ref>.</p><p>WiC-TSV inherits some of the desirable properties of WiC, such as independence from external sense inventories and the binary classification nature of the task. However, though our benchmark draws ideas from the Word-in-Context benchmark, it provides a different evaluation setting with additional flavors. The main difference with respect to our dataset lies in the presence of relevant information such as hypernyms and definitions, which makes our dataset more realistic and a direct proxy for downstream evaluation: in WiC-TSV the ambiguous target word in a single context is compared against a specific target sense (indicated by provided hypernyms and definitions), in contrast to the comparison of the intended senses of the target word in two different contexts. Also, the task is more targeted at word-level representation, as in one of the tasks (i.e. hypernymy task) the model is not provided with any contextual information and, therefore, needs to have a clear understanding of the word to be able to make correct judgements. Moreover, WiC-TSV includes instances from three domains (cocktails, medicine, and computer science) in its test set, which makes the benchmark more challenging and comparable to a real setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WiC-TSV: The Benchmark</head><p>A goal of this benchmark is to evaluate the ability of a model to verify the target sense of a word in a context without the usage of an external sense inventory, i.e., without knowing all possible senses of the target word. Another model quality that is aimed at with the presented benchmark is the ability to transfer the intrinsic knowledge into a specific domain. As for most areas, domain-specific training data is hard to obtain, being able to learn on general purpose data and still perform well on domain-specific data is a huge advantage in a real world setting.</p><p>To this end, we constructed a benchmark satisfying following requirements:</p><p>1. Knowledge of only a single sense of the target word;</p><p>2. Knowledge of the definition and/or hypernyms of the target sense;</p><p>3. Ability to test the models capability to disambiguate both general purpose and domainspecific senses;</p><p>4. Ability to test the models capability to classify usages of previously unseen words;</p><p>Formally, each instance in the dataset consists of a target word w, a context c containing the target word w, and its corresponding target sense s represented by either its definition (Task 1), its hypernym/s (Task 2), or both definition and hypenyms (Task 3). The task aims to determine whether the intended sense of the word w used in the context c matches the target sense s. <ref type="table" target="#tab_1">Table 1</ref> contains examples of instances from the WiC-TSV test set. Furthermore, a small sample of 10 instances is available online in the form of a survey 2 , where the achieved score is shown to the user after the submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Construction</head><p>In this section we detail the construction of the dataset. First, we describe the construction of the training and development set (Section 3.1.1) and then the test set (Section 3.1.2), with a special focus on the creation of the domain-specific subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Training and Development Set</head><p>Instances in the training and development set do not focus on a specific domain. As basis served the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019), which contains a target word w and two contexts c 1 and c 2 for each instance. The contexts from WiC for noun instances come from two resources: WordNet and Wiktionary. To maintain the desirable characteristics of the WiC dataset (e.g., balanced data, not having repeated contexts across instances), the splits of the original training and development sets were treated separately in the following way: starting from a noun-only sub-sample, for each context c i , the sense of the target word w was mapped to the corresponding synset of WordNet, adding a sense identifier. Each WiC instance was then split into two instances, one for each context. For initial negative instances (i.e. w has different intended senses in c 1 and c 2 ), the sense identifiers of these two instances were switched. To avoid information leakage, only one of the two instances were kept for the WiC-TSV dataset 3 . Finally, for each sense, the definition and hypernyms were derived from WordNet using the sense identifiers. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Test Sets</head><p>To make the dataset more challenging and realistic, the test set incorporates both general purpose and domain-specific instances.</p><p>General Purpose (WNT/WKT). The general purpose instances were generated analogously to 3.1.1. Hence, this test set is composed of both WordNet and Wiktionary examples, with definitions and hypernyms extracted from WordNet.</p><p>In the following we describe the construction of the domain-specific subsets. The main difference between domain-specific and WNT/WKT test sets is that in the former the target sense remains the same. That means, that even though "fork" might have different senses within the computer science Python is an interpreted, high-level, general-purpose programming language object oriented programming language F The present paper compares the recently studied pythons with those examined 20 years ago , and uses the combined dataset to assess the ecological sustainability .</p><p>Python is an interpreted, high-level, general-purpose programming language object oriented programming language domain, we are only interested in one of these senses.</p><p>Cocktails (CTL). For the cocktails instances the target words were taken from the "All about cocktails" thesaurus 5 . The thesaurus contains 300 entries describing not only cocktails, but also beverages, garnishes and glassware, among others. For instances obtained from this resource, the hypernym "cocktail" is used in the WiC-TSV dataset, while the definition is derived from the thesaurus.</p><p>Medical Subjects (MSH). For medical subject instances we use terms, definitions and hypernyms from the MeSH thesaurus 6 . This thesaurus is used for indexing medical articles and therefore contains a wide variety of terms in this domain. We considered various types, such as diseases, symptoms and body parts as target words. 5 vocabulary.semantic-web.at/cocktails 6 www.nlm.nih.gov/mesh/ Computer Science (CPS). Target words in the computer science domain were gathered manually, without a readily available thesaurus. The definitions were derived from the lead section of the corresponding Wikipedia page, while hypernyms were created by the consensus of two domain experts.</p><p>In order to create the domain-specific instances, first a list of ambiguous words and their domainspecific target senses was fixed for each domain. Then, we used the Wikilinks dataset <ref type="bibr" target="#b17">(Singh et al., 2012</ref>) as a basis for collecting different contexts containing the target words. This dataset contains documents -blog posts scraped from the web -and the links from these documents to the Wikipedia pages, which were used to assign the intended sense (i.e., target sense or other sense) to the target word. Where needed, additional contexts were collected manually by incorporating a search engine to find contexts for the target word. The intended senses for these instances were as-signed manually.</p><p>Postprocessing. After creating the initial domain-specific instances, the subsets were checked manually to remove non-suitable and unsolvable instances. To maintain a rather realistic evaluation setup, data was not completely cleaned, meaning that contexts can contain noisy elements such as headings or meta-info derived from the websites (e.g., "posted by").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Cleaning</head><p>While the quality of the domain-specific instances is assured due to their manual creation process, an additional data cleaning step was introduced in which general purpose instances were manually curated. The instances from the test set were split into four sets with an overlap of 20%. Each set was evaluated by an annotator regarding correctness and solvability of the instances. For example, when the hypernym of an instance was too generic to help in the disambiguation process, or the context itself was too ambiguous, the instance was marked as "to filter out". Each marked instance was reviewed by a second annotator, who could either confirm, or reject the request of removal. Instances marked by both annotators were removed.</p><p>An example of such a removed instance would be the context "The zero sign in American Sign Language is considered rude in some cultures ." for the target word "zero" with the target definition 'a mathematical element that when added to another number yields the same number'. In American Sign Language (ASL), "zero sign" is a ringshaped hand sign using the thumb and pointing finger, similar to the OK-gesture. The provided instance mixes two senses of "zero sign". On the one hand, it refers to the hand gesture itself (synonymous to OK-gesture) which does not fully match the target sense. On the other hand, it also refers to the sign of the digit zero in ASL, which does match the target sense.</p><p>Other examples of filtered instances involve sentences where the target word may have been used metaphorically.</p><p>This procedure resulted in 106 instances which were removed. About 8% of these instances were part of evaluation sets created to measure the human performance (see 3.4) 7 : the annotators achieved a mean accuracy of only 56% on these 7 Annotations for these instances were removed before calculating the metrics presented in 3.4  instances. This shows that the data cleaning step was necessary in order to ensure the data quality of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistics</head><p>A statistical overview of the dataset and their splits is shown in <ref type="table" target="#tab_3">Table 2</ref>. The totality of 3832 available instances were split into train, development and test sets with a ratio of 56:10:34 which allows a sophisticated analysis of the generalisation capabilities of tested systems, while still providing an appropriately sized training set. The test set contains around 55% general purpose instances and 45% from specific domains. For each domain, the number of unique target words is relatively low compared to the general domain subset, which results in a higher number of instances per target word. However, for domain specific words, a great variety of senses is used in the contexts, yielding a big diversity among the instances. For all three splits, positive and negative instances are approximately balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Human Performance</head><p>To estimate the human performance upper bound, a sub-sample of the test set was manually annotated. The performance was evaluated on the setting of Task 3, meaning that both the definition and the hypernyms were provided to disambiguate. A random selection of 250 instances were split into two evaluation sets of the size of 150, resulting in a 20% overlap. Each evaluation set was assigned to a non-expert annotator with English as native language. No additional information -especially  not from the respective ontology or about other senses of the target -was provided to the annotators and they were instructed not to use external knowledge sources (e.g. if they are not familiar with the domain-specific sense of a word).</p><p>Results of the human performance evaluation can be found in <ref type="table" target="#tab_5">Table 3</ref>. The mean accuracy for the evaluated datasets was 85%, with individual scores of 81% and 89%. To estimate the interannotator reliability, the agreement of the two annotators on the overlapping instances was calculated: for 42 instances (84%) the annotators agreed on the label.</p><p>When evaluating the instances per domain, it can be seen that the general purpose instances were more difficult than the domain-specific ones, as annotators achieved an average accuracy of 82% (individual scores of 77% and 87%) on the general purpose instances, while the mean accuracy on the domains were 89% (83% and 96%), 92% (88% and 96%), and 86.5% (89% and 84%) for MSH, CTL, and CPS, respectively. This performance difference is even more evident when comparing to the performance of non-native speakers: an additional experiment showed, that evaluators whose mother language is not English only achieved an average accuracy of about 77% on the WNT/WKT instances, while performances on the domain specific subsets were comparable to native speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section we evaluate the performance of different baseline models on our WiC-TSV benchmark. For our experiments we considered two main systems, namely BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and FastText <ref type="bibr" target="#b7">(Joulin et al., 2017)</ref>, as well as unsupervised baselines adapted to the corresponding tasks in WiC-TSV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Tasks</head><p>The benchmark provides three different tasks depending on the input information available: definition-based (Section 4.1.1), hypernym-based (Section 4.1.2), and both (Section 4.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Task 1: Definition Information</head><p>In this task, the goal is to identify if the intended sense of the target word in the context matches the target sense described by the definition.</p><p>In other words, the model has to check if the sense represented by the definition can fit within the given context. For this task, the system is provided with a context (in which the target word is marked) along with a definition (which describes one of the possible senses of the word).</p><p>Baselines. The first baseline is based on the pre-trained transformer-based language model BERT 8 . It consists of a simple classification layer on top of the BERT model which is responsible for encoding the input. For this task, we concatenate the context and the definition and feed the whole sequence to BERT. Then, the classification layer takes as input the concatenation of three different vectors, all provided by BERT: the [CLS] token representation, the representation of the target word in the context and the average representation of the words in the definition. This is similar to the baseline BERT model employed in Super-GLUE <ref type="bibr" target="#b20">(Wang et al., 2019)</ref>. It is worth mentioning that BERT is originally trained using Word-Piece tokenization <ref type="bibr" target="#b21">(Wu et al., 2016)</ref>, which means that each word can be broken down into more than one sub-word. Therefore, in order to have a fixed length representation for each word, we take the average of its sub-word representations. Finally, the whole model is fine-tuned on the training set.</p><p>For the FastText-based baseline, we first extract the corresponding embeddings for each word in the context and definition, respectively. Then, the representation is simply computed as the average of the corresponding embeddings it contains. Next, these two representations are concatenated together to form a fixed length vector which we then feed to a fully connected layer. Finally, we put a simple classification layer on top of this fully connected layer and train the model on the training set.</p><p>We also evaluated GlossBERT <ref type="bibr" target="#b6">(Huang et al., 2019)</ref> on our dataset. The authors describe a weak supervision algorithm that consists in surrounding the target word with special symbols -quotation marks are used in the available implementation 9 . We provide results both with (GBERT ws ) and without (GBERT) weak supervision. We chose the hyper-parameters as suggested by the authors, trained for 6 epochs and achieved the highest scores on the 4th epoch.</p><p>Unsupervised baselines U-BERT and U-dBERT, which do not make use of the training set, are simple threshold-based classifier which take the cosine distance of a target word representation and a definition representation into account. As source for these vectors serve BERT and DistilBERT, respectively.</p><p>Similar to before, we derive the target word vector by taking the embedding of the target word in the context and the definition vector by averaging over all embeddings of the definition. The threshold is tuned on the development set with a step size of 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Task 2: Hypernym Information</head><p>For this task, the system is provided with a target word (in a context) and a set of hypernyms for the target sense. The task is to identify if the intended sense given through the context is the hyponym of the provided hypernyms. Note that, unlike Task 1, no definition is involved in this setting and the task is directed only by hypernymy information.</p><p>Baselines. We used baseline models similar to those used in the previous task. The only difference lies in how we shape the inputs fed to these models.</p><p>For the supervised and unsupervised BERTbased models, we put together the context with the hypernyms to form the input. Similarly, for the FastText-based model, the hypernyms' embeddings are concatenated with the context's representation and fed to the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Task 3: Both Sources of Information</head><p>In the third task systems are provided with both definition and hypernymy information.</p><p>Baselines. For this task, we concatenate the definition and the hypernyms, and feed the generated sequence together with the context to BERT. 9 https://github.com/HSLCY/GlossBERT  Then, the concatenation of the [CLS] token representation, the representation of the target word in the context and the average representation of the words in the definition/hypernyms sequence is fed to the classification layer. For the unsupervised model we use the same BERT input and take the representation of the target in context and the average over the definition and hypernyms as input vectors. For the FastTextbased baseline, the hypernyms' embeddings are concatenated with both the context's representation and the definition representation and the combination is fed to the classifier. <ref type="table" target="#tab_7">Table 4</ref> shows the overall results for the three tasks. As can be observed, GlossBERT performs best in terms of accuracy and F 1 . BERT-L is a little worse, but achieves the best recall. The worst supervised baseline -FastText -does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT-based models are far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition in- <ref type="table">Table 5</ref>: Performance for the baseline models for the three tasks (i.e., T1: definition-based, T2: hypernymybased, and T3: both sources of information) split by domain: General (WNT/WKT), Cocktails (CTL), Medical Subjects (MSH), and Computer Science (CPS). Baseline True is a naive baseline that always returns "True". Human performance in terms of accuracy is estimated to be 82.1% (WNT/WKT), 92.0% (CTL), 89.1% (MSH) and 86.5% (CPS) as described in Section 3.4. formation is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results. Yet GlossBERT reaches a better performance with definition only 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The unsupervised models only perform well with hypernyms. Though U-dBERT reaches the best precision in Task 1, the recall remains very low and therefore the overall performance.</p><p>Another point to highlight is the high recall of BERT-based models, in contrast to its precision. This is mainly attributed to the domainspecific subsets as it will be analysed below. As for the comparison between BERT-based models, the larger model (BERT-L) performs as expected better than the base model (BERT-B) overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>In order to better understand the results, in this section we perform a focused analysis on the performance split by domain. <ref type="table">Table 5</ref> presents the results split by domain. Fast-Text faces a massive challenge in adapting to new domains and generalising from WNT/WKT to the <ref type="bibr">10</ref> We did not evaluate GlossBERT with hypernyms as suchconfiguration was not considered by the authors in the original system and its integration would not be straightforward. other domains. However, BERT-based models show to be much more robust to domain changes. In fact, the results on the domain-specific instances are in the same ballpark as the WNT/WKT test set. This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task <ref type="bibr" target="#b11">(Magnini et al., 2002)</ref>. On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in <ref type="bibr">Word-Net)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Domain-based Analysis</head><p>Surprisingly, unsupervised DistilBERT achieves the best accuracy over all tasks and classifiers on MSH. However, both unsupervised models do not perform well on WNT/WKT and CTL. We can observe that supervised models are significantly more reliable and produce similar scores on different tasks and datasets than unsupervised models.</p><p>In general, for BERT-based models, recall is substantially higher than precision on the domainspecific subsets. This is desirable in a retrieval setting where a high-coverage retrieval of relevant cases is of more importance. Interestingly, among the two BERT alternatives, the smaller model performs better on the domain-specific subsets, suggesting that it is more robust to domain changes. This is an important observation which needs fur-ther careful investigation in future work, given that most evaluation benchmarks (on which the larger model consistently outperforms the smaller one) comprise in-domain test sets, which cannot reveal robustness across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">In-domain Few-shot Analysis</head><p>Although the availability of big annotated domainspecific training sets is quite rare, the presence of a small training set forms a realistic scenario. Incorporating these domain-specific instances in the model training could potentially increase its prediction performance. To investigate this theory, we performed an additional analysis focusing on the usage of in-domain examples in the learning process, where for each domain 100 instances from the test sets were used as a training set. To enforce the assumption that not all target senses would be seen during the training process, we put aside all instances of 3 target words for each domain test set. 11 Two additional domain-based strategies were considered: (1) few-shot learning: only using the domain-specific instances, and (2) continued learning: extending the existing generalpurpose training set with the domain-specific instances.</p><p>For this analysis we focused on Task-3 and BERT-large, which performed better overall. <ref type="table">Table 6</ref> shows the F1 results. In general, few-shot learning works surprisingly well overall (achieving the best overall performance in the CTL and MSH domains). On CTL pairs unseen during training, it even performs considerably better than the same BERT model trained in the continued learning setting. In the CPS domain, for both few-shot in-domain learning and continued learning the performance on seen target words is quite high, while the prediction of unseen target words produces relatively low F1 scores, which indicates a low ability to generalise to new senses. As for the model trained on the general-domain dataset, it performs best in the CPS domain, but performs considerably lower than the domain-tuned alternatives in the CTL domain. Indeed, the domaintuned BERT systems clearly outperform the same model trained on the general domain on seen pairs, proving the importance of obtaining word-specific examples to boost performance. However, this may not be realistic in practice, and therefore fur-  <ref type="table">Table 6</ref>: F1 score for the in-domain few-shot analysis (Task-3) using BERT-L trained on general domain (WNT), domain-specific (Dom) and general domain fine-tuned on the target domain (WNT+D). In addition to the full test set (All), results are split on seen (See) and unseen (Uns), as per the presence or absence of the target word in the domain-specific training set.</p><p>ther research should be devoted in improving the generalization capabilities of disambiguation systems, and language models in particular. These findings are consistent with the results of an experiment conducted with GlossBERT in the few-shot learning setting on Task-1: the overall accuracy increase ranged from 0.1% (CPS) to 13.6% (CTL) compared to the model trained solely on general domain instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper we have introduced the Target Sense Verification task, a re-formulation of WSD where the equivalence of the intended sense of a word in context and a single given sense is evaluated. Furthermore, we presented WiC-TSV, a multidomain benchmark which differs from existing WSD datasets in three main ways: (1) it is based on TSV and therefore framed as a binary classification task where only one target sense needs to be verified, (2) it is independent from external sense inventories, and (3) its test set contains instances from three specific and heterogeneous domains are included: cocktails, medical subjects and computer science. Our benchmark therefore opens the floor for different disambiguation algorithms that do not require modeling the entirety of a sense inventory. This characteristic also provides a crucial advantage in enterprise and domainspecific settings as it facilitates the development of systems which are only aimed at modelling the domain at hand. Moreover, having these out-ofdomain test instances makes our benchmark more robust and generalisable, preventing (or making it harder) for statistical models to learn spurious correlations from the training set, which has been proven to be an issue in standard NLP tasks <ref type="bibr">(Po-liak et al., 2018;</ref><ref type="bibr" target="#b5">Gururangan et al., 2018;</ref><ref type="bibr" target="#b9">Linzen, 2020)</ref>. In our initial experiments we found that current state-of-the-art disambiguation techniques based on pre-trained language models such as BERT are very accurate at handling ambiguity, even in specialised domains. However, there is still room for improvement as highlighted by the gap with the human performance. This benchmark therefore opens up avenues for future research on domaintransfer and on developing general-purpose solutions which can perform well on a variety of domains without the need for large amounts of training data.</p><p>As future work, we are planning to further investigate and analyse the robustness of pre-trained models with respect to domain changes. Also, it would be interesting to develop hybrid models which take both definition and hypernymy information into account -in this paper we combined both sources in BERT in a simple manner, but more complex models should lead to further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Sample instances from WiC-TSV. Target words are marked in bold. Tags: T (True) and F (False).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of training, development and testing splits of WiC-TSV, including total number of instances (Total), unique number of target words (N w ) and percentage of positive instances (R + ).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average human accuracy for native English annotators, on different subsets of the dataset: general purpose, i.e., WNT/WKT, and the domain specific, i.e., MSH, CTL, and CPS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Test set performance of the baseline models on WiC-TSV, in terms of accuracy, precision, recall, and F1, on the three different tasks. Baseline True is a naive baseline that always returns True and the human performance is computed as described in Section 3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>All See Uns All See Uns All See Uns WNT 76.5 74.7 75.5 77.4 82.9 76.0 74.9 77.8 74.8 WNT+D 80.2 89.5 75.5 78.5 86.2 76.6 73.6 88.9 72.9 Dom 84.2 88.8 82.6 78.7 88.6 75.5 70.7 93.3 69.1 Base True 58.5 54.8 59.1 68.2 70.9 67.5 62.6 58.3 62.8</figDesc><table><row><cell>Train</cell><cell>CTL</cell><cell>MSH</cell><cell>CPS</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The last update in WordNet dates back to June 2011.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">h t t p s : //w w w.s u r v e y m o n k e y .c o m/r/ LHYWXPV</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">If both instances were kept, the label could have been predicted with a high accuracy by counting the appearance of the target sense (even=True, odd=False).4  WordNet sense identifiers are omitted in the final dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We used the implementation of BERT available at https://github.com/CyberZHG/keras-bert for the base(BERT-B)and large (BERT-L) pre-trained models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">To add robustness to the results, three different random samples were considered for this experiment, with the results being averaged after the three different runs.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 17: All-words word sense disambiguation on a specific domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)<address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="123" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new minimally-supervised framework for domain word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1411" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enterprise knowledge graphs: A semantic approach for knowledge management in the next generation of enterprise information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">SÃ¶ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">MarÃ­a</forename><forename type="middle">Esther</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Scerri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICEIS 2017 -Proceedings of the 19th International Conference on Enterprise Information Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GlossBERT: BERT for word sense disambiguation with gloss knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3507" to="3512" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00141</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How can we accelerate progress towards human-like linguistic generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language modelling makes sense: Propagating representations through WordNet for full-coverage word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AlÃ­pio</forename><surname>Jorge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5682" to="5691" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The role of domain information in word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pezzulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WiC: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1267" to="1273" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypothesis only baselines in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>RÃ¶der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<title level="m">Gerbil -Benchmarking named entity recognition and linking consistently. Semantic Web</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UFSAC: Unification of Sense Annotated Corpora and Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LoÃ¯c</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sense vocabulary compression through the semantic knowledge of wordnet for neural word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LoÃ¯c</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Global WordNet Conference</title>
		<meeting>the 10th Global WordNet Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
