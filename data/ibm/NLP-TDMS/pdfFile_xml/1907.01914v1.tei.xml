<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention model for articulatory features detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ievgen</forename><surname>Karaulov</surname></persName>
							<email>ekaraulov@sciforce.solutions</email>
							<affiliation key="aff0">
								<address>
									<settlement>Sciforce</settlement>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Tkanov</surname></persName>
							<email>dtkanov@sciforce.solutions</email>
							<affiliation key="aff0">
								<address>
									<settlement>Sciforce</settlement>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention model for articulatory features detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: manners of articulation</term>
					<term>places of articulation</term>
					<term>sequence-to-sequence model</term>
					<term>multitask learning</term>
					<term>low-resource speech recognition</term>
					<term>phones recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Articulatory distinctive features, as well as phonetic transcription, play important role in speech-related tasks: computerassisted pronunciation training, text-to-speech conversion (TTS), studying speech production mechanisms, speech recognition for low-resourced languages. End-to-end approaches to speech-related tasks got a lot of traction in recent years. We apply Listen, Attend and Spell (LAS) [1] architecture to phones recognition on a small small training set, like TIMIT <ref type="bibr" target="#b1">[2]</ref>. Also, we introduce a novel decoding technique that allows to train manners and places of articulation detectors end-to-end using attention models. We also explore joint phones recognition and articulatory features detection in multitask learning setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end approaches emerged in neural translation and later significantly changed automatic speech recognition (ASR) and TTS. While conventional pipelines still provide decent results, especially on smaller datasets, end-to-end models quickly catchup and are already state-of-the-art on some tasks <ref type="bibr" target="#b2">[3]</ref>. Endto-end models are typically sequence-to-sequence models that output words, subword units, graphemes or phonemes directly.</p><p>Articulatory features <ref type="bibr" target="#b3">[4]</ref> play an important role in describing speech production mechanisms. They can be divided in voicing, manners and places of articulation. A combination of a manner, a place and voicing defines any sound that human can produce in a unique way. For example, by convention "s" in the word "sea" is a voiceless alveolar sibilant which contrasts it to other phones.</p><p>By their nature, articulatory features are language agnostic and open many research oportunities in crosslingual and multilingual speech recognition, as linguistic features in speech synthesis etc. Particularly, Interspeech 2017 paper by Abraham <ref type="bibr" target="#b4">[5]</ref> explores these features for ASR in low-resource setting. Automatic Speech Attribute Transcription (ASAT) system <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> also works on producing speech-to-text model based on indicators detection (subset of these indicators are articulatory features; hereafter both terms are used with the same meaning).</p><p>Another application of phones recognition and articulatory features estimation is Computer-Assisted Pronunciation Training. Some of the approaches are described in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref>. This paper dwells on applications of attention-based models to articulatory features detection. We introduce a novel decoding technique that allows training manners and places of articulation detectors end-to-end with the help of attention-based models. We also explore joint phones recognition and articulatory features detection in a multitask learning setting. Contrary to other works in this domain, we focus on producing sequences of features instead of frame-or segment-level labels. Sequencelevel data is simpler to work with in applications where precise alignment with original waveform is not important. It also may serve as input for the encoder in sequence-to-sequence-based speech synthesis. Besides, articulatory features are languageindependent, and our approach can potentially be applied to zero-resource speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>The conventional approach to estimation of phonological features is akin to the standard ASR pipeline. It requires forced alignment of phones to utterances. As a result, training is usually done either on fine-labeled data with alignments or on data that have good acoustic models available. This limits research to well-studied mainstream languages or enforces usage of cross-language models.</p><p>Reasearch on detection of places and manners of articulation is spread across different languages and it is challenging to select a single baseline. We decided to use TIMIT in our experiments as it is an overall well-studied corpus with well-known published results for phones recognition <ref type="bibr" target="#b9">[10]</ref> and articulatory features detection <ref type="bibr" target="#b10">[11]</ref>.</p><p>While we focus on detection, there is still a noteworthy series of recent works on manners discrimination using zero-time windowing, e.g. <ref type="bibr" target="#b11">[12]</ref>, which might be used in feature engineering for encoder inputs.</p><p>For non-English baselines, a good starting point is work by Merkx and Scharenborg <ref type="bibr" target="#b12">[13]</ref> that describes positive impact of CNNs on ariculatory features classification in Dutch.</p><p>Another end-to-end approach to articulatory features detection is described in <ref type="bibr" target="#b13">[14]</ref>, where authors focus on connectionist temporal networks (CTC) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention-based models</head><p>Typical end-to-end models in speech domain are based on sequence to sequence neural networks. Most common architectures are CTC, recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b9">[10]</ref> and encoder-decoder with attention <ref type="bibr" target="#b15">[16]</ref>. In this work we will focus on attention-based models. One of important features of attention-based models is that they provide a link between the encoder (acoustic data) and the decoder (textual data or indicators) steps. For training this link results in faster convergence to lower loss values. Besides, during inference, attentions enables building a distribution that can relate the sound data and the textual data, essentially providing fuzzy alignment between the decoder outputs and the encoder inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Articulatory features</head><p>Articulatory features describe production of sounds via the interaction of different components within a human vocal tract. Our approach rests largely on the seminal paper by Simon King <ref type="bibr" target="#b10">[11]</ref> that used so-called Spoken Patterns of English (SPE <ref type="bibr" target="#b3">[4]</ref>) features and manners as targets instead of phones for training a neural network.</p><p>We used a combination of SPE, manners and places of articulation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-task learning</head><p>Multi-task learning (MTL) <ref type="bibr" target="#b16">[17]</ref> improves learning efficiency and model generalization for the task-specific models by learning several related tasks at the same time. All tasks usually share a part of representation. Each new task contributes to the model learning by adding information and transferring knowledge. MTL approach is applied to neural networks by sharing some of the hidden layers between different tasks.</p><p>In our work, we share encoder parameters between two tasks: articulatory features classification and phonemes classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder for indicators</head><p>In this work we train a LAS model on phone and indicator targets. For phone targets, the setup is rather straightforward: decoder outputs ARPABET symbols directly at each step. We do not use any language model, so the decoder outputs are passed for the error rate calculation as is.</p><p>For indicators detection, we modified the decoder. We use a sigmoid activation function for the projection layer, since these features are not mutually exclusive. At each step, the decoder outputs posteriograms for places and manners of articulation. Our approach has two options for the indicators decoder. The first option outputs samples from posteriors of indicators. Alternatively, the indicators posteriors are transformed to phones posteriors via the mapping matrix M = {mij}, i = 1..M , j = 1..N , where mij is a binary indicator of presence or absence of the corresponding feature fi ∈ F in the phone sj ∈ S; M is a phones count in the lexicon, N is an indicators count.</p><p>Let's describe the second approach in detail. For clarity, we are describing all the operations during inference. Let Φ be a set of posterior probabilities φi of all indicators at encoder's output and decoder's time step t. Assuming binary features' independence, we can obtain log probabilities of observing phones P = {log pi} using the following equation:</p><formula xml:id="formula_0">P = log Φ · M + log (1 − Φ) · (1 − M )<label>(1)</label></formula><p>After obtaining log posteriors of phones P , we then find one with the highest probabilityf = fĵ,ĵ = argmaxP , and perform its reverse mapping on articulatory features vector by just getting columnĵ from M . This new vector of refined indicators is passed to the decoder's input for calculating output at decoder's next time step t + 1. Thus, binary features posteriors are rounded to the values corresponding to the closest phone, filtering out invalid combinations which never occur in the language (for example, front and back, stop and continuant, etc.).</p><p>It is important to note that the set S of all possible phones is not constrained by phonetic inventory of training language and thus can be used in cross-lingual low resource speech recognition. So, by specifying a mapping matrix for a different language than used at training time, we can perform cross-lingual phones recognition.</p><p>Finally, we apply MTL for both phone targets and indicators. Both tasks share the encoder, however, the attention and the decoder are distinct for them. Losses of both tasks contribute equally to the overall loss on each step. In our experiments, the joint model needs 20% less steps for convergence than separate models. A possible explanation is that a loss for indicators targets of similar phones (e.g. target "n" and network output "ng") will have lower value than a loss for distant phones (e.g. "n" and "ow"), due to matching articulatory features, even at early training steps. At the same time, for phone targets, network outputs that are closer to targets will have a higher loss, at least at early stages of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and features</head><p>All experiments were performed on the TIMIT corpus. For training we used the standard 462-speaker set without SA records. Test results in tables below correspond to the core test set of 192 utterances. A development set was collected from the remaining part of the test set, i.e. the non-core part. We explicitly checked that the train, development and test sets had non-overlapping speakers. Also, the test core set does not share either speakers, or phrases with the train and development sets.</p><p>There are 61 phone types in the corpus, yet a common practice is to report results on a reduced 39-phone set with allophones mapped to a common phonetic label following <ref type="bibr" target="#b17">[18]</ref>. Also most papers perform training on the full phone set and do 61 to 39 mapping for evaluation. In our experiments we did not find any benefits of this approach against direct training on the reduced phones set. Both ways (the full set with mapping and the plain reduced set) yielded near identical results.</p><p>For features extraction we tried several approaches. We followed <ref type="bibr" target="#b9">[10]</ref>: 20 ms window and 10 ms step mel-scale filterbanks plus log-energy feature. Also we experimented with mel-frequency cepstral coefficients (MFCC) and Lyon's auditory model <ref type="bibr" target="#b18">[19]</ref>. In each case of base acoustic features we computed deltas and double-deltas. All inputs were globally normalized to have zero mean and unit variance for each input feature.  <ref type="table" target="#tab_0">Table 1</ref> compares three models with around 6.5 million weights and identical architectures: 3 layers of 256 units in encoder, 1 layer of 256 units in decoder, Luong attention, 20% dropout and 10% scheduled sampling probabilities. We do not do any forced alignment and do not use temporal alignments for phones provided with TIMIT, thus phone error rates (PER) reported in this paper are essentially edit distances normalized by ground truth length. After conducting experiments we concluded that with other parameters fixed, MFCC and melfilterbanks perform similar in terms of phone error rate while Lyon's cochleogram provides a slightly better accuracy. Still, to make comparison with other baselines more fair, we conducted further training on MFCC features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>We performed training on a single NVIDIA GTX1080 GPU. On average, one training run takes up 3-5 hours depending on model size. We tried to keep a batch size of 32, yet occasionally had to lower it for some experiments to 16 due to memory limitations. In all experiments we used a decoder with a single layer. As to the encoder-decoder connection, we noticed that attention provides enough information to the decoder and passing the final encoder state to the decoder does not yield any performance improvements.</p><p>TIMIT is a small dataset with a few hours of speech in all sets combined. Thus, deep learning models overfit easily and regularization plays a crucial role. We relied on dropout and L2 weight decay for regularization. We used drop probability in 20 -40% range and decay constant between 10 −5 and 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>In our experiments, we did not use external language models or speaker adaptation. We used LSTMs in the encoder without any initial convolution layers. Adding any of this improvements should positively impact accuracy of the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Phone recognition</head><p>We used the results from <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b15">[16]</ref> as baselines to compare our results against them. Their phone error rates for pretrained transducer and attention-based models are among the lowest reported for sequence-to-sequence models that do not rely on external language models.  <ref type="table" target="#tab_1">Table 2</ref> shows the comparison of our models against baselines. At first glance, our implementation of a multitask attention-based model (LAS-MTL-S and LAS-MTL-M) for joint articulatory features and phones recognition performs worse than baselines. We attribute this to a suboptimal choice of hyperparameters, because the non-multitask model (LAS) trained on just phone recognition yields results comparable to the multitask model. Therefore, adding indicators as an additional target doesn't decrease the phone recognition accuracy. Another interesting observation is that the model LAS-F trained directly on articulatory features without complementary phone targets results in a higher error rate than LAS-MTL models. In this case, PER serves as integral accuracy for all indicators together. A review of individual features accuracy also showed that the multitask learning approach yields higher accuracy than just using articulatory features targets.</p><p>Besides, our experiments show that sampling from indicators posteriors (LAS-MTL-S) performs better than using a mapping matrix (LAS-MTL-M). A possible explanation is that articulatory features can express minor variations in phones pro-nunciation (such as assimilation or coarticulation) that are not reflected in the phonetic alphabet.</p><p>An important takeaway from this experiment is that a sequence-to-sequence articulatory features detection model, besides actual speech indicators, also yields phone recognition results close to strong baselines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Manners and places of articulation detection</head><p>In <ref type="figure" target="#fig_2">figure 2</ref> we can see outputs of the multitask model decoder for indicators on SX100 utterance. These are raw values without "rounding" to the closest phones. To the best of our knowledge, there is no previously published results for sequence-level articulatory features detection on TIMIT. In <ref type="bibr" target="#b13">[14]</ref> the authors report articulatory features detection results on Wall Street Journal (WSJ) corpus <ref type="bibr" target="#b19">[20]</ref>. The accuracy is in 91-96% range, yet to do a proper comparison, we might need to train a model on WSJ.</p><p>Having no other options, we compare to frame-level baselines available. For King and Taylor paper <ref type="bibr" target="#b10">[11]</ref>, we provide results from experiment 2 which has a broader features set that is closer to what we used. ASAT <ref type="bibr" target="#b5">[6]</ref> paper mentions only a few accuracy values and some of them are suboptimal, for example, 75% for close and 68% for mid places. The reported phone error rate for ASAT is 29.4%.</p><p>Using alignments from attention, it is possible to map sequence-level posteriors to acoustic frames from the original waveform. It is important to understand that a direct projection of sequence outputs to frames is not precise. Still, we provided accuracy of a frame-level projection to show that even without direct training on frame targets, alignment makes sense in spite of pyramidal stacks of recurrent layers in the encoder. <ref type="table">Table 3</ref>: Manner and places of articulation on Timit corpus. Ours -LAS-MTL-M model, KT -King and Taylor paper <ref type="bibr" target="#b10">[11]</ref>. Accuracy rates are reported at frame and sequence levels. For frames level, our model accuracy is computed with a direct projection to frames and with a projection to markup segments.  <ref type="table">Table 3</ref> shows detection accuracy for manners and places of articulation. Our attention-based model for articulatory features provides accurate sequence-level estimates with most manners and places being in 95%+ range. PER and indicators accuracy imply that even if the decoder yields a wrong phone, most of articulatory features inferred would align with ground truth.</p><p>To calculate frame accuracies presented in the "Ours frames" column, we used the following phone-to-frame mapping algorithm. Soft attention was converted to hard attention using the dynamic time warping algorithm (DTW) <ref type="bibr" target="#b20">[21]</ref>. Ambiguities in phone-to-frame mapping were resolved by leaving points with higher attention weights on the alignment path.</p><p>Inaccuracies on the frame level ("Ours frames" column) are almost exclusively caused by boundary frames: attention out-puts a peak at the prominent acoustic part of a phone sound and the projection for the remaining part becomes ambiguous. This effect is especially clear for long phones, e.g. vowels, and for long silences where a single "sil" symbol is mapped to 1 or 2 starting frames from the final encoder layer.</p><p>It is possible to do a more precise projection by using attention peaks as a phone nuclei estimator and combining them with other waveform segmentation methods. In this manner, the frame-level accuracy would be much closer to the sequence level. To illustrate this point, we projected predicted indicators to waveform segments from TIMIT markup using attention peaks (the "Ours markup frames" column in <ref type="table">table 3</ref>). As a result, accuracy for all features increased significantly. For some features, e.g. vowels, accuracy went up by 20 percent points. Theregore, a combination of a LAS-based model with an external waveform segmentation model may lead to more precise alignments of detected features with the acoustic signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>A LAS encoder is a stack of pyramidal layers. As a result, the top-most layer has a typical window step of 40 -80 ms depending on the number of encoders layers. This interval determines inaccuracy that would be even in case of the ideal projection of sequence symbols to frames through attention. One way to infer more accurate phone boundaries within 80ms superframes is to use conventional methods for speech segmentation.</p><p>An interesting side note is that independent from input features or model hyperparameters, a single most common mistake made by our models was "ah" -"ih" substitution. It accounted for more than 0.5 percent point of all mistakes.</p><p>It is worth noting that TIMIT has explicit phone sequences for each utterance, yet it is possible to perform training on corpuses with just textual transcript by applying grapheme-tophoneme conversion. One way to do this is to use the "espeakng 1 " software that has various models for conversion from graphemes to International Phonetic Alphabet. As a result, it is possible even to train a multilingual model on several corpuses for different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The paper proposes a novel approach to end-to-end articulatory features detection. The resulting model yields posteriorgrams for articulatory features, rough alignments with acoustic data and competitive phone error rates even in low-resource settings.</p><p>In future, we would like to study the possibilities of applying our approach to recognition of phones and articulatory features on a zero-resource language using the model trained on a higher-resource language(s). Besides, we would like to study more carefully the ways to improve time alignment of the model outputs with the waveform.</p><p>The code we used to train and evaluate our model is available at https://github.com/sciforce/phones-las .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Attention output for phrase SX100 from test set: "The best way to learn is to solve extra problems".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>shows the output of attention for phrase SX100 "The best way to learn is to solve extra problems" from the test set. The model learned to align indicators with acoustic data. Mapping articulatory features to phones yields the following transcription: [sil dh ah sil b ae s sil t w ey sil t ah dh er n ih z sil t ih s aa l v eh sil k s sil t er sil p r aa sil p l m s sil].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Articulatory features posteriors for phrase SX100 from the test set: "The best way to learn is to solve extra problems". Ground truth features are marked with white circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Phone error rate (PER) comparison for different feature sets on TIMIT corpus.</figDesc><table><row><cell>Feature</cell><cell>PER</cell></row><row><cell cols="2">Melfilterbanks 21.7%</cell></row><row><cell>MFCC</cell><cell>21.8%</cell></row><row><cell>Lyon</cell><cell>21.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Phones recognition on TIMIT corpus. LAS -a model trained on phone targets. LAS-F -a model trained on articulatory features targets. LAS-MTL-S -a multitask model with sampling from the decoder outputs. LAS-MTL-M -a multitask model with the decoder inputs converted using a mapping matrix.</figDesc><table><row><cell>Paper</cell><cell>Model</cell><cell cols="2">Parameters PER</cell></row><row><cell cols="2">Baseline [10] Transducer</cell><cell>4.3M</cell><cell>17.7%</cell></row><row><cell cols="2">Baseline [16] ARSG</cell><cell>around 6M</cell><cell>18.7%</cell></row><row><cell>Ours</cell><cell>LAS</cell><cell>5.6M</cell><cell>20.2%</cell></row><row><cell>Ours</cell><cell>LAS-F</cell><cell>5.6M</cell><cell>23.4%</cell></row><row><cell>Ours</cell><cell>LAS-MTL-S</cell><cell>7.1M</cell><cell>20.4%</cell></row><row><cell>Ours</cell><cell cols="2">LAS-MTL-M 7.1M</cell><cell>20.8%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/espeak-ng/espeak-ng 2 https://github.com/WindQAQ/listen-attend-and-spell</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>We would like to thank Tzu-Wei Sung from National Taiwan University for his implementation of "Listen, Attend and Spell" model 2 that we used as a starting point for our experiments. Also we would like to thank Olga Zvyeryeva for preparing mappings from phones to articulatory features.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TIMIT Acoustic-phonetic Continuous Speech Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LDC93S1, Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018 -19 th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Hyderabad, India, Proceedings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Sound Pattern of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Estimation of Articulatory Features and Acoustic models for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2017 -18 th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Stockholm, Sweden, Proceedings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Detection-Based ASR in the Automatic Speech Attribute Transcription Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bromberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An attribute detection based approach to automatic speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Loquens</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An overview of spoken language technology for education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mispronunciation Diagnosis of L2 English at Articulatory Level Using Articulatory Goodness-Of-Pronunciation Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7 th ISCA Workshop on Speech and Language Technology in Education</title>
		<meeting>7 th ISCA Workshop on Speech and Language Technology in Education</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Detection of Phonological Features in Continuous Speech using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="333" to="353" />
		</imprint>
	</monogr>
	<note>Computer Speech &amp; Language</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminating Nasals and Approximants in English Language Using Zero Time Windowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018 -19 th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Hyderabad, India, Proceedings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Articulatory Feature Classification Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merkx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Scharenborg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018 -19 th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Hyderabad, India, Proceedings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining Articulatory Features with End-to-end Learning in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>27th International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23 rd International Conference on Machine Learning</title>
		<meeting>the 23 rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speaker-independent phone recognition using hid-den Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1989-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Computational Model of Filtering, Detection, and Compression in the Cochlea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The design for the Wall Street Journalbased CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FastDTW: Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="561" to="580" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
