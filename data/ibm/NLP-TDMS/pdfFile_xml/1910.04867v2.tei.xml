<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning promises to unlock deep learning for the long tail of vision tasks without expensive labelled datasets. Yet, the absence of a unified evaluation for general visual representations hinders progress. Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error). We present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. With VTAB, we conduct a large-scale study of many popular publicly-available representation learning algorithms. We carefully control confounders such as architecture and tuning budget. We address questions like: How effective are ImageNet representations beyond standard natural datasets? How do representations trained via generative and discriminative models compare? To what extent can self-supervision replace labels? And, how close are we to general visual representations? * Equal contribution . Correspondence to: Neil Houlsby &lt;neil-houlsby@google.com&gt;.</p><p>Preprint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Distributed representations learned from raw pixels have enabled unprecedented performance on many visual understanding tasks. Hand-crafted features have been replaced with hand-annotated datasets, with thousands to millions of examples <ref type="bibr">(Krizhevsky, 2009;</ref><ref type="bibr" target="#b8">Russakovsky et al., 2015)</ref>. By contrast, humans learn a wide range vision tasks using just a few examples per task. A key research challenge is to close this gap in sample efficiency, and unlock deep learning for the long tail of problems without many labels.</p><p>Improving sample efficiency has been approached from many angles: few-shot learning <ref type="bibr">(Fei-Fei et al., 2006)</ref>, transfer learning <ref type="bibr">(Pan &amp; Yang, 2009</ref>), domain adaptation <ref type="bibr">(Wang &amp; Deng, 2018)</ref>, and representation learning <ref type="bibr">(Bengio et al., 2013)</ref>. Representation learning is studied in many contexts: supervised pre-training <ref type="bibr" target="#b12">(Sharif Razavian et al., 2014)</ref>, selfsupervised learning <ref type="bibr">(Doersch et al., 2015)</ref>, semi-supervised learning <ref type="bibr">(Chapelle et al., 2009)</ref>, generative modeling <ref type="bibr">(Donahue et al., 2017)</ref>, and disentanglement learning <ref type="bibr">(Higgins et al., 2017)</ref>. Each sub-domain has its own evaluation protocol, and the lack of a common benchmark impedes progress. Benchmarks have been critical in other sub-fields, such as RL <ref type="bibr">(Mnih et al., 2013)</ref>, image classification <ref type="bibr" target="#b8">(Russakovsky et al., 2015)</ref>, and NLP <ref type="bibr">(Wang et al., 2018)</ref>. Inspired by these successes, we propose a benchmark with similar principles: (i) minimal constraints to encourage creativity, (ii) a focus on practical considerations, and (iii) make it challenging.</p><p>We present the Visual Task Adaptation Benchmark (VTAB). Using VTAB, we perform the first extensive cross sub-field study of representation learning. VTAB defines a good representation as one that can be used to solve many diverse previously unseen tasks with the fewest possible labels. We consider low sample complexity to be the key objective of representation learning. Task diversity is also crucial to assess generality. Therefore, VTAB goes beyond standard natural tasks, and includes those related to sensorimotor control, medical imaging, and scene understanding.</p><p>In our study, we investigate many representation learning algorithms, pre-trained on ImageNet. We carefully control confounding factors such as tuning budget, architecture, and pre-training data. This study quantifies existing intuitions and reveals new insights: (i) Supervised ImageNet pretraining yields excellent representations for natural image classification tasks. Interestingly, it also yields a smaller, but consistent improvement on specialized tasks (e.g. medical imaging). However, these representations are extremely limited for tasks that require structured understanding. (ii) Selfsupervised is less effective than supervised learning overall, but surprisingly, can improve structured understanding. arXiv:1910.04867v2 [cs.CV] 21 Feb 2020 (iii) Combining supervision and self-supervision is effective, and to a large extent, self-supervision can replace, or compliment labels. (iv) Discriminative representations appear more effective than those trained as part of a generative model, with the exception of adversarially trained encoders.</p><p>(v) GANs perform relatively better on data similar to their pre-training source (here, ImageNet), but worse on other tasks. (vi) Evaluation using a linear classifier leads to poorer transfer and different conclusions. We seek algorithms that perform well on a wide variety of unseen visual understanding tasks with few labels per task. <ref type="bibr">1</ref> We first formalize this objective and then specify a practical benchmarking procedure to measure progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Visual Task Adaptation Benchmark</head><p>A dataset D n is a set of n instances {(x i , y i )} n i=1 with observations x i ∈ X and labels y i ∈ Y . A prediction function is any mapping F : X → Y (e.g. a classifier). A (learning) algorithm, A, takes as input a dataset and outputs a prediction function. For example, A may be a pre-trained network coupled with a training mechanism. An evaluation procedure, E T , takes F and outputs a scalar measuring F 's performance (e.g. test-set accuracy). We seek the algorithm that maximizes the expected performance over a distribution of tasks P T , where a task T is a tuple containing a taskspecific dataset distribution D T and evaluation procedure. Given only n samples per task we want to maximize:</p><formula xml:id="formula_0">SCORE n (A) = E T ∼P T E T [A(D n T )] ,<label>(1)</label></formula><p>This general formulation requires a few clarifications. First, the task distribution needs to be appropriate; we desire a spectrum that covers tasks solvable by a vision algorithm with human-like capabilities. Second, n may be varied to measure an algorithm's sample complexity. In practice we choose n to be similar to a modest labelling budget (Section 3.1). Third, we assume that P T is known, and we aim to build an algorithm with the best inductive biases for solving samples from it. We desire an "open world", where evaluation data is extremely diverse and can always be freshly sampled. Unfortunately, in practice the benchmark must contain a finite number of test tasks. Therefore, we must ensure that the algorithms are not pre-exposed to specific evaluation samples, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A Practical Benchmark</head><p>We now describe the Visual Task Adaptation Benchmark (VTAB), which is designed to be the best possible proxy for Eq. (1). <ref type="figure" target="#fig_0">Fig. 1</ref> present an overview.</p><p>Task Distribution We aim for universal visual understanding, so we informally define P T as "Tasks that a human can solve, from visual input alone.". We validate this empirically, see Appendix B. Intuitively, such tasks should benefit from visual representations learned by observing and interacting with the natural world. The second clause eliminates tasks that require external knowledge that is (currently) unreasonable for a vision algorithm to acquire -an extreme example would be to classify objects grouped by their spelling in a natural language. Section 2.2 details the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation Over Tasks</head><p>We approximate the expectation over tasks by an empirical average over a number of hand-picked samples. Ideally, we would sample a new task for each evaluation, as is possible in procedural environments, e.g. <ref type="bibr">(Finn et al., 2017)</ref>. However, real-world vision tasks are expensive to collect, so we define a fixed, representative set of samples from P T . While fixing the set reduces variance, it introduces a risk of meta-overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mitigating Meta-Overfitting</head><p>To reduce metaoverfitting, we treat the evaluation tasks like a test set, and consider them unseen. Therefore, algorithms that use pre-training must not pre-train on any of the evaluation tasks (even their unlabelled images). Upstream training should provide useful inductive biases for any draw from P T , so should not use test samples. Fortunately, despite numerous test re-evaluations on popular ML benchmarks, such as ImageNet, progress seems to transfer to new data <ref type="bibr" target="#b4">(Recht et al., 2018;</ref><ref type="bibr">Kornblith et al., 2019)</ref>.</p><p>Unified Implementation With many diverse tasks, usage could become impractical. As discussed, the algorithms must have no prior knowledge of the downstream tasks; while it is permitted to run a hyperparameter search on each task, the search space cannot be task-dependent. To get meaningful results, we need to define hyperparameter searches that work well across the benchmark.</p><p>For this, we convert all tasks into classification problems. For example, a detection task requiring localization of an object can be mapped to classification of the (x, y, z) coordinates. With a homogeneous task interface, we may control for possible confounding factors. For example, we may use the same architecture and hyperparameter sweep everywhere. Not all tasks can be efficiently modeled as image-level classification, such as those requiring per-pixel predictions. Nonetheless, we design the tasks such that success on VTAB requires learning of diverse set of visual features: object identification, scene classification, pathology detection, counting, localization, and 3D geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tasks</head><p>VTAB contains 19 tasks which cover a broad spectrum of domains and semantics. Appendix A contains details. These are grouped into three sets: NATURAL, SPECIALIZED, and STRUCTURED.</p><p>The NATURAL group represents classical vision problems. These tasks contain natural images captured using standard cameras. The classes may represent generic, fine-grained, or abstract objects. The group includes: Caltech101, CIFAR-100, DTD, Flowers102, Pets, Sun397, and SVHN.</p><p>The SPECIALIZED group also contains images of the world, but captured through specialist equipment. These images have different invariances to those in the NATURAL tasks. Nonetheless, humans recognize the structures therein, thus generic visual representations should also capture the visual concepts. We have two sub-groups: remote sensing, and medical. Remote sensing includes Resisc45 and Eu-roSAT: aerial images of the earth captured using satellites or aerial photography. Medical includes Patch Camelyon, metastases detection from microscopy images, and Diabetic Retinopathy, retinopathy classification from fundus images.</p><p>The STRUCTURED group assesses comprehension of the structure of a scene, for example, object counting, or 3D depth prediction. Most of these tasks are generated from simulated environments, whose structure is easy for a human to determine, but whose domain differs greatly to datasets like ImageNet. These tasks are intended as a step towards useful representations for perceptual control. We include: Clevr: Simple shapes rendered in a 3D scene, with two tasks: counting and depth prediction. dSprites: Simple black/white shapes rendered in 2D, with two tasks: location and orientation prediction. SmallNORB: Artificial objects viewed under varying conditions, with two tasks: objectazimuth and camera-elevation prediction. DMLab: Frames from a rendered 3D maze. The task involves predicting the time for a pre-trained RL agent to navigate to an object. KITTI: frames captured from a car driver's perspective. We predict the depth of the nearest vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Representation and Transfer Learning</head><p>Success on VTAB, and ultimately optimizing Eq. (1), requires some knowledge of P T . For example, CNN architectures have a useful inductive bias for vision. While these biases are usually manually designed (e.g. through architecture choice), they can also be learned. For example by learning: architectures Zoph &amp; Le (2017), optimization algorithms <ref type="bibr">Bello et al. (2017)</ref>, initialization distributions <ref type="bibr" target="#b0">Raghu et al. (2019)</ref>, or pre-trained data representations. Whilst many strategies merit investigation, we focus on representation learning. Human visual perception is refined through years of observation and interaction with the world, resulting in a system that solves new tasks with few indomain labels. Likewise, we aim to pre-train a network that extracts useful features, or representations, from raw data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Strategy</head><p>The representations are not pretrained on the evaluation tasks themselves (which VTAB forbids), so they must be adapted to solve the new tasks (using limited in-domain data). The simplest adaptation strategy in deep representation learning is to first pre-train the network, then freeze the network's weights and train another -usually smaller -model on top. When the upstream and downstream datasets differ significantly, fine-tuning the original weights is more effective <ref type="bibr">(Yosinski et al., 2014;</ref><ref type="bibr">Kornblith et al., 2019)</ref>. VTAB does not constrain the transfer strategy; here we use fine-tuning as it tends to perform best.</p><p>Upstream Training Representation learning literature often focuses on unsupervised learning which may be applied to any dataset. However, supervised data, where available, can yield good representations. Indeed, the most popular models used in practice are pre-trained on ImageNet labels <ref type="bibr">(Huh et al., 2016, and refs therein)</ref>. VTAB does not constrain the type of data used for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Large-Scale Study</head><p>With VTAB, we evaluate many popular, publicly-available representation learning algorithms across the different types of tasks. Here, we study the pre-training losses, and therefore, control for other factors such as architecture, preprocessing, transfer hyperparameters, and pre-training data. Of course, VTAB may be used subsequently to study and improve these aspects as well. Finally, we provide additional analysis of various of VTAB's design choices and assess other evaluation protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setup</head><p>Downstream Data Size Section 2.2 describes the tasks. VTAB aims to assess adaptation with limited data, so we evaluate primarily using 1,000 labelled examples per task (called VTAB-1k). We define train (800 samples) and validation (200 samples) sets for users' convenience. We emphasize that to avoid meta-overfitting, one should not use extra images for hyperparameter selection, but may use the 1,000 examples in any manner. We also use the full datasets (VTAB-full). This allows us to assess the value of representation learning as in-domain data increases, and to check performance against prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Learning Algorithms</head><p>We evaluate supervised, semi-supervised, self-supervised, and generative models, selecting popular, public methods from each class. We also compare to "from-scratch" models, which use no pre-training, yielding a total of 18 algorithms. All models (except "from-scratch") are pre-trained on ImageNet (1.28M labelled images). Some use the ImageNet labels, others use some or none of the labels.</p><p>We train two supervised models: one using 100% of the ImageNet labels (SUP-100%), and one using 10% (SUP-10%). For self-supervised learning, we include both imagebased and patch-based models. The image-based models include <ref type="bibr">ROTATION (Gidaris et al., 2018)</ref> and EXEM-PLAR <ref type="bibr">(Dosovitskiy et al., 2014)</ref>. Patch-based include REL-ATIVE PATCH <ref type="bibr">LOCATION (Doersch et al., 2015)</ref> and JIG-SAW <ref type="bibr">(Noroozi &amp; Favaro, 2016)</ref>. We use the public implementations of <ref type="bibr">Kolesnikov et al. (2019)</ref>. The patch-based models are converted into image-level classifiers by averaging the representations from a 3 × 3 grid of patches (see Appendix C). The semi-supervised models are trained using 10% of the ImageNet labels with an auxiliary loss on all of the data, see <ref type="bibr">(Zhai et al., 2019)</ref>. We use either rotation (SUP-ROTATION-10%) or Exemplar (SUP-EXEMPLAR-10%) auxiliary losses. These models can also use all of the labels, denoted SUP-ROTATION-100% and SUP-EXEMPLAR-100%. For the generative models, we evaluate GANs and VAEs. As is common, we take the GAN's image representations from the discriminator, replacing the final linear layer with a classification layer. We use both the label-conditional and unconditional BigGAN discriminators (Brock et al., 2019) (COND-BIGGAN and UNCOND-BIGGAN) using the implementations of Chen et al. <ref type="formula" target="#formula_0">(2019)</ref>; <ref type="bibr">Lucic et al. (2019)</ref>. We also evaluate the encoder from <ref type="bibr">BIGBIGAN (Donahue &amp; Simonyan, 2019)</ref>. For the autoencoders, we use the encoder as the representation. We evaluate VAEs <ref type="bibr">(Kingma &amp; Welling, 2014)</ref>, and WAEs with three distribution matching losses: <ref type="bibr">GAN, MMD (Tolstikhin et al., 2018)</ref>, and UKL <ref type="bibr" target="#b7">(Rubenstein et al., 2019)</ref>.</p><p>Transfer Algorithm VTAB permits any method of transfer. Here, we fine-tuning the entire network on the task data, since that performs best <ref type="bibr">(Kornblith et al., 2019)</ref>. It is also popular to add a linear model to a frozen network, so we compare to this approach in Section 3.4.</p><p>Hyperparameters Upstream, we contol the data (Ima-geNet) and architecture. We find that bigger architectures perform better on VTAB <ref type="bibr">(Appendix L and Kolesnikov et al. (2019)</ref>). For comparability, we use ResNet50-v2, or similar, in this study. For the supervised and semi-supervised methods we use the architecture in <ref type="bibr">He et al. (2016)</ref>. For GANs and auto-encoders we use very similar deep ResNets, but with appropriate modifications to pre-train them successfully as a generative model. Appendix C contains details.</p><p>Downstream, the transfer hyperparameters influence performance, and different datasets require different settings. We run VTAB in two modes: lightweight and heavyweight. Lightweight mode performs a restricted per-task hyperparameter search. Many hyperparameters are fixed, including optimizer, batch size, pre-processing, and weight decay. For each task, the lightweight mode sweeps: 2 initial learning rates, and 2 learning rate schedules. See Appendix I for details. We choose short schedules to limit cost, but show in Section 3.5 that these yield near-optimal performance.</p><p>In heavyweight mode we perform a large random search over learning rate, schedule, optimizers, batch size, train preprocessing functions, evaluation pre-processing, and weight decay. We include longer training schedules and higher resolution images. This mode is used to understand the impact of a larger computational budget on the performance, and establish performance upper bounds on the methods used. Appendix I contains details.</p><p>We perform our main study in lightweight mode. In Section 3.3 we show that although extensive tuning improves all methods, the relative performances are mostly unaffected. For future study using VTAB, we recommend defining a similar lightweight setup (or using the one here) that facilitates fair comparison without high cost. If ample computational resources are available, a heavyweight mode may be used to improve the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tuning and Evaluation Protocol</head><p>We perform adaptation on the training set and perform model selection for each task on the pre-defined validation sets. We re-train the best model on the union of training and validation sets and evaluate on the test set. Note, for VTAB-1k we define custom train set with 800 examples and validation set with 200 examples. We run the final evaluation on the test set with three random seeds and report the median score.</p><p>Metrics We evaluate with top-1 accuracy. We consider other metrics in Appendix D, and find the conclusions are the same. To aggregate scores across tasks, we take the mean accuracy. We investigate more complex aggregation strategies in Section 3.5, but the relative performances are unaffected, so we use mean accuracy for simplicity.  <ref type="figure">Figure 2</ref>. The methods are divided into five groups: Generative, training from-scratch, all methods using 10% labels <ref type="bibr">(Semi Supervised)</ref>, and all methods using 100% labels (Supervised), For each task group, bars show the mean accuracy across all methods with 3 repeats. Error bars indicate standard deviation across methods. The BigBiGAN outlier causes the high variance in generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VTAB Results</head><p>We run all 18 methods on 19 tasks for both VTAB-1k and VTAB-full using the lightweight sweep. <ref type="figure">Fig. 2</ref> shows aggregate performance of each method group (generative, self-sup, etc.) on each VTAB task group ( NATURAL, etc.). <ref type="figure">Fig. 3</ref> presents a breakdown of the performance of each algorithm on each taks group. <ref type="figure">Fig. 4</ref> shows two detailed per-task comparisons: SUP-100% (ImageNet) versus FROM-SCRATCH, and SUP-ROTATION-100% (best overall on VTAB-1k) versus SUP-100%. Appendix G, <ref type="table">Table 6</ref> contains the complete results table.</p><p>Generative Models Overall, these perform worst, <ref type="figure">Fig. 2</ref>. The autoencoders perform more poorly than training from scratch. Despite SOTA generative quality on ImageNet, the GANs' discriminators do not provide useful representations. The GAN discriminators appear unstable, with large error bars in <ref type="figure">Fig. 3</ref>. The BIGBIGAN encoder stands out, performing much better -similar to the best self-supervised models, <ref type="figure">Fig. 3</ref>. Interestingly, BIGBIGAN performs relatively well on natural tasks, but less so on structured tasks. UNCOND-GAN shows a similar pattern. This indicates that GANs fit more strongly to ImageNet's domain (natural images), than self-supervised alternatives. Despite huge advances in generative quality, these models do not yield great representation quality. Similarly, <ref type="bibr" target="#b1">Ravuri &amp; Vinyals (2019)</ref> show that models with high sample quality fail to generate data from which an accurate classifier can be trained. However, the BigBiGAN result holds promise for adversarially-trained encoders, and consideration of downstream representation quality may lead to further progress.</p><p>Self-supervised All self-supervised representations outperform from-scratch training. The best, ROTATION, attains 59.6% on VTAB-1k, while FROM-SCRATCH attains 42.1% (Appendix G, <ref type="table">Table 6</ref>). Methods applied to the entire image (ROTATION, EXEMPLAR) outperform patch-based methods (JIGSAW, REL.PAT.LOC.). However, the patch based methods perform slightly better on DTD (Describable Textures) and Retinopathy (Fundus images), see Appendix G, <ref type="table">Table 6</ref>. Intuitively, these tasks require sensitivity to local textures, which are indeed captured by patch-based methods. On NATURAL tasks, self-supervision is far behind supervised methods <ref type="figure">(Fig. 2)</ref>. On SPECIALIZED, the performances are similar, and most interestingly, on STRUCTURED selfsupervised methods performs slightly better than supervised. This indicates supervised ImageNet models are invariant to useful features required for structured understanding, but self-supervised methods can capture these to some degree.</p><p>(Semi-)Supervised Overall, supervised models perform best. The benefits are most pronounced on the NATURAL tasks, whose domain and semantics are arguably most similar ImageNet classification. However, with self-supervision, the benefit can be attained with fewer labels. SUP-10% (120k labels) attains 61.6% on VTAB-1k , while SUP-100% (1.2M labels) attains 65.6%. With self-supervision, SUP-ROTATION-10% closes 80% of the gap, attaining 64.8%. Recent work has shown in-domain benefits of semisupervised learning <ref type="bibr">(Hénaff et al., 2019;</ref><ref type="bibr">Berthelot et al., 2020)</ref>; our study reveals that semi-supervised methods also learn good representations for transfer to new tasks. Finally, additional self-supervision even improves on top of 100% labelled ImageNet, particularly on STRUCTURED tasks ( <ref type="figure">Fig. 4</ref>. SUP-100% attains 65.6%, whereas SUP-ROTATION-100% attains 67.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Heavyweight Hyperparameter Sweeps</head><p>We evaluate FROM-SCRATCH and the best models: SUP-100%, SUP-ROTATION-100%, and SUP-EXEMPLAR-100% using the heavyweight hyperparameter search: Table 1. Appendix I shows the selected hyperparameters. As expected, all methods improve significantly. Prior work <ref type="bibr">(He et al., 2018)</ref> shows that with sufficient data and training time, from-scratch training is competitive for detection. However, we observe that across all task groups, pre-trained representations are better than a tuned from-scratch model. On a couple of tasks FROM-SCRATCH performs competitively -Clevr-Dist and sNORB-Elev, which require localization and camera elevation respectively -indicating that even the best ImageNet representations fail to capture these aspects. The ranking of the best methods is similar, but not identical, with a combination supervision and self-supervision  <ref type="figure">Figure 3</ref>. Average top-1 accuracy across the tasks in each group for VTAB-1k. The x-axis indexes the methods, ordered ordered according to their average accuracy across all tasks (dashed curve).</p><p>Error bars indicate 95% confidence interval using bootstrap resampling from the 3 experiment repeats. (SUP-EXEMPLAR-100%) getting the best performance.</p><p>We check our results against those recently reported in the literature (Appendix H, <ref type="table">Table 8</ref>). Our results are comparable; behind on highly popular tasks on which complex architectures have been optimized (e.g. CIFAR), but ahead in others. In Appendix L we show that simply increasing the architecture size improves VTAB performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Frozen Features Extractors</head><p>Representations are often evaluated by training a linear layer on a frozen model <ref type="bibr">(Kolesnikov et al., 2019)</ref>. ImageNet is often used for linear evaluation, but this is meaningless for the models we consider because many use ImageNet labels for pre-training. However, linear evaluation is also used in a transfer setting (Goyal et al., 2019a), so we apply this protocol to the VTAB tasks, and contrast it to finetuning. The full protocol resembles the lightweight finetuning sweep, see Appendix K.</p><p>Fig <ref type="figure">. 5</ref> shows the per-task correlation between linear and fine-tuning. Appendix K, <ref type="table">Table 9</ref> contains the full table of results. We first note that linear evaluation significantly lowers performance, even when downstream data is limited to 1000 examples. SUP-100% attains 65.6% with fine-tuning (lightweight sweep), but 57.3% with li-near. Linear transfer would not by used in practice unless infrastructural constraints required it. Second, <ref type="figure">Fig. 5</ref> shows that on many datasets, particularly SPECIALIZED and STRUCTURED, the correlation is low. Linear evaluation may lead to different conclusions, for example, COND-BIGGAN attains 43.3% (1000-examples) on linear, outperforming both REL.PAT.LOC and JIGSAW. Yet when fine-tuned these self-supervised methods significantly outperform the GAN discriminator. Another discrepancy is between semi-supervised and supervised. SEMI-ROTATION-10% and SEMI-EXEMPLAR-10% are 1 − 2% behind SUP-100% with fine-tuning, but 4−5% behind with linear evaluation. These self-supervised methods extract useful representations, just without linear separability. Previous works <ref type="bibr">(Kornblith et al., 2019;</ref><ref type="bibr">Kolesnikov et al., 2019)</ref> claim that linear evaluation results are sensitive to additional factors that we do not vary, such as ResNet version or pre-training regularization parameters. Overall linear evaluation is a poor proxy for overall reduced sample complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Analysis</head><p>Metrics We explore alternatives to using top-1 accuracy, and aggregation of scores accross tasks using the mean. Instead of top-1 accuracy, we used mean-per-class accuracy and Cohen's quadratic kappa. These metrics create only  <ref type="figure">Figure 4</ref>. Absolute difference in top-1 accuracy between method pairs for each dataset, using 1k examples. The bar colour indicates the task group: NATURAL, SPECIALIZED, and STRUCTURED. Left: SUP-100% versus FROM-SCRATCH -supervised pretraining yields a large improvement on the NATURAL datasets and some others. Right: SUP-ROTATION-100% versus SUP-100%the additional self-supervised loss on top of supervised loss yields improvements, especially on the STRUCTURED tasks. Note that the y-scales differ. minor ranking differences, leaving the overall picture unchanged. Kendall's ranking correlation with respect to top-1 accuracy is always &gt; 0.97. Details are in D. For aggregation across tasks, we consider seven alternative strategies: (i) Macro-average across datasets (grouping tasks with the same input data). (ii) Macro-average across groups (merging tasks in the same group). (iii) Geometric mean. (iv) Average rank. (v) Average rank after small perturbation of the scores with noise. (vi) Robust (binned) average rank.</p><p>(vii) Elimination rank -equivalent to an "Exhaustive Ballot". All strategies have a high Kendall's correlation with the vanilla mean across tasks (τ &gt; 0.87). Appendix E contains details. Based on these results we choose mean top-1 for VTAB since it is simple, interpretable, can be computed for each task independently, and is highly correlated with more complex appraoches.</p><p>Representative Subset For prototyping, a representative subset of VTAB may be useful. We compute the rank correlation between the mean scores produced by each 20 5 subsets of five tasks, and the full suite. The top-5 subsets tend to span different domains, but differ to each other. Ap-  pendix F contains the results. Although a subset might be useful for screening models, these were computed on our set of models, and may correlate less well in other experiments. Using subsets also increases the risk of meta-overfitting, which VTAB aims to avoid by having many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limiting Evaluation Cost</head><p>The cost is near linear in the schedule length. To determine a minimal, yet meaningful, schedule, we sweep over schedules ranging from 40 to 40,000 steps, using batch size 512. <ref type="figure">Fig. 6</ref> summarizes the results, details Figs. 19 and 20, Appendix M. Most runs reach their optimal performance within 1,000 steps and do not improve significantly when trained for longer.</p><p>Finetuning and evaluating a given ResNet-50 model on a single Nvidia P100 GPU (batch size of 64 images, 0.01 initial learning rate) takes 3 hours for VTAB-1k. To reduce the time and cost we use Google Cloud TPU-v3-16 accelerators and verify that we obtain similar resuts in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Vision Benchmarks The Visual Decathlon <ref type="bibr" target="#b3">(Rebuffi et al., 2017)</ref> contains ten classification tasks: Omniglot, and nine using natural images. Four overlap with VTAB, but VTAB includes several other domains and tasks. Importantly, these benchmarks have opposite evaluation protocols: The Visual Decathlon allows direct joint optimization on the tasks, but forbids external data. By contrast, VTAB forbids multi-task learning on the downstream tasks, but permits transfer from any arbitrary dataset -this simulates one-off training of representations which are then applied to novel tasks. Further, VTAB considers a low-sample regime (1000-examples), which reflects performance under a reasonable labelling budget. We conduct experiments showing that the methods ranked according to VTAB are more likely to transfer to new tasks, than those ranked according to the Visual Decathlon (Appendix N).</p><p>The Facebook AI SSL challenge <ref type="bibr">(Goyal et al., 2019b)</ref> was proposed to evaluate self-supervision. It contains four tasks on two datasets of natural images, including classification, detection, and low-label classification. The original paper (Goyal et al., 2019a) also includes navigation and surface normal estimation. VTAB uses classification tasks only to admit task-independent implementations, and attains diversity with many alternative domains and semantics (localization, counting, etc.). Our study includes self-supervision, but are not restricted to it. Indeed, one challenge is to outperform supervised ImageNet on VTAB. Most importantly, Facebook SSL requires transfer via a shallow network stacked on a fixed CNN, whereas VTAB permits any transfer, and focuses on performance with few samples.</p><p>Meta-Dataset (Triantafillou et al., 2019) contains natural image classification tasks. The protocol differs to VTAB: most of Meta-Dataset's evaluation datasets are also in the training set, and the train/test split is made across classes. Meta-Dataset is designed for few-shot learning, rather than 1000 examples, which may entail different solutions.</p><p>Representation Learning Evaluation Popular evaluations for representation learning are linear/MLP and semisupervised. In linear/MLP evaluation, widely used in for self-supervised representations <ref type="bibr">(Doersch et al., 2015;</ref><ref type="bibr">Zhang et al., 2016;</ref><ref type="bibr">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr">Doersch &amp; Zisserman, 2017)</ref>, the weights of a pre-trained network are frozen, and a linear layer/MLP is trained on top to predict the labels. Evaluation is often performed on the same dataset that was used to train the representations, but using all labels, defeating the goal of sample efficiency. A semi-supervised protocol is more realistic and performs better <ref type="bibr">(Zhai et al., 2019)</ref>. This evaluation is performed on a single dataset, by contrast VTAB concerns transfer to unseen tasks. Linear evaluation is sensitive to small upstream training details, such as the ResNet version or regularization, that make little difference when training end-to-end <ref type="bibr">(Kornblith et al., 2019;</ref><ref type="bibr">Kolesnikov et al., 2019)</ref>. Other intrinsic scores have been proposed to measure the disentanglement of representations <ref type="bibr">(Locatello et al., 2019)</ref>, or the mutual information between the input and representations <ref type="bibr">(Hjelm et al., 2018)</ref>. Both were shown to be weakly correlated with representation utility <ref type="bibr">(Locatello et al., 2019;</ref><ref type="bibr">Tschannen et al., 2019)</ref>.</p><p>Generative Evaluation Evaluation of generative quality is a difficult goal with numerous proxy metrics, such as reconstruction error, FID <ref type="bibr">(Heusel et al., 2017)</ref>, IS <ref type="bibr" target="#b10">(Salimans et al., 2016)</ref>, precision-recall <ref type="bibr" target="#b9">(Sajjadi et al., 2018)</ref>, log likelihood (for auto-regressive models), or the ELBO (for VAEs). Sometimes generative models are also evaluated using linear classification <ref type="bibr" target="#b10">(Radford et al., 2016;</ref><ref type="bibr">Donahue et al., 2017;</ref><ref type="bibr">Dumoulin et al., 2017)</ref>, or semi-supervised learning <ref type="bibr">(Kingma et al., 2014;</ref><ref type="bibr">Narayanaswamy et al., 2017;</ref><ref type="bibr">Tschannen et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our study answers important and timely questions on representation learning. First, how effective are supervised ImageNet representations? ImageNet labels are indeed effective for natural tasks. Further, although the literature reports mixed results <ref type="bibr">(Liu et al., 2017;</ref><ref type="bibr" target="#b0">Raghu et al., 2019;</ref><ref type="bibr">Neumann et al., 2019)</ref>, we find that they also work for specialized tasks despite the domain-shift (medical or remote sensing). However, for structured understanding, supervised representations can be poorer than unsupervised ones. As future work, training on diverse data sources, such as video, may enable moving beyond the "ImageNet-like" tasks.</p><p>Second, how do representations trained via generative and discriminative models compare? Generation is a useful task in of itself, but generative losses, in their current form, seem less promising as means towards learning how to represent data. BigBiGAN is a notable recent exception, that is on par with self-supervision and warrants more exploration.</p><p>Third, to what extent can self-supervision replace labels? Self-supervised learning appears promising, even outperforming supervision on some tasks that require structured understanding. Interestingly, self-supervision can almost (but not quite) replace 90% of ImageNet labels; the gap between pre-training on 10% labels with self-supervison, and 100% labels, is small. Further, self-supervision adds value on top of ImageNet labels on the same data. Overall, it seems we are quite far from general visual representations; simply adding more data on the SPECIALIZED and STRUCTURED tasks is better than the pre-training strategies we evaluated. However, self-supervison can be applied to any dataset of images or videos, so combining large scale open-domain self-supervision with ImageNet or other label sources may be promising future work.</p><p>VTAB measures representation quality as the ability to adapt with few examples to diverse, unseen tasks. Here, we provide a large study of upstream training losses and control other factors such as hyperparameter sweep, architecture, transfer algorithm, preprocessing, and pre-training data. However, VTAB may be used to analyze and optimze many factors involved in learning generalizable representations. Varying other factors to improve VTAB is valuable future research. To isolate the effect of each factor, confounders such as hyperparameter sweep size should be controlled. The only approach that is out-of-bounds is to condition the algorithm explicitly on the VTAB tasks, since this would compromise representation generalization.</p><p>Code and data to run VTAB are made available, and progress is monitored at github.com/google-research/ task_adaptation. The models presented here are released. We hope that VTAB drives representation learning towards making deep learning accessible to the many problems without vast labelling budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Raphael Marinier and Anton Raichuk for help building the DMLab classification task, and Samy Bengio and Kevin Murphy for useful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Balduzzi, D., Tuyls, K., Perolat, J., and Graepel, T. Reevaluating evaluation. In Advances in Neural Information Processing Systems, 2018.   EuroSAT (Helber et al., 2019) The task consists in classifying Sentinel-2 satellite images into 10 different types of land use (Residential, Industrial, River, Highway, etc). The spatial resolution corresponds to 10 meters per pixel, and the image size is 64x64 pixels.</p><p>Resisc45 <ref type="figure" target="#fig_0">(Cheng et al., 2017)</ref> The Remote Sensing Image Scene Classification (RESISC) dataset is a scene classification task from remote sensing images. There are 45 classes, containing 700 images each, including tennis court, ship, island, lake, parking lot, sparse residential, or stadium. The image size is RGB 256x256 pixels.</p><p>Patch Camelyon <ref type="figure" target="#fig_0">(Veeling et al., 2018)</ref> The Patch Camelyon dataset contains 327,680 images of histopathologic scans of lymph node sections. The classification task consists in predicting the presence of metastatic tissue in given image (i.e., two classes). All images are 96x96 pixels.</p><p>Retinopathy <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human Evaluation</head><p>We define P (T ) informally as "All tasks that a human can solve using visual input alone". Intuitively, the tasks in Appendix A satisfy this property because the task semantics involve simple visual concepts (for a human), and the images contain recognisable objects -either in a natural or artificial environment. However, we also check empirically that humans can solve the types of tasks using in VTAB from examples alone. We therefore evaluate human raters on a representative subset of the tasks used in VTAB: Pets (natural images, fine-grained object labels), DTD (natural, textures), Camelyon (specialized, medical images), EuroSAT (specialized, aerial images), DMLab (structured, distance prediction), and Clevr-count (structured, object counting).</p><p>For each human-evaluated dataset, raters are given 20 random examples for each class, taken from the training split. Raters are asked to classify between 50 and 100 images each, for a total of 1K images (except for DMLab: 534 images), randomly taken from the test split, based on the provided training examples. Beside the examples, no hints nor explanations are given on the nature of the tasks. The raters have to deduce which properties of each image should be used to classify those, for example: the breed of the animal, the distance between the camera and the objects, or the number of objects. The raters are asked not to exchange on the tasks, so each rater produces independent work.</p><p>All datasets rated by humans are the same as the one rated by the models, except for DMLab, where we only assess the distance prediction aspect and not object type. This is because there are too many object types for the raters to learn the two groups of objects from 20 examples per class. Therefore, we asses distance-prediction alone. The human DMLab task contains only 3 classes, each of which contains many object types, and differ only in object distance.</p><p>Note that this evaluation is not meant to quantify the relative performances of humans and machines, since protocol differences e.  <ref type="table">Table 3</ref>. Human evaluation scores, measured using mean-per-class accuracy.</p><p>We measure human performance using mean-per-class accuracy. This is because the human training sets are class-balanced, so the raters cannot learn the class priors which algorithms that see an i.i.d sample of the training set could. <ref type="table">Table 3</ref> shows the results. The results indicate that some tasks are harder due to more subtle distinctions (Camelyon) or noise (DMLab), and some very easy (Clevr-count). However, in all cases the raters perform significantly better than random guessing. This demonstrates that the types of tasks used in VTAB are ones for which human-like visual representations are useful to solve them, using few labels and visual-input alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architectures</head><p>All of the methods (except for BigGAN models) we evaluate in the paper use the standard <ref type="bibr">ResNet50-v2 (He et al., 2016)</ref> architecture.</p><p>This architecture consists of 50 convolutional layers. The representation produced by this model has 2048 dimensions. Exact details of this architecture can be found in <ref type="bibr">(He et al., 2016)</ref>.</p><p>In the Cond-BigGAN and Uncond-BigGAN models we use publicly available 2 implementation by <ref type="bibr">(Lucic et al., 2019)</ref> and <ref type="bibr">(Chen et al., 2019)</ref> of the custom ResNet-like architecture proposed and described in <ref type="bibr">(Brock et al., 2019)</ref>. It has 1536 dimensions in the final representation layer.</p><p>We use a specialized procedure for evaluating patch-based models <ref type="bibr">(Relative Patch Location (Doersch et al., 2015)</ref> and Jigsaw <ref type="figure" target="#fig_0">(Noroozi &amp; Favaro, 2016)</ref>). These models use ResNet50 model with the overall stride reduced from 64 to 16 (by substituting the first and the last strided convolution of the standard ResNet50 model by a convolution with stride one). During the adaptation phase, we apply ResNet50 with reduced stride in the following way (assuming that input image size is 224 × 224):</p><p>• Perform central crop of size 192 × 192.</p><p>• Cut image into 3 × 3 patches of size 64 × 64.</p><p>• Apply the ResNet50 model independently to every patch.</p><p>• Output the final representation and element-wise average of 9 individual patch representations.  <ref type="figure">Figure 7</ref>. Ranking of the methods using the average scores across datasets (validation split) using three different metrics: top-1 accuracy (left), mean-per-class accuracy (center), Cohen's quadratic kappa (right). The methods on the x-axis are sorted according to the highest scores according to each metric. Although there are some minor changes in the ranking between top-1 and Cohen's quadratic kappa, the overall performance of groups of methods remains unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Alternative Metrics</head><p>In previous works, some datasets are reported using alternative metrics to top-1 accuracy: mean-per-class accuracy (Caltech101) or Cohen's quadratic kappa (Retinopathy). We study whether these metrics reveal different results to top-1 accuracy. We find that, although there are minor ranking differences, the overall picture remains unchanged. Kendall's ranking correlation scores, with respect to top-1 accuracy are 1.0 (mean per-class accuracy) and 0.97 (quadratic kappa). <ref type="figure">Figure 7</ref> shows different test metric scores (top-1 accuracy, mean-per-class accuracy, and Cohen's quadratic kappa), averaged across all datasets in the benchmark, for the different methods studied. The methods were ranked in the x-axis according to their score in each metric. Observe that top-1 and mean-per-class accuracy give exactly the same ranking. There are subtle differences when using Cohen's quadratic kappa, but the overall picture remains unchanged: supervised methods outperform semi-supervised methods by a small margin, followed by self-supervised methods, etc. Kendall's ranking correlation scores, with respect to top-1 accuracy, are: 1.0 for mean per-class accuracy, and 0.97 for Cohen's quadratic kappa. This confirms that our conclusions hold even if we are not using the standard metrics for a few datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Alternative weighting and ranking schemes for models</head><p>Mean top-1 accuracy across tasks creates an implicit weighting. First, some tasks use the same input data (e.g Clevr-Count and Clevr-Dist), thus upweighting those domains. Second, the task groups differ in size. Third, the tasks exhibit different performance ranges across methods. Therefore, we compare seven different ranking aggregation strategies: (i) Macroaverage across datasets (grouping tasks with the same input data). (ii) Macro-average across groups (merging tasks in the same group). (iii) Geometric mean. (iv) Average rank. (v) Average rank after small perturbation of the scores with noise.</p><p>(vi) Robust (binned) average rank. (vii) Elimination rank -equivalent to an "Exhaustive Ballot".</p><p>All strategies have a high Kendall's correlation with the vanilla mean across tasks (τ &gt; 0.87). The most dissimilar strategy is the average rank, with τ = 0.873. Therefore, we use mean top-1 accuracy because it is interpretable and can be computed independently for each method, unlike rank-based aggregation.</p><p>Throughout, we use (unweighted) mean top-1 accuracy across all tasks to rank the different models. This assumes that the samples represent our desired task distribution P T in an unbiased manner. However, there may be other sensible weighting schemes that are not uniform. Here we explore three alternative weighting schemes (i) assigning equal weight to every dataset -there are three data sets that are used in two tasks, and the others are used in one task, (ii) assigning equal weight to each task group: NATURAL, SPECIALIZED, and STRUCTURED, and (iii) the weighting scheme introduced in Balduzzi et al. <ref type="bibr">(2018)</ref>.</p><p>Another issue inherent in (weighted or unweighted) mean accuracy is that the mean accuracies of individual tasks vary significantly depending on the task's difficulty. Since maximum accuracy is bounded, this may limit the range of performances, implicitly upweighting tasks with more "headroom". Therefore, we explore a few simple alternative ranking strategies (see <ref type="bibr">Dwork et al. (2001)</ref> for an introduction raking aggregation methods):</p><p>(i) Ranking according to the geometric mean.</p><p>(ii) The average rank obtained by ranking the models for each task according to accuracy and then computing the average rank across tasks.</p><p>(iii) A noise-perturbed version of (i) in which the accuracies are perturbed by Gaussian noise with variance 1.0 prior to ranking.</p><p>(iv) A robust variant of the average rank, where, prior to averaging ranks across tasks, the accuracy is binned into buckets of size 1% and all models in the same bucket obtain the same rank.</p><p>(v) An elimination ranking scheme equivalent to the "Exhaustive Ballot" voting system, see e.g. <ref type="bibr" target="#b11">(Shahandashti, 2016)</ref>.</p><p>We measure the agreement between these ranking strategies using the Kendall rank correlation coefficient <ref type="bibr">(Kendall, 1945)</ref>.</p><p>To account for the training/evaluation stochasticity in VTAB, we sample (independently for every pair of model and task) one out of the three test-set repetitions, and compute the rank correlation between ranking according to the mean accuracy and each alternative ranking. We average over 100 such samples. The results are in <ref type="table">Table 4</ref>. The mean rank correlation between the ranking according to the mean across tasks correlates very well with alternative ranking schemes. In particular, weighted means for 1000 samples and the full dataset as well as the geometric mean have a rank correlation with the ranking according to the mean that exceeds 0.95. The agreement with different types of average rank somewhat lower for 1000 samples, but still considerable. We hence conclude that the mean accuracy across tasks is a fairly robust metric to rank models in VTAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Method 1000 Samples Full Dataset</head><p>Reweighted data sets 0.959 ± 0.017 0.978 ± 0.015 Reweighted groups 0.969 ± 0.017 0.991 ± 0.010 Balduzzi et al. (2018) weighting 0.891 ± 0.027 0.950 ± 0.018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric mean</head><p>0.951 ± 0.020 0.973 ± 0.016 Average rank 0.873 ± 0.023 0.925 ± 0.019 Average rank (perturbed) 0.874 ± 0.024 0.926 ± 0.028 Average rank (robust) 0.877 ± 0.024 0.957 ± 0.016 Elimination rank 0.930 ± 0.031 0.929 ± 0.018 <ref type="table">Table 4</ref>. Kendall rank correlation coefficient (Kendall, 1945) (with standard deviation) measuring the agreement of the ranking of models according to the mean accuracy across tasks with different types of weighted means, the geometric mean, and different types of average ranks. Agreement of ranking according to mean accuracy with alternative ranking schemes is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Representative Subset of Tasks</head><p>We explore another direction related to ranking of models: Which subset of tasks is most representative of the performance of the entire benchmark? Such a subset allows for cheap iteration which is beneficial during early development of new models.</p><p>We search the most representative 5 tasks from the full set of 20 tasks by performing exhaustive search over all 20 5 subsets. For each subset we compute the mean accuracy and compute the Kendall rank correlation coefficient between the resulting ranking of models and the ranking according to the mean over all tasks (averaged over 10 trials sampled as described in Appendix E). <ref type="table">Table 5</ref> shows the subsets which produce the highest rank correlation. There are many subsets which lead to an excellent agreement with the ranking according to the full mean. These subsets can be quite different provided that they are sufficiently diverse.</p><p>To assess how well we can expect this approach generalize to unseen models, we perform the following experiment. For each pair of models, we perform subset selection based on the accuracies of the remaining models, and check whether the ranking of the left out pair based on the mean across all data sets agrees with the ranking of the pair according to the mean over the subset. The mean outcome of this binary test across all pairs is an estimate of the probability for correctly ranking an unseen pair of models: 0.952 for the full data sets and 0.956 for 1000 examples. The subset selection hence generalizes to an unseen pair of models with high probability.</p><p>The representative subsets in <ref type="table">Table 5</ref> may be used for rapid prototyping of new methods before running the full VTAB. However, a caveat to the above analyses is that we evaluate the ranking of a very diverse set of models, from those whose performance is worse than from-scratch training to performance combinations of self-supervision and supervision. Therefore, the ranking is reasonably robust. To make fine-grained distinctions between models of a similar class, the representative subsets above may be less reliable. Repeated iteration on just a few tasks exposes one more to the risk of meta-overfitting.  <ref type="table">Table 5</ref>. Task subsets of size 5 that lead to the largest rank correlation with the mean accuracy across all tasks. There are many different subsets that lead to an high rank correlation with the full mean.  <ref type="table">Table 6</ref>. Top-1 accuracy of all the models evaluated on VTAB in lightweight mode. Each entry represents the median score of three runs with different random seeds evaluated on the test set. Within each dataset-size group (1000-example and full), the methods are sorted from best to worst according to their mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Rank Correlation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Lightweight experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Heavyweight experiments</head><p>The literature results may use substantially more complex architectures, or additional task-specific logic. For example, the Retinopathy result uses three neural networks combined with decision trees, and uses additional information, such as combining the images from two eyes of the same patient.   <ref type="table">Table 8</ref>. Comparison of the best method on the full datasets using heavyweight sweep (SUP-EXEMPLAR-100%) to results published in the literature (where available). For some datasets prior work does not use top-1 accuracy, therefore, we present our the performance of SUP-EXEMPLAR-100% using the same metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Hyperparameter Sweeps</head><p>Lightweight sweep The lightweight mode performs a restricted hyperparameter search for all tasks, permitting fair comparison with few resources. It uses fixed values for most hyperparameters. We set the batch size to 512 and use SGD with momentum of 0.9. When fine-tuning, we do not use weight decay, and when training from scratch we set it to 0.01 times the learning rate <ref type="bibr">(Loshchilov &amp; Hutter, 2019)</ref>. We resize all images to 224 × 224, except for generative models, where we resize to 128 × 128 since training at a higher resolution is currently very challenging.</p><p>Lightweight mode sweeps four hyperparameters:</p><p>• Learning rate: {0.1, 0.01}</p><p>• Traning schedule: In all cases we decay the learning rate by a factor of 10 after 1 3 and 2 3 of the training time, and one more time shortly before the end. We train for {2 500, 10 000} training steps (i.e. model updates).</p><p>Note that when training on 1000 examples, we perform model selection using the regular validation set for each task. This setting is somewhat unrealistic, since in practice one would train on the larger validation set. However, we use this setting, which is common in prior art <ref type="bibr">(Oliver et al., 2018)</ref>, because it significantly reduces evaluation noise. <ref type="bibr">Further, Zhai et al. (2019)</ref> show that using a small validation yields the same conclusions.</p><p>Heavyweight sweep We define a relatively large search-space over relevant hyperparameters for the fine-tuning adaptation phase and perform an individual random search for each downstream task's validation set in that space. The random search consists of 100 independent trials. Specifically, we search:</p><p>• Batch-size: {128, 256, 512}</p><p>• Training schedule: In all cases we decay the learning rate by a factor of 10 after 1 3 and 2 3 of the training time, and one more time shortly before the end. We train for either any of {100, 200, 300, 400, 500} epochs over the dataset, or any of {2 500, 5 000, 10 000, 20 000, 40 000} training steps (i.e. model updates).</p><p>• Preprocessing: during training, optionally include random horizontal flipping.</p><p>• Preprocessing: during training, optionally include random color distortions.</p><p>• Preprocessing: during training, use any of the following techniques for resizing: res.: resize the image to 224; res.|crop: resize the image to 256 and take a random crop of size 224; res.sma|crop: resize the image keeping its aspect ratio such that the smaller side is 256, then take a random crop of size 224; inc.crop: "inception crop" from <ref type="bibr" target="#b13">(Szegedy et al., 2015)</ref>; cif.crop: resize the image to 224, zero-pad it by 28 on each side, then take a random crop of size 224.</p><p>• Preprocessing: for evaluation, either resize the image to one of {224, 320, 384}, or resize it such that the smaller side is of size {256, 352, 448} and then take a central crop of size {224, 320, 384}.</p><p>• Weight decay: we sample log-uniform randomly in [10 −5 , 10 −1 ].</p><p>• Optimizer: we randomly choose between -SGD with learning-rate chosen log-uniform randomly in [0.001, 0.0], or -Adam with learning-rate chosen log-uniform randomly in [10 −5 , 10 −2 ].</p><p>We then choose the set of hyperparameters with the best performance at the end of training for each task individually.</p><p>For both training from scratch, and from a pre-trained supervised ImageNet training, we present the best set of hyperparameters for each task in Figs. 8 to 10 as a red dot. The green background shows the distribution of hyperparameters which performed within 2% of the best one for each task, providing a sense for the importance of tuning that hyperparameter.</p><p>In <ref type="figure" target="#fig_0">Figs. 12 and 13</ref>, we show the achieved performance for the full distribution of hyperparameters as violin plots for the CIFAR-100 and DMLab downstream tasks, both for the small and full variants. The supervised pre-training not only achieves better accuracy than training from scratch, but it also displays much smaller variance across hyperparameter values.     <ref type="figure" target="#fig_0">Figure 15</ref>. Absolute difference in top-1 accuracy between pairs of methods for each dataset. The bar colour denotes the task group as usual. Top: Lightweight hyperparameter sweep. Bottom: Heavyweight hyperparameter sweep. Left: SUP-100% versus FROM-SCRATCH -supervised pre-training yields a substantial improvement on the NATURAL datasets and some others. Right: SUP-ROTATION-100% versus SUP-100% -the additional self-supervised loss yields better representations for the STRUCTURED tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Lightweight versus Heavyweight Searches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Linear Evaluation</head><p>Here we describe the setup for linear evaluation. We follow <ref type="bibr">(Kolesnikov et al., 2019)</ref> and evaluate the frozen representation by training a linear logistic regression model. We use exactly the same hyperparameters as described in lightweight sweep from Section I. The only difference here is that only the linear layer is trained instead of fine tuning the whole network.  <ref type="figure" target="#fig_0">Figure 17</ref>. VTAB performance on the scaled up architectures. Here the architecture is represented by different color and width is represented by different circle size. The performance increases when switching from the standard ResNet50 architecture to ResNet152 2x wider architecture.</p><p>In this section, we study the problem of "is scaling up the architectures helpful?". <ref type="figure" target="#fig_0">Figure 18</ref> shows top-1 accuracy on ImageNet public validation set when scaling up the architectures. Widening factor is the multiplier on the network width, where ×1 stands for the standard ResNet architecture. Depth stands for the number of architecture layers. As expected, the model accuracy goes up with either wider or deeper architectures. <ref type="figure" target="#fig_0">Figure 17</ref> shows the results on VTAB benchmark, where the 2x wider ResNet152 architecture performs consistently better than the standard ResNet50 model.  <ref type="figure" target="#fig_0">Figure 19</ref>. Top-1 accuracy for each task attained by SUP-100% and ROTATION with respect to the number of fine-tuning steps on the 1000-example datasets. More steps is usually better, but performance is stable after 1000 steps.  <ref type="figure">Figure 20</ref>. Top-1 accuracy for each task attained by SUP-100% and ROTATION with respect to the number of fine-tuning steps on the full datasets. More steps is usually better, but performance is stable after 1000 steps.  <ref type="figure" target="#fig_0">Figure 21</ref>. Absolute differences in Kendall's rank correlation score between the "gold" ranking in each dataset, and the ranking obtained with either VTAB or Visual Decathlon. Bar colors indicate the category as usual, and yellow indicates datasets only present in Visual Decathlon. Positive values indicate that VTAB ranking is closer than Visual Decathlon ranking, when compared against the "gold" ranking of a particular dataset. The average ranking correlation for VTAB is 0.76, and 0.70 for Visual Decathlon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N. Comparison to Visual Decathlon</head><p>In Section 4 we describe the differences between VTAB and Visual Decathlon protocols. Here we answer a more practical question: which benchmark should one use to compare the adaptation abilities of a set of methods to unseen tasks? We will show that the rank correlation with an unseen task, is expected to be higher for the ranking obtained using VTAB, than using Visual Decathlon.</p><p>First, we fine-tuned each of our 16 baseline models in each of the Visual Decathlon datasets (using the lightweight hyperparameter search described in Section 3.1), which we downloaded directly from the competition's website. All datasets in the Visual Decathlon are provided so that the shorter size of the image is 72 pixels, while our baseline models were trained in much larger resolutions. To overcome this difference in resolution, we resize the Visual Decathlon images to the resolution required by each model. We checked that our baseline models obtained reasonable results on the Visual Decathlon benchmark. In fact, our best model, SUP-ROTATION-100%, reports a test average accuracy of 78.22%, and a decathlon score of 3580, which are both slightly better than the best results reported in <ref type="bibr">(Rebuffi et al., 2018) (78.08% and 3412, respectively)</ref>, and other works (e.g. <ref type="bibr" target="#b3">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b6">Rosenfeld &amp; Tsotsos, 2018)</ref>). However, notice that comparing these numbers is delicate, since we did not use any data augmentation during training and all our models are based on the Resnet50 architecture, while these works use heavy data augmentation (that depends on the dataset), and Resnet26-like architectures. Then, for each task T in the union of VTAB and Visual Decathlon, we rank the 16 baseline methods according to their accuracy on task T . If we consider T as the unseen dataset, this is the "gold" ranking of the studied methods. Now, we obtain two alternative rankings: one based on the mean accuracy in VTAB and another on Visual Decathlon, excluding task T in both cases to avoid any bias. We can then compute, for each "unseen" dataset, the Kendall's ranking correlation between the "gold" ranking and each of the alternative rankings. <ref type="figure" target="#fig_0">Figure 21</ref> shows the absolute differences in the rank correlation score between the VTAB-based ranking and Visual Decathlon-based ranking.</p><p>For most datasets, the difference is positive, which means that the ranking according to VTAB correlates better with the "gold", than the raking obtained using Visual Decathlon. Notice that even tasks that were not part of VTAB (colored in gold), are better represented by VTAB's ranking than that of Visual Decathlon. These results are not surprising, since VTAB contains a more representative set of tasks than Visual Decathlon. The average ranking correlation for VTAB is 0.76, and 0.70 for Visual Decathlon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the VTAB evaluation protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Kendall's correlation between fine-tuning and linear evaluation on each dataset. Performance under various time constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>ImageNet supervised representation fine-tuned on the full datasets. Best hyperparameter values are marked in red and those within 2% in green to show sensitivity. Heavy parameter sweep on CIFAR-100 dataset. Top 2 rows show the results of from scratch algorithm and bottom 2 rows show the results of supervised 100% algorithm. Each plot shows the violin plot of the results with respect to a given parameter. Here we show the results evaluated on full datasets and 1k datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 13 .</head><label>13</label><figDesc>Heavy parameter sweep on DMLab dataset. Top 2 rows show the results of from scratch algorithm and bottom 2 rows show the results of supervised 100% algorithm. Each box shows the violin plot of the results with respect to a given parameter. We show the results evaluated on full datasets and 1k datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 18 .</head><label>18</label><figDesc>Top-1 accuracy on ImageNet public validation set when scaling up the architectures. The accuracy goes up with either wider or deeper architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy of the best models and ResNet50 from-scratch using the heavyweight hyperparameter sweep. Rot-100% 80.0 42.6 66.5 91.5 88.6 32.7 90.0 78.5 96.4 82.1 78.4 96.2 58.1 55.3 89.3 71.1 69.1 39.3 53.6 71.Sup-Rot-100% 93.5 84.0 76.8 97.4 92.6 75.6 97.4 86.5 99.1 96.3 83.7 100.0 96.8 79.6 100.0 96.8 82.4 100.0 96.7 91.</figDesc><table><row><cell></cell><cell></cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>DTD</cell><cell>Flowers102</cell><cell>Pets</cell><cell>Sun397</cell><cell>SVHN</cell><cell>Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Retinopathy</cell><cell>Clevr-Count</cell><cell>Clevr-Dist</cell><cell>DMLab</cell><cell>dSpr-Loc</cell><cell>dSpr-Ori</cell><cell>KITTI-Dist</cell><cell>sNORB-Azim</cell><cell>sNORB-Elev</cell><cell>Mean</cell></row><row><cell></cell><cell>From-Scratch</cell><cell cols="19">61.2 20.4 46.4 73.4 48.0 13.1 82.5 77.0 91.5 59.5 73.4 66.0 59.2 36.9 88.2 62.6 63.8 22.6 79.1 59.2</cell></row><row><cell>1000</cell><cell>Sup-100% Sup-</cell><cell cols="19">83.6 48.9 65.9 93.0 90.4 31.1 87.0 80.2 95.8 82.0 79.0 78.8 58.3 53.8 89.0 71.2 73.9 34.2 57.8 71.2</cell></row></table><note>5 Sup-Ex-100% 83.3 49.3 61.8 94.2 90.2 35.2 87.6 84.2 95.7 81.0 78.7 96.4 56.3 56.1 90.0 69.8 73.3 38.6 59.5 72.7 Full From-Scratch 74.5 77.8 67.1 85.8 70.9 70.1 97.0 91.2 98.8 94.3 82.8 99.8 96.7 76.6 100.0 96.7 68.4 99.9 94.0 86.4 Sup-100% 93.0 83.4 73.7 97.3 92.7 75.6 97.5 87.3 98.8 96.1 83.4 100.0 97.0 78.8 100.0 96.8 81.0 100.0 98.5 91.13 Sup-Ex-100% 93.8 83.1 76.5 97.8 92.9 75.3 97.5 86.5 99.0 96.3 83.7 99.9 96.8 79.3 100.0 96.7 82.8 100.0 99.1 91.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H.,Lefrancq, A., Green, S., Valdés, V., Sadik, A., et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. Bello, I., Zoph, B., Vasudevan, V., and Le, Q. V. Neural optimizer search with reinforcement learning. In International Conference on Machine Learning, 2017. Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. International Conference on Learning Representations, 2019. Chapelle, O., Scholkopf, B., and Zien, A. Semi-supervised learning. IEEE Transactions on Neural Networks, 2009. Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Self-supervised gans via auxiliary rotation loss. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. Proceedings of the IEEE, 2017. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., , and Vedaldi, A. Describing textures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition, 2014. Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation policies from data. In IEEE Conference on Computer Vision and Pattern Recognition. 2019. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. International Journal of Robotics Research, 2013. Goyal, P., Mahajan, D., Gupta, A., and Misra, I. Scaling and benchmarking self-supervised visual representation learning. arXiv preprint arXiv:1905.01235, 2019a. Goyal, P., Mahajan, D., and Misra, I. Facebook AI Self-Supervision Challenge, 2019b. URL https://sites.google.com/corp/view/ fb-ssl-challenge-iccv19/home. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European Conference on Computer Vision. Springer, 2016. He, K., Girshick, R., and Dollár, P. Rethinking ImageNet pre-training. arXiv preprint arXiv:1811.08883, 2018. Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965, 2018. Huh, M., Agrawal, P., and Efros, A. A. What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614, 2016. Kaggle and EyePacs. Kaggle diabetic retinopathy detection, July 2015. URL https://www.kaggle.com/c/ diabetic-retinopathy-detection/data. Kendall, M. G. The treatment of ties in ranking problems. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop 2013, 2013. Narayanaswamy, S., Paige, T. B., Van de Meent, J.-W., Desmaison, A., Goodman, N., Kohli, P., Wood, F., and Torr, P. Learning disentangled representations with semisupervised deep generative models. In Advances in Neural Information Processing Systems, 2017. Teh, E. W. and Taylor, G. W. Metric learning for patch classification in digital pathology. In Medical Imaging with Deep Learning, 2019. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. International Conference on Learning Representations, 2018. Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017.</figDesc><table><row><cell cols="2">Supplementary Material: A Large-scale Study of Representation Learning</cell></row><row><cell cols="2">with the Visual Task Adaptation Benchmark</cell></row><row><cell>A. Tasks</cell><cell></cell></row><row><cell></cell><cell>Biometrika, 1945.</cell></row><row><cell cols="2">Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE Trans-actions on Pattern Analysis and Machine Intelligence, 2013. Berthelot, D., Carlini, N., Cubuk, E. D., Kurakin, A., Sohn, K., Zhang, H., and Raffel, C. Remixmatch: Semi-supervised learning with distribution alignment and aug-mentation anchoring. 2020. Category Dataset Train size Classes Reference visual learning. In IEEE International Conference on Computer Vision, 2017. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised vi-sual representation learning by context prediction. In IEEE International Conference on Computer Vision, 2015. Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. International Conference on Learning Represen-tations, 2014. Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative mod-els. In Advances in Neural Information Processing Sys-tems, 2014. Kolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised visual representation learning. In IEEE Con-ference on Computer Vision and Pattern Recognition, 2019. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. Neumann, M., Pinto, A. S., Zhai, X., and Houlsby, N. In-domain representation learning for remote sensing. arXiv preprint arXiv:1911.06721, 2019. Natural Caltech101 3,060 102 (Li et al., 2006) Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Xu, K., Natural CIFAR-100 50,000 100 (Krizhevsky, 2009) Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019. Tschannen, M., Bachem, O., and Lucic, M. Recent advances in autoencoder-based representation learning. arXiv preprint arXiv:1812.05069, 2018. Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lucic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019. , and Bowman, S. R. Glue: A multi-task benchmark and analy-sis platform for natural language understanding. Interna-tional Conference on Learning Representations, 2018. Wang, M. and Deng, W. Deep visual domain adaptation: A survey. Neurocomputing, 2018. Wang, Z., Yin, Y., Shi, J., Fang, W., Li, H., and Wang, X. Zoom-in-net: Deep mining lesions for diabetic retinopa-thy detection. In Medical Image Computing and Com-puter Assisted Intervention, 2017. Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision Natural DTD 3,760 47 (Cimpoi et al., 2014) Natural Flowers102 2,040 102 (Nilsback &amp; Zisserman, 2008) Natural Pets 3,680 37 (Parkhi et al., 2012) Natural Sun397 87,003 397 (Xiao et al., 2010) Natural SVHN 73,257 10 (Netzer et al., 2011) Specialized EuroSAT 21,600 10 (Helber et al., 2019) Specialized Resisc45 25,200 45 (Cheng et al., 2017) Specialized Patch Camelyon 294,912 2 (Veeling et al., 2018) Specialized Retinopathy 46,032 5 (Kaggle &amp; EyePacs, 2015) Structured Clevr/count 70,000 8 (Johnson et al., 2017) Structured Clevr/distance 70,000 6 (Johnson et al., 2017) Structured dSprites/location 663,552 16 (Matthey et al., 2017) Structured dSprites/orientation 663,552 16 (Matthey et al., 2017) Structured SmallNORB/azimuth 36,450 18 (LeCun et al., 2004) Structured SmallNORB/elevation 36,450 9 (LeCun et al., 2004) Structured DMLab 88,178 6 (Beattie et al., 2016) Brock, A., Doersch, C. and Zisserman, A. Multi-task self-supervised Structured KITTI/distance 5,711 4 (Geiger et al., 2013)</cell></row><row><cell>and Pattern Recognition, 2010.</cell><cell></cell></row><row><cell>Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Ad-vances in Neural Information Processing Systems. 2014.</cell><cell>Loshchilov, I. and Hutter, F. Decoupled weight decay regu-larization. 2019.</cell></row><row><cell>Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S 4 L:</cell><cell>Lucic, M., Tschannen, M., Ritter, M., Zhai, X., Bachem, O., and Gelly, S. High-fidelity image generation with fewer</cell></row><row><cell>Self-Supervised Semi-Supervised Learning. IEEE Inter-</cell><cell>labels. In International Conference on Machine Learning,</cell></row><row><cell>national Conference on Computer Vision, 2019.</cell><cell>2019.</cell></row></table><note>Cheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art.Donahue, J. and Simonyan, K. Large scale adversarial representation learning. arXiv preprint arXiv:1907.02544, 2019. Donahue, J., Krähenbühl, P., and Darrell, T. Adversarial feature learning. In International Conference on Learning Representations, 2017. Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in Neu- ral Information Processing Systems, 2014. Dumoulin, V., Belghazi, I., Poole, B., Mastropietro, O., Lamb, A., Arjovsky, M., and Courville, A. Adversari- ally learned inference. In International Conference on Learning Representations, 2017. Dwork, C., Kumar, R., Naor, M., and Sivakumar, D. Rank aggregation methods for the web. In International Con- ference on World Wide Web, 2001. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories.Gidaris, S., Singh, P., and Komodakis, N. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018.Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Se- lected Topics in Applied Earth Observations and Remote Sensing, 2019. Hénaff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Klambauer, G., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a Nash equi- librium. In Advances in Neural Information Processing Systems, 2017. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta- vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, volume 3, 2017. Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. International Conference on Learning Representations, 2018. Huang, Y., Cheng, Y., Chen, D., Lee, H., Ngiam, J., Le, Q. V., and Chen, Z. Gpipe:Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: A diag- nostic dataset for compositional language and elementary visual reasoning. In IEEE Conference on Computer Vi- sion and Pattern Recognition, 2017.Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet models transfer better? IEEE Conference on Computer Vision and Pattern Recognition, 2019. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. LeCun, Y., Huang, F. J., and Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. In IEEE Conference on Computer Vision and Pattern Recognition, 2004. Li, F.-F., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006. Liu, Y., Gadepalli, K., Norouzi, M., Dahl, G. E., Kohlberger, T., Boyko, A., Venugopalan, S., Timofeev, A., Nelson, P. Q., Corrado, G. S., et al. Detecting cancer metas- tases on gigapixel pathology images. arXiv preprint arXiv:1703.02442, 2017. Locatello, F., Bauer, S., Lucic, M., Gelly, S., Schölkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. International Conference on Machine Learning, 2019.Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. dsprites: Disentanglement testing sprites dataset.https://github.com/deepmind/dsprites-dataset/, 2017.Nilsback, M.-E. and Zisserman, A. Automated flower clas- sification over a large number of classes. In Indian Con- ference on Computer Vision, Graphics and Image Pro- cessing, Dec 2008. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, 2016. Oliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Good- fellow, I. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems. 2018. Pan, S. J. and Yang, Q. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 2009. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012. Radford, A., Metz, L., and Chintala, S. Unsupervised rep- resentation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations, 2016.Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation equivariant cnns for digital pathol- ogy. In International Conference on Medical Image Com- puting and Computer-Assisted Intervention, 2018. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O.Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European Conference on Computer Vision, 2016.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Description of the datasets used for the tasks in VTAB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>provides statistics and references for all of the tasks in VTAB. Some examples include apples, bottles, dinosaurs, and bicycles. The image size is 32x32. DTD (Cimpoi et al., 2014) The task consists in classifying images of textural patterns (47 classes, with 120 training images each). Some of the textures are banded, bubbly, meshed, lined, or porous. The image size ranges between 300x300 and 640x640 pixels.SVHN (Netzer et al., 2011) This task consists in classifying images of Google's street-view house numbers (10 classes, with more than 1000 training images each). The image size is 32x32 pixels.</figDesc><table><row><cell>We now provide a brief description of each task.</cell></row><row><cell>Flowers102 (Nilsback &amp; Zisserman, 2008) The task consists in classifying images of flowers present in the UK (102</cell></row><row><cell>classes, with between 40 and 248 training images per class). Azalea, Californian Poppy, Sunflower, or Petunia are</cell></row><row><cell>some examples. Each image dimension has at least 500 pixels.</cell></row><row><cell>Pets (Parkhi et al., 2012) The task consists in classifying pictures of cat and dog breeds (37 classes with around 200 images</cell></row><row><cell>each), including Persian cat, Chihuahua dog, English Setter dog, or Bengal cat. Images dimensions are typically 200</cell></row><row><cell>pixels or larger.</cell></row></table><note>Caltech101 (Li et al., 2006) The task consists in classifying pictures of objects (101 classes plus a background clutter class), including animals, airplanes, chairs, or scissors. The image size varies, but it typically ranges from 200-300 pixels per edge.CIFAR-100 (Krizhevsky, 2009) The task consists in classifying natural images (100 classes, with 500 training images each).Sun397 (Xiao et al., 2010) The Sun397 task is a scenery benchmark with 397 classes and, at least, 100 images per class. Classes have a hierarchy structure, and include cathedral, staircase, shelter, river, or archipelago. The images are (colour) 200x200 pixels or larger.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>DMLab (Beattie et al., 2016) The DMLab (DeepMind Lab) is a set of control environments focused on 3D navigation and puzzle-solving tasks. The Dmlab dataset contains frames observed by the agent acting in the DeepMind Lab environment, which are annotated by the distance between the agent and various objects present in the environment. The goal is to evaluate the ability of a visual model to reason about distances from the visual input in 3D environments. The Dmlab dataset consists of 360x480 color images in 6 classes. The classes are {close, far, very far} × {positive reward, negative reward} respectively. KITTI-Dist (Geiger et al., 2013) The KITTI task consists in predicting the (binned) depth to the vehicle (car, van, or truck) in the image. There are 4 bins / classes.</figDesc><table><row><cell>Kaggle &amp; EyePacs, 2015) The Diabetic Retinopathy dataset consists of image-label pairs with high-resolution</cell></row><row><cell>retina images, and labels that indicate the presence of Diabetic Retinopahy (DR) in a 0-4 scale (No DR, Mild, Moderate,</cell></row><row><cell>Severe, or Proliferative DR).</cell></row></table><note>Clevr/count (Johnson et al., 2017) CLEVR is a visual question and answer dataset designed to evaluate algorithmic visual reasoning. We use just the images from this dataset, and create a synthetic task by setting the label equal to the number of objects in the images. Clevr/distance (Johnson et al., 2017) Another synthetic task we create from CLEVR consists of predicting the depth of the closest object in the image from the camera. The depths are bucketed into size bins.dSprites/location (Matthey et al., 2017) The dSprites dataset was originally designed to asses disentanglement properties of unsupervised learning algorithms. In particular, each image is a 2D shape where six factors are controlled: color, shape, scale, rotation, and (x,y) center coordinates. Images have 64x64 black-and-white pixels. This task consists in predicting the x (horizontal) coordinate of the object. The locations are bucketed into 16 bins.dSprites/orientation (Matthey et al., 2017) We create another task from dSprites consists in predicting the orientation of each object, bucketed into 16 bins. SmallNORB/azimuth (LeCun et al., 2004) The Small NORB dataset contains images of 3D-toys from 50 classes, including animals, human figures, airplanes, trucks, and cars. The image size is 640x480 pixels. In this case, we define labels depending on the azimuth (angle of horizontal deviation), in intervals of 20 degrees (18 classes). SmallNORB/elevation (LeCun et al., 2004) Another synthetic task we create from Small NORB consists in predicting the elevation in the image. There are 9 classes, corresponding to 9 different elevations ranging from 30 to 70 degrees, in intervals of 5 degrees.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>g. number of training examples, render the performance incomparable. Instead, it is meant as a verification that the kinds of domains (natural images, aerial imagery, etc.) and semantics (object type classification, localization, counting, etc.) are possible to learn from examples alone using human-level visual representations.</figDesc><table><row><cell></cell><cell cols="2">Random Guess Human</cell></row><row><cell>Pets</cell><cell>2.7%</cell><cell>63.1%</cell></row><row><cell>DTD</cell><cell>2.1%</cell><cell>64.0%</cell></row><row><cell>Camelyon</cell><cell>50%</cell><cell>68.0%</cell></row><row><cell>EuroSAT</cell><cell>10%</cell><cell>86.5%</cell></row><row><cell>DMLab</cell><cell>33.3%</cell><cell>49.0%</cell></row><row><cell>Clevr-count</cell><cell>12.5%</cell><cell>99.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 .</head><label>7</label><figDesc>Top-1 accuracy of all the models evaluated on VTAB in heavyweight mode.</figDesc><table><row><cell></cell><cell>SUP-EX.-100%</cell><cell>Result</cell><cell>Reference</cell></row><row><cell>Caltech101</cell><cell>90.4 / 95.1 *</cell><cell cols="2">86.9 / 95.1 * Cubuk et al. (2019) / Kornblith et al. (2019)</cell></row><row><cell>CIFAR-100</cell><cell>83.1</cell><cell>91.7</cell><cell>Tan &amp; Le (2019)</cell></row><row><cell>DTD</cell><cell>76.5</cell><cell>78.1</cell><cell>Kornblith et al. (2019)</cell></row><row><cell>Flowers102</cell><cell>97.8</cell><cell>98.8</cell><cell>Tan &amp; Le (2019)</cell></row><row><cell>Pets</cell><cell>92.9</cell><cell>95.9</cell><cell>Huang et al. (2018)</cell></row><row><cell>SVHN</cell><cell>97.5</cell><cell>99.0</cell><cell>Cubuk et al. (2019)</cell></row><row><cell>Sun397</cell><cell>75.3</cell><cell>72.0</cell><cell>Wang et al. (2017)</cell></row><row><cell>Camelyon</cell><cell>86.5</cell><cell>90.6</cell><cell>Teh &amp; Taylor (2019)</cell></row><row><cell>EuroSAT</cell><cell>99.0</cell><cell>96.4</cell><cell>Helber et al. (2019)</cell></row><row><cell>Resisc45</cell><cell>96.3</cell><cell>90.4</cell><cell>Cheng et al. (2017)</cell></row><row><cell>Retinopathy</cell><cell>74.7  †</cell><cell>85.4  †</cell><cell>Wang et al. (2017)</cell></row></table><note>* Mean per-class accuracy † Quadratic Kappa</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Training from scratch with only 1000 examples. Best hyperparameter values are marked in red and those within 2% in green to show sensitivity. Training from scratch on the full datasets. Best hyperparameter values are marked in red and those within 2% in green to show sensitivity. ImageNet supervised representation fine-tuned on only 1000 examples. Best hyperparameter values are marked in red and those within 2% in green to show sensitivity.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Preprocessing Preprocessing</cell><cell></cell><cell>Weight Weight</cell><cell>Learningrate Learningrate</cell></row><row><cell></cell><cell>Batch Batch</cell><cell></cell><cell></cell><cell>Schedule Schedule</cell><cell></cell><cell>FlipCol. Train FlipCol. Train</cell><cell>Eval Eval</cell><cell></cell><cell>Decay SGD Adam Decay SGD Adam</cell></row><row><cell>Caltech101 Caltech101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-100 CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DTD DTD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flowers102 Flowers102</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pets Pets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVHN SVHN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sun397 Sun397</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Camelyon Camelyon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EuroSAT EuroSAT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Resisc45 Resisc45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retinopathy Retinopathy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clevr-Count Clevr-Count</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clevr-Dist Clevr-Dist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMLab DMLab</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KITTI-Dist KITTI-Dist</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dSpr-Loc dSpr-Loc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dSpr-Orient dSpr-Orient</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sNORB-Azim sNORB-Azim</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sNORB-Elev Sum Figure 8. sNORB-Elev sNORB-Azim dSpr-Orient dSpr-Loc KITTI-Dist DMLab Clevr-Dist Clevr-Count Retinopathy Resisc45 EuroSAT Camelyon Sun397 SVHN Pets Flowers102 DTD CIFAR-100 Caltech101 Sum Sum Sum Figure 9. sNORB-Elev sNORB-Azim dSpr-Orient dSpr-Loc KITTI-Dist DMLab Clevr-Dist Clevr-Count Retinopathy Resisc45 EuroSAT Camelyon Sun397 SVHN Pets Flowers102 DTD CIFAR-100 Caltech101 Figure 10. sNORB-Elev</cell><cell>256 Batch 128 512 128 256 512 256 128 512 256 128 512 Batch</cell><cell>100 epochs 100 epochs 100 epochs 100 epochs</cell><cell>200 epochs 200 epochs 200 epochs 200 epochs</cell><cell>300 epochs Schedule 400 epochs 500 epochs 2500 steps 5000 steps 10000 steps 20000 steps 300 epochs 400 epochs 500 epochs 2500 steps 5000 steps 10000 steps 20000 steps 300 epochs 400 epochs 500 epochs 2500 steps 5000 steps 10000 steps 20000 steps 300 epochs 400 epochs 500 epochs 2500 steps 5000 steps 10000 steps 20000 steps Schedule</cell><cell>40000 steps 40000 steps 40000 steps 40000 steps</cell><cell cols="2">none FlipCol. Train horizontal raw distort res. res.|crop res.sma|crop inc.crop cif.crop none horizontal raw distort res. res.|crop res.sma|crop inc.crop cif.crop Preprocessing res.@224 res.@320 Eval res.@384 cen.crop@224 cen.crop@320 res.@224 res.@320 res.@384 cen.crop@224 cen.crop@320 none horizontal raw distort res. res.|crop res.sma|crop inc.crop cif.crop res.@224 res.@320 res.@384 cen.crop@224 cen.crop@320 none horizontal raw distort res. res.|crop res.sma|crop inc.crop cif.crop res.@224 res.@320 res.@384 cen.crop@224 cen.crop@320 FlipCol. Train Eval Preprocessing</cell><cell>cen.crop@384 cen.crop@384 cen.crop@384 cen.crop@384</cell><cell>1e-5 Weight 1e-4 1e-3 1e-2 1e-1 Decay SGD Adam 0.001 0.01 0.1 1.0 1e-5 1e-4 1e-3 1e-2 1e-5 1e-4 1e-3 1e-2 1e-1 0.001 0.01 0.1 1.0 1e-5 1e-4 1e-3 1e-2 Learningrate 1e-5 1e-4 1e-3 1e-2 1e-1 0.001 0.01 0.1 1.0 1e-5 1e-4 1e-3 1e-2 1e-5 1e-4 1e-3 1e-2 1e-1 0.001 0.01 0.1 1.0 1e-5 1e-4 1e-3 1e-2 Decay SGD Adam Weight Learningrate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Per-dataset, absolute difference in top-1 accuracy of two hyper parameter sweep strategies. Top left: FROM-SCRATCH. Top right: SUP-100%. Bottom left: SUP-ROTATION-100%. Bottom right: SUP-EXEMPLAR-100% The bar colour denotes the task group.</figDesc><table><row><cell></cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">From-Scratch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Sup-100%</cell><cell></cell><cell></cell></row><row><cell>Top-1 Accuracy Delta</cell><cell>5 10 15 20 25 30 35 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Heavy</cell><cell>Top-1 Accuracy Delta</cell><cell>0 10 20 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Heavy</cell></row><row><cell></cell><cell>0</cell><cell cols="2">dSpr-Loc</cell><cell cols="2">DMLab</cell><cell cols="2">sNORB-Azim</cell><cell cols="2">Camelyon</cell><cell cols="2">Clevr-Dist</cell><cell cols="2">EuroSAT</cell><cell>Sun397</cell><cell>CIFAR-100</cell><cell>Retinopathy</cell><cell>Resisc45</cell><cell>SVHN</cell><cell>DTD</cell><cell>Caltech101</cell><cell>dSpr-Orient</cell><cell>KITTI-Dist</cell><cell>Clevr-Count</cell><cell>Flowers102</cell><cell>Pets</cell><cell>sNORB-Elev</cell><cell>Light</cell><cell></cell><cell>−10</cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>Sun397</cell><cell>KITTI-Dist</cell><cell>DTD</cell><cell>EuroSAT</cell><cell>Camelyon</cell><cell>SVHN</cell><cell>Resisc45</cell><cell>Pets</cell><cell>sNORB-Azim</cell><cell>Flowers102</cell><cell>Clevr-Dist</cell><cell>Retinopathy</cell><cell>dSpr-Loc</cell><cell>DMLab</cell><cell>sNORB-Elev</cell><cell>dSpr-Orient</cell><cell>Clevr-Count</cell><cell>Light</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Sup-Rotation-100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Sup-Exemplar-100%</cell></row><row><cell>Top-1 Accuracy Delta</cell><cell cols="2">−10 0 10 20 30 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Heavy</cell><cell>Top-1 Accuracy Delta</cell><cell>0 10 20 30 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Heavy</cell></row><row><cell></cell><cell cols="2">−20</cell><cell cols="2">CIFAR-100</cell><cell cols="2">KITTI-Dist</cell><cell cols="2">Caltech101</cell><cell cols="2">dSpr-Loc</cell><cell cols="2">DTD</cell><cell>Camelyon</cell><cell>Resisc45</cell><cell>Pets</cell><cell>Sun397</cell><cell>EuroSAT</cell><cell>Clevr-Dist</cell><cell>Flowers102</cell><cell>SVHN</cell><cell>Retinopathy</cell><cell>DMLab</cell><cell>sNORB-Azim</cell><cell>sNORB-Elev</cell><cell>dSpr-Orient</cell><cell>Clevr-Count</cell><cell>Light</cell><cell></cell><cell>−10</cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>DTD</cell><cell>KITTI-Dist</cell><cell>SVHN</cell><cell>Clevr-Dist</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Sun397</cell><cell>Pets</cell><cell>Camelyon</cell><cell>sNORB-Azim</cell><cell>Flowers102</cell><cell>dSpr-Loc</cell><cell>Retinopathy</cell><cell>DMLab</cell><cell>sNORB-Elev</cell><cell>dSpr-Orient</cell><cell>Clevr-Count</cell><cell>Light</cell></row><row><cell cols="5">Figure 14. dSpr-Loc −10 10 20 30 40 50 60 70 80 Top-1 Accuracy Delta 0</cell><cell cols="2">Clevr-Dist</cell><cell cols="2">sNORB-Elev</cell><cell cols="2">Clevr-Count</cell><cell cols="2">Camelyon</cell><cell>DMLab</cell><cell>Retinopathy</cell><cell>dSpr-Orient</cell><cell>EuroSAT</cell><cell>sNORB-Azim</cell><cell>SVHN</cell><cell>Sun397</cell><cell>KITTI-Dist</cell><cell>Resisc45</cell><cell>DTD</cell><cell>CIFAR-100</cell><cell>Flowers102</cell><cell>Caltech101</cell><cell>Pets</cell><cell>Sup-100% From-Scratch</cell><cell>Top-1 Accuracy Delta</cell><cell>−2 0 2 4 6 8 10 12 −4</cell><cell>sNORB-Elev</cell><cell>sNORB-Azim</cell><cell>Sun397</cell><cell>CIFAR-100</cell><cell>Pets</cell><cell>Caltech101</cell><cell>EuroSAT</cell><cell>Retinopathy</cell><cell>Camelyon</cell><cell>Flowers102</cell><cell>SVHN</cell><cell>Resisc45</cell><cell>dSpr-Orient</cell><cell>DTD</cell><cell>KITTI-Dist</cell><cell>Clevr-Dist</cell><cell>DMLab</cell><cell>Clevr-Count</cell><cell>dSpr-Loc</cell><cell>Sup-Rotation-100% Sup-100%</cell></row><row><cell>Top-1 Accuracy Delta</cell><cell cols="2">−30 −20 −10 0 10 20 30 40 50</cell><cell cols="2">sNORB-Elev</cell><cell cols="2">Clevr-Dist</cell><cell cols="2">dSpr-Loc</cell><cell cols="2">Camelyon</cell><cell cols="2">EuroSAT</cell><cell>SVHN</cell><cell>Retinopathy</cell><cell>dSpr-Orient</cell><cell>KITTI-Dist</cell><cell>sNORB-Azim</cell><cell>Clevr-Count</cell><cell>DMLab</cell><cell>Sun397</cell><cell>DTD</cell><cell>Flowers102</cell><cell>Caltech101</cell><cell>Resisc45</cell><cell>CIFAR-100</cell><cell>Pets</cell><cell>Sup-100% From-Scratch</cell><cell>Top-1 Accuracy Delta</cell><cell>−5 0 5 10 15 20 −10</cell><cell>CIFAR-100</cell><cell>KITTI-Dist</cell><cell>sNORB-Elev</cell><cell>Caltech101</cell><cell>Pets</cell><cell>Camelyon</cell><cell>Flowers102</cell><cell>Retinopathy</cell><cell>Clevr-Dist</cell><cell>dSpr-Orient</cell><cell>Resisc45</cell><cell>dSpr-Loc</cell><cell>EuroSAT</cell><cell>DTD</cell><cell>DMLab</cell><cell>Sun397</cell><cell>SVHN</cell><cell>sNORB-Azim</cell><cell>Clevr-Count</cell><cell>Sup-Rotation-100% Sup-100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Kendall rank correlation coefficient between finetuning and linear evaluation on each dataset.</figDesc><table><row><cell>Kendall Rank Correlation</cell><cell>−0.2 0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">1000-Example Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kendall Rank Correlation</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Full Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>−0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">−0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SVHN</cell><cell>Flowers102</cell><cell>DTD</cell><cell>Pets</cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>Sun397</cell><cell>Retinopathy</cell><cell>EuroSAT</cell><cell>Camelyon</cell><cell>Resisc45</cell><cell>Clevr-Dist</cell><cell>sNORB-Azim</cell><cell>dSpr-Loc</cell><cell>Clevr-Count</cell><cell>DMLab</cell><cell>sNORB-Elev</cell><cell>dSpr-Orient</cell><cell>KITTI-Dist</cell><cell></cell><cell></cell><cell>SVHN</cell><cell>Caltech101</cell><cell>CIFAR-100</cell><cell>DTD</cell><cell>Sun397</cell><cell>Flowers102</cell><cell>Pets</cell><cell>Retinopathy</cell><cell>Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Clevr-Dist</cell><cell>dSpr-Loc</cell><cell>dSpr-Orient</cell><cell>sNORB-Azim</cell><cell>Clevr-Count</cell><cell>sNORB-Elev</cell><cell>KITTI-Dist</cell><cell>DMLab</cell></row><row><cell></cell><cell cols="5">Figure 16. Caltech101 CIFAR-100</cell><cell></cell><cell>DTD</cell><cell></cell><cell cols="2">Flowers102</cell><cell></cell><cell>Pets</cell><cell></cell><cell>Sun397</cell><cell></cell><cell cols="2">SVHN</cell><cell></cell><cell>Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Retinopathy</cell><cell></cell><cell>Clevr-Count</cell><cell></cell><cell cols="2">Clevr-Dist</cell><cell></cell><cell>DM-Lab</cell><cell></cell><cell>dSpr-Loc</cell><cell></cell><cell>dSpr-Ori</cell><cell></cell><cell></cell><cell>KITTI-Dist</cell><cell></cell><cell>sNORB-Azim</cell><cell>sNORB-Elev</cell><cell>Mean</cell></row><row><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In some practical settings additional unlabelled data may be available for unseen tasks in addition to the labelled data. We omit this setting, leaving it to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google/compare_gan/blob/master/compare_gan/architectures/resnet_ biggan.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transfusion: Understanding transfer learning with applications to medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07208</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10887</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">cifar-10classifiersgeneralizetocifar-10?arXivpreprintarXiv:1806.00451</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical and consistent estimation of f-divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Electoral systems used around the world. Real-world electronic voting: Design, analysis and deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Shahandashti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlsson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pat</surname></persName>
		</author>
		<idno>56.2 42.2 66.0 34.2 63.8 32.9 31.7 51.1 Exemplar 69.1 12.5 48.4 66.7 42.0 14.5 88.0 76.8 94.7 68.1 73.1 48.3 62.0 45.3 92.0 45.5 73.4 30.9</idno>
		<imprint/>
	</monogr>
	<note>9 50.8 Jigsaw 66.2 14.8 50.7 65.3 34.0 11.4 54.9 73.0 91.5 66.7 71.3 44.1. 8 57.5 BigBiGAN 80.8 39.2 56.6 77.9 44.4 20.3 76.8 77.4 95.6 74.0 69.3 53.9 55.6 38.7 70.6 46.7 71.4 27.2 46.3 59.1 Rotation 77.1 27.4 52.6 66.1 49.0 11.0 89.6 77.8 93.9 70.3 72.3 44.4 58.6 46.1 87.0 49.2 77.2 39.3 42.5 59.5</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pat</surname></persName>
		</author>
		<idno>Loc. 79.9 65.7 65.2 78.8 66.8 58.0 93.7 85.3 97.8 91.5 79.8 99.5 87.7 71.5 100.0 90.4 75.0 99.7 92.6 83.1 BigBiGAN 89.2 75.1 67.2 85.6 62.7 62.1 94.3 83.0 98.4 91.4 78.2 95.2 89.8 62.3 100.0 84.7 75.8 95.9</idno>
		<imprint/>
	</monogr>
	<note>3 83.3 Exemplar 81.9 70.7 61.1 79.3 67.8 58.2 96.7 84.7 98.5 93.5 79.0 99.8 93.3 74.7 100.0 96.5 78.2 99.9 97.4 84.8 Rotation 88.3 73.6 63.3 83.4 71.8 60.5 96.9 86.4 98.3 93.4 78.6 99.8 93.3 76.8 100.0 96.5 82.6 99.9 98.0 86.4</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sup-Rot</surname></persName>
		</author>
		<idno>100% 94</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pat</surname></persName>
		</author>
		<idno>Loc. 62.0 15.5 44.7 47.9</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
	<note>5 86.6 53.7 71.6 41.0 46.9 34.8 28.8 19.7 46.4 21.3 25.6 41.8</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cond-Biggan</surname></persName>
		</author>
		<idno>63.4 20.7 34.5 60.6 23.2 11.9 47.3 71.4 75.3 42.8 63.6 40.6 48.0 31.0 40.4 42.9 35.6 24.1 31.3 42.5 Exemplar 57.0 20.7 42.7 55.8 26.0 13.5 37.8 80.8 88.4 59.4 73.7</idno>
		<imprint/>
	</monogr>
	<note>45.4 49.6 33.8 55.0 28.9 58.7 18.7 32.2 46.2 Rotation 67.7 23.0 44.5 48.1 18.3 13.4 53.0 78.2 86.6 50.8 70.3 41.2 50.9 33.1 60.0 30.2 60.3 21.7 37.2 46.8</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pat</surname></persName>
		</author>
		<idno>Loc. 73.2 29.0 55.5 57.5 29.7 27.8 50.2 77.4 90.7 70.2 74.5 49.7 57.7 42.8 38.5 20.7 53.0 32.7 36.3 50.9</idno>
		<imprint/>
	</monogr>
	<note>Exemplar 68.2 49.2 52.2 67.4 38.3 43.7 59.3 81.5 94.2 79.9 74.8 56.3 58.8 41.4 70.6 34.2 62.8 32.3 45.0 58.4</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cond-Biggan</surname></persName>
		</author>
		<idno>73.6 47.9 43.5 70.1 29.4 39.4 67.7 76.3 84.6 66.9 69.7 52.1 60.4 39.7 75.5 64.9 43.7 67.3 52.7 59.2 Rotation 77.5 48.4 56.2 60.2 30.6 42.1 71.6 82.0 93.4 74.7 74</idno>
		<imprint/>
	</monogr>
	<note>7 56.6 64.8 46.0 75.1 36.1 62.0 38.3 52.8 60.2 BigBiGAN 88.9 56.7 67.1 84.2 54.7 46.2 80.0 81.3 95.8 85.1 75.4 62.7 64.2 49.2 79.6 38.6 67.7 29.9 42.6 65.8</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Rot-10% 86.7 65.6 65.5 83.6 84.6 58.8 62.2 84.0 96.0 87.6 74.6 56.8 53.1 45.9 69.7 45.7 64.9 41.8 44.1 66.9</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semi-Ex</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>10% 86.8 64.6 63.6 80.7 85.3 57.7 68.7 83.2 94.5 82.8 74.3 52.1 54.6 46.8 67.2 65.2 74.1 68.8 41.7 69.1</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Top-1 accuracy of all the models with linear evaluation on VTAB</title>
	</analytic>
	<monogr>
		<title level="m">Table 9</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
