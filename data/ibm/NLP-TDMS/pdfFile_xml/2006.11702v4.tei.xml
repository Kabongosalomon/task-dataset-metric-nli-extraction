<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Universal Representation Transformer Layer for Few-Shot Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian AI Institute</orgName>
								<orgName type="institution">UTS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hamilton</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian AI Institute</orgName>
								<orgName type="institution">UTS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian AI Institute</orgName>
								<orgName type="institution">UTS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team Correspondence to</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mila</surname></persName>
						</author>
						<title level="a" type="main">A Universal Representation Transformer Layer for Few-Shot Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization. Our code is available at https://github.com/liulu112601/URT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning tasks from small data remains a challenge for machine learning systems, which show a noticeable gap compared to the ability of humans to understand new concepts from few examples. A promising direction to address this challenge is developing methods that are capable of performing transfer learning across the collective data of many tasks. Since machine learning systems generally improve with the availability of more data, a natural assumption is that few-shot learning systems should benefit from leveraging data across many different tasks and domains-even if each individual task has limited training data available. This research direction is well captured by the problem of multi-domain few-shot classification. In this setting, training and test data spans a number of different domains, each represented by a different source dataset. A successful approach in this multi-domain setting must not only address the regular challenge of few-shot classification-i.e., the challenge of having only a handful of examples per class. It must also discover how to leverage (or ignore) what is learned from different domains, achieving generalization and avoiding cross-domain interference. strategies <ref type="bibr" target="#b18">[19]</ref> and more flexible meta-learning algorithms <ref type="bibr" target="#b17">[18]</ref>. Most notable is SUR (Selecting Universal Representation) <ref type="bibr" target="#b6">[7]</ref>, a method that relies on a so-called universal representation, extracting from a collection of pre-trained and domain-specific neural network backbones. SUR prescribes a hand-crafted feature-selection procedure to infer how to weight each backbone for each task at hand, and produces an adapted representation for each task. This was shown to lead to some of the best performances on Meta-Dataset.</p><p>In SUR, the classification procedure for each task is fixed and not learned. Thus, except for the underlying universal representation, there is no transfer learning performed with regards to how classification rules are inferred across tasks and domains. Yet, cross-domain generalization might be beneficial in that area as well, in particular when tasks have only few examples per class.</p><p>Present work. To explore this question, we propose the Universal Representation Transformer (URT) layer, which can effectively learn to transform a universal representation into task-adapted representations. The URT layer is inspired from Transformer networks <ref type="bibr" target="#b22">[23]</ref> and uses an attention mechanism to learn to retrieve or blend the appropriate backbones to use for each task. By training this layer across few-shot tasks from many domains, it can support transfer across these tasks.</p><p>We show that our URT layer on top of a universal representation's pre-trained backbones sets a new state-of-the-art performance on Meta-Dataset. It succeeds at outperforming SUR on 4 dataset sources without impairing accuracy on the others. This leads to top performance on 7 dataset sources when comparing to a set of competing methods. To interpret the strategy that URT learns to weigh the backbones from different domains, we visualize the attention scores for both seen and unseen domains and find that our model generates meaningful weights for the pre-trained domains. A comprehensive analysis on variants and ablations of the URT layer is provided to show the importance of various components of URT, notably the number of attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Few-Shot Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setting</head><p>In this section, we will introduce the problem setting for few-shot classification and the formulation of meta-learning for few-shot classification. Few-shot classification aims to classify samples where only few examples are available for each class. We describe a few-shot learning classification task as the pair of examples, comprising of a support set S to define the classification task and the query set Q of samples to be classified.</p><p>Meta-learning is a technique that aims to model the problem of few-shot classification as learning to learn from instances of few-shot classification tasks. The most popular way to train a meta-learning model is with episodic training. Here, tasks T = (Q, S) are sampled from a larger dataset by taking subsets of the dataset to build a support set S and a query set Q for the task. A common approach is to sample N -way-K-shot tasks, each time selecting a random subset of N classes from the original dataset and choosing only K examples for each class to add to the support set S.</p><p>The meta-learning problem can then be formulated by the following optimization:</p><formula xml:id="formula_0">min Θ E (S,Q)∼p(T ) [L(S, Q, Θ)] , L(S, Q, Θ) = 1 |Q| (x,y)∼Q − log p(y|x, S; Θ) + λΩ(Θ),<label>(1)</label></formula><p>where p(T ) is the distribution of tasks, Θ are the parameters of the model and p(y|x, S; Θ) is the probability assigned by the model to label y of query example x (given the support set S), and Ω(Θ) is an optional regularization term on the model parameters with factor λ.</p><p>Conventional few-shot classification targets the setting of N -way-K-shot, where the number of classes and examples are fixed in each episode. Popular benchmarks that follow this approach include Omniglot <ref type="bibr" target="#b9">[10]</ref>) or benchmarks made of subsets of ImageNet, such as miniImageNet <ref type="bibr" target="#b23">[24]</ref> and tieredImageNet <ref type="bibr" target="#b16">[17]</ref>. In such benchmarks, the tasks used for training cover a set of classes that is disjoint from the classes found in the test set of tasks. However, with the training and test sets tasks coming from a single dataset/domain, the distribution of tasks found in either sets is similar and lacks variability, which may be unrealistic in practice.</p><p>It is in this context that Triantafillou et al. <ref type="bibr" target="#b21">[22]</ref> proposed Meta-Dataset, as a further step towards large-scale, multi-domain few shot classification. Meta-Dataset includes ten datasets (domains), with eight of them available for training. Additionally, each task sampled in the benchmark varies in the number of classes N , with each class also varying in the number of shots K. As in all few-shot learning benchmarks, the classes used for training and testing do not overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Background and Related Work</head><p>Transfer by fine-tuning A simple and effective method for few-shot classification is to perform transfer learning by first learning a neural network classifier on all data available for training and using its representation to initialize and then fine-tune neural networks on the few-shot classification tasks found at test time <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>. Specifically, Saikia et al. <ref type="bibr" target="#b18">[19]</ref> have shown that competitive performance can be reached using a strong hyper-parameter optimization method applied on a carefully designed validation metric appropriate for few-shot learning.</p><p>Meta-Learning Another approach is to use meta-learning to more directly train a model to learn to perform few-shot classification, in an end-to-end way. A large variety of such models have been explored, inspired by memory-augmented networks <ref type="bibr" target="#b19">[20]</ref>, LSTMs <ref type="bibr" target="#b14">[15]</ref> and metric-based classifiers <ref type="bibr" target="#b23">[24]</ref>. The two most popular methods are Prototypical Networks <ref type="bibr" target="#b20">[21]</ref> and Model Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b7">[8]</ref>. Prototypical Networks assume that every class can be represented as a prototype in a learned embedding space (represented by a neural network). Prototypes are calculated as the average of the representations of the support examples of each class. A 1-nearest centroid classifier is then adopted for classification and the neural network representation is trained to facilitate classification in few-shot tasks directly. MAML models the procedure of learning to learn as a bilevel optimization, where an outer loop backpropagates loss gradients through an optimization-based inner loop to learn its initialization. Triantafillou et al. <ref type="bibr" target="#b21">[22]</ref> showed that prototypical networks and MAML could be combined by leveraging prototypes for the initialization of the output weights value in the inner loop. Requeima et al. <ref type="bibr" target="#b17">[18]</ref> also proposed Conditional Neural Adaptive Processes (CNAPs) for few-shot classification, which can be seen as extending prototypical networks with a more sophisticated architecture that allows for improved task adaptation. This architecture was later improved further by Bateni et al. <ref type="bibr" target="#b0">[1]</ref> with Simple CNAPS, leading to one of the current best methods on Meta-Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universal Representatons</head><p>In contrast, our work instead builds on that of Dvornik et al. <ref type="bibr" target="#b6">[7]</ref> and their method SUR (Selecting from Universal Representations). Bilen and Vedaldi <ref type="bibr" target="#b1">[2]</ref> introduced the term universal representation to refer to a representation that supports good performance in multiple domains. One proposal towards such a representation is to train different neural networks backbones separately on the data of each available domain, then simply to concatenate the representation learned by each. Another is to introduce some parameter sharing between the backbones, by having a single network conditioned on the domain of the provenance of each batch of training data <ref type="bibr" target="#b15">[16]</ref>, e.g. using Feature-wise Linear Modulate (FiLM) <ref type="bibr" target="#b13">[14]</ref>. SUR proposes to leverage a universal representation in few-shot learning tasks with a feature selection procedure that assigns different weights to each of the domain-specific subvectors of the universal representation. The objective is to assign high weights only to the domain-specific representations that are specifically useful for each few-shot task at hand. The weights are inferred by optimizing a loss on the support set that encourages high accuracy of a nearest-centroid classifier. As such, the method does not involve any meta-learning-a choice motivated by the concern that meta-learning may struggle in generalizing to domains that are dissimilar to the training domains. SUR achieved some of the best performances on Meta-Dataset. However, a contribution of our work is to provide evidence that meta-learning can actually be used to replace SUR's hand-designed inference procedure and improve performance further.</p><p>Transformer Networks Our meta-learning approach to leverage universal representations is inspired directly from Transformer networks <ref type="bibr" target="#b22">[23]</ref>. Transformer networks are neural network architectures characterized by the use self-attention mechanisms to solve tasks. Our model structure is inspired by the structure of the dot-product self-attention in the Transformer, which we adapted here to multidomain few-shot learning by designing appropriate parametrizations for queries, keys and values. Self-attention was explored in the single-domain training regime by Ye et al. <ref type="bibr" target="#b24">[25]</ref>, however for a different purpose, where each representation of individual examples in a task support set is influenced by all other examples. Such a strategy is also employed by CNAPs, but with the latter using FiLM as the conditioning mechanism, instead of self-attention. Regardless, the aim of this paper is to propose a different strategy. Rather than using self-attention between individual examples in the support set, our model uses self-attention to select between different domain-specific backbones. </p><formula xml:id="formula_1">r(S 1 ) r(S c ) r i (S c ) c Backbones {r i } c Backbones {r i } c Backbones {r i } c Backbones {r i } c Backbones {r i } α i,c α 2,1 α 3,1 α 4,1 α 1,2 α 2,2 α 3,2 α 4,2 α 1 α 2 α 3 α 4</formula><p>Inferring adapted representation for task Universal Representations <ref type="figure">Figure 1</ref>: Illustration of how a single-head URT layer uses a universal representation to produce a task-specific representation. This example assumes the use of four backbones, with each color illustrating their domain-specific sub-vector representation in the universal representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Universal Representation Transformer Layer</head><p>In this section, we describe our proposed URT layer, which uses meta-learning episodic training to learn how to combine the domain-specific backbones of a universal representation for any given few-shot learning classification task.</p><p>Conceptually, the proposed model views the support set S of a task as providing information on how to query and retrieve from the set {r i } of m pre-trained backbones the most appropriate backbone to build an adapted representation φ for the task.</p><p>We would like the model to support a variety of strategies on how to retrieve backbones. For example, it might be beneficial for the model to retrieve a single backbone from the set, especially if the domain of the given task matches perfectly that of a domain found in the training set. Alternatively, if some of the training domains benefit from much more training data than others, a better strategy might be to attempt some cross-domain generalization towards the few-shot learning task by blending many backbones together, even if none matches the domain of the task perfectly.</p><p>This motivates us to use dot-product self-attention, inspired by layers of Transformer networks <ref type="bibr" target="#b22">[23]</ref>. For this reason, we refer to our model as a Universal Representation Transformer (URT) layer. Additionally, since each class of the support set might require a different strategy, we perform attention separately for each class and their support set S c = {x|(x, y) ∈ S and y = c}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-Head URT Layer</head><p>We start by describing an URT layer consisting of a single attention head. An illustration of a single-head URT layer is shown in <ref type="figure">Figure 1</ref>. Let r i (x) be the output vector of the backbone for domain i. We then write the universal representation as r(x) = concat(r 1 (x), . . . , r m (x)).</p><p>(</p><p>This representation provides a natural starting point to obtain a representation of a support set class. Specifically, we will note</p><formula xml:id="formula_3">r(S c ) = 1 |S c | x∈Sc r(x)<label>(3)</label></formula><p>as the representation for the set S c . From this, we can describe the URT layer by defining the queries 3 , keys, the attention mechanism and output of the layer:</p><p>Queries q c : For each class c, we obtain a query through q c = W q r(S c ) + b q , where we have a learnable query linear transformation represented by matrix W q and bias b q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training of URT layer</head><p>Input: Number of tasks τ total , m pre-trained backbones ; <ref type="bibr">1:</ref> for τ ∈ {1, · · · , τ total } do 2:</p><p>Sample a few-shot task T with support set S and query set Q;</p><p>3:</p><p># Infer adapted representation for task from S 4:</p><p>For each class, obtain representation using m pre-trained backbones as in Eq. (3);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Obtain attention scores using Eq. (4,5) for each head using support set S; <ref type="bibr">6:</ref> # Use adapted representation to predict labels in Q from support set S Compute loss as in <ref type="figure">Eq. (1,8)</ref> and perform gradient descent step on URT parameters Θ; 10: end for Keys k i,c : For each domain i and class c, we define keys as k i,c = W k r i (S c ) + b k , using a learnable linear transformation W k and b k and where r i (S c ) = 1/|S c | x∈Sc r i (x), using a similar notation as for r(S c ).</p><p>Attention scores α i : as for regular Transformer layers, we use scaled dot-product attention</p><formula xml:id="formula_4">α i,c = exp(β i,c ) i exp(β i ,c ) , β i,c = q c k i,c √ l ,<label>(4)</label></formula><p>where l is the dimensionality of the keys and queries. Then, these per-class scores are aggregated to obtain scores for the full support set by averaging</p><formula xml:id="formula_5">α i = c α i,c N .<label>(5)</label></formula><p>Equipped with these attention scores, the URT layer can now produce an adapted representation for the task (for the support and query set examples) by computing</p><formula xml:id="formula_6">φ(x) = i α i r i (x) .<label>(6)</label></formula><p>As we can see, this approach has the flexibility of either selecting a single domain-specific backbone (by assigning α i = 1 for a single domain) or blending different domains together (by having α i &gt;&gt; 0 for multiple backbones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Head URT Layer</head><p>The URT layer described so far can only learn to retrieve a single backbone (or blending of backbones). Yet, it might be beneficial to retrieve multiple different (blended) backbones, especially for a few-shot task that would include many classes of varying complexity.</p><p>Thus, to achieve such diversity in the adapted representation, we also consider URT layers with multiple heads, i.e. where each head corresponds to the calculation of Equation <ref type="bibr" target="#b5">6</ref> and each head has its own set of parameters (W q , b q , W k , b k ). Denoting each head now as φ h , a multi-head URT layer then produces as its output the concatenation of all of its heads:</p><formula xml:id="formula_7">φ(x) = concat(φ 1 (x), . . . , φ H (x)).<label>(7)</label></formula><p>Empirically we found that the randomness in the initialization of head weights alone did not lead to uniqueness and being complimentary between the heads, so inspired by Lin et al. <ref type="bibr" target="#b11">[12]</ref>, we add a regularizer to avoid duplication of the attention scores:</p><formula xml:id="formula_8">Ω(Θ) = (AA − I) F 2 ,<label>(8)</label></formula><p>where · F is the Frobenius norm of a matrix and A ∈ R n×m is the matrix for attention scores, with A h being the vector of all scores α i for head h. The identity matrix I regularizes each set of attention scores to be more focused so that multiple heads can attend to different domain-specific backbones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Strategy</head><p>We train representations produced by the URT layer by following the approach of Prototypical Networks <ref type="bibr" target="#b20">[21]</ref>, where the probability of a label y for a query example x given the support set of a task is modeled as:</p><formula xml:id="formula_9">p(y = c|x, S; Θ) = exp(−d(φ(x) − p c )) N c =1 exp(−d(φ(x) − p c )) ,<label>(9)</label></formula><p>where d is a distance metric and p c = 1/|S c | x∈Sc φ(x) corresponds to the centroid of class c, referred to as its prototype. We use (negative) cosine similarity as the distance. The full training algorithm is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we seek to answer three key experimental questions:</p><p>Q1 How does URT compare with previous state-of-the-art on Meta-Dataset for multi-domain fewshot classification? Q2 Do the URT attention heads generate interpretable and meaningful attention scores? Q3 Does the URT layer provide consistent benefits, even when pre-trained backbones are trained in different ways?</p><p>In addition, we investigate architectural choices made, such as our models for keys/queries and their regularization, and study their contribution to achieving strong performance with URT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setup</head><p>We test our methods on the large-scale few-shot learning benchmark Meta-Dataset <ref type="bibr" target="#b21">[22]</ref>. It consists of ten datasets with various data distributions across different domains, including natural images (Birds, Fungi, VGG Flower), hand-written characters (Omniglot, Quick Draw), and human created objects (Traffic Signs, Aircraft). Among the ten datasets, eight provide data that can be used during either training, validation and testing (with each class assigned to only one of those sets), while two datasets are solely used for testing. Following Bateni et al. <ref type="bibr" target="#b0">[1]</ref>, Requeima et al. <ref type="bibr" target="#b17">[18]</ref>, we also report results on MNIST <ref type="bibr" target="#b10">[11]</ref>, CIFAR10 and CIFAR100 <ref type="bibr" target="#b8">[9]</ref> as additional unseen test datasets. Following Triantafillou et al. <ref type="bibr" target="#b21">[22]</ref>, few-shot tasks are sampled with varying number of classes N , varying number of shots K and class imbalance. The performance is reported as the average accuracy over 600 sampled tasks. More details of Meta-Dataset can be found in Triantafillou et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>The domain-specific backbones are pre-trained following the setup in <ref type="bibr" target="#b6">[7]</ref>. Then, we freeze the backbone and train the URT layer for 10,000 episodes, with an initial learning rate of 0.01 and a cosine learning rate scheduler. Following Chen et al. <ref type="bibr" target="#b4">[5]</ref>, the training episodes have 50% probability coming from the ImageNet data source. Since different pre-trained backbones may produce representations with different vector norms, we normalize the outputs of the backbones as in Dvornik et al. <ref type="bibr" target="#b6">[7]</ref>. URT is trained with parameter weight decay of 1e-5 and with a regularization factor λ = 0.1. The number of heads (H in <ref type="figure">Equation 7</ref>), is set to 2 and the dimension of the keys and queries (l in <ref type="figure">Equation 4</ref>) is set to 1024. We choose the hyper-parameters based on the performance of the validation set. Details of the hyper-parameter selection and how the performance is influenced by them are outlined in Section 4.5. <ref type="table" target="#tab_0">Table 1</ref> presents a comparison of URT with SUR, as well as other baselines based on transfer learning by fine-tuning <ref type="bibr" target="#b18">[19]</ref> or meta-learning (Prototypical Networks <ref type="bibr" target="#b20">[21]</ref>, first-order MAML <ref type="bibr" target="#b7">[8]</ref>, ProtoMAML <ref type="bibr" target="#b21">[22]</ref>, CNAPs <ref type="bibr" target="#b17">[18]</ref>) and Simple CNAPS <ref type="bibr" target="#b0">[1]</ref>. We observe in <ref type="table" target="#tab_0">Table 1</ref> that URT establishes a new stateof-the-art on Meta-Dataset, by achieving the top performance on 7 out of the 10 dataset sources. When comparing to its predecessor, URT outperforms SUR on 4 datasets without compromising performance on others, which is challenging to achieve in the multidomain setting. Of note, the average inference time for URT is 0.04 second per task, compared to 0.43 for SUR, on a single V100. Thus, getting rid of the optimization procedure for every episode with our meta-trained URT layer also significantly increases the latency, by more than 10×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Previous Approaches</head><p>We also report performances on the MNIST, CIFAR-10 and CIFAR-100 dataset sources in <ref type="table" target="#tab_1">Table 2</ref>, and compare with the subset of methods that have reported on these datasets. There, URT neither improves nor gets worse performance than SUR, yeilding top performance on the MNIST domain but not on the CIFAR-10/CIFAR-100 domain, on which Simple CNAPS has the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interpreting and Visualizing Attention by URT</head><p>To better understand how the URT model of Section 4.2 uses its two heads to build adapted representations, we visualize the attention scores produced on the test tasks of Meta-Dataset in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>The blue (first head) and orange (second head) heatmaps summarize the values of the attention scores (Equation 5), averaged across several tasks for each test domain. Specifically, the element on row t and column i is the averaged attention scores α i computed on test set domain t for the backbone from domain i. Note that the last two rows are the two unseen domain datasets. We found that for datasets from the seen domains, i.e. the first eight rows, one head (right, orange) consistently puts most of its weight on the backbone pre-trained on the same domain, while the other head (left, blue) learns relatively smoother weight distributions that blends other related domains. For unseen datasets, the right head puts half of its weight on ImageNet and the left head learned to blend the representations from four backbones. As additional evidence of the benefit of URT on universal representations, we also present experiments based on a different set of backbone architectures. Following SUR <ref type="bibr" target="#b6">[7]</ref>, we consider the backbones from a parametric network family, obtained by training a base backbone on one dataset (ILSVRC) and then learning separate FiLM layers <ref type="bibr" target="#b13">[14]</ref> for each other dataset, to modulate the backbone so it is adapted to the other domains. These backbones collectively have only 0.5% more parameters than a single backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">URT using FiLM Modulated Backbones</head><p>A comparison between SUR and URT using these backbones (referred to as SUR-pf and URT-pf) is presented in <ref type="table" target="#tab_2">Table 3</ref>. Once again, URT improves the performance on three datasets without sacrificing performance on others. Additionally, URT-pf now achieves better performance than BOHB-E on VGGFlower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-Parameter and Ablation Studies</head><p>We analyze the importance of the various components of URT's attention mechanism structure and training strategy in <ref type="table" target="#tab_3">Table 4</ref>. First we analyze the importance of using the support set to model queries and/or keys. To this end, we consider setting the matrices W q / W k of the query / key linear transformation to 0, which only leaves the bias term. We found that the support set representation is most crucial for building the keys (row w/o W k in the table) and has minor benefits for queries (row w/o W q ) in the table. This observation is possibly related to the success of attention-based models with learnable constant queries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. We also found that adding a regularizer Ω(Θ) as in Equation 8 is important for some datasets, specifically VGG Flower and Birds. An important hyper-parameter in URT is the number of heads H. We chose this hyper-parameter based on the performance on validation set of tasks in Meta-Dataset. In <ref type="table" target="#tab_4">Table 5</ref>, we show the validation performance of URT for varying number of heads. As suggested by Triantafillou et al. <ref type="bibr" target="#b21">[22]</ref>, we considered looking at the rank of the performance achieved by each choice of H for each validation domains, and taking the average across domains as a validation metric. However, since the performances when using two to four heads are similar and yield the same average rank, we instead simply consider the average accuracy as the selection criteria. In general, we observe a large jump in performance when using multiple heads instead of just one. However, since the number of heads controls the capacity, predictably we also observe that having too many heads leads to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed the URT layer to effectively integrate representations from multiple domains and demonstrated improved performance in multi-domain few-shot classification. Notably, our URT approach was able to set a new state-of-the-art on Meta-Dataset, and never performs worse than its predecessor (SUR) while also being 10× more efficient at inference. This work suggests that combining meta-learning with pre-trained universal representations is a promising direction for new few-shot learning methods. Specifically, we hope that future work can investigate the design of richer forms of universal representations that go beyond simply pre-training a single backbone for each domain, and developing meta-learners adapted to those settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our URT model may present an interesting element of solution for applications that present difficulties in the collection and sharing of data. This could include settings where each user of an application has limited private data, and as such desires that a classification task be executed directly and solely on their devices. Any deployment of the proposed model however should be preceded by an analysis of the potential biases captured by the dataset sources used for training and the correction of any such undesirable biases captured by the pre-trained backbones and model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 : 8 :</head><label>78</label><figDesc>Compute adapted representation of examples in S and Q as in Eq. (6,7); Compute probabilities of label of examples in Q using Prototypical Network as in Eq. (9); 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Average attention scores generated by URT with two heads. Rows correspond to the domain of the test tasks and the columns correspond to the pre-trained backbones r i (x) trained on the eight training domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test performance (mean+CI%95) over 600 few-shot tasks. URT and the most recent methods, which are listed in the first row, are compared on Meta-Dataset<ref type="bibr" target="#b21">[22]</ref>, which are listed in the first row. The numbers in bold have intersecting confidence intervals with the most accurate method.ProtoMAML[22] 46.5±1.1 82.7±1.0 75.2±0.8 69.9±1.0 68.3±0.8 66.8±0.9 42.0±1.2 88.7±0.7 52.4±1.1 41.7±1.1 5.4 CNAPs[18] 52.3±1.0 88.4±0.7 80.5±0.6 72.2±0.9 58.3±0.7 72.5±0.8 47.4±1.0 86.0±0.5 60.2±0.9 42.6±1.1 5.1 BOHB-E[19] 55.4±1.1 77.5±1.1 60.9±0.9 73.6±0.8 72.8±0.7 61.2±0.9 44.5±1.1 90.6±0.6 57.5±1.0 51.9±1.0 3±1.1 93.1±0.5 85.4±0.7 71.4±1.0 71.5±0.8 81.3±0.6 63.1±1.0 82.8±0.7 70.4±0.8 52.4±1.1 2.5 SimpleCNAPS[1] 58.6±1.1 91.7±0.6 82.4±0.7 74.9±0.8 67.8±0.8 77.7±0.7 46.9±1.0 90.7±0.5 73.5±0.7 46.</figDesc><table><row><cell></cell><cell>I L S V R C</cell><cell>O m n i g l o t</cell><cell>A i r c r a f t</cell><cell>B i r d s</cell><cell>T e x t u r e s</cell><cell>Q u i c k D r a w</cell><cell>F u n g i</cell><cell>V G G F l o w e r</cell><cell>T r a f fi c S i g n s</cell><cell>M S C O C O</cell><cell>a v g . r a n k</cell></row><row><cell>MAML[8]</cell><cell cols="10">37.8±1.0 83.9±1.0 76.4±0.7 62.4±1.1 64.1±0.8 59.7±1.1 33.5±1.1 79.9±0.8 42.9±1.3 29.4±1.1</cell><cell>8.0</cell></row><row><cell>ProtoNet[21]</cell><cell cols="10">44.5±1.1 79.6±1.1 71.1±0.9 67.0±1.0 65.2±0.8 64.9±0.9 40.3±1.1 86.9±0.7 46.5±1.0 39.9±1.1</cell><cell>7.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.4</cell></row><row><cell>TaskNorm[3]</cell><cell cols="10">50.6±1.1 90.7±0.6 83.8±0.6 74.6±0.8 62.1±0.7 74.8±0.7 48.7±1.0 89.6±0.6 67.0±0.7 43.4±1.0</cell><cell>3.8</cell></row><row><cell>SUR[7]</cell><cell cols="10">56.2±1.1</cell><cell>2.4</cell></row><row><cell>URT</cell><cell cols="10">55.7±1.0 94.4±0.4 85.8±0.6 76.3±0.8 71.8±0.7 82.5±0.6 63.5±1.0 88.2±0.6 69.4±0.8 52.2±1.1</cell><cell>1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Test performance (mean+CI%95) over 600 few-shot tasks on</cell></row><row><cell>additional datasets.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MNIST CIFAR10 CIFAR100 avg. rank</cell></row><row><cell>CNAPs[18]</cell><cell>92.7 ± 0.4 61.5 ± 0.7 50.1 ± 1.0</cell><cell>4.7</cell></row><row><cell>TaskNorm[3]</cell><cell>92.3 ± 0.4 69.3 ± 0.8 54.6 ± 1.1</cell><cell>3.3</cell></row><row><cell>SUR[7]</cell><cell>94.3 ± 0.4 66.8 ± 0.9 56.6 ± 1.0</cell><cell>2.3</cell></row><row><cell cols="2">SimpleCNAPS[1] 93.9 ± 0.4 74.3 ± 0.7 60.5 ± 1.0</cell><cell>1.7</cell></row><row><cell>URT</cell><cell>94.8 ± 0.4 67.3 ± 0.8 56.9 ± 1.0</cell><cell>2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Performance comparison using para-</cell></row><row><cell cols="2">metric network family (pf) backbones.</cell></row><row><cell></cell><cell>SUR-pf [7] URT-pf VS.</cell></row><row><cell>ILSVRC</cell><cell>56.4 ± 1.2 55.5 ± 1.1 =</cell></row><row><cell>Omniglot</cell><cell>88.5 ± 0.8 90.2 ± 0.6 +</cell></row><row><cell>Aircraft</cell><cell>79.5 ± 0.8 79.8 ± 0.7 =</cell></row><row><cell>Birds</cell><cell>76.4 ± 0.9 77.5 ± 0.8 =</cell></row><row><cell>Textures</cell><cell>73.1 ± 0.7 73.5 ± 0.7 =</cell></row><row><cell cols="2">Quick Draw 75.7 ± 0.7 75.8 ± 0.7 =</cell></row><row><cell>Fungi</cell><cell>48.2 ± 0.9 48.1 ± 0.9 =</cell></row><row><cell cols="2">VGG Flower 90.6 ± 0.5 91.9 ± 0.5 +</cell></row><row><cell cols="2">Traffic Signs 65.1 ± 0.8 67.5 ± 0.8 +</cell></row><row><cell>MSCOCO</cell><cell>52.1 ± 1.0 52.1 ± 1.0 =</cell></row><row><cell>MNIST</cell><cell>93.2 ± 0.4 93.9 ± 0.4 =</cell></row><row><cell>CIFAR10</cell><cell>66.4 ± 0.8 66.1 ± 0.8 =</cell></row><row><cell>CIFAR100</cell><cell>57.1 ± 1.0 57.3 ± 1.0 =</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Meta-Dataset performance variation on ablations of elements of the URT layer.</figDesc><table><row><cell></cell><cell cols="8">ILSVRC Omniglot Aircraft Birds Textures Draw Fungi Flower Signs MSCOCO</cell></row><row><cell>w/o W q</cell><cell>+0.2</cell><cell>-0.2</cell><cell>-0.6</cell><cell>-0.1</cell><cell>-0.3</cell><cell>-0.2 0.0</cell><cell>-0.2 -0.8</cell><cell>-0.1</cell></row><row><cell>w/o W k</cell><cell>-14.2</cell><cell>-2.8</cell><cell cols="3">-10.7 -18.1 -7.6</cell><cell cols="2">-9.3 -22.4 -3.6 -0.26</cell><cell>-10.9</cell></row><row><cell cols="2">w/o r(S c ) -14.2</cell><cell>-2.8</cell><cell cols="3">-10.7 -18.1 -7.6</cell><cell cols="2">-9.2 -22.4 -3.6 -0.26</cell><cell>-10.9</cell></row><row><cell>w/o Ω(Θ)</cell><cell>0.0</cell><cell>-0.9</cell><cell>-0.4</cell><cell>-3.3</cell><cell>-1.2</cell><cell cols="2">-0.2 +0.3 -9.0 -2.0</cell><cell>0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Validation performance on Meta-Dataset using different number of heads Average Accuracy 74.605 77.145 76.943 76.984 76.602 75.906 75.454 74.473 Average Rank 2.875 1.000 1.000 1.000 2.250 2.250 2.25 2.50</figDesc><table><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Unable to avoid the unfortunate double usage of the term "query" due to conflicting conventions, we highlight the difference between the query sets Q of few-shot tasks and the queries qc of an attention mechanism.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Tianyi Zhou for paper review and suggestions. The computation support for this project is provided by Compute Canada and Google Cloud. This project was supported by the Canada CIFAR AI Chairs program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<title level="m">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tasknorm: Rethinking batch normalization for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Selecting relevant features from a universal representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09338</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional lstm model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multidomain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7957" to="7968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimized generic feature learning for few-shot classification across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
