<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61/CSIRO</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61/CSIRO</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Au</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition with CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is timeconsuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an endto-end trainable network with streams which learn the IDTbased BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to 'translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields stateof-the-art results on four publicly available datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action Recognition (AR) pipelines have transitioned from the use of handcrafted descriptors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b66">66]</ref> to CNN models such as the two-stream network <ref type="bibr" target="#b56">[56]</ref>, 3D spatio-temporal features <ref type="bibr" target="#b60">[60]</ref>, spatio-temporal ResNet <ref type="bibr" target="#b16">[17]</ref> and the I3D network pre-trained on Kinetics-400 <ref type="bibr" target="#b3">[4]</ref>. Such CNNs operate on RGB/optical flow videos thus failing to capture some domain-specific information which sophisticated low-level representations capture by design. One prominent example are Improved Dense Trajectory (IDT) descriptors <ref type="bibr" target="#b66">[66]</ref> which are typically encoded with Bag-of- * Both authors contributed equally. This paper is accepted by the ICCV'19. Please respect the authors' efforts by not copying/plagiarizing bits and pieces of this work for your own gain (we will vigorously pursue dishonest authors). If you find anything inspiring in this work, be kind enough to cite it thus showing you care for the CV community.</p><p>Words (BoW) <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b11">12]</ref> or Fisher Vectors (FV) <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref> and fused with CNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b9">10]</ref> at the classifier which improves results due to several sophisticated steps of IDT: (i) camera motion estimation, (ii) motion descriptor modeling along motion trajectories estimated by the optical flow, (iii) pruning inconsistent matches, (iv) focusing on human motions via a human detector, (v) combination of IDT with powerful and highly complementary to each other video descriptors such as Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, Histogram of Optical Flow (HOF) <ref type="bibr" target="#b12">[13]</ref> and Motion Boundary Histogram (MBH) <ref type="bibr" target="#b65">[65]</ref> e.g., HOF and MBH contain zero-and first-order motion statistics <ref type="bibr" target="#b66">[66]</ref>.</p><p>However, extracting dense trajectories and corresponding video descriptors is costly due to several off-line/CPUbased steps. Motivated by this shortcoming, we propose simple trainable CNN streams on top of a CNN network (in our case I3D <ref type="bibr" target="#b3">[4]</ref>) which learn to 'translate' the I3D output into IDT-based BoW and FV global video descriptors. We can even 'translate' the I3D RGB output into I3D Optical Flow Features (OFF). At the testing stage, our socalled BoW, and FV and OFF streams (on top of I3D) are able to hallucinate such global descriptors which we feed into the final layer preceding a classifier. We show that IDT/OFF representations can be synthesized by our network thus removing the need of actually computing them which simplifies the AR pipeline. With a handful of convolutional/FC layers and basic CNN building blocks, our representation rivals sophisticated AR pipelines that aggregate features frame-by-frame e.g., HOK <ref type="bibr" target="#b7">[8]</ref> and rank-pooling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b9">10]</ref>. Below, we detail our contributions: I. We are the first to propose that old-fashioned IDT-based BoW and FV global video descriptors can be learned via simple dedicated CNN-streams at the training stage and simply hallucinated for classification with a CNN action recognition pipeline during testing.</p><p>II. We show that even the I3D optical flow stream can be easily hallucinated from the I3D RGB stream.</p><p>III. We study various aspects of our model e.g., the count sketch <ref type="bibr" target="#b49">[49]</ref> of features to avoid overfitting when fusing several streams and Power Normalization <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref> to prevent so-called burstiness in BoW, FV and CNNs, and we perform several experiments on four datasets. <ref type="figure" target="#fig_3">Figure 1</ref>: The overview of our pipeline. We remove the prediction and the last 1D conv. layers from I3D RGB and optical flow streams, concatenate (⊕) the 1024×7 feature representations X (rgb) and X (opt.) , and feed them into our Fisher Vector (FV), Bag-of-Words (BoW), and the High Abstraction Features (HAF) streams followed by the Power Normalization (PN) blocks. The resulting feature vectorsψ (f v1) , ψ (f v2) ,ψ <ref type="bibr">(bow)</ref> and ψ (haf ) are concatenated (⊕) and fed into our Prediction Network (PredNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Below, we describe handcrafted spatio-temporal video descriptors and their encoding strategies, optical flow, and deep learning pipelines for video classification.</p><p>Handcrafted video representations. Early AR relied on spatio-temporal interest point detectors <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b64">64]</ref> and spatio-temporal descriptors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b66">66]</ref> which capture various appearance and motion statistics.</p><p>Spatio-temporal interest point detectors were developed for the task of identifying spatio-temporal regions of videos rich in motion patterns relevant to classification, thus providing sampling locations for local descriptors. The number of sampling points had a significant influence on the processing speed due to the volumetric nature of videos. Har-ris3D <ref type="bibr" target="#b43">[43]</ref>, one of the earliest detectors, performs a search for extreme points in the spatio-temporal domain via the so-called structure tensor and the determinant-to-trace ratio test. Cuboid <ref type="bibr" target="#b13">[14]</ref>, a faster detector, applies Gaussian and Gabor filters in spatial and temporal domains, respectively. Selective STIP <ref type="bibr" target="#b5">[6]</ref> extracts initial key-point candidates with the Harris corner detector followed by the candidate suppression with a so-called surround suppression mask. Hes-STIP, a more recent detector, uses integral videos and Hessian matrix to search the scale-space for local maxima of the signal. Evaluations and further reading on spatio-temporal detectors can be found in surveys <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b69">69]</ref>.</p><p>One drawback of spatio-temporal interest point detectors is the sparsity of key-points and inability to capture longterm motion patterns. Thus, a Dense Trajectory (DT) <ref type="bibr" target="#b64">[64]</ref> approach densely samples feature points in each frame to track them in the video (via optical flow). Then, multiple descriptors are extracted along trajectories to capture shape, appearance and motion cues. As DT cannot compensate for the camera motion, the IDT <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b65">65]</ref> estimates the camera motion to remove the global background motion. IDT also removes inconsistent matches via a human detector.</p><p>For spatio-temporal descriptors, IDT employs HOG <ref type="bibr" target="#b21">[22]</ref>, HOF <ref type="bibr" target="#b12">[13]</ref> and MBH <ref type="bibr" target="#b65">[65]</ref>. HOG <ref type="bibr" target="#b21">[22]</ref> contains statistics of the amplitude of image gradients w.r.t. the gradient orientation. Thus, it captures the static appearance cues while its close cousin, HOG-3D <ref type="bibr" target="#b30">[31]</ref>, is designed for spatiotemporal interest points. In contrast, HOF <ref type="bibr" target="#b12">[13]</ref> captures histograms of optical flow while MBH <ref type="bibr" target="#b65">[65]</ref> captures derivatives of the optical flow, thus it is highly resilient to the global camera motion whose cues cancel out due to derivatives. Thus, HOF and MBH contain the zero-and first-order optical flow statistics. Other spatio-temporal descriptors include SIFT3D <ref type="bibr" target="#b54">[54]</ref>, SURF3D <ref type="bibr" target="#b73">[73]</ref> and LTP <ref type="bibr" target="#b75">[75]</ref>.</p><p>In this work, we follow the standard practice, that is, we use the Improved Dense Trajectories <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> and we encode them together with HOG, HOF, and MBH descriptors via BoW <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b11">12]</ref> and FV <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref> which we describe below. Descriptor encoding. BoW <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b11">12]</ref>, a global image representation, is likely the oldest encoding strategy for local descriptors. It consists of (i) clustering with k-means for a collection of descriptor vectors from the training set to build so-called visual vocabulary, (ii) assigning each descriptor to its nearest cluster center from the visual dictionary, and (iii) aggregating the one-hot assignment vectors via average pooling. Similar models such as Soft Assignment (SA) <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b33">33]</ref> and Localized Soft Assignment (LcSA) <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b38">38]</ref> use the Component Membership Probability (CMP) of GMM to assign each descriptor with some probability to visual words followed by average or non-linear pooling <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b70">70]</ref>.</p><p>In this paper, we chose the simplest BoW model <ref type="bibr" target="#b11">[12]</ref>  with Power Normalization <ref type="bibr" target="#b38">[38]</ref> detailed in Section 3. BoW can be seen as zero-order statistics of FV <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>, thus we also employ FV to capture first-and second-order statistics of local descriptors. FV builds a visual dictionary from training data via GMM. Then, a displacement/square displacement of each descriptor vector w.r.t. each GMM component center is taken, normalized by its GMM standard deviation/variance to capture the first/second-order terms, and then soft-assigned via CMP to each GMM component. Optical flow. As a key concept in AR from videos, optical flow is the distribution of velocities of movement of brightness pattern across frames <ref type="bibr" target="#b25">[26]</ref> such as the pattern of motion of objects, surfaces and edges in a visual scene caused by the relative motion between an observer and a scene <ref type="bibr" target="#b26">[27]</ref>. Early optical flow coped with small displacements via energy minimization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">46]</ref>. However, to capture informative motions of subjects/objects, optical flow needs to cope with large displacements <ref type="bibr" target="#b0">[1]</ref>. As energy-based methods suffer from the local minima, local descriptor matching is used in Large Displacement Optical Flow (LDOF) <ref type="bibr" target="#b2">[3]</ref>. Recent methods use non-rigid descriptor matching <ref type="bibr" target="#b72">[72]</ref>, segment matching <ref type="bibr" target="#b1">[2]</ref> or even edge-preserving interpolation <ref type="bibr" target="#b51">[51]</ref>.</p><p>In this work, we are not concerned with the use of the newest possible optical flow. Thus, we opt for LDOF <ref type="bibr" target="#b46">[46]</ref>. CNN-based action recognition. The success of AlexNet <ref type="bibr" target="#b40">[40]</ref> and ImageNet <ref type="bibr" target="#b53">[53]</ref> sparked studies into AR with CNNs. Early models extracted per-frame representations followed by average pooling <ref type="bibr" target="#b29">[30]</ref> which discards the temporal order. To fix such a shortcoming, frame-wise CNN scores were fed to LSTMs <ref type="bibr" target="#b14">[15]</ref>. Two-stream networks <ref type="bibr" target="#b56">[56]</ref> compute representations per RGB frame and per 10 stacked optical flow frames. However, a more obvious extension is to model spatio-temporal 3D CNN filters <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">63]</ref>.</p><p>The recent I3D model <ref type="bibr" target="#b3">[4]</ref> draws on the two-stream networks, 'inflates' 2D CNN filters pre-trained on ImageNet to spatio-temporal 3D filters, and implements temporal pooling across the inception module. In this paper, we opt for the I3D network but our proposed layers are independent of the CNN design. We are concerned with 'absorbing' the old yet powerful IDT representations and/or optical flow features into CNN and hallucinating them at the test time. Temporal aggregation. While two-stream networks <ref type="bibr" target="#b56">[56]</ref> discard the temporal order and others use LSTMs <ref type="bibr" target="#b14">[15]</ref>, many AR pipelines address the spatio-temporal aggregation. Rank pooling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> projects frame-wise feature vectors onto a line such that the temporal order of vectors is preserved along the line. Subspace and kernel rank pooling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b67">67]</ref> use projections into the RKHS in which the temporal order of frames is preserved. Another aggregation family captures second-or higher-order statistics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this paper, we are not concerned with temporal pooling. Thus, we use a 1D convolution (as in I3D <ref type="bibr" target="#b3">[4]</ref>). Power Normalization family. BoW, FV and even CNN-based descriptors have to deal with the so-called burstiness defined as 'the property that a given visual element appears more times in an image than a statistically independent model would predict' <ref type="bibr" target="#b27">[28]</ref>, a phenomenon also present in video descriptors. Power Normalization <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b36">36]</ref> is known to suppress the burstiness, and it has been extensively studied in the context of BoW <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref>. Moreover, a connection to Max-pooling was found in survey <ref type="bibr" target="#b38">[38]</ref> which also shows that the so-called MaxExp pooling is in fact a detector of 'at least one particular visual word being present in an image'. According to papers <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref>, many Power Normalization functions are closely related. We outline Power Normalizations used in our work in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In our work, we use BoW/FV (training stage), as well as Power Normalization <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b37">37]</ref> and count sketches <ref type="bibr" target="#b71">[71]</ref>. Notations. We use boldface uppercase letters to express matrices e.g., M , P , regular uppercase letters with a subscript to express matrix elements e.g., P ij is the (i, j) th element of P , boldface lowercase letters to express vectors, e.g. x, φ, ψ, and regular lowercase letters to denote scalars. Vectors can be numbered e.g., m 1 , ..., m K or x n , etc., while regular lowercase letters with a subscript express an element of vector e.g., m i is the i th element of m. Operators ';' and ⊕ concatenate vectors e.g.,</p><formula xml:id="formula_0">⊕ i∈I K v i = [v 1 ; ...; v K ] while I d denotes an index set of integers {1, ..., d}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Descriptor Encoding Schemes</head><p>Bag-of-Words <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b11">12]</ref> assigns each local descriptor x to the closest visual word from M = [m 1 , ..., m K ] built via k-means. In order to obtain mid-level feature φ, we solve:</p><formula xml:id="formula_1">φ = arg min φ x − M φ 2 2 , s. t. φ ∈ {0, 1}, 1 T φ = 1.</formula><p>(1)</p><p>Fisher Vector Encoding <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref> uses a Mixture of K Gaussians from a GMM used as a dictionary. It performs descriptor coding w.r.t. to Gaussian components G(w k , m k , σ k ) which are parametrized by mixing probability, mean, and on-diagonal standard deviation. The firstand second-order features φ k , φ k ∈ R D are :</p><formula xml:id="formula_2">φ k = (x−m k )/σ k , φ k = φ 2 k −1. (2) Concatenation of per-cluster features φ * k ∈ R 2D forms the mid-level feature φ ∈ R 2KD : φ = [φ * 1 ; ...; φ * K ] , φ * k = p (m k |x, θ) √ w k φ k ; φ k / √ 2 ,<label>(3)</label></formula><p>where p and θ are the component membership probabilities and parameters of GMM, respectively. For each descriptor x of dimensionality D (after PCA), its encoding φ is of 2KD dim. as φ contains first-and second-order statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pooling a.k.a. Aggregation</head><p>Traditionally, pooling is performed via averaging midlevel feature vectors φ(x) corresponding to (local) descriptors x ∈ X from a video sequence X , that is ψ = avg x∈X φ(x), and (optionally) applying the 2 -norm normalization. In this paper, we work with either sequences X (for which the above step is used) or subsequences. Proposition 1. For subsequence pooling, let X s,t = X 0,t \ X 0,s−1 , where X s,t denotes a set of descriptors in the sequence X counting from frame s up to frame t, where 0 ≤ s ≤ t ≤ τ , X 0,−1 ≡ ∅, and τ is the length of X . Moreover, let us compute an integral mid-level feature φ t = φ t−1 + x∈X t,t φ(x) which aggregates mid-level feature vectors from frame 0 to frame t, and φ −1 is an all-zeros vector. Then, the pooled subsequence is given by:</p><formula xml:id="formula_3">ψ s,t = (φ t −φ s−1 )/( φ t −φ s−1 2 + ),<label>(4)</label></formula><p>where 0 ≤ s ≤ t ≤ τ are the starting and ending frames of subsequence X s,t ⊆ X and is a small constant. We normalize the pooled sequences/subseq. as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Power Normalization</head><p>As alluded to in Section 2, we apply Power Normalizing functions to BoW and FV streams which hallucinate these two modalities (and HAF/OFF stream explained later). We investigate three operators g(ψ, ·) detailed by Remarks 1-3.</p><p>Remark 1. AsinhE function <ref type="bibr" target="#b39">[39]</ref> is an extension of a well-known Power Normalization (Gamma) <ref type="bibr" target="#b39">[39]</ref> defined as g(ψ, γ) = Sgn(ψ)|ψ| γ for 0 &lt; γ ≤ 1 to the operator with a smooth derivative and a parameter γ . AsinhE is defined as the normalized Arcsin hyperbolic function:</p><formula xml:id="formula_4">g(ψ, γ ) = arcsinh(γ ψ)/ arcsinh(γ ).<label>(5)</label></formula><p>Remark 2. Sigmoid (SigmE), a Max-pooling approximation <ref type="bibr" target="#b39">[39]</ref>, is an extension of the MaxExp operator defined as g(ψ, η) = 1 − (1 − ψ) η for η &gt; 1 to the operator with a smooth derivative, a response defined for real-valued ψ (rather than ψ ≥ 0), a parameter η and a small const. :</p><formula xml:id="formula_5">g(ψ, η ) = 2 1+e −η ψ/( ψ 2 + ) −1.<label>(6)</label></formula><p>Remark 3. AxMin, a piece-wise linear form of SigmE <ref type="bibr" target="#b39">[39]</ref>, is given as g(ψ, η ) = Sgn(ψ) min(η ψ/( ψ 2 + ), 1) for η &gt; 1 and a small constant .</p><p>Despite the similar role of these three pooling operators, we investigate each of them as their interplay with end-toend learning differs. Specifically, lim ψ→±∞ g(ψ, ·) for As-inhE and SigmE are ±∞ and ±1, resp., thus their asymptotic behavior differs. Moreover, AxMin is non-smooth and relies on the same gradient re-projection properties as ReLU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Count Sketches</head><p>Sketching vectors by the count sketch <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b71">71]</ref> is used for their dimensionality reduction which we use in this paper. </p><formula xml:id="formula_6">P ij = s i if h i = j, 0 otherwise,<label>(7)</label></formula><p>and the sketch projection p : R d → R d is a linear operation given as p(ψ) = P ψ (or p(ψ; P ) = P ψ to highlight P ).</p><p>Proof. It directly follows from the definition of the count sketch e.g., see Definition 1 <ref type="bibr" target="#b71">[71]</ref>.</p><p>Remark 4. Count sketches are unbiased estimators:</p><formula xml:id="formula_7">E h,s (p(ψ, P (h, s)), p(ψ , P (h, s))) = ψ, ψ . As vari- ance V h,s (p(ψ), p(ψ )) ≤ 1 d ψ, ψ 2 + ψ 2 2 ψ 2 2</formula><p>, we note that larger sketches are less noisy. Thus, for every modality we compress, we use a separate sketch matrix P . As video modalities are partially dependent, this implicitly leverages the unbiased estimator and reduces the variance.</p><p>Proof. For the first and second property, see Appendix A of paper <ref type="bibr" target="#b71">[71]</ref> and Lemma 3 <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>Our pipeline is illustrated in <ref type="figure" target="#fig_3">Figure 1</ref>. It consist of (i) the Fisher Vector and Bag-of-Words hallucinating streams denoted as FV and BoW (shown in dashed red), respectively, (ii) the High Abstraction Features stream denoted as HAF, and (iii) the Prediction Network abbreviated as PredNet.</p><p>The role of BoW/FV streams is to take I3D intermediate representations generated from the RGB and optical flow frames and learn to hallucinate BoW/FV representations. For this purpose, we use the MSE loss between the ground-truth BoW/FV and the outputs of BoW/FV streams. The role of the HAF stream is to further process I3D intermediate representations before they are concatenated with hallucinated BoW/FV. PredNet fuses the concatenated BoW/FV/HAF and learns class concepts. <ref type="figure" target="#fig_0">Figure 2</ref> shows our pipeline for hallucinating the OFF representation (I3D optical flow). Below, we describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BoW/FV Hallucinating Streams</head><p>BoW/FV take as input the I3D intermediate representations X (rgb) and X (opt.) of size 1024×7 which were obtained by stripping the classifier and the last 1D conv. layer of I3D pre-trained on Kinetics-400. The latter dimension of X (rgb) and X (opt.) can be thought of as the temporal size. We concatenate X (rgb) and X (opt.) along the third mode and obtain X which has dimensionality 1024×7×2. As FV contains the first-and second-order statistics, we use a separate stream per each type of statistics, and a single stream for BoW. For the practical choice of BoW/FV pipelines, we use either a Fully Connected (FC) unit shown in <ref type="figure" target="#fig_1">Figure 3a</ref> or a Convolutional (Conv) pipeline in <ref type="figure" target="#fig_1">Figure 3b</ref>. Thus, we investigate the following hallucinating stream combinations: (i) BoW-FC and FV-FC, (ii) BoW-Conv and FV-FC, or (iii) BoW-Conv and FV-Conv. Where indicated, we also equip each stream with Power Normalization (PN). For specific PN realizations, we investigate AsinhE, SigmE, and AxMin variants from Remarks 1, 2 and 3. Below we detail how we obtained ground-truth BoW/FV. Ground-truth BoW/FV. To train Fisher Vectors, we computed 256 dimensional GMM-based dictionaries on descriptors resulting from IDT <ref type="bibr" target="#b66">[66]</ref> according to steps described in Sections 2 and 3.1. We applied PCA to trajectories (30 dim.), HOG (96 dim.), HOF (108 dim.), MBHx (96 dim.) and MBHy (96 dim.), and we obtained the final 213 dim. local descriptors. We applied encoding as in Eq. <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, the aggregation from Section 3.2 and Power Normalization from Section 3.3. Thus, our encoded first-and second-order FV representations, each of size 256 × 213 = 54528, had to be sketched to 1000 dimensions. To this end, we followed Section 3.4, prepared matrices P (f v1) and P (f v2) as in Proposition 2, and fixed both of them throughout experiments. The sketched first-and second-order representations ψ (f v1) = P (f v1) ψ (f v1) and ψ (f v2) = P (f v2) ψ (f v2) can be readily combined next with the MSE loss functions detailed in Section 4.5.</p><p>For BoW, we followed Section 3.1 and applied k-means to build a 1000 dim. dictionary from the same descriptors which were employed to pre-compute FV. Then, the descriptors were encoded according to Eq. (1), aggregated according to steps described in Section 3.2 and normalized by Power Normalization from Section 3.3. Where indicated, we used 4000 dim. dictionary and thus applied sketching on such BoW to limit its vector size to 1000 dim.</p><p>We note that we use ground-truth BoW/FV descriptors only at the training stage to train our hallucination streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">High Abstraction Features</head><p>High Abstraction Features (HAF) take as input the I3D intermediate representations X (rgb) and X (opt.) . Practical realizations of HAF pipelines are identical to those of BoW/FV/OFF. Thus, we have a choice of either FC or Conv units illustrated in <ref type="figure" target="#fig_1">Figures 3a and 3b</ref>. We simply refer to those variants as HAF-FC and HAF-Conv, respectively. Similar to BoW/FV/OFF streams, the HAF representation also uses Power Normalization and it is of size 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optical Flow Features</head><p>For pipeline in <ref type="figure" target="#fig_0">Figure 2</ref>, the I3D intermediate representation X (rgb) only is fed to hallucination/HAF streams. I3D Optical Flow Features X (opt.) are pre-computed as the training ground-truth for the OFF layer (the MSE loss is used). Matrix P (tot) is prepared according to Proposition 2 and fixed throughout experiments. As sketching is a linear projection, we can backpropagate through it with ease. When also hallucinating OFF as in <ref type="figure" target="#fig_0">Figure 2</ref>, we additionally concatenate ψ (of f ) with other feature vectors to obtain ψ (tot) . PredNet. The final unit of our overall pipeline, PredNet, is illustrated in <ref type="figure" target="#fig_1">Figure 3c</ref>. On input, we take ψ (tot) (no sketching) or (ψ (tot) ) if sketching is used, pass it via the batch normalization and then an FC layer which produces a C dim. representation passed to the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Combining Hallucinated BoW/FV/OFF and HAF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Objective and its Optimization</head><p>During training, we combine MSE loss functions responsible for training hallucination streams with the class. loss: * (X , y;Θ) = α |H| i∈H ψ i −ψ i  , (of f )} is our set of hallucination streams which can be modified to multiple/few such streams depending on the task at hand. Moreover, g(·, η) is a Power Normalizing function chosen from the family described in Section 3.3, f (·; Θ (pr) ) is the PredNet module with parameters Θ (pr) which we learn, { (·, Θ i ), i ∈ H} are the hallucination streams while {ψ i , i ∈ H} are the corresponding hallucinated BoW/FV/OFF representations. Moreover, (·, Θ (haf ) ) is the HAF stream with the output denoted by ψ (haf ) . For the hallucination streams, we learn parameters {Θ i , i ∈ H} while for HAF, we learn Θ (haf ) . The full set of parameters we learn is defined asΘ ≡ ({Θ i , i ∈ H}, Θ (haf ) , Θ (pr) , Θ ( ) ). Furthermore, {P i , i ∈ H} are the projection matrices for count sketching of the ground-truth BoW/FV/OFF feature vectors {ψ i , i ∈ H} while {ψ i , i ∈ H} are the corresponding sketched/compressed representations. Finally, P (tot) is the projection matrix for hallucinated BoW/FV/OFF representations concatenated with each other and HAF, that is, for ψ (tot) = ⊕ i∈Hψi ; ψ (haf ) which results in the sketched counterpart ψ (tot) that goes into the PredNet module f . Section 3.4 details how to select matrices P . If sketching is not needed, we simply set a given P to be the identity projection P = I. In our experiments, we simply set α = 1. Optimization. We minimize * (X , y;Θ) w.r.t. parameters of each stream, that is {Θ i , i ∈ H} for hallucination streams, Θ (haf ) for the HAF stream, Θ (pr) for PredNet and Θ ( ) for the classification loss. In practice, we perform a simple alternation over two minimization steps shown in <ref type="figure">Figure 4</ref>. In each iteration, we perform one forward and backward pass regarding the MSE losses to update the parameters {Θ i , i ∈ H} of the hallucination streams. Then, we perform one forward and backward pass regarding the classification loss . We update all network streams during this pass. Thus, one can think of our network as multitask learning with BoW/FV/OFF and label learning tasks. </p><formula xml:id="formula_8">lim γ→0 V (γ) = 1 d lim γ→0 ψ γ , ψ γ 2 ψ γ 2 2 ψ γ 2 2 +1 = 2 d .<label>(9)</label></formula><p>Now, assume that d dimensional ψ and ψ are actually 2norm normalized. Then, we have the following ratio of variances:</p><formula xml:id="formula_9">κ = V/V (γ) = 2/( ψ, ψ 2 +1),<label>(10)</label></formula><p>Thus, 1 ≤ κ ≤ 2 depends on (ψ, ψ ), and κ varies smoothly between [1; 2] for 1 ≤ γ ≤ 0 of Gamma, a monotonically increasing function. For typical γ = 0.5, we measured for the actual data that κ ≈ 1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Protocols</head><p>HMDB-51 <ref type="bibr" target="#b41">[41]</ref> consists of 6766 internet videos over 51 classes; each video has ∼20-1000 frames. Following the protocol, we report the mean accuracy across three splits. YUP++ <ref type="bibr" target="#b18">[19]</ref> dataset contains so-called video textures. It has 20 scene classes, 60 videos per class, and its splits contain scenes captured with the static or moving camera. We follow the standard splits (1/9 dataset for training). MPII Cooking Activities <ref type="bibr" target="#b52">[52]</ref> consist of high-resolution videos of people cooking various dishes. The 64 distinct activities from 3748 clips include coarse actions e.g., opening refrigerator, and fine-grained actions e.g., peel, slice, cut apart. We use the mean Average Precision (mAP) over 7-fold cross validation. For human-centric protocol <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, we use Faster-RCNN <ref type="bibr" target="#b50">[50]</ref>    <ref type="table">Table 3</ref>: Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF+BoW/FV halluc.) approach on different architectures used for HAF and BoW/FV streams such as (FC) and (Conv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluations</head><p>We start our experiments by investigating various aspects of our pipeline and then we present our final results. HAF, BoW and FV streams. Firstly, we ascertain the gains from our HAF and BoW/FV streams. We evaluate the performance of (i) the HAF-only baseline pipeline without IDT-based BoW/FV information (HAF only), (ii) the HAFonly baseline with exact ground-truth IDT-based BoW/FV added at both training and testing time (HAF+BoW/FV exact), and (iii) the combined HAF plus IDT-based BoW/FV streams (HAF+BoW/FV halluc.). We also perform evaluations on (iv) HAF plus IDT-based BoW stream (HAF+BoW halluc.) and HAF plus IDT-based FV stream (HAF+FV halluc.) to examine how much gain IDT-based BoW and FV bring, respectively. As Section 4.1 suggests that each stream can be based on either the Fully Connected (FC) or Convolutional (Conv.) pipeline, we firstly investigate the use of FC unit from <ref type="figure" target="#fig_1">Figure 3a</ref>, that is, we use HAF-FC, BoW-FC and HAF-FC streams. PredNet also uses FC. For ground-truth FV, we use 1000 dim. sketches. <ref type="table" target="#tab_1">Table 1</ref> presents results on the HMDB-51 dataset. As expected, the (HAF only) is the poorest performer while (HAF+BoW/FV exact) is the best performer determining the upper limit on the accuracy. Hallucinating (HAF+BoW halluc.) outperforms (HAF+FV halluc.) marginally. We expect FV to perform close to BoW due to the significant compression with sketching by factor ∼ 52.5×. Approaches (HAF+FV/BoW halluc.) and (HAF+BoW/FV exact) achieve the best results, and outperform (HAF only) by 1.35% and 1.48% accuracy. These result show that our hallucination strategy (HAF+FV/BoW halluc.) can mimic (HAF+BoW/FV exact) closely. Our 82.37% accuracy is the new state of the art. Below we show larger gains on YUP++. <ref type="table" target="#tab_3">Table 2</ref> presents similar findings on the YUP++ dataset. Our (HAF+FV halluc.) brings the improvement of ∼ 2.2 and ∼ 6.5% over (HAF+BoW halluc.) and (HAF only) on scenes captured with the moving camera (dynamic). Our (HAF+BoW/FV halluc.) yields ∼ 8.0% over (HAF only) thus demonstrating again the benefit of hallucinating BoW/FV descriptors. The total gain for (HAF+BoW/FV halluc.) over (HAF only) equals 4.1%. Our (HAF+FV/BoW halluc.) matches results of (HAF+BoW/FV exact) without explicitly computing BoW/FV during testing. Below, we investigate different architectures of our streams. Fully Connected/Convolutional streams. <ref type="figure" target="#fig_1">Figures 3a and  3b</ref> show two possible realizations of HAF, BoW and FV streams. While FC and Conv. architectures are not the only possibilities, they are the simplest ones. <ref type="table">Table 3</ref> shows that using FC layers (FC) for HAF and BoW/FV streams, denoted as (HAF-FC+BoW/FV-FC halluc.) outperforms Convolutional (Conv) variants by up to ∼ 1.5% accuracy. Thus, we use only the FC architecture in what follows. Sketching and Power Normalization. As PredNet uses FC layers (see <ref type="figure" target="#fig_1">Figure 3c</ref>), we expect that limiting the input size to this layer via count sketching from Section 3.4 should benefit the performance. Moreover, as visual and video representations suffer from so-called burstiness, we investigate AsinhE, SigmE and AxMin from Remarks 1, 2 and 3. <ref type="figure" target="#fig_6">Figure 5a</ref> investigates the classification accuracy on the HMDB-51 dataset (split 1) when our HAF and BoW/FV feature vectors {ψ i , i ∈ H} and ψ (haf ) (described in Sections 4.4 and 4.5) are passed via Power Normalizing functions AsinhE, SigmE and AxMin prior to concatenation (see <ref type="figure" target="#fig_3">Figure 1</ref>). From our experiment it appears that all PN functions perform similarly and improve results from the baseline 82.29% to ∼ 83.20% accuracy. We observe a similar gain from 93.15% to 94.44% acc. on YUP++ (static). In what follows, we simply use AsinhE for PN. <ref type="figure" target="#fig_6">Figure 5b</ref> illustrates on the HMDB-51 dataset (split 1) that applying count sketching on concatenated HAF and BoW/FV feature vectors ψ (tot) , which produces ψ (tot) (see Section 4.5 for reference to symbols), improves results from 82.88% to 83.92% accuracy for d = 2000. This is expected as reduced size of ψ (tot) results in fewer parameters of the FC layer of PredNet and less overfitting. Similarly, for the YUP++ dataset and the split (static), we see the performance increase from 93.15% to 94.81% accuracy. Comparisons with other methods. Below we present our final results and we contrast them against the state of the art. <ref type="table" target="#tab_5">Table 4</ref> shows results on the HMDB-51 dataset. For       our method, we used sketching of ψ (tot) with d = 2000 and PN. Our (HAF+BoW/FV halluc.) model yields 82.48% acc. which beats results in the literature to the best of our knowledge. If we tune PN per split, our results reach 82.78% accuracy. However, we do not advise such tuning due to danger of overfitting. We note that we outperform more complex methods such as Adversarial Discriminative Learning (ADL) with I3D <ref type="bibr" target="#b67">[67]</ref> and Fully Fine-Tuned I3D <ref type="bibr" target="#b3">[4]</ref>. <ref type="table" target="#tab_6">Table 5</ref> shows results on the YUP++ dataset. Our (HAF+BoW/FV halluc.) model yields very competitive results on the static protocol and outperforms competitors on the dynamic and mixed protocols. With 92.2% mean accuracy over static and dynamic scores (mean stat/dyn), we outperform more complex ADL+I3D <ref type="bibr" target="#b67">[67]</ref> and T-ResNet <ref type="bibr" target="#b18">[19]</ref>. <ref type="table">Table 6</ref> shows results for the MPII dataset for which we use HAF with/without the BoW (4000 dim.) hallucination stream (no FV stream). As MPII contains subsequences, we use integral pooling from Prop. 1. Our basic model (HAF+BoW halluc.) scores ∼ 71.9% mAP. Applying sketching and PN (HAF+BoW halluc.+SK/PN) yields 73.6% mAP. Unlike GRP+IDT <ref type="bibr" target="#b6">[7]</ref> and KRP-FS+IDT <ref type="bibr" target="#b8">[9]</ref>, our first two experiments do not use any human-or motioncentric pre-processing. With human-centric crops, denoted with (*), our baseline without BoW (HAF* only) achieves 74.8% mAP. The model with BoW (HAF+BoW halluc.) scores 77.8% mAP. By utilizing 4 sketches for BoW and 4 BoW streams with Power Normalization (HAF*+BoW hal.+MSK/PN), we obtain 80.4% mAP. Hallucinating Optical Flow. For (HAF • +BoW hal.+MSK/ PN) in <ref type="table">Table 6</ref>, we increased the resolution of RGB frames 2× to obtain larger human-centric crops and 2× larger optical flow res., which yielded 81.7% mAP. In the same setting, hallucinating optical flow feat. (ditto+OFF hal.) yielded 81.84% mAP, the new state of the art. Charades. In <ref type="table" target="#tab_8">Table 7</ref>, baselines (HAF only) and (HAF+Bo -W/FV exact) score 37.2% and 41.9% mAP. Moreover, our best pipeline (HAF+BoW/FV/OFF halluc.+MSK×8/PN) that hallucinates IDT BoW/FV and I3D optical flow features (OFF) with 8 sketches per BoW/FV/OFF and PN yielded 43.1% (a much more complex feature banks <ref type="bibr" target="#b74">[74]</ref> yield 43.4%). Finally, if 25% of this dataset was dedicated to testing, ∼55h of computations would be saved. Discussion. There exist several reasons explaining why our pipeline works well e.g., sophisticated IDT trajectory modeling is unlikely to be captured by off-the-shelf CNNs unless a CNN is enforced to learn IDT. We perform translation of the I3D output into IDT-based BoW/FV descriptors thus enforcing I3D to implicitly learn IDT which coregularizes I3D which resembles Domain Adaptation (DA) methods: a source network co-regularizes a target network <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">42]</ref> by the alignment of feature statistic of both streams. Related to DA is Multi-task Learning (MTL) known for boosting generalization/preventing overfitting of CNNs due to task specific losses <ref type="bibr" target="#b4">[5]</ref>. MTL training on related tasks is known to boost individual task accuracies beyond a vanilla feature fusion <ref type="bibr" target="#b59">[59]</ref>. Finally, our pipeline uses self-supervision e.g., IDT BoW/FV and OFF descriptors represent easy to obtain self-information about videos. We train our halluc./last I3D layers via task-specific losses (similar to MTL). However, our halluc. layers distill the domain specific cues which are fed back into the network (PredNet) which boosts our results by further ∼2.7% compared to vanilla (I3D+BoW MTL • ) in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a simple yet powerful strategy that learns IDT-based descriptors (and even optical flow features) and hallucinates them in a CNN pipeline for AR at the test time. With state-of-the-art results, we hope our method will spark a renewed interest in IDT-like descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hallucination Quality</head><p>Below, we provide an analysis of the quality of hallucination of the BoW/FV streams compared to the groundtruth BoW/FV feature vectors. <ref type="figure" target="#fig_7">Figure 6</ref> presents histograms of the square difference between the hallucinated features and ground-truth ones. Specifically, we plot histograms of {(ψ (bow),mn − ψ (bow),mn ) 2 , m ∈ I 1000 , n ∈ N }, where index m runs over features m ∈ I 1000 and n ∈ N runs over each video. Counts for training and testing splits are normalized by 1000 (the number of features) and the number of training and testing videos, respectively. The histograms are computed over bins of size 0.01 thus allowing us to simply plot continuously looking lines instead of bins. <ref type="figure" target="#fig_7">Figure 6a</ref> shows that the BoW ground-truth descriptors for the training split are learnt closely by our BoW hallucinating unit based on FC layers (FC). We capture histograms for epochs 1, 5, 15, 25 in colors interpolated from red to blue. As one can see, in early epochs, the peak around the first bin is small. As the epochs progress, the peak around the first bin becomes prominent while further bins decrease in size. This indicates that as the training epochs progress, the approximation error becomes smaller and smaller. <ref type="figure" target="#fig_7">Figure 6b</ref> shows that the BoW ground-truth descriptors for the testing split are also approximated closely by the hallucinated BoW descriptors.</p><p>We compared histograms for testing and training slits for BoW, first-and second-order FV and observed small differences only. Such a comparison can be conducted by computing the ratio of testing to training bins and it reveals variations between 0.8× and 1.25×. Thus, without the loss of clarity, we skip showing plots for FV testing splits. <ref type="figure" target="#fig_7">Figures 6c and 6d</ref> show that the first-and second-order FV terms (FV1) and (FV2) can be also learnt closely by our hallucinating units. We show only the quality of approximation on the training split as behavior on testing splits matches closely the behavior on training splits. <ref type="figure" target="#fig_7">Figures 6e, 6f, 6g and 6h</ref> show the similar learning/approximation trend for BoW training and testing splits, and the first-and second-order FV terms (training only) given our hallucinating unit based on FC layers (FC) with no sketching or PN (-SK/PN).  <ref type="table">Table 8</ref>: Evaluations on MPII. The (HAF*+BoW halluc.) is our pipeline with the BoW stream, (*) denotes humancentric pre-processing for 256 pixels (height) while (HAF*+BoW hal.+MSK/PN) denotes our pipeline with multiple sketches per BoW followed by Power Norm (PN). By analogy, ( • ) denotes human-centric pre-processing for 512 pixels (height).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Higher Resolution Frames on MPII</head><p>For human-centric pre-processing on MPII denoted by (*), we observed that the bounding boxes used for extraction of the human subject are of low resolution. Thus, we decided to firstly resize RGB frames to 512 pixels (height) rather than 256 pixels and then compute the corresponding optical flow, and perform extraction of human subjects for which the resolution thus increased 2×.</p><p>The (HAF*+BoW halluc.), our pipeline with the BoW stream, and (HAF*+BoW hal.+MSK/PN) with multiple sketches and PN are computed for the standard 256 pixels (height) denoted by (*) are given in <ref type="table">Table 8</ref>.</p><p>The (HAF • +BoW halluc.), our pipeline with the BoW stream, and (HAF • +BoW hal.+MSK/PN) pipeline are analogous pipelines but computed for the increased 512 pixel resolution (height) denoted by ( • ). According to the table, increasing the resolution 2× prior to human detection, extracting subjects in higher resolution and scaling (to the 256 size for shorter side) yields 1.3% improvement in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Pre-processing</head><p>For HMDB-51 and YUP++, we use the data augmentation strategy described in the original authors' papers (e.g., random crop of videos, left-right flips on RGB and optical flow frames. For testing, center crop, no flipping are used.</p><p>For the MPII dataset with human-centric pre-processing, human detector is used first. Then, we crop randomly around the bounding box of human subject (we include it). Finally, we allow scaling, zooming in, and left-right flips. For longer videos, we sample sequences to form a 64-frame sequence. For short videos (less than 64 frames), we loop the sequence many times to fit its length to the expected input length. Lastly, we scale the pixel values of RGB and optical flow frames to the range between −1 and 1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Hallucinating the Optical Flow Features (OFF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Stream types used in our network. Figures 3a and 3b show Fully Connected and Convolutional variants used for the practical realization of the FV, BoW, OFF and HAF streams. Figure 3c shows our PredNet. Note that we indicate the type of operation and its parameters in each block e.g., conv2d and its number of filters/size, or Power Normalization (PN). Beneath arrows, we indicate the size of input, intermediate or output representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 2 .</head><label>2</label><figDesc>Let d and d denote the dimensionality of the input and sketched output vectors, respectively. Let vector h ∈ I d d contain d uniformly drawn integer numbers from {1, ..., d } and vector s ∈ {−1, 1} d contain d uniformly drawn values from {−1, 1}. Then, the sketch projection matrix P ∈ {−1, 0, 1} d ×d becomes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1</head><label>1</label><figDesc>indicates that FV (first-and second-order), BoW and HAF feature vectorsψ (f v1) ,ψ (f v2) ,ψ (bow) and ψ (haf ) are concatenated (via operator ⊕) to obtain ψ (tot) and subsequently sketched (if indicated so during experiments), that is, ψ (tot) = P (tot) ψ (tot) which reduces the size of the total representation from d = 4000 to 500 ≤ d ≤ 2000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 2 +</head><label>2</label><figDesc>f(ψ (tot) ; Θ (pr) ), y; Θ ( ) ,where: ∀i ∈ H,ψ i = g( (X , Θ i ), η) , ψ i = P i ψ i , ψ (haf ) = g (X ,Θ (haf )), η , ψ (tot) = P (tot) ⊕ i∈Hψi ; ψ (haf ) .<ref type="bibr" target="#b7">(8)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 : 2 2</head><label>42</label><figDesc>Optimization. In each step, we have (i) forward/backward passes via BoW/FV (optionally OFF) streams for the MSE loss followed by (ii) forward/backward passes via BoW/FV (opt. OFF), and HAF streams and PredNet for the classification loss.The above equation is a trade-off between the MSE loss functions { ψ i −ψ i , i ∈ H} and the classification loss (·, y; Θ ( ) ) with some label y ∈ Y and parameters Θ ( ) ≡ {W , b}. The trade-off is controlled by a constant α ≥ 0 while MSE is computed over hallucination streams i ∈ H, and H ≡ {(f v1), (f v2), (bow)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>γ ′ , η ′ , η ′′ accuracy Evaluations of (fig. 5a) Power Normalization and (fig. 5b) sketching on the HMDB-51 dataset (split 1 only).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6 :</head><label>6</label><figDesc>Evaluations of (top) our (HAF+BoW halluc.) pipeline without sketching/PN, with sketching/PN (SK/PN). The (HAF* only) is our baseline without the BoW stream, (*) denotes humancentric pre-processing while (MSK/PN)in pipeline (HAF*+BoW hal.+MSK/PN) denotes multiple sketches per BoW followed by Power Norm (PN). (bottom) Other methods on the MPII dataset. HAF HAF+BoW/ HAF+BoW/FV/OFF HAF+BoW/FV/OFF HAF+BoW/FV/OFF only FV exact halluc. +MSK×2/PN halluc. +MSK×4/PN halluc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6 :</head><label>6</label><figDesc>FV2 FC (train), -SK/PN Figure Evaluation of the square difference between the hallucinated and ground truth representations on HMDB-51 (split 1). Experiments in the top row use (FC) streams with sketching and PN. Two leftmost plots in the bottom row use (Conv) streams. Two rightmost plots in the bottom row use (FC) streams without sketching/PN (-SK/PN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). By !, we indicate that the three Mean Square Error (MSE) losses are only applied at the training stage to train our FV (first-and second-order components) and BoW hallucinating streams (indicated in dashed red). By %, we indicate that the MSE losses are switched off at the testing stage. Thus, we hallucinateψ (f v1) ,</figDesc><table /><note>ψ (f v2) andψ (bow) , and pass them to PredNet together with ψ (haf ) to obtain labels y. The original training FV and BoW feature vectors (used only during training) are denoted by ψ (f v1) , ψ (f v2) and ψ (bow) , while P are count sketch projecting matrices (see text for details).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>83% 80.78% 80.45% 81.02% HAF+BoW/FV exact 83.00% 82.80% 81.70% 82.50% HAF+BoW halluc. 82.29% 81.24% 80.98% 81.50% HAF+FV halluc. 82.68% 81.05% 79.93% 81.22% HAF+BoW/FV halluc. 82.88% 82.74% 81.50% 82.37% Evaluations of pipelines on the HMDB-51 dataset. We compare (HAF only) and (HAF+BoW/FV exact) which show the lower-and upper bound on the accuracy, and our (HAF+BoW/FV halluc.), (HAF+BoW halluc.) and (HAF+FV halluc.).Furthermore, we use the Adam minimizer with 10 −4 initial learning rate which we halve every 10 epochs. We run our training for 50-100 epochs depending on the dataset. Sketching the Power Normalized vectors. Proposition 3. Sketching PN vectors increases the sketching variance ( 2 -normalized by vec. norms) by 1 ≤ κ ≤ 2.</figDesc><table><row><cell>sp1</cell><cell>sp2</cell><cell>sp3 mean acc.</cell></row><row><cell cols="3">HAF only 81.Proof. Normalize variance V from Remark 4 by the norms</cell></row><row><cell cols="3">ψ 2 2 ψ 2 2 . Consider V (γ) which is the variance for d di-mensional vectors {(ψ γ , ψ γ ) : ψ ≥ 0, ψ ≥ 0} power nor-</cell></row><row><cell cols="3">malized by Gamma from Remark 1, and divide it accord-</cell></row><row><cell>ingly by ψ γ 2 2 ψ γ 2</cell><cell></cell><cell></cell></row></table><note>2 . For extreme PN (γ → 0), we have:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>to crop video around humans. Charades [55] consist of of 9848 videos of daily indoors activities, 66500 temporal annotations and 157 classes. static dynamic mixed mean acc. HAF only 92.03% 81.67% 89.07% 87.59% HAF+BoW/FV exact 93.30% 89.82% 92.41% 91.84% HAF+BoW halluc. 92.69% 85.93% 92.41% 90.34% HAF+FV halluc. 92.69% 88.15% 91.48% 90.77% HAF+BoW/FV halluc. 93.15% 89.63% 92.31% 91.69%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Eval. of pipelines on YUP++. See Table 1 for the legend. FV-FC halluc. 81.96% 80.39% 80.52% 80.95% HAF-FC+BoW/FV-Conv halluc. 82.42% 81.30% 81.50% 81.74% HAF-FC+BoW/FV-FC halluc. 82.88% 82.74% 81.50% 82.37%</figDesc><table><row><cell>sp1</cell><cell>sp2</cell><cell>sp3 mean acc.</cell></row><row><cell>HAF-Conv+BoW/</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>83% 80.78% 80.45% 81.02% HAF+BoW/FV halluc. 83.46% 82.61% 81.37% 82.48% ADL+ResNet+IDT 74.3% [67] STM Network+IDT 72.2%<ref type="bibr" target="#b17">[18]</ref> ADL+I3D 81.5%<ref type="bibr" target="#b67">[67]</ref> Full-FT I3D 81.3%<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell></cell><cell>sp1</cell><cell>sp2</cell><cell>sp3 mean acc.</cell></row><row><cell>HAF only</cell><cell>81.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluations of (top) our (HAF+BoW/FV halluc.) and (bottom) comparisons to the state of the art on HMDB-51.</figDesc><table><row><cell></cell><cell cols="3">static dynamic mixed mean mean stat/dyn all</cell></row><row><cell>HAF only</cell><cell cols="3">92.03% 81.67% 89.07% 86.8% 87.6%</cell></row><row><cell cols="4">HAF+BoW/FV halluc. 94.81% 89.63% 93.33% 92.2% 92.6%</cell></row><row><cell cols="4">T-ResNet [19] 92.41% 81.50% 89.00% 87.0% 87.6%</cell></row><row><cell cols="2">ADL I3D [67] 95.10% 88.30%</cell><cell>-</cell><cell>91.7% -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Evaluations of (top) our (HAF+BoW/FV halluc.) and (bottom) comparisons to the state of the art on YUP++. HAF*+BoW hal.+MSK/PN 80.1 79.2 84.8 83.9 80.9 78.5 75.5 80.4% HAF • +BoW hal.+MSK/PN 80.8 80.9 85.0 83.9 82.0 79.8 79.6 81.7% KRP-FS 70.0% [9] KRP-FS+IDT 76.1% [9] GRP 68.4% [7] GRP+IDT 75.5% [7]</figDesc><table><row><cell></cell><cell>sp1 sp2 sp3 sp4 sp5 sp6 sp7 mAP</cell></row><row><cell>HAF+BoW halluc.</cell><cell>73.9 71.6 76.2 70.7 76.3 71.9 63.4 71.9%</cell></row><row><cell cols="2">HAF+BoW halluc.+SK/PN 73.9 75.8 72.2 73.9 77.0 73.6 68.8 73.6%</cell></row><row><cell>HAF* only</cell><cell>74.6 73.2 77.0 75.1 76.1 75.6 71.9 74.8%</cell></row><row><cell>HAF*+BoW halluc.</cell><cell>78.8 75.0 84.1 76.0 77.0 78.3 75.2 77.8%</cell></row><row><cell>ditto+OFF hal.</cell><cell>81.2 81.2 84.9 83.4 84.2 78.9 79.1 81.8%</cell></row><row><cell>I3D+BoW MTL •</cell><cell>79.1 78.1 83.6 78.7 79.1 78.6 76.5 79.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Evaluations of our methods on the Charades dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>sp1 sp2 sp3 sp4 sp5 sp6 sp7 mAP HAF*+BoW halluc. 78.8 75.0 84.1 76.0 77.0 78.3 75.2 77.8% HAF*+BoW hal.+MSK/PN 80.1 79.2 84.8 83.9 80.9 78.5 75.5 80.4% HAF • +BoW halluc. 78.8 78.3 84.2 77.4 77.1 78.3 75.2 78.5% HAF • +BoW hal.+MSK/PN 80.8 80.9 85.0 83.9 82.0 79.8 79.6 81.7%</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank CSIRO Scientific Computing for their help and NVIDIA for GPUs. We thank Dr. M. R. Mansour for early discussion on related topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>Below we asses (i) the hallucination quality, (ii) provide additional results for higher resolution frames on MPII, and (iii) we provide further details of our pre-processing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reliable estimation of dense optical flow fields with large displacements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general dense image matching framework combining direct and feature-based costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Braux-Zin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action Recognition? A New Model and the Kinetics Dataset. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997-07" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Selective spatio-temporal interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonzàlez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="396" to="410" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higherorder pooling of CNN features via kernel linearization for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-linear temporal subspace representations for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PoTion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1530" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Human Detection Using Oriented Histogram of Flow and Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Computer Communications and Networks</title>
		<meeting>the 14th International Conference on Computer Communications and Networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumava</forename><surname>Kumar Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal residual networks for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning end-to-end video classification with rank-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Orientation histograms for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<idno>TR94-03</idno>
		<imprint>
			<date type="published" when="1994-12-01" />
			<pubPlace>Cambridge, MA 02139</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MERL -Mitsubishi Electric Research Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluation of interest point detectors and feature descriptors for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Gauglitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Höllerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<title level="m">Dimensionality reduction on SPD manifolds: The emergence of geometry-aware methods. TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Min-max statistical alignment for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Book review: Thinking in perspective: Critical essays in the study of thought processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="358" to="359" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">On the Burstiness of Visual Elements. CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Spatio-Temporal Descriptor Based on 3D-Gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">BMCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tensor representations via kernel linearization for action recognition from 3D skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Soft Assignment of Visual Words as Linear Coordinate Coding and Optimisation of its Reconstruction Error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Museum exhibit identification challenge for the supervised domain adaptation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<title level="m">Higher-order Occurrence Pooling on Midand Low-level Features: Visual Concept Detection</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Higher-order occurrence pooling for bagsof-words: Visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Comparison of Mid-Level Feature Coding Approaches And Pooling Strategies in Visual Concept Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5774" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">ImageNet classification with deep convolutional neural networks. NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Siamese networks: The tale of two manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Soumava Kumar Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human action recognition using multi-velocity STIPs and motion energy orientation histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="312" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Lingqiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Defence of Soft-assignment Coding. ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fisher kernels on visual vocabularies for image categorization. CVPR, 0:1-8</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A 3-Dimentional SIFT Descriptor and its Application to Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CNN-based action recognition and supervised domain adaptation on 3D body skeletons via kernel feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Is learning the n-th thing any easier than learning the first? NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<title level="m">Learning Spatiotemporal Features with 3D Convolutional Networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ionut Cosmin Duta, Negar Rostamzadeh, and Nicu Sebe. Realtime Video Classification using Dense HOF/HOG. ICMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cor</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<title level="m">Visual word ambiguity. TPAMI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Longterm temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Cheng-Lin</surname></persName>
		</author>
		<title level="m">Action Recognition by Dense Trajectories. CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Dense Trajectories and Motion Boundary Descriptors for Action Recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Action Recognition with Improved Trajectories. ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="716" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Master&apos;s thesis, School of the Computer Science and Software Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>The University of Western Australia</orgName>
		</respStmt>
	</monogr>
	<note>Analysis and evaluation of Kinect-based action recognition algorithms</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A comparative review of recent kinect-based action recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Loss switching fusion with similarity search for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa Reda</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Local trinary patterns for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lahav</forename><surname>Yeffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="492" to="497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
