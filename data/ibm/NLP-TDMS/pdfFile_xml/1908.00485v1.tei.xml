<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Adapt Invariance in Memory for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Learning to Adapt Invariance in Memory for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-identification</term>
					<term>Domain Adaptation</term>
					<term>Invariance Learning</term>
					<term>Exemplar Memory</term>
					<term>Graph-based Positive Prediction !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work considers the problem of unsupervised domain adaptation in person re-identification (re-ID), which aims to transfer knowledge from the source domain to the target domain. Existing methods are primary to reduce the inter-domain shift between the domains, which however usually overlook the relations among target samples. This paper investigates into the intra-domain variations of the target domain and proposes a novel adaptation framework w.r.t three types of underlying invariance, i.e., Exemplar-Invariance, Camera-Invariance, and Neighborhood-Invariance. Specifically, an exemplar memory is introduced to store features of samples, which can effectively and efficiently enforce the invariance constraints over the global dataset. We further present the Graph-based Positive Prediction (GPP) method to explore reliable neighbors for the target domain, which is built upon the memory and is trained on the source samples. Experiments demonstrate that 1) the three invariance properties are indispensable for effective domain adaptation, 2) the memory plays a key role in implementing invariance learning and improves the performance with limited extra computation cost, 3) GPP could facilitate the invariance learning and thus significantly improves the results, and 4) our approach produces new state-of-the-art adaptation accuracy on three re-ID large-scale benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ERSON re-identification (re-ID) <ref type="bibr" target="#b58">[59]</ref> is an image retrieval task, that aims at seeking matched persons of the query from a disjoint-camera database. The predominant methods have demonstrated dramatic performance when trained and tested on the same data distribution. However, they may suffer a significant degradation in the performance when evaluated on a different domain, due to dataset shifts from the changes of scenario, season, illumination, camera deployment, et al. It raises a domain adaptation problem that often encountered in real world applications and attracts increasing attention in the community <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. In this work, we study the problem of unsupervised domain adaptation (UDA) in re-ID. The goal is to improve the generalization ability of models on a target domain, using a labeled source domain and an unlabeled target domain.</p><p>Conventional methods of UDA are mainly designed for a closed-set setting, where the source and target domains share a common label space, i.e. the classes of two domains are exactly the same. A popular approach is to align the feature distributions of both domains, but it does not readily apply to the context of re-ID. Since domain adaptation in re-ID is a special open-set problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, where the source and target domains have completely disjoint classes/identities. For such label constraint, directly aligning the feature distributions of two domains will align the samples from different classes and may be detrimental to the adaptation accuracy.</p><formula xml:id="formula_0">• Z.</formula><p>To address the challenges of domain adaptive re-ID, recent works concentrate on aligning the source-target distributions in a common space, such as pixel-level space <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b46">[47]</ref> and attribute label space <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Despite their success, these works only consider the overall inter-domain shift between the source and target domains, but largely overlook the intra-domain variations of the target domain. In the re-ID system, the intra-domain variations are important factors that affect the performance. Without considering the intradomain variations of the target domain, an adapted model will produce poor performance, when the intra-domain variations in the target testing set are seriously different from the source domain.</p><p>In this work, we explicitly consider the intra-domain variations of the target domain and design our framework w.r.t three types of underlying invariance, i.e., Exemplar-Invariance (EI), Camera-Invariance (CI), and Neighborhood-Invariance (NI), as described below.</p><p>Exemplar-Invariance (EI): The first property is motivated by the retrieval results of re-ID. Given a re-ID model trained on a labeled source training set, we evaluate it on a source/target testing set. On the one hand, we observe that the top-ranked retrieval results (both positive and negative samples) always are visually similar to the query when tested on the source set. A similar phenomenon is shown in image classification <ref type="bibr" target="#b50">[51]</ref>. This indicates that the model has learned to distinguish persons by apparent similarity for the source domain. On the other hand, when tested on the target arXiv:1908.00485v1 [cs.CV] 1 Aug 2019 (a) Exemplar-invariance (b) Camera-invariance (c) Neighborhood-invariance set, the top-ranked results often include many samples that are visually dissimilar to the query. This suggests that the ability of the model to distinguish persons by apparent similarity is degraded on the target domain. In reality, each person exemplar could differ significantly from others even shared the same identity. Therefore, it is possible to enable the model to capture the apparent representation by learning to distinguish individual exemplars. To achieve this goal, we introduce the exemplar-invariance (EI) to improve the discrimination ability of the model on the target domain, by encouraging each exemplar to be close to itself while away from others. Camera-Invariance (CI): Camera style (CamStyle) difference is a critical factor for re-ID that can be clearly identified, since the appearance of a person may change largely under different cameras <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Due to the camera deployments of the source and target domains are usually different, the model trained on the source domain may suffer from the variations caused by the target cameras. To address this problem, Zhong et al. <ref type="bibr" target="#b63">[64]</ref> introduce camera-invariance (CI) by enforcing an target example and its corresponding Cam-Style transferred images to be close to each other. Inspired by them, we integrate the camera-invariance learning into our model by classifying an target example and its CamStyle counterparts to a same class.</p><p>Neighborhood-Invariance (NI): Apart from the easily identified camera variance, some other latent intra-domain variations are hard to explicitly discern without fine-grained labels, such as the changes of pose, view, and background. To overcome this difficulty, we attempt to generalize the model with the neighbors of target samples. Suppose we are given an appropriate model trained on the source and target domains, a target sample and its nearest-neighbors in the target set may share the same identity with a higher potential. Considering this trait, we introduce the neighborhoodinvariance (NI) to learn a model that is more robust to overcome the latent intra-domain variations of the target domain. We accomplish this constraint by encouraging an exemplar and its reliable neighbors to be close to each other. Examples of the three types of invariance are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Intuitively, a straightforward way to implement the three invariance properties is to constrain them with contrastive/triplet loss <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref> within a training mini-batch. However, the number of samples in a mini-batch is relatively small compared with the entire training set. In this manner, it is difficult to form a mini-batch with ideal examples, and the overall relations between training samples cannot be considered thoroughly during the network adaptation procedure. To deal with this issue, we develop a novel framework to effectively accommodate the three invariance properties for domain adaptive re-ID. Specifically, we introduce an exemplar memory module into the network to store the up-to-date representations of all training samples. The memory enables the network to enforce the invariance constraints over the entire/global target training data instead of the current mini-batch. With the memory, the invariance learning of the target domain can be effectively implemented with a non-parametric classification loss, considering each target sample as an individual class.</p><p>In our previous work <ref type="bibr" target="#b64">[65]</ref>, we directly select top-k nearest neighbors from the memory for the learning of NI. This straightforward strategy ignores the underlying relations between samples in the memory. As a result, the similarity estimation of hard samples may not be accurate when the model has inferior discriminative ability. As a notable extension of our previous work <ref type="bibr" target="#b64">[65]</ref>, we propose a graphbased positive prediction (GPP) approach to address this problem, thereby promoting the invariance learning. GPP is built upon the memory module and designed by graph convolutional networks (GCNs), which aims to predict positive neighbors from the memory for a training target sample. In addition to the target memory, we also construct a memory for saving features of the source domain. This enables us to imitate the neighbor exploring process of target invariance learning and thus learn GPP on the labeled source domain. The learned GPP is then directly applied to the unlabeled target domain for facilitating the learning of NI.</p><p>In summary, our contribution is as follow:</p><p>• This work comprehensively investigates the intradomain variations of the target domain and studies three underlying properties of target invariance. The experiment demonstrates that the three properties are indispensable for improving the transferable ability of the model in the context of re-ID.</p><p>• This work proposes a novel framework equipped with a memory module that can effectively enforce the three constraints into the network. The memory enables us to fully exploit the sample relations over the whole training set instead of the mini-batch. With the memory, the performance can be significantly improved, requiring very limited extra computation cost and GPU memory.</p><p>• This work introduces a Graph-based Positive Prediction (GPP) approach to leverage the relationships between candidate neighbors and infer accurate positive neighbors for the training target sample. The experiment shows that GPP is beneficial to the learning of neighborhood-invariance and could consistently improve the results, especially the mAP.</p><p>• In addition, we analyze the mechanism of the three invariance properties, which helps us to understand how these three properties encourage the network to adapt to the target domain.</p><p>• Experiments demonstrate the effectiveness and superiority of the proposed method over the state-of-theart UDA approaches. Our results outperform state of the art by a large margin on three datasets, including Market-1501, DukeMTMC-reID and MSMT17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Unsupervised domain adaptation. This work is closely related to unsupervised domain adaptation (UDA). In classical UDA, methods are designed under the assumption of the closed-set scenario, where the classes of the source and target domains are precisely the same. For this problem, recent popular approaches are mainly focused on distribution alignment learning, which attempts to reduce the domain discrepancy by using Maximum Mean Discrepancy (MMD) minimization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b52">[53]</ref> or domain adversarial training <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b42">[43]</ref>. However, these methods are usually not applicable to the open-set scenario, where unknown classes exist in the source/target domain. Since samples of unknown classes should not be aligned with the ones of known classes. It raises to the problem of open set UDA, introduced by Busto and Grall <ref type="bibr" target="#b3">[4]</ref>. To tackle this problem, Busto and Grall <ref type="bibr" target="#b3">[4]</ref> develop a method to learn a mapping from the source domain to the target domain by jointly predicting unknownclass target samples and discarding them. Saito et al. <ref type="bibr" target="#b34">[35]</ref> introduce an adversarial learning framework to separate target samples into known and unknown classes. Meanwhile, unknown classes are rejected during feature alignment. Recently, Sohn et al. <ref type="bibr" target="#b37">[38]</ref> consider a more challenging setting of open-set UDA, where the source and target domain belong disjoint label spaces, i.e. the classes of two domains are completely different. In practice, many tasks match such setting, e.g., cross-ethnicity face recognition and UDA in re-ID considered in our work. To address this problem, Sohn et al. first reformulate the disjoint classification task to a binary verification one. Then, a Feature Transfer Network (FTN) is proposed to transfer the source feature space to a new space and align with the target feature space. In this paper, we study this problem in the aspect of intra-domain variations in the target domain and propose an effective framework to improve the generalization ability of the model by target invariance learning.</p><p>Unsupervised person re-identification (re-ID). Recent supervised methods have made great achievements in re-ID <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b61">[62]</ref>, benefiting from rich-labeled data and the increasing ability of deep networks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>. However, the performance of these strong methods may have a large drop when tested on an unseen (target) dataset, due to the dataset shift. To address this problem, many hand-craft features based methods are designed for unsupervised re-ID, such as ELF <ref type="bibr" target="#b11">[12]</ref>, LOMO <ref type="bibr" target="#b24">[25]</ref> and BOW <ref type="bibr" target="#b57">[58]</ref>. These methods can be directly applied to any dataset without training, but fail to obtain fulfilling performance on large-scale, complex scenarios. Although labeling re-ID data is difficult, it is relatively easy to collect sufficient unlabeled data in the target domain. Recently, many works are proposed to transfer the knowledge of a labeled data to an unlabeled one. These works mainly can be divided into two categories: 1) discovering pseudo labels for target dataset, and 2) reducing the source-target discrepancy in a common label space. For the first category, methods use a labeled source dataset to initialize a re-ID model and explore pseudo labels for target dataset based on clustering <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b54">[55]</ref>, associating label with labeled source dataset <ref type="bibr" target="#b56">[57]</ref>, assigning label with nearest-neighbors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b53">[54]</ref>, or regarding camera style counterparts as positive sample <ref type="bibr" target="#b63">[64]</ref>. These methods are closely related to our work in that using the relationship between target samples to refine the model. The main difference is that our work comprehensively considers three latent invariance constraints and enforces them over the entire dataset. We show the mutual benefit among the three invariance properties and the effectiveness of exploiting the global sample relationship. Methods of the second category attempt to align the distributions of the source and target domains in a common space, such as image-level space <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b66">[67]</ref> and attribute label space <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b44">[45]</ref>. These methods only consider the overall discrepancy between the source and target domains, but largely ignore the intra-domain variations in the target domain. In this work, we explicitly consider the invariance properties in the target domain to address the problem of domain adaptive re-ID.</p><p>Neural networks with augmented-memory. Learning neural networks with augmented-memory is proposed to address various tasks, such as question answering <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref>, few-shot learning <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b43">[44]</ref>, and video understanding <ref type="bibr" target="#b48">[49]</ref>. The augmented-memory enables the networks to store the intermediate knowledge into a structural and addressable table. The community of learning with augmented-memory mainly can be divided into two categories. The first category aims to augment neural networks with a fully differentiable memory module, such as Neural Turing Machine <ref type="bibr" target="#b10">[11]</ref> and Memory Networks <ref type="bibr" target="#b47">[48]</ref>. Memory Networks introduce a long-term memory component that can be read and written. The memory is utilized to query and retrieve fact for the task of question answering. Another category focus on developing a non-parametric memory <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> which can directly save the features of samples into a feature bank and be updated during training. These approaches then use the attention mechanism to calculate the similarities between the query and instances in the memory. We draw inspiration from these methods and develop a memorybased framework for unsupervised domain adaptive re-ID.</p><p>Graph convolutional network (GCN). In real-world applications, data are usually represented in the form of graphs or networks, such as knowledge graphs and social networks. Recently, many works concentrate on extending the deep convolutional neural network for handling graph data. In light of the objectives on graph data, these works are generally divided into spectral based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> and spatial based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The spectral based methods are developed based on graph spectral theory. These methods usually handle the entire graph to learn a domain-dependent graph network, leading them difficult to apply on large graphs and can not be applied to a different structure. The spatial based methods extend the convolution operation of CNNs to the graph structure. The graph convolution is directly performed based on the graph nodes and their spatial neighbors. This work is mostly We feed-forward the inputs into the feature extractor to obtain the up-to-date representations. Subsequently, two branches are designed to optimize the framework with the source data and the target data, respectively. In each branch, we introduce a memory to store the features of corresponding domain data. The source branch aims to learn basic representation for the model with identity classification loss Lscr, as well as, to learn a graphbased positive prediction (GPP) network with binary classification loss Lgpp. For training of GPP, we first select top-ranked features of the input from the source memory. Then, the selected features are regarded as candidate neighbors and are used to train the GPP network. The GPP network is trained to predict the positive and negative neighbors of the input. The target branch attempts to enforce the invariance learning on the target data. The invariance learning loss Ltgt is calculated by estimating the similarities between the target sample and whole features in the target memory. In addition, we employ the GPP network to infer reliable positive neighbors for the target sample, thereby facilitating the invariance learning. In testing, the L2-normalized output of the global average pooling (GAP) is used as the feature of an image.</p><p>inspired by spatial based methods and proposes a graphbased positive prediction (GPP) approach to predict reliable neighbors across graphs formed by different candidates. The proposed GPP is also related to LBFC <ref type="bibr" target="#b45">[46]</ref> and SGGNN <ref type="bibr" target="#b36">[37]</ref>, which are designed for face clustering and re-ID re-ranking, respectively. The main difference is that GPP is designed to explore reliable neighbors that contribute to generalize re-ID networks. Importantly, GPP is trained on hard negative samples selected from the whole dataset, while LBFC and SGGNN are trained with limited samples from a mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>This paper aims to address the problem of unsupervised domain adaptation (UDA) in person re-identification (re-ID). In the context of UDA in re-ID, we are provided with a fully labeled source domain {X s , Y s } and an unlabeled target domain X t . The source domain includes N s person images of M identities. Each person image x s i is associated with an identity y s i . The target domain contains N t person images. The identity annotation of the target domain is not available. In general, the source and target domains are drawn from different distributions, and the identities in the source and target domains are completely different. In this paper, our goal is to learn a transferable deep re-ID model that generalizes well on the target testing set. The labeled source domain and unlabeled target domain are exploited during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Framework</head><p>The framework of the proposed method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. In our method, the inputs are sampled from the labeled source domain and the unlabeled target domain. The inputs are first fed-forward into the feature extractor to obtain the up-to-date features. The feature extractor is composed of convolutional layers, global average pooling (GAP) <ref type="bibr" target="#b25">[26]</ref> and a batch normalization layer <ref type="bibr" target="#b19">[20]</ref>. The convolutional layers are the residual blocks of ResNet-50 <ref type="bibr" target="#b16">[17]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. The output of feature extractor is 2,048 dimensional. Subsequently, the source branch and the target branch are developed for training our model with the source data and the target data, respectively. Each branch includes an exemplar memory module. The memory module is served as a feature-storage that saves the up-to-date output of feature extractor for each source/target sample. The aims of the source branch are twofold. On the one hand, we use the identity classifier with cross-entropy loss to learn basic representations for the feature extractor. On the other hand, we first select the top-ranked features of the input from the source memory. These top-ranked features are regarded as candidate neighbors and are utilized to learn a Graph-based Positive Prediction (GPP) network. The GPP network consists several graph convolution layers <ref type="bibr" target="#b20">[21]</ref> and a positive classifier, which aims to predict the probability of a candidate that belongs to a positive sample of the input. The target branch is designed to enforce the proposed three properties of invariance on unlabeled target data, i.e. exemplar-, camera-and neighborhood-invariance. Given a target sample, the invariance learning loss is calculated by estimating the similarities between the target sample and whole target samples in the target memory. During invariance learning, we use the learned GPP to infer reliable positive neighbors of the target sample. These neighbors are selected from the candidates of the target memory, according to the probabilities obtained by the positive classifier of GPP. Note that, the GPP network is trained with only the source data. Besides, the loss of the GPP network is not utilized to update the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised Learning for Source Domain</head><p>Given the labeled source data, we are able to learn a basic discriminative model in a supervised way. In this paper, we treat the training process of the source domain as an identity classification problem <ref type="bibr" target="#b58">[59]</ref>. Thus, the supervised loss of the source domain is calculated by cross-entropy loss, formulated as,</p><formula xml:id="formula_1">L src = − log p(y s i |x s i ),<label>(1)</label></formula><p>where p(y s i |x s i ) is the predicted probability that the source image x s i belongs to identity y s i . p(y s i |x s i ) is obtained by the identity classifier of the source branch.</p><p>It is reported that the model trained using the labeled source data produces a high accuracy on the same distributed testing set. However, without adapting the model, the performance will deteriorate significantly when tested on an unseen (target) testing set. This deterioration is mainly caused by the domain shift and will be more serious with the increase of domain shift. Next, we will introduce the exemplar memory based target invariance learning method to improve the transferability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Invariance Learning for Target Domain</head><p>The deep re-ID model trained with only the source data is usually sensitive to the intra-domain variations of the target domain. In fact, the variations are critical factors influencing the performance in target testing set. Therefore, it is important and necessary to consider the intra-domain variations of the target domain during transferring the knowledge from the source domain to the target domain. To this end, this work investigates three underlying properties of target invariance and adapts the network with the constraints of the three properties. The three properties of target invariance are Exemplar-Invariance (EI), Camera-Invariance (CI) and Neighborhood-Invariance (NI), which are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Three Properties of Target Invariance</head><p>Exemplar-invariance. Given a well-trained re-ID model, the top-ranked results are usually visually similar to the query. This suggests that the model has learned to discriminate persons by apparent similarity. However, this phenomenon may no longer apply when tested on a different distributed dataset. In fact, the appearance of each person image may be very different from others, even if they share the same identity. We call this property as exemplar-invariance, where each person image should be close to itself while far away from others. Therefore, by enforcing the exemplarinvariance on the target images, it is possible to enable the model to improve the ability of distinguishing person images based on the apparent representation.</p><p>Camera-invariance. Camera style variation is a natural and important factor in re-ID, where a person image may encounter significant changes in appearance under different cameras. A model trained using labeled source data can learn the camera-invariance for the source cameras, but may suffer from the variations caused by the target cameras. This is because the camera settings of the two domains will be very different. Inspired by HHL <ref type="bibr" target="#b63">[64]</ref>, we achieve the property of target camera-invariance by learning with unlabeled target images and their camera style transferred counterparts. These counterparts are in the style of different cameras but share the same identity with the original image. Real Camera style-transferred <ref type="figure">Fig. 3</ref>. Example of camera style-transferred images on DukeMTMC-reID. A real image collected from a certain camera is transferred to images in the styles of other cameras. In this process, the identity information is preserved to some extent. The real image and its corresponding fake images are assumed to belong to the same class during camera-invariance learning.</p><p>In the constraint of camera-invariance, a target image and its camera-style transferred counterparts are encouraged to be close to each other. Suppose we have C cameras in the target set, we train CamStyle model <ref type="bibr" target="#b65">[66]</ref> for the target domain with CycleGAN <ref type="bibr" target="#b67">[68]</ref> or StarGAN <ref type="bibr" target="#b5">[6]</ref>. For Market-1501 <ref type="bibr" target="#b57">[58]</ref> and DukeMTMC-reID <ref type="bibr" target="#b60">[61]</ref> that have less cameras, we use CycleGAN to train CamStyle model. For MSMT17 that has many cameras, we apply StarGAN to train CamStyle model.</p><p>With the learned CamStyle model, each real target image collected from camera c is augmented with C − 1 images in the styles of other cameras while remaining the original identity. Examples of real images and fake images generated by the CamStyle model are shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighborhood-invariance.</head><p>Besides the camerainvariance property that is natural and can be easily identified, there are many crucial intra-domain variations that are hard to overcome without the annotation of identity, such as the variations of pose and background. In fact, for each target image, there may exist a number of positive samples in the target dataset. If we could exploit these positive samples during the adapting process, we are able to further improve the robustness of re-ID model in overcoming the variations in the target domain. Assuming that we are given an appropriate transferred model and a query target sample. The nearest neighbors of the query in the target set are most likely to be positive samples of the query. In light of this, we introduce the neighborhoodinvariance under the assumption that a target image and its reliable neighbors will share the same identity and should be close to each other.</p><p>Next, we will introduce the loss functions of enforcing the three invariance constraints into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Target Invariance Learning with Memory</head><p>An essential step of enforcing the three properties of invariance is to estimate the similarities (relationships) among target samples. Intuitively, a straightforward solution is to calculate the similarities between samples within a training mini-batch and enforce the three constraints by contrastive loss <ref type="bibr" target="#b17">[18]</ref> or triplet loss <ref type="bibr" target="#b21">[22]</ref>. However, the number of samples in a training mini-batch is far less than that in the entire training set due to the limited GPU memory. This will cause two problems during invariance learning. First, it is hard to compose an appropriate mini-batch that includes various samples, and their corresponding CamStyle counterparts and neighbors. Second, the similarities between mini-batch samples are local and limited compared to the overall dataset. To address these problems, we introduce an exemplar memory into the network to store the up-to-date representations of the entire target samples. The memory enables the network to directly estimate the similarities between a training target sample and whole target samples, thereby effectively implementing the invariance constraints globally.</p><p>Exemplar Memory. The exemplar memory is a feature bank <ref type="bibr" target="#b51">[52]</ref> that stores the up-to-date features of the entire dataset. Given an unlabeled target data including N t images, we construct an exemplar memory (F t ) which has N t slots. Each slot stores the L2-normalized output of the feature extractor for the corresponding target image. In the initialization, we initialize the values of all the features in the memory to zeros. During each training iteration, for a target training sample x t i , we forward it through the model and obtain the L2-normalized output of the feature extractor, f (x t i ). During the back-propagation, we update the feature in the memory for the training sample x t i through,</p><formula xml:id="formula_2">F t [i] ← αF t [i] + (1 − α)f (x t i ),<label>(2)</label></formula><formula xml:id="formula_3">where F t [i] is the feature of image x t i in the i-th slot. The hyper-parameter α ∈ [0, 1] controls the updating rate. F t [i] is then L2-normalized via F t [i] ← F t [i] 2 .</formula><p>Next, we will introduce the approach of memory based invariance learning.</p><p>Exemplar-invariance learning. The exemplar-invariance enforces a target image to be close to itself while far away from others. To achieve this goal, we regard the N t target images as N t different classes and apply a nonparameterized manner to classify each image into its own class. For simplicity, we assign the corresponding index as the class of each target sample. Specifically, given a target image x t i , we first compute the cosine similarities between the embedding f (x t i ) and features saved in the memory F t . Then, the predicted probability that x t i belongs to class i is calculated using softmax function,</p><formula xml:id="formula_4">p(i|x t i ) = exp(F t [i] T f (x t i )/β) N t j=1 exp(F t [j] T f (x t i )/β) ,<label>(3)</label></formula><p>where β ∈ (0, 1] is temperature fact that balances the scale of distribution. The objective of exemplar-invariance learning is to minimize the negative log-likelihood over the target image x t i , as</p><formula xml:id="formula_5">L ei = − log p(i|x t i ).<label>(4)</label></formula><p>Camera-invariance learning. The camera-invariance enforces a target image to be close to its style-transferred counterparts. To introduce the camera-invariance learning into the model, we try to classify each real image and its style-transferred counterparts to the same class. The loss function of camera-invariance is explained as,</p><formula xml:id="formula_6">L ci = − log p(i|x t i ),<label>(5)</label></formula><p>wherex t i is a target image randomly sampled from the styletransferred images of x t i . In this way, images in different camera styles of the same sample are forced to be close to each other.</p><p>Neighborhood-invariance learning. The neighborhoodinvariance enforces a target image to be close to its reliable neighbors. To endow this property into the network, we classify the target image x t i into the classes of its reliable neighbors. Supposing the selected reliable neighbors of x t i in the memory are denoted as K(x t i ). We assign the weight of the probability that x t,i belongs to the class j as,</p><formula xml:id="formula_7">w i,j = 1 |K(x t i )| , j = i 1, j = i , ∀j ∈ K(x t i ),<label>(6)</label></formula><p>where |K(x t i )| denotes the size of K(x t i ). The objective of neighborhood-invariance learning is formulated as a softlabel loss,</p><formula xml:id="formula_8">L ni = − j =i w i,j log p(j|x t i ), ∀j ∈ K(x t i ).<label>(7)</label></formula><p>Note that, in order to distinguish between exemplarinvariance learning and neighborhood-invariance learning, x t i is not classified to its own class in Eq. 7.</p><p>The key of neighborhood-invariance learning is including as many positive samples as possible in K(x t i ) while rejecting negative samples. In Section 3.4, we will introduce a vanilla neighbor selection (VNS) method and a graphbased positive prediction (GPP) method for exploring reliable neighbors K(x t i ). Overall loss of invariance learning. By jointly considering the exemplar-invariance, camera-invariance and neighborhood-invariance, the overall loss of invariance learning over a target training image can be written as,</p><formula xml:id="formula_9">L tgt = − j w i,j log p(j| x t i ), j ∈ K( x t i ),<label>(8)</label></formula><p>where x t i is an image randomly sampled from the union set of x t i and its camera style-transferred images. In Eq. 8, when i = j, we optimize the network with the exemplarinvariance learning and camera-invariance learning by classifying x t i into its own class. When i = j, the network is optimized with the neighborhood-invariance learning by leading x t i to be close to its reliable neighbors in K( x t i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph-based Positive Prediction</head><p>Vanilla Neighbor Selection (VNS). A naive way to select neighbors of a target image x t i is based on the cosine similarities between f (x t i ) and features saved in the memory. In the vanilla neighbor selection (VNS) method, we directly select the top-k ranked samples from the memory as the K(x t i ). However, in such simple approach, the similarities between f (x t i ) and features are estimated independently, neglecting the relationships among the features in the memory. As a consequence, the similarity estimation of hard positive and negative samples may not be accurate, especially when the model has poor performance on the target domain.</p><p>To overcome this problem, next, we will propose a novel neighbor selection method, called graph-based positive prediction (GPP). The network of GPP is constructed by graph convolutional networks (GCNs) <ref type="bibr" target="#b20">[21]</ref> and positive classifier. </p><formula xml:id="formula_10">! " ! # ! $ ! % ! &amp; ! ' ! " ! # ! $ ! % ! &amp; ! '</formula><p>Graph Convs <ref type="figure">Fig. 4</ref>. The pipeline of graph-based positive prediction (GPP). Given the embedding of an input sample, 1) we first compute the similarities between the input and features in the memory. 2) The top-k ranked samples are selected as candidate neighbors and utilized to construct a graph for positive prediction.</p><p>3) The features of nodes are refined by graph convolutional networks (GCNs) on the graph. 4) The positive classifier is employed to predict positive probabilities of every node. For the source domain, the positive classification loss is computed for training the network of GPP. For the target domain, we select reliable neighbors for the input target sample, depending on the predicted positive probabilities of nodes.</p><p>The goal of GPP is to refine the similarities between f (x t i ) and features in the memory by leveraging the relationships among the features. To achieve this goal, the network of GPP is trained to predict probabilities of samples in the memory that belong to positive class of the input sample. We train the GPP network with labeled source samples and then apply it to infer reliable neighbors for target training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Training GPP on Source Domain</head><p>To train the network of GPP, we simulate the positive prediction process on the source domain. In the implementation, we construct a source memory F s with N s slots for storing the features of source samples. The network of GPP includes several graph convolutional layers and a positive classifier. The training process of GPP is illustrated in <ref type="figure">Fig. 4</ref>, which is divided into four steps as described below.</p><p>1. Similarity computation. Given a training source sample x s i , we first extract its feature f (x s i ), and compute the cosine similarities between f (x s i ) and features in the source memory F s .</p><p>2. Graph construction. We select k-nearest-neighbors from the ranked-list as candidate neighbors of x s i , which is denoted as V = {v 1 , v 2 , ..., v k }. Then, we construct a complete undirected graph G(V, E), where V denotes the set of nodes and E indicates the set of edges. We denote</p><formula xml:id="formula_11">H = {F s [v 1 ], F s [v 2 ], ..., F s [v k ]} as the node features.</formula><p>In order to encode the information of x s i , we normalize H by subtracting f (x s i ),</p><formula xml:id="formula_12">H = {F s [v 1 ] − f (x s i ), ..., F s [v k ] − f (x s i )}.<label>(9)</label></formula><p>In practice, the H can be represented by a matrix with a size of k × d, where d is the feature dimension of each node. As well, the weights of E are represented by an adjacency matrix A ∈ R k×k , where</p><formula xml:id="formula_13">A i,j = H T i H j , ∀i, j ∈ V.<label>(10)</label></formula><p>Lastly, each row of A is normalized by softmax function.</p><p>3. Feature updating with GCNs. In this step, we aim to improve the node representations with the neighbor relations, so that can accurately predict positive samples from the candidates. To achieve this goal, we employ the graph convolution network (GCNs) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b45">[46]</ref> to update node features on G(V, E). The input to the GCNs is a set of node features H and an adjacency matrix A, and the output is a new set of node features Z. Every graph convolutional layer in the GCNs can be written as a non-linear function,</p><formula xml:id="formula_14">H (l+1) = ReLU([AH (l) ||H (l) ]W (l) ),<label>(11)</label></formula><p>with H (0) = H and Z = H (L) . L is the number of graph convolutional layers. || is the matrix concatenation operation. W (l) ∈ R 2din×dout is a learnable weight matrix for the l-th graph convolutional layer. d in and d out are the dimensions of input feature and out feature, respectively. In this paper, we adopt 4 graph convolutional layers to form the GCNs. The output Z is used to predict the positive probabilities for nodes by a positive classifier, as described in the next step. 4. Prediction with positive classifier. Given the updated features Z, we use a positive classifier to predict the probability that a node belongs to the positive sample of the input x s i . For training of the GPP network, the loss function on the source domain is formulated by,</p><formula xml:id="formula_15">L gpp = − 1 k j (y * j log p * j + (1 − y * j ) log(1 − p * j )),<label>(12)</label></formula><p>where j ∈ V . p * j is the predicted probability that node j belongs to the positive of the input x s i . y * i is the groundtruth binary label, defined as,</p><formula xml:id="formula_16">y * j = 0, y s j = y s i 1, y s j = y s i , ∀j ∈ V.<label>(13)</label></formula><p>We optimize the network of GPP with L gpp computed on the labeled source data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Reliable Neighbors Selection on Target Domain</head><p>We infer the network of GPP on the target domain with the same steps as the training process on the source domain, except the loss computation in the last step. For a target sample x t i , we first compute the similarities between f (x t i ) and features in the target memory F s with step 1. Then, we obtain the positive probabilities for the candidate neighbors V with the remaining steps. Finally, the reliable neighbors</p><p>Step 1</p><p>Step 3</p><p>Step 5</p><p>Step 4</p><p>Final Step 6</p><p>Step 2 <ref type="figure">Fig. 5</ref>. Toy example of invariance learning. Dot colors denote classes. In each step, an input and its reliable neighbors (highlighted in circle) are enforced to be close by neighborhood-invariance learning, while an input and other samples (out of circle) are enforced to be far away by exemplarinvariance learning. With the interaction of exemplar-invariance and neighborhood-invariance, samples with the same class are gradually grouped closer, while dissimilar groups are separated.</p><p>K(x t i ) are selected depending on the positive probabilities, defined as,</p><formula xml:id="formula_17">K(x t i ) = {j|j ∈ V ∧ p * j ≥ µ},<label>(14)</label></formula><p>where µ is threshold value that controls whether a candidate should be selected as a reliable neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Final Loss for Network</head><p>By combining the losses of supervised learning, invariance learning and graph-based positive prediction learning, the final loss for the overall network is formulated as,</p><formula xml:id="formula_18">L = L src + L tgt + L gpp .<label>(15)</label></formula><p>To this end, we introduce a framework for UDA in person re-ID, in which L src aims to maintain a basic representation for person. Meanwhile, L tgt attempts to take the knowledge from the labeled source domain and incorporate the invariance properties of the target domain into the network. L gpp tries to learn a reliable neighbor prediction network that could facilitate the invariance learning. Note that, L gpp is only calculated on the source domain and is only used to update the network of GPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion on the Three Invariance Properties</head><p>We analyze the advantage and disadvantage for the three invariance properties, and the mutual effectiveness among them. The exemplar-invariance enforces each exemplar away from each other. It is beneficial to enlarge the distance between exemplars from different identities. However, exemplars of the same identity will also be far apart, which is harmful to the system. On the contrast, neighborhoodinvariance encourages each exemplar and its neighbors to be close to each other. It is beneficial to reduce the distance between exemplars of the same identity. However, neighborhood-invariance might also pull closer images of different identities, because we could not guarantee that each neighbor shares the same identity with the input exemplar. Therefore, there exists a trade off between exemplarinvariance and neighborhood-invariance, where the former aims to lead the exemplars from different identities to be far away while the latter attempts to encourage exemplars of the same identity to be close to each other. In other words, the interaction process between exemplar-invariance and neighborhood-invariance can be considered as a kind of local-clustering. On the one hand, when enforcing neighborinvariance, samples with the same identity would be progressively grouped closer, through the neighborhood relation and the connection of their shared neighbors. On the other hand, exemplar-invariance encourages dissimilar samples to be pushed away from each other, so that dissimilar groups would be separated. Camera-invariance has the similar effect as the exemplar-invariance and also leads the exemplar and its camera-style transferred samples to share the same representation. A toy example of invariance learning is shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate the proposed method on three large-scale person re-identification (re-ID) benchmarks: Market-1501 <ref type="bibr" target="#b57">[58]</ref>, DukeMTMC-reID <ref type="bibr" target="#b60">[61]</ref> and MSMT17 <ref type="bibr" target="#b46">[47]</ref>. Market-1501 <ref type="bibr" target="#b57">[58]</ref> includes 32,668 labeled person images of 1,501 identities collected from six non-overlapping camera views. For evaluation, the dataset is divided into 12,936 images of 751 identities for training, 3,368 query images and 19,732 images of 705 identities for testing.</p><p>DukeMTMC-reID <ref type="bibr" target="#b60">[61]</ref> is a re-ID benchmark collected from the DukeMTMC dataset <ref type="bibr" target="#b33">[34]</ref>. The dataset is captured from eight cameras, including 36,411 person images from 1,812 identities. It contains 16,522 images of 702 identities for training, 2,228 query images of 702 identities and 17,611 gallery images for testing.</p><p>MSMT17 <ref type="bibr" target="#b46">[47]</ref> is a newly released large-scale person re-ID benchmark. It is composed of 126,411 person images from 4,101 identities collected by an 15-camera system. The training set consists of 32,621 images of 1,041 identities, and the testing set contains 11,659 images as query and 82,161 images as gallery. The dataset has serious variations of scene and lighting, and is more challenging than the other two benchmarks.</p><p>Evaluation Protocol. During training, we use a labeled training dataset as the source domain and an unlabeled training dataset as the target domain. In testing, performance is evaluated on the target testing set by the cumulative matching characteristic (CMC) and mean Average Precision (mAP). For CMC, we use rank-1, rank-5, rank-10, and rank-20 as metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Deep re-ID model. We adopt ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the backbone of the feature extractor and initialize the model with the parameters pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. Specifically, we fix the first two residual layers and set the stride size of last residual layer to 1. After the global average pooling (GAP) layer, we add a batch normalization (BN) layer <ref type="bibr" target="#b19">[20]</ref> followed by ReLU <ref type="bibr" target="#b30">[31]</ref> and Dropout <ref type="bibr" target="#b38">[39]</ref>. The identity classifier is an M -dimensional FC layer followed by softmax function. The input image is resized to 256 ×128. During training, we perform random flipping, random cropping and random erasing <ref type="bibr" target="#b62">[63]</ref> for data augmentation. The probability of dropout is set to 0.5. We train the re-ID model with a learning rate of 0.01 for ResNet-50 base layers and of 0.1 for the others in the first 40 epochs. The learning rate is divided by 10 for the next 20 epochs. The SGD optimizer is used to train the re-ID model with a mini-batch size of 128 for both source and target domains. We initialize the updating rate of memory α to 0.01 and increase α linearly with the number of epochs, i.e., α = 0.01 × epoch. Without specification, we set the temperature fact β = 0.05. For vanilla neighbor selection, we set k = 8 and directly select the top-k nearestneighbors from the memory as the reliable neighbors. For GPP, we set the number of candidate neighbors k = 100 and neighbor selection threshold µ = 0.9. We train the model with exemplar-invariance and camera-invariance learning at the first 10 epochs and add the neighborhood-invariance learning for the rest epochs. In testing, we extract the L2normalized output of GAP as the image feature and adopt the Euclidean metric to measure the similarities between query and gallery images.</p><p>Network of graph-based positive prediction (GPP). The GPP network contains four graph convolutional layers and a positive classifier. The input and out dimensions of these graph convolutional layers are: 2048→2048, 2048→512, 512→256, 256→256. The positive classifier is composed of a 256-dimensional FC layer, a BN layer, a PReLU layer <ref type="bibr" target="#b15">[16]</ref>, and a 2-dimensional FC layer. We utilize SGD optimizer to update the GPP network after the 5th epoch. The learning rate is initialized as 0.01 and divided by 10 after the 40th epoch.</p><p>Baseline setting. We set the model as the baseline when trained the network using only the identity classifier. We employ the baseline in two ways: 1) Train on target, training the baseline on the labeled target training data and testing on the target testing set, and 2) Source only, training the baseline on a source labeled training set and directly applying to a target testing set without modification. The "Train on target" and the "Source only" can be considered as the upper bond and lower bond of our method, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Analysis</head><p>We first investigate the sensitivities of our approach to three important hyper-parameters, i.e., the temperature fact β in Eq. 3, the number of candidate neighbors k in graph G(V, E) and the neighbor selection threshold µ in Eq. 14. In order to clearly analyze the impact of every parameter, we vary the value of one parameter and keep fixed the others. Experiments are evaluated on the setting of transferring between Market-1501 and DukeMTMC-reID.  Temperature fact β. In <ref type="table" target="#tab_1">Table 1</ref>, we report the impact of the temperature fact β in Eq. 3. Assigning a lower value to β gives rise to a lower entropy, which commonly produces better results. However, when assigning a extremely low value to β, the model does not converge and produces a poor performance, e.g., β = 0.01. The best results are obtained at β = 0.05.</p><p>Number of candidate neighbors k. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we evaluate the performance of using a different number of candidate neighbors in the graph G(V, E). When k = 0, our approach reduces to the model trained with only exemplarinvariance and camera-invariance. It is evident that injecting neighborhood-invariance into the network (k &gt; 0) can significantly improve accuracy, and our method is insensitive to the changes of k. The rank-1 accuracy and mAP first improve with the increase of k and reach stable when k &gt; 100. This improvement tendency is reasonable, because: 1) Using a larger k will include more positive samples in the graph and GPP might discover more hard positive samples for neighborhood-invariance learning; 2) Most positive samples are within the top-100 nearest neighbours, so it is unnecessary to include too many candidate samples. Considering the trade-off between accuracy and speed, we set k = 100 in our approach.</p><p>Threshold of positive neighbor selection µ. In <ref type="figure" target="#fig_5">Fig. 7</ref>, we compare the performance of using different values of µ in Eq. 14. On the one hand, assigning a too high value to µ would only select easy positive samples for neighborinvariance learning, e.g., µ = 0.99. This will result in the model suffers from hard positive samples in testing. Note that, when µ = 1, our method reduces to the model trained without neighbor-invariance learning. Since extremely few  On the other hand, giving a too low value to µ might include too many false positive samples for neighbor-invariance learning, e.g., µ &lt; 0.7. Approaching a sample to false positive samples would undoubtedly have deleterious effects on the results. Our approach reaches the best result when µ is around 0.9. According to above analysis, we set β = 0.05, k = 100 and µ = 0.9 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation and Analysis</head><p>Performance of baseline in domain adaptation. In <ref type="table" target="#tab_2">Table 2</ref>, we report the results of the baseline when transferring between Market-1501 and DukeMTMC-reID. When trained on the labeled target training set, the baseline (Train on target) achieves high accuracy. However, we observe a serious drop in performance when the baseline (Source only) is trained only using the labeled source set and directly applied to the target testing set. For the case of testing on Market-1501, the baseline (Train on target) achieves a rank-1 accuracy of 87.6%, but the rank-1 accuracy of the baseline (Source only) declines to 43.1%. A similar drop can be observed when transferred from Market-1501 to DukeMTMC-reID. This decline in accuracy is mainly caused by the domain shifts between datasets.</p><p>Ablation study on invariance learning. To study the effectiveness of the proposed invariance learning of target domain, we conduct ablation experiments in <ref type="table" target="#tab_2">Table 2</ref>. We start from the basic model (Our method w/ EI), which enforces exemplar-invariance learning into the baseline (Source only) model, and then add camera-invariance, neighborhoodinvariance, and both.</p><p>First, we show the effect of adding exemplar-invariance learning. As shown in <ref type="table" target="#tab_2">Table 2</ref>, "Ours w/ EI" consistently improves the results over the baseline (Source only). Specifically, the rank-1 accuracy improves from 43.1% to 48.7% and 28.9% to 34.2% when tested on Market-1501 and DukeMTMC-reID, respectively. This demonstrates that exemplar-invariance learning is an effective way to improve the discrimination of person descriptors for the target domain. Second, we validate the effectiveness of camerainvariance learning over the basic model (ours w/ EI). In <ref type="table" target="#tab_2">Table 2</ref>, we observe significant improvement when adding camera-invariance learning into the system. For example, "Ours w/ EI+CI" achieves a rank-1 accuracy of 63.1% when transferred from DukeMTMC-reID to Market-1501. This is higher than "Ours w/ EI" by 14.4% in rank-1 accuracy. The improvement demonstrates that the image variations caused by target cameras severely impact the performance in target testing set. Injecting camera-invariance learning into the model could effectively improve the robustness of the system to camera style variations. In <ref type="table" target="#tab_3">Table 3</ref>, we also report the results of using different number of camera style samples for each target image. Even with only one camera style sample for each target image, the results of our approach can be considerable improved and are slightly lower than that of using all camera style samples. This indicates that the number of camera style samples in our approach can be agnostic to the number of cameras, and thus our approach is scalable to a scenario with many cameras.</p><p>Third, we evaluate the effect of neighborhood-invariance learning. As reported in <ref type="table" target="#tab_2">Table 2</ref>, neighborhood-invariance significantly improves performance of the basic model (Ours w/ EI). When adding neighborhood-invariance to the basic model, "Ours w/ EI+NI" obtains 67.2% rank-1 accuracy and 48.3% mAP when transferred from Market-1501 to DukeMTMC-reID. This increases the results of the basic model by 33% in rank-1 accuracy and by 29.7% in mAP, respectively. A similar improvement is achieved when transferred to Market-1501.</p><p>Finally, we demonstrate the mutual benefit among the  4% when transferred to DukeMTMC-reID, which is lower than "Train on target" by 1.6% in rank-1 accuracy and by 3.4% in mAP. This demonstrates that our approach has a strong capacity of bridging the gap between person re-ID domains.</p><p>Benefit of exemplar memory. In <ref type="table" target="#tab_4">Table 4</ref>, we validate the effectiveness of the exemplar memory. When training our model without memory, we enforce the invariance learning within a mini-batch. Specifically, the inputs of the target mini-batch are composed of the target samples, corresponding CamStyle samples and corresponding k-nearest neighbors. Without the memory, we could not employ GPP in our model. Therefore, for a fair comparison, we train our memory based model without GPP, i.e. select the reliable neighbors K(x t i ) by vanilla neighbor selection (VNS) method. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the exemplar memory based method clearly outperforms the mini-batch based method. This demonstrates the effectiveness of leveraging relations among whole datasets by memory. It is noteworthy that using the exemplar memory introduces limited additional training time (≈ + 0.02 s/iter) and GPU memory (≈ + 200 MB) compared to using the mini-batch.</p><p>Effectiveness of graph-based positive prediction. In <ref type="table" target="#tab_4">Table 4</ref>, we report the results of training our method with and without graph-based positive prediction (GPP). Without GPP, our method reduces to training with vanilla neighbor selection (VNS), i.e. directly select k-nearest neighbors from memory as reliable neighbors K(x t i ). It is clearly that, adding GPP into the system significantly improves the  accuracy, requiring extra 0.11 s/iter training time and 2,800 MB GPU memory.</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we also evaluate two other neighbor selection methods that are constructed as follows:</p><p>• Variant VNS: We train a positive prediction classifier as the same way as GPP, but without using graph convolution layers to update features of candidate samples. K(x t i ) is selected according to the predicted positive probability and threshold µ. We set µ = 0.9 for variant VNS.</p><p>• Variant GPP: We use the same architecture as the proposed GPP. The top-k samples are selected as K(x t i ), according to the positive prediction of GPP. We set k = 8 for variant GPP. <ref type="table" target="#tab_5">Table 5</ref> shows that: (1) GPP based methods consistently outperforms VNS based methods, whether selecting K(x t i ) by top-k samples or threshold of predicted positive probability. This demonstrates the effectiveness of GPP, which updates features with the neighbor relations. (2) For GPP based methods, it is better to selecting reliable neighbors based on threshold than fix top-k samples. Because the number of positive samples of each sample is very different, using fix top-k samples might include negative samples that have low positive probability or ignore positive samples that have high positive probability. While threshold based strategy is more flexible and will always select samples with high positive probability. (3) The improvement of threshold based strategy is limited or even negative for the VNS method. Because the inputs of positive prediction classifier are unrelated features, which do not consider the relations Unsupervised person re-ID performance comparison with state-of-the-art methods on Market-1501 and DukeMTMC-reID. Market: Market-1501 <ref type="bibr" target="#b57">[58]</ref>. Duke: DukeMTMC-reID <ref type="bibr" target="#b60">[61]</ref>. MSMT: MSMT17 <ref type="bibr" target="#b46">[47]</ref>. SyRI: SyRI <ref type="bibr" target="#b0">[1]</ref>. Multi: a combination of seven datasets. among candidates. In this case, the positive prediction classifier fails to refine the similarities between the input and candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>To further validate the effectiveness of GPP, we evaluate the selected reliable neighbors K(x t i ) throughout the training for VNS and GPP. Two metrics are applied: recall and precision. Recall is the fraction of true positive samples in K(x t i ) over the total amount of positive samples in the dataset. Precision is the fraction of true positive samples in K(x t i ) among the number of samples in K(x t i ). <ref type="figure" target="#fig_7">Fig. 8</ref> shows that: 1) GPP consistently produces higher precision than VNS method. 2) The recall of GPP is lower than VNS in the early training epochs. However, the recall of GPP consistently grows with the training epochs and is largely higher than that of VNS in later training epochs. This confirms the superiority of GPP over VNS in terms of reliable neighbor selection and adaptation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-art Methods</head><p>In <ref type="table" target="#tab_6">Table 6</ref> and <ref type="table" target="#tab_8">Table 7</ref>, we compare our approach with stateof-the-art unsupervised learning methods when tested on Market-1501, DukeMTMC-reID and MSMT17.</p><p>SOTA on Market-1501 and DukeMTMC-reID. <ref type="table" target="#tab_6">Table 6</ref> reports the comparisons when tested on Market-1501 and DukeMTMC-reID. We compare with two hand-crafted feature based methods without transfer learning: LOMO <ref type="bibr" target="#b24">[25]</ref> and BOW <ref type="bibr" target="#b57">[58]</ref>, four unsupervised methods that use a labeled source data to initialize the model but ignore the labeled source data during learning feature for the target domain: CAMEL <ref type="bibr" target="#b54">[55]</ref>, DECAMEL <ref type="bibr" target="#b55">[56]</ref>, UMDL <ref type="bibr" target="#b32">[33]</ref> and PUL <ref type="bibr" target="#b9">[10]</ref>, and nine unsupervised domain adaptation approaches: PTGAN <ref type="bibr" target="#b46">[47]</ref>, SPGAN <ref type="bibr" target="#b8">[9]</ref>, MMFA <ref type="bibr" target="#b26">[27]</ref>, TJ-AIDL <ref type="bibr" target="#b44">[45]</ref>, DASy <ref type="bibr" target="#b0">[1]</ref>, MAR <ref type="bibr" target="#b56">[57]</ref>, CamStyle <ref type="bibr" target="#b66">[67]</ref>, HHL <ref type="bibr" target="#b63">[64]</ref> and ECN <ref type="bibr" target="#b64">[65]</ref>.</p><p>We first compare with hand-crafted feature based methods, which neither require learning on labeled source set nor unlabeled target set. These two hand-crafted features have demonstrated the effectiveness on small datasets, but fail to produce competitive results on large-scale datasets. For example, the rank-1 accuracy of LOMO is 12.3% when tested on DukeMTMC-reID, which is much lower than transferring learning based methods. Next, we compare with four unsupervised methods. Benefit from initializing model from the labeled source data and learning with unlabeled target data, the results of these three unsupervised approaches are commonly superior to hand-crafted methods. Such as, PUL obtains rank-1 accuracy of 45.5% when using DukeMTMC-reID as source set and tested on Market-1501, surpassing BOW by 9.7% in rank-1 accuracy.</p><p>Finally, we compare with the domain adaptation based approaches, which produce state-of-the-art results. As can be seen, our approach outperforms them by a large margin on both datasets. Specifically, our method achieves rank-1 accuracy = 84.1% and mAP = 63.8% when using DukeMTMC-reID as the source set and tested on Market-1501, and, obtains rank-1 accuracy = 74.0% and mAP = 54.4% vice-versa. It is worth noting that our approach significantly outperforms MAR <ref type="bibr" target="#b56">[57]</ref> which uses a larger dataset (MSMT17) as the source domain. Compared to the current best method (ECN <ref type="bibr" target="#b64">[65]</ref>), our method surpasses ECN by 20% and 14.4% in mAP when tested on Market-1501 and DukeMTMC-reID, respectively. SOTA on MSMT17. We also evaluate our approach on a larger and more challenging dataset, i.e., MSMT17.</p><p>Since it is a newly released dataset, there are only three unsupervised domain adaptation methods (PTGAN <ref type="bibr" target="#b46">[47]</ref>, DECA <ref type="bibr" target="#b56">[57]</ref>, and ECN <ref type="bibr" target="#b64">[65]</ref>) reported results on MSMT17. In addition, we further compare with two semi-supervised methods, TAUDL <ref type="bibr" target="#b21">[22]</ref> and UTAL <ref type="bibr" target="#b22">[23]</ref>, which use withincamera identity annotations of the target domain. As shown in <ref type="table" target="#tab_8">Table 7</ref>, our approach clearly exceeds both unsupervised domain adaptation methods and semi-supervised methods. For instance, our method produces rank-1 accuracy = 42.5% and mAP = 16.0% when using DukeMTMC-reID as the source set, which is higher than ECN by 12.3% in rank-1 accuracy and by 5.8% in mAP. Similar superiority of our method can be observed when using the Market-1501 as the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an exemplar memory based unsupervised domain adaptation (UDA) framework for person re-ID task. With the exemplar memory, we can directly evaluate the relationships between target samples. And thus we could effectively enforce the underlying invariance constraints of the target domain into the network training process. Moreover, the memory enables us to design a graph-based positive prediction (GPP) method that can infer reliable neighbors from candidate neighbors. Experiment demonstrates the effectiveness of the invariance learning, memory and GPP for improving the transferable ability of deep re-ID model. Our approach produces new state of the art in UDA accuracy on three large-scale domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of three underlying properties of invariance. Colors indicate identities. (a) Exemplar-invariance: an input exemplar (denoted by ) is enforced to be away from others. (b) Camera-invariance: an input exemplar (denoted by ) and its CamStyle transferred images (with dashed outline) are encouraged to be close to each other. (c) Neighborhood-invariance: an input exemplar (denoted by ) and its reliable neighbors (highlighted in dashed circle) are forced to be close to each other. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of the proposed method. During training, the inputs are drawn from the labeled source domain and the unlabeled domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-2 Cam-3 Cam-4 Cam-6 Cam-7 Cam-8 Cam-1 Cam-2 Cam-3 Cam-4 Cam-5 Cam-6 Cam-7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Evaluation with different number of candidate samples for graphbased positive prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Evaluation with different values of µ in Eq. 14 samples would be selected as reliable positive neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Market-1501 → DukeMTMC-reID</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>The curve of selected reliable neighbors in (a) recall and (b) precision throughout the training. Results are compared between vanilla neighbor selection (VNS) and graph-based positive prediction (GPP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zhong is with the Department of Artificial Intelligence, Xiamen University, Xiamen 361005, China and also with the Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW 2007, Australia (e-mail: zhunzhong007@gmail.com). • L. Zheng is with the Research School of Computer Science, The Australian National University, Canberra, ACT 0200, Australia (e-mail: liangzheng06@gmail.com).</figDesc><table /><note>• Z. Luo is with the Postdoc Center of Information and Communication Engineering, Xiamen University, Xiamen 361005, China (e-mail: zhim- ing.luo@xmu.edu.cn).• S. Li (corresponding author) is with the Department of Artifi- cial Intelligence, Xiamen University, Xiamen 361005, China (e-mail: szlig@xmu.edu.cn).• Yi Yang is with the Centre for Artificial Intelligence, Univer- sity of Technology Sydney, Ultimo, NSW 2007, Australia. E-mail: yee.i.yang@gmail.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Evaluation with different values of β in Eq. 3.</figDesc><table><row><cell></cell><cell>β</cell><cell></cell><cell cols="5">Duke → Market Rank-1 mAP</cell><cell></cell><cell>Market → Duke Rank-1 mAP</cell></row><row><cell cols="2">0.01</cell><cell></cell><cell></cell><cell>50.1</cell><cell></cell><cell></cell><cell>21.1</cell><cell></cell><cell>45.6</cell><cell>21.5</cell></row><row><cell cols="2">0.03</cell><cell></cell><cell></cell><cell>79.4</cell><cell></cell><cell></cell><cell>54.5</cell><cell></cell><cell>68.1</cell><cell>45.6</cell></row><row><cell cols="2">0.05</cell><cell></cell><cell></cell><cell>84.1</cell><cell></cell><cell></cell><cell>63.8</cell><cell></cell><cell>74.0</cell><cell>54.4</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell>75.7</cell><cell></cell><cell></cell><cell>46.5</cell><cell></cell><cell>63.0</cell><cell>39.8</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>59.0</cell><cell></cell><cell></cell><cell>30.9</cell><cell></cell><cell>51.0</cell><cell>28.0</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>57.9</cell><cell></cell><cell></cell><cell>30.3</cell><cell></cell><cell>44.6</cell><cell>23.8</cell></row><row><cell>Rank-1 accuracy (%)</cell><cell>40 50 60 70 80 90</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell cols="3">40 Duke-&gt;Market 50 60</cell><cell>k</cell><cell>80 100 120 140 160 180 200 Market-&gt;Duke</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP (%)</cell><cell>40 50 60 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Duke-&gt;Market</cell><cell>Market-&gt;Duke</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>k</cell><cell>80 100 120 140 160 180 200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Methods comparison when transferred to Market-1501 and DukeMTMC-reID. Train on target: baseline model trained with the labeled target training data. Source Only: baseline model trained with only labeled source data. EI: exemplar-invariance. CI: camera-invariance. NI: neighborhood-invariance.</figDesc><table><row><cell>Methods</cell><cell cols="3">Invariance EI CI NI</cell><cell>R-1</cell><cell cols="3">DukeMTMC-reID → Market-1501 R-5 R-10 R-20</cell><cell>mAP</cell><cell>R-1</cell><cell cols="3">Market-1501 → DukeMTMC-reID R-5 R-10 R-20</cell><cell>mAP</cell></row><row><cell>Train on target</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.6</cell><cell>95.5</cell><cell>97.2</cell><cell>98.3</cell><cell>69.4</cell><cell>75.6</cell><cell>87.3</cell><cell>90.6</cell><cell>92.9</cell><cell>57.8</cell></row><row><cell>Source only</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.1</cell><cell>58.8</cell><cell>67.3</cell><cell>74.3</cell><cell>17.7</cell><cell>28.9</cell><cell>44.0</cell><cell>50.9</cell><cell>57.5</cell><cell>14.8</cell></row><row><cell>Ours</cell><cell></cell><cell>-</cell><cell>-</cell><cell>48.7</cell><cell>67.4</cell><cell>74.0</cell><cell>80.2</cell><cell>21.0</cell><cell>34.2</cell><cell>51.3</cell><cell>58</cell><cell>64.2</cell><cell>18.7</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>-</cell><cell>63.1</cell><cell>79.1</cell><cell>84.6</cell><cell>89.1</cell><cell>28.4</cell><cell>53.9</cell><cell>70.8</cell><cell>76.1</cell><cell>80.7</cell><cell>29.7</cell></row><row><cell>Ours</cell><cell></cell><cell>-</cell><cell></cell><cell>71.8</cell><cell>83.1</cell><cell>87.1</cell><cell>90.6</cell><cell>45.7</cell><cell>67.2</cell><cell>80.0</cell><cell>83.8</cell><cell>86.7</cell><cell>48.3</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>84.1</cell><cell>92.8</cell><cell>95.4</cell><cell>96.9</cell><cell>63.8</cell><cell>74.0</cell><cell>83.7</cell><cell>87.4</cell><cell>90.0</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Evaluation with different number of camera style samples for each target image. C is the number of cameras in the target domain. Model is trained with all three invariance constraints.</figDesc><table><row><cell># CamStyle</cell><cell cols="2">Duke → Market</cell><cell cols="2">Market → Duke</cell></row><row><cell>samples</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>0</cell><cell>71.8</cell><cell>45.7</cell><cell>67.2</cell><cell>48.3</cell></row><row><cell>1</cell><cell>83.5</cell><cell>61.1</cell><cell>72.1</cell><cell>52.3</cell></row><row><cell>3</cell><cell>84.0</cell><cell>62.8</cell><cell>73.0</cell><cell>53.0</cell></row><row><cell>C-1</cell><cell>84.1</cell><cell>63.8</cell><cell>74.0</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Results and computational cost analysis of the exemplar memory and graph-based positive prediction (GPP).</figDesc><table><row><cell>Module</cell><cell></cell><cell>Training time</cell><cell>GPU memory</cell><cell cols="2">Duke→ Market</cell><cell cols="2">Market→ Duke</cell></row><row><cell cols="2">Memory GPP</cell><cell>(s/iter)</cell><cell>(MB)</cell><cell>R-1</cell><cell>mAP</cell><cell>R-1</cell><cell>mAP</cell></row><row><cell>-</cell><cell>-</cell><cell>≈0.50</cell><cell>≈6,800</cell><cell>70.2</cell><cell>40.6</cell><cell>55.3</cell><cell>33.7</cell></row><row><cell></cell><cell>-</cell><cell>≈0.52</cell><cell>≈7,000</cell><cell>77.4</cell><cell>45.5</cell><cell>65.4</cell><cell>42.6</cell></row><row><cell></cell><cell></cell><cell>≈0.63</cell><cell>≈9,800</cell><cell>84.1</cell><cell>63.8</cell><cell>74.0</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Results of using different neighbor selection methods. VNS: Vanilla neighbor selection. GPP: graph-based positive prediction.</figDesc><table><row><cell>Neighbor selection</cell><cell>Condition</cell><cell cols="2">Duke→ Market R-1 mAP</cell><cell cols="2">Market→Duke R-1 mAP</cell></row><row><cell>VNS</cell><cell>top-8</cell><cell>77.4</cell><cell>45.5</cell><cell>65.4</cell><cell>42.6</cell></row><row><cell>Variant VNS</cell><cell>p ≥ 0.9</cell><cell>77.0</cell><cell>46.4</cell><cell>64.5</cell><cell>42.8</cell></row><row><cell>Variant GPP</cell><cell>top-8</cell><cell>79.8</cell><cell>51.3</cell><cell>68.2</cell><cell>45.3</cell></row><row><cell>GPP</cell><cell>p ≥ 0.9</cell><cell>84.1</cell><cell>63.8</cell><cell>74.0</cell><cell>54.4</cell></row><row><cell cols="6">three invariance properties. As shown in the last row in</cell></row><row><cell cols="6">Table 2, the three invariance properties are complementary</cell></row><row><cell cols="6">to each other. The integration of them achieves obvious im-</cell></row><row><cell cols="6">provement over independently adding camera-invariance</cell></row><row><cell cols="6">or neighborhood-invariance to the basic model. For ex-</cell></row><row><cell cols="6">ample, "Ours w/ EI+CI+NI" achieves rank-1 accuracy of</cell></row><row><cell cols="6">84.1% when transferred from DukeMTMC-reID to Market-</cell></row><row><cell cols="6">1501, outperforming "Ours w/ EI+CI" by 21% and "Ours</cell></row><row><cell cols="6">w/ EI+NI" by 12.3%. Similar improvement is observed</cell></row><row><cell cols="6">when transferred to DukeMTMC-reID. Particularly, our fi-</cell></row><row><cell cols="6">nal model (Ours w/ EI+CI+NI) has only a small gap with</cell></row><row><cell cols="6">the upper bond model (Train on target). For instance, our</cell></row><row><cell cols="6">final model reaches rank-1 accuracy of 74.0% and mAP of</cell></row><row><cell>54.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Unsupervised/semi-supervised person re-ID performance comparison with state-of-the-art methods on MSMT17. *: Using within-camera identity annotations of target dataset. Multi: a combination of seven datasets.</figDesc><table><row><cell>Methods</cell><cell>Reference</cell><cell>Source</cell><cell>R-1</cell><cell cols="3">MSMT17 R-5 R-10 mAP</cell></row><row><cell>TAUDL [22] UTAL [23]</cell><cell>ECCV 18 TPAMI 19</cell><cell>MSMT*</cell><cell>28.4 31.4</cell><cell>--</cell><cell>--</cell><cell>12.5 13.1</cell></row><row><cell>DECA [56]</cell><cell>TPAMI 19</cell><cell>Multi</cell><cell>30.3</cell><cell>-</cell><cell>-</cell><cell>11.1</cell></row><row><cell>PTGAN [47]</cell><cell>CVPR 18</cell><cell></cell><cell>10.2</cell><cell>-</cell><cell>24.4</cell><cell>2.9</cell></row><row><cell>ECN [65] Ours</cell><cell>CVPR 19 This paper</cell><cell>Market</cell><cell cols="2">25.3 36.3 40.4 53.1</cell><cell>42.1 58.7</cell><cell>8.5 15.2</cell></row><row><cell>PTGAN [47]</cell><cell>CVPR 18</cell><cell></cell><cell>11.8</cell><cell>-</cell><cell>27.4</cell><cell>3.3</cell></row><row><cell>ECN [65] Ours</cell><cell>CVPR 19 This paper</cell><cell>Duke</cell><cell cols="2">30.2 41.5 42.5 55.9</cell><cell>46.8 61.5</cell><cell>10.2 16.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep association learning for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo Ha2 Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sampleproblem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="737" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
		<editor>Prco. BMVC</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multitarget, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Leveraging virtual and real person for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02074</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Crossview asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep asymmetric metric embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<title level="m">Person reidentification: Past, present and future</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Random erasing data augmentation. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Camstyle: A novel data augmentation method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
