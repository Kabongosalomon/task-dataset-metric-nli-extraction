<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
							<email>lijiang@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
							<email>hszhao@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent YouTu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With increasing capability of 3D sensing hardware, it is now easy to capture 3D data in many scenarios. Compared with 2D images, 3D data provides richer information about the environment. 3D data is in general view-independent and captures 3D structure, making it possible to incorporate geometry information in scene understanding tasks.</p><p>Learning-based approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref> were proposed to solve various 3D vision problems, e.g., shape classification, scene semantic/instance segmentation, and 3D object detection. Unlike 2D images, in which pixel grids are regular with object color information, 3D object data scatters, with most space actually not occupied. Therefore, directly voxelizing 3D scenes and extending deep neural network operations from 2D to 3D is inefficient. Several voxel-based methods, such as Submanifold Sparse Convolution <ref type="bibr" target="#b2">[3]</ref> and O-CNN <ref type="bibr" target="#b22">[23]</ref>, improve the 3D convolution efficiency. However, since voxelization is accom-  panied by loss of information, high-resolution 3D models are needed to uphold the data precision, even though it unavoidably costs large memory and computation resource.</p><p>From another perspective, PointNet <ref type="bibr" target="#b9">[10]</ref> directly processes 3D points in a network, only considering regions covered by the 3D points. PointNet++ <ref type="bibr" target="#b11">[12]</ref> further adopts a hierarchical encoder-decoder structure to consider local regions, which downsamples point clouds in layers first and gradually interpolates them to the original resolution. This framework just utilizes weak connection between each point and its local context, since point features are extracted independently by the multi-layer perceptrons (MLP). In segmentation tasks, it is commonly known that local context is crucial for labeling the semantic categories. This motivates us to further explore the semantic relation between points and their local contextual neighbors to extract more discriminative features for 3D semantic scene labeling.</p><p>Our Contributions To explore the semantic relation between points in a local region and utilize the contextual information, we explicitly build edges between points and their contextual neighbors and establish a hierarchical edge branch with an auxiliary edge loss, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Specifically, besides the encoder-decoder point branch as in PointNet++, our new edge branch accepts point features from different layers and progressively produces edge features, which are then fed to point branch for fusing information in local graphs. For each point, the corresponding edge features provide local intrinsic geometric and regional semantic information to enhance point representation.</p><p>Instead of building isolated graphs for points in each layer, we design a hierarchical graph construction process to gradually take point features at different layers into the edge branch. Edge features of adjacent layers are connected by an operator, named "edge upsample". Consequently, edges on full-resolution point cloud encode multi-layer features, providing comprehensive data for final prediction. We regularize the final edge features considering semantic consistency of the two connected points, which helps increase the discrimination ability between inter-and intracategory feature pairs, implicitly pulling points with the same semantic label closer in the feature space.</p><p>The decent performance of our method compared with all existing point-based neural networks on the large-scale scene labeling datasets, i.e., Stanford Large Scale 3D Indoor Space (S3DIS) <ref type="bibr" target="#b0">[1]</ref> and ScanNet <ref type="bibr" target="#b1">[2]</ref>, manifests the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Representation</head><p>To process 3D data, one typical approach is to store the data in volume grids and adopt 3D convolutions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. Since most voxels are unoccupied, Submanifold Sparse Convolution Network <ref type="bibr" target="#b2">[3]</ref> defines a sparse convolution operation to process spatially-sparse 3D data. OctNet <ref type="bibr" target="#b12">[13]</ref>, on the other hand, represents the data using unbalanced octrees and defines network operations on these octrees to enable deeper neural networks without sacrificing the precision. Similarly, O-CNN <ref type="bibr" target="#b22">[23]</ref> uses an octree to enable 3D CNN on high-resolution 3D data.</p><p>Another approach is to use multi-view 2D images, to which 2D convolutions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11]</ref> can be directly applied. However, these approaches overlook the geometric structure in objects and scenes, especially the view-occluded 3D structures. Other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref> consider 3D object surface and apply convolutions on it for semantic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Point-based Deep Neural Network</head><p>PointNet <ref type="bibr" target="#b9">[10]</ref> is the first deep neural network to directly process 3D point coordinates, with MLPs and maxpooling for extracting features. Since max-pooling is a global operation on all the points, PointNet lacks local region understanding. PointNet++ <ref type="bibr" target="#b11">[12]</ref> further applies a hierarchical structure and uses k-NN followed by maxpooling to capture regional information. Since it aggregates local features simply via a max-pooling, regional information is not yet fully utilized.</p><p>Recently, much effort has been made for effective local feature aggregation. SPLATNet <ref type="bibr" target="#b16">[17]</ref> maps points into a high-dimensional sparse lattice and performs convolution on it. RSNet <ref type="bibr" target="#b3">[4]</ref> projects features of unordered points into an ordered sequence of feature vectors and applies Recurrent Neural Network layers to model local depen-dency. PointCNN <ref type="bibr" target="#b6">[7]</ref> explores convolution on point clouds and addresses the point ordering issue by permuting and weighting input points and features with the X -Conv operator. Besides, methods of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref> explore local context based on graphs.</p><p>Graph-based Methods ECC <ref type="bibr" target="#b15">[16]</ref> organizes point clouds as graphs and uses graph convolutions to dynamically learn weights to combine local features. DGCNN <ref type="bibr" target="#b25">[26]</ref> proposes the EdgeConv module to generate edge features that describe the connection between a point and its nearest neighbors. PointWeb <ref type="bibr" target="#b28">[29]</ref> further connects every point pairs in a local region to obtain more representative region features. KCNet <ref type="bibr" target="#b13">[14]</ref> creates k-nearest neighbor graphs and applies kernel correlation to learn local structures over point neighborhood. PCCN <ref type="bibr" target="#b23">[24]</ref> and PointConv <ref type="bibr" target="#b26">[27]</ref> connect each point with its k-nearest neighbors and extend the convolution operation from regular grids to irregular point clouds by adaptively projecting the relative position of two points to a convolution weight. Compared to PCCN, PointConv additionally considers point distribution density. Spectral Graph Convolution <ref type="bibr" target="#b21">[22]</ref> performs graph convolution after a graph Fourier transform. Superpoint Graph (SPG) <ref type="bibr" target="#b5">[6]</ref> splits the point cloud into geometrically-homogeneous partitions and builds a super-point graph, followed by a graph neural network to produce semantic labels.</p><p>In our work, we also propose a graph for point cloud processing, and yet focus particularly on exploring the semantic relation between points and their contextual neighbors for semantic segmentation through explicit edges. The key distinction of our method from other graph-based frameworks is that instead of fixing the graph and point resolution (e.g., PCCN <ref type="bibr" target="#b23">[24]</ref> and KCNet <ref type="bibr" target="#b13">[14]</ref>) or building independent graphs at each scale (e.g., PointConv <ref type="bibr" target="#b26">[27]</ref>, PointWeb <ref type="bibr" target="#b28">[29]</ref> and ECC <ref type="bibr" target="#b15">[16]</ref>), our graph is hierarchically constructed. We construct an edge branch, in which we fuse multi-scale point features and propagate edge features over multiple scales to enable longer distances of message passing hierarchically over edges without large memory overhead. Moreover, we propose edge loss aiming to encode the edges with exact semantic consistency information and increase the discrimination power among point features with different categories.</p><p>With meaningful edge features, we further feed edge features into each scale of the point branch to offer contextual information. To pass messages via edges, PointConv <ref type="bibr" target="#b26">[27]</ref> and PCCN <ref type="bibr" target="#b23">[24]</ref> adaptively learn weights from edges to fuse point features, while KCNet <ref type="bibr" target="#b13">[14]</ref> defines a point-set kernel and kernel correlation to aggregate local features along edges. Different from these methods, our approach concatenates each point feature with the max-pooled corresponding edge features. Our approach requires less parameters to learn and preserves the distinctiveness of individual point features (Section 4.4 provides more discussions).</p><formula xml:id="formula_0">!×# $ Encoding Stage ! % ×# % ! % ×3 Point Module ! '() ×# '() ! '() ×3 Point Module ! ' ×# ' ! ' ×3 Point Module Edge Module |+ % |×, % Point Feature Edge Feature Point Feature Edge Module |+ '() |×, '() Edge Feature Edge Module |+ ' |×, ' Edge Feature Point Feature ⋯ ⋯ ⋯ ⋯ !×#./00 Decoding Stage |+| MLP MLP Skip Connection (a) Point Branch (b) Edge Branch !×# !×3 Edge Module |+|×, Max Pool ! ' ×# ' $ ! '1) ×# '1) $ ! ) ×# ) $ Point Feature : Concatenation Edge Feature Figure 2.</formula><p>Overall architecture. N denotes the number of points in the original point cloud. The subscript of N is the layer index. Larger indices indicate layers with more points. C denotes the number of point feature channels. K denotes the number of edge feature channels. E denotes the edge set. The edge feature is encoded from the coarsest layer 0, and is gradually refined with the point features from later layers. Edge features in different layers also participate in the corresponding point modules to provide contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We design a hierarchical edge branch collaborating with the point prediction branch for point cloud semantic segmentation, as shown in <ref type="figure">Fig. 2</ref>. We progressively enlarge the graph, upsample edge features, and accept point features in different layers to refine the edge features. Edge features in different layers then provide extra contextual information for point feature learning. The final edge features are regularized with semantic consistency of their two-end points, which serve as auxiliary supervision for point features.</p><p>In this section, we first introduce the new edge branch, covering especially the interaction between point and edge branches, in Section 3.1. Then the hierarchical graph construction framework, which enables integration of differentlayer information for edge prediction is described in Section 3.2. Section 3.3 depicts the loss regularizing both category prediction of each point and semantic-consistency prediction of each edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Edge Branch</head><p>Given a point cloud with N points P = {p 1 , p 2 , ..., p N }, we construct a directed graph G = (V, E), where V = P and E includes the edges that connect each point to its contextual points. Here, G is hierarchically constructed in a coarse to fine manner. We denote the graph in layer L as G L . A larger L indicates a layer with more points, and layer 0 is the coarsest layer with the least points. The detailed graph construction process is depicted later in Section 3.2.</p><p>Here, we first introduce the constitution of edge branch and how it interacts with the point branch.</p><p>As shown in <ref type="figure">Fig. 2</ref>, for the point branch, we follow PointNet++ <ref type="bibr" target="#b11">[12]</ref> to create a hierarchical encoder-decoder structure with previous features in point encoder connected to the corresponding point decoder layers through skipconnection, thus passing detailed low-level information. The point cloud is downsampled and then upsampled in</p><formula xml:id="formula_1">! " ×$ " ! " ×3 |' "() |×* "() Edge Feature: ℍ ,-./ (a) Edge Module (1 "() , ' "() ) 4 "() Graph Construct Edge Upsample (1 " , ' " ) 4 " |' " |×* "() Edge Encoder |' " |×* " (1 " , ' " ) 4 "</formula><p>Edge Feature:</p><formula xml:id="formula_2">ℍ ,- Point Feature: 5 6-</formula><p>Edge Encoder for 7 <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> : 8 " Indexing for 7 <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> : 9 " ; 8,9</p><p>"()→" = &gt;?@&gt; :</p><formula xml:id="formula_3">ℝ C-×ℝ C-→ ℝ D - E (1 " , ' " ) 4 " ! " ×$ "</formula><p>Point Feature:  the process. Meanwhile, we construct an edge branch with consecutive edge modules, taking both features from the corresponding point module and the previous edge module.</p><p>The procedure is to extract edge features from the coarsest layer to grab high-level information with the largest receptive field, and progressively fuse point features from finer layers into edges, in parallel with the point decoding stage. Point features from the encoder layers are also used in the process, along with skip-connection to the corre-sponding decoder layers.</p><p>Although both abstract global features from the coarser layers and detailed information from finer layers are important, the most essential data for edge prediction is from the last layer with the most refined point features. With this consideration, edge features are encoded in a coarseto-fine manner, making point features in the finest layer fused at last. The hierarchical edge features are also fed to the corresponding point modules to provide additional contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Edge Module</head><p>At the decoding stage, for layer L, we denote the graph as G L = (V L , E L ) and the number of points as N L . The edge module accepts the L-layer point features F V L and (L − 1)-layer edge features H E L−1 as arguments and returns the edge features in layer L. As shown in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, the edge module is expressed as</p><formula xml:id="formula_4">H E L = M encoder (F V L , M upsample (H E L−1 )),<label>(1)</label></formula><p>where M encoder denotes the edge encoder and M upsample is the edge upsampling module, which maps edge features in graph G L−1 to graph G L . The graph construction and edge upsampling process will be described in Section 3.2.</p><p>For each edge e i,j = (p i , p j ) ∈ E L , its edge feature at layer L is written as</p><formula xml:id="formula_5">H L i,j = M encoder (F L i , F L j , H L−1→L i,j ),<label>(2)</label></formula><p>where F L i and F L j are the point features of p i and p j , respectively. H L−1→L i,j is the edge feature upsampled from layer L − 1 to layer L.</p><p>As illustrated in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, M encoder for a single edge can be expanded as</p><formula xml:id="formula_6">H L i,j = f (1) ext ([f (2) ext (f edge (F L i , F L j )), H L−1→L i,j ]),<label>(3)</label></formula><p>where [·, ·] represents concatenation. The feature extractor f ext : R n → R m can be any differentiable function. In our implementation, we apply MLP as f ext . The edge function f edge takes the two point features it connects as input and outputs a feature for the edge. We formulate f edge as</p><formula xml:id="formula_7">f edge (F L i , F L j ) = [(p j − p i ), F L j , F L i ],<label>(4)</label></formula><p>where [·, ·, ·] concatenates the three elements, and p i , p j here represent 3D point coordinates. The two point features are concatenated for completely preserving information of the two points. Also, we provide (p j − p i ) to indicate the relative position between the two points. Other implementations of f edge are discussed in the experiment part.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Incorporation of Edges in Point Prediction</head><p>For layer L, every point in graph G L links to other contextual points. So corresponding edges are expected to pass the contextual information back to the point. To this end, the edge features with respect to point p i are operated by max-pooling as a region guidance. Let E L (p i ) denote the set containing all edges starting from p i , the corresponding set of edge features is</p><formula xml:id="formula_8">H E L (pi) = {H L i,j |(p i , p j ) ∈ E L (p i )}.<label>(5)</label></formula><p>The point feature F L i is then updated by <ref type="figure" target="#fig_5">Fig. 4</ref> gives an illustration of the process. By incorporating edge information in point features, we enlarge the message passing range. The local region feature provided by the edges allows the point feature extractor to see farther in each layer. Additional contextual information including intrinsic geometry and semantic relation in the local region is incorporated in the region feature to benefit segmentation. We experiment with other schemes for message passing. Section 4.4 gives more discussions.</p><formula xml:id="formula_9">(F L i ) new = [F L i , MaxPool(H E L (pi) )].<label>(6)</label></formula><p>By helping feature extraction in the other branch, point and edge features become more powerful in final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Graph Construction</head><p>Instead of building graphs separately at each layer, we build the graph hierarchically, as shown in <ref type="figure">Fig. 5</ref>. By designing the "edge upsample" operation with each edge aware of associated edges in previous layer, we enlarge the receptive field and enable longer-range message passing for edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Graph Initialization</head><p>As shown in <ref type="figure">Fig. 5</ref>, the graph is initialized in the coarsest layer (layer 0). The initial graph G 0 is constructed by</p><formula xml:id="formula_10">! " ×3 Point Module ! %&amp;' ×3 Point Module ! % ×3 Point Module Layer 0 Layer ( −1 ⋯ ⋯ !×3 + " Graph Initial + %&amp;' Graph Construct + % Graph Construct ⋯ + Graph Construct ⋯ Final Graph</formula><p>Decoding Stage Layer ( <ref type="figure">Figure 5</ref>. Hierarchical Graph Construction. The graph is initialized in the coarsest layer and is progressively enlarged by considering both point coordinates in the current layer and the graph in previous layer.</p><p>connecting each point with its nearest k 0 points. Mathematically, G 0 = (V 0 , E 0 ) is formulated as</p><formula xml:id="formula_11">V0 = P0, E0 = {(pi, pj)| pi ∈ P0, pj ∈ N k 0 (pi)},<label>(7)</label></formula><p>where P 0 is the point set in layer 0, which is downsampled from the original point set with farthest point sampling (FPS) in encoding layers. N k0 (p i ) is the set of the k 0nearest neighbors of point p i , including itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hierarchical Architecture</head><p>Along with the decoding process of point features, we gradually enlarge the graph and enrich the edge features with more details. The process is illustrated in <ref type="figure">Fig. 5</ref>. </p><formula xml:id="formula_12">E L−1 ne (e i,j ) = {(p i , p j )| p i ∈ N L−1 k (p i ), p j ∈ N L−1 k (p j )}, where N L−1 k (p i ) ⊆ V L−1 is the k-nearest neighbors of p i ∈ V L in layer L − 1. p i is included in N L−1 k (p i ) if p i ∈ V L−1 .</formula><p>We then check whether edges in E L−1 ne (e i,j ) exist in E L−1 -the edge set of G L−1 . If edge e i,j connects two distant points, for which even in the coarser layer L−1 there is no connection between the two corresponding regions, we do not take the edge into consideration in layer L. Hence, if</p><formula xml:id="formula_13">E L−1 ne (e i,j ) ∩ E L−1 = Ø, edge e i,j is discarded from E (0)</formula><p>L . Following this principle, the final graph G L = (V L , E L ) has an edge set of</p><formula xml:id="formula_14">E L = pi∈V L E L (p i ),</formula><p>where E L (p i ) (edges starting from p i ) is expressed as</p><formula xml:id="formula_15">E L (p i ) = {(p i , p j )|p j ∈ N k L (p i ), E L−1</formula><p>ne (e i,j )∩E L−1 = Ø}. Note that at least e i,i is reserved in E L (p i ) in some extreme cases. Edge Upsampling In PointNet++ <ref type="bibr" target="#b11">[12]</ref>, point feature of p i in layer L is propagated from layer L − 1 by interpolating feature values of its k nearest neighbors in layer L − 1 as</p><formula xml:id="formula_16">F L−1→L i = f p interp ({F L−1 j | p j ∈ N L−1 k (p i )}).<label>(8)</label></formula><p>We similarly propagate edge features in layer L − 1 to layer L as</p><formula xml:id="formula_17">H L−1→L i,j = f e interp ({H L−1 i ,j | (p i , p j ) ∈ E L−1 ne (e i,j )∩E L−1 })</formula><p>. A demonstration is given in <ref type="figure" target="#fig_7">Fig. 6</ref>.</p><p>The interpolation weights are based on the inverse distance of the two pairs of end points. For H L−1 i ,j , the weight is formulated as</p><formula xml:id="formula_18">w i ,j = 1 ( p i − p i t + ) · ( p j − p j t + ) ,<label>(9)</label></formula><p>where p i , p j ∈ V L−1 , p i , p j ∈ V L represent point coordinates, = 1e − 8 and t is set to 2. The weights are then normalized as</p><formula xml:id="formula_19">w n i ,j = w i ,j (p i ,p j )∈E L−1 ne (ei,j )∩E L−1 w i ,j .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>We optimize the point and edge branches jointly with the combined loss on the two branches as</p><formula xml:id="formula_20">L = λ 1 L point + λ 2 L edge ,<label>(11)</label></formula><p>where λ 1 and λ 2 adjust the ratio of the two losses.</p><p>Point Loss The final point features are followed by an MLP to produce point-wise semantic predictions. We further use the final edge predictions as weights to aggregate point scores and get refined point predictions. Cross entropy loss is applied to constrain the point predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Loss</head><p>The edge features in the final graph G are regularized by the edge labels, which represent whether the two-end points of the edge are in the same category or not. The label for edge e i,j = (p i , p j ) ∈ E is set as</p><formula xml:id="formula_21">l e i,j = 1, if l p i = l p j 0, if l p i = l p j .<label>(12)</label></formula><p>where l p i and l p j are the point semantic labels of p i and p j . An MLP is adopted to produce the per-edge prediction. Binary cross entropy loss is chosen for the edge loss as L edge = − 1 |E| e i,j ∈E (l e i,j log(pred e i,j )+α(1−l e i,j ) log(1−pred e i,j )),</p><p>where pred e i,j is the edge prediction for e i,j , and α balances the two kinds of edges, as there are more intra-class edges than inter-class ones considering the local neighborhood.</p><p>The final edge feature for each edge can be deemed as a function on features of the two regions centered at the two-end points. Information from different layers are taken into account. More details are preserved by encoding at last. Hence, the edge loss guides the edge encoder to seek difference between the intra-and inter-class feature pairs, and implicitly serves as auxiliary supervision for point features. It increases the discrimination power among point features in different categories. Also, with the edge supervision, more exact contextual information is passed to points via edges to enhance point features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted experiments on two representative and challenging large-scale scene labeling datasets, i.e., S3DIS <ref type="bibr" target="#b0">[1]</ref> and ScanNet v2 <ref type="bibr" target="#b1">[2]</ref>, with ablation analysis presented on the ScanNet v2 val set and S3DIS Area 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The point branch contains an encoder with four downsampling layers and a decoder with four upsampling layers. The numbers of points, N 0 , N 1 , N 2 , N 3 , N 4 = N , in the decoder are 16, 64, 256, 1,024, and 4,096, respectively. The edge branch has five blocks with k (number of nearest neighbors) set to 4, 6, 10, 14, 16 from layer 0 to 4. k is chosen as 3 for point and edge feature interpolation.</p><p>The whole network was trained in an end-to-end manner using the SGD optimizer with batch size 16 and base learning rate 0.05. For S3DIS, we train the network for 100 epochs and decay the rate by 0.1 for every 25 epochs. For ScanNet, we train the network for 120 epochs and decay the rate by 0.1 for every 30 epochs. The momentum and weight decay are set to 0.9 and 0.0001 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>S3DIS The dataset <ref type="bibr" target="#b0">[1]</ref> has 6 areas with a total of 271 rooms. Each room is provided as points with RGB information. Each point has a semantic label from 13 categories of floor, window, door, etc. In each training iteration, we randomly sample blocks in the training areas, with 4,096 points randomly selected per block. We set the block size as 0.8m × 0.8m with 0.1m padding. Also, we represent each point as a 9D vector with XY Z, RGB, and normalized position in room. All points in the test areas are used in evaluation. Two settings are adopted <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>: (i) splitting Area 5 as the test set and using others for training; and (ii) adopting 6-fold cross validation, with each of the 6 areas taking as the test set once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ScanNet v2</head><p>The dataset has 1,613 scans with a train/validation/test split of 1,201/312/100. Excluding the 'unannotated' points, each point in the scans has a label from 20 categories of wall, shower curtain, etc. To prepare the input data, we follow previous work <ref type="bibr" target="#b11">[12]</ref> to randomly sample blocks in rooms and sample 4,096 points per block. Again, we use 0.8m × 0.8m block size and 0.1m padding. Here, each input point feature is a 6D vector (XY Z &amp; RGB). We evaluated on both the validation and test sets. Since the semantic annotation for the test sets is not publicly available, we submitted our predictions to the official server to obtain the evaluation results.</p><p>Evaluation Metric It includes the class-wise mean of intersection over union (mIoU), class-wise mean of accuracy (mAcc) and point-wise overall accuracy (OA). <ref type="table">Table 1</ref> lists quantitative results of different methods on S3DIS Area 5. Compared to previous approaches, ours yields the highest scores in terms of all the three metrics. Specifically, our model yields mIoU 61.85%, exceeding the former best by 3.58%. <ref type="table">Table 2</ref>   <ref type="table">Table 1</ref>. Semantic segmentation results evaluated on S3DIS Area 5. Most methods do not perform well on the "beam" category, which has few points (0.029%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground Truth Ours <ref type="figure">Figure 7</ref>. Visualization of the semantic segmentation results on the S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>OA mAcc mIoU PointNet <ref type="bibr" target="#b9">[10]</ref> 78 among different architectures on 6-fold cross validation. Ours also reaches the first place for all the three items. <ref type="table" target="#tab_2">Table 3</ref> lists results of our framework and other pointbased methods on ScanNet v2 test set. All methods use only point clouds with RGB color as input without voxelization. Our approach outperforms others by a large margin: 6.2% higher in absolute mIoU and 11.2% better relatively. Visual results are shown in Figs. 7 and 8. Our method segments objects even in complex scenes. It is notable that several detailed structures are classified and segmented from the surroundings, manifesting the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>For ScanNet v2, the models are trained on training set and evaluated on validation set. For S3DIS, the models are Method mIoU PointNet++ <ref type="bibr" target="#b11">[12]</ref> 33.9 SPLATNet <ref type="bibr" target="#b16">[17]</ref> 39.3 PointCNN <ref type="bibr" target="#b6">[7]</ref> 45.8 PointConv <ref type="bibr" target="#b26">[27]</ref> 55.6 Our Method 61.8   Message Passing by Edges Besides the approach described in Section 3.1.2, we also experimented with another scheme which is inspired by graph convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, where the edge features are further encoded to form weights for the linked points. The point features are then updated as a weighted sum of the adjacent point features. We denote this scheme as adaptive aggregation (AdaAggre) and test the two settings, with and without softmax, for the weights. <ref type="table">Table 5</ref> lists the experimental results on ScanNet v2 validation set.</p><formula xml:id="formula_23">f edge (F L i , F L j ) = [(p j − p i ), F L j , (F L j − F L i )]. (14)</formula><p>The performance gain for the graph-convolution-style methods is lower than max-pooling followed by concatenation. It may be because during the point decoding, it is not very helpful to mix point features in each local neighborhood. Instead, the combined contextual feature reveals the relation of a point with its neighborhood. It can better preserve the point's own distinctiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Graph Construction and Edge Upsampling</head><p>We build connection between edge features of adjacent layers by "edge upsample". We also experimented on ScanNet dataset with removing hierarchical graph construction and building the graph of each layer separately without edge upsampling.</p><p>The mIoU/mAcc/OA (%) results are 57.01/66.52/83.57 respectively, much lower than our full framework with 63.36/72.61/86.13. The connected edge branch optimally incorporates the point features in different layers, enabling effective learning for the edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have designed a hierarchical point-edge interaction network, in which an edge branch is proposed to work with the encoder-decoder point branch for point cloud semantic segmentation. The proposed hierarchical graph framework enables the edge branch to progressively integrate differentlayer point features. Also, the generated edge features are incorporated into the point branch to provide contextual information. The final edge features are supervised by the semantic consistency of related points to implicitly regularize the point features. All these steps make semantic relationship with local context well utilized via edges.</p><p>With the high-quality point prediction results and generality of the framework applicable to different datasets, we believe the proposed method will broadly benefit 3D understanding in the community. In the future, we will explore multi-range edge construction to gather both closerange and long-distance contextual information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Simple illustration of our framework. The point and edge branches work together to predict the semantic labels. Selfconnected edges and edge directions are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Edge Encoder for a Single Edge : Concatenation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Architecture of the Edge Module. (b) Edge Encoder block in (a). KL and CL represent the channel numbers in edge and point features in layer L, respectively. For simplicity, we only illustrate the edge encoding process for a single edge in (b). Edge features for all the edges in EL constitute HE L .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the Point Module. KL denotes the channel number of the L-layer edge features, while CL denotes the channel number of the L-layer point feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>GraphL</head><label></label><figDesc>Construction of Layer L Consider two adjacent layers L−1 and L with vertices V L−1 and V L as the point set in that layer, respectively. The graph G L is constructed by first finding the k L nearest neighbors for each point in V L . Let G ) denote such initial L-layer graph. For each edge e i,j = (p i , p j ) ∈ E (0) L , we consider the set consisting of possible neighboring edges in layer L − 1 as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>"#$ = "#$ , "#$ Points in Layer − 1 Two points selected from Layer Edge -,. in Layer Edges in Layer − 1 for interpolating feature of -,. : 23 "#$ ( -,. ) ∩ "#$ Indicating the NN of -, . in layer − 1 ( Demonstration of edge upsampling. Points in layer L−1 (blue ones) also exist in layer L. Self-connected edges are omitted. For edge ei,j in layer L, we propagate edge features in layer L − 1 by finding its neighboring edges in EL−1 and interpolating features of these edges. Red arrows represent edges in GL−1 for interpolation, which denote intersection of EL−1 (blue arrows) and E L−1 ne (ei,j) (yellow arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>shows the comparison Methods OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter PointNet [10] -48.98 41.09 88.80 97.33 69.80 0.05 3.92 46.26 10.76 58.93 52.61 5.85 40.28 26.38 33.22 SegCloud [20] -57.35 48.92 90.06 96.05 69.86 0.00 18.37 38.35 23.12 70.40 75.89 40.88 58.42 12.96 41.60 PointCNN [7] 85.91 63.86 57.26 92.31 98.24 79.41 0.00 17.60 22.77 62.09 74.39 80.59 31.67 66.67 62.05 56.74 SPGraph [6] 86.38 66.50 58.04 89.35 96.87 78.12 0.00 42.81 48.93 61.58 84.66 75.41 69.84 52.60 2.10 52.22 PCCN [24] -67.01 58.27 92.26 96.20 75.89 0.27 5.98 69.49 63.45 66.87 65.63 47.28 68.91 59.10 46.22 Our Method 87.18 68.30 61.85 91.47 98.16 81.38 0.00 23.34 65.30 40.02 75.46 87.70 58.45 67.78 65.61 49.36</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Semantic segmentation results on ScanNet v2 test set. trained on Areas 1-4 &amp; 6 and evaluated on Area 5.</figDesc><table><row><cell>Edge Function We explore different ways of incorpo-</cell></row><row><cell>rating point information into edges, including Subtraction,</cell></row><row><cell>Summation, Hadamard product, 'ConcatSub', and Concate-</cell></row><row><cell>nation. Here 'ConcatSub' is defined as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>shows comparison of the results. Overall, concatenation yields the best result due to preservation of most point information. Summation, Subtraction, and Hadamard Product all cause information loss in the level of point features. 'ConcatSub' achieves similar performance with Concatenation, since the two-point features can be restored in this type of operations. Hadamard Product 59.07 / 58.79 68.02 / 65.27 85.31 / 86.16 ConcatSub 63.09 / 59.37 71.82 / 66.19 86.12 / 86.53 Concatenation 63.36 / 61.85 72.61 / 68.30 86.13 / 87.18</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Figure 8. Visualization of the semantic segmentation results on the ScanNet v2 dataset.</cell></row><row><cell>Methods</cell><cell>mIoU</cell><cell>mAcc</cell><cell>OA</cell></row><row><cell>Subtraction</cell><cell cols="3">58.31 / 58.85 67.95 / 65.66 84.02 / 86.44</cell></row><row><cell>Summation</cell><cell cols="3">57.86 / 58.96 67.25 / 65.87 83.69 / 86.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Ablation study results for edge function f edge on ScanNet v2 and S3DIS. The results are shown in format of ScanNet v2 / S3DIS. The ablation on two datasets share similar observation. Ablation results for message passing by edges.</figDesc><table><row><cell>Methods</cell><cell>mIoU mAcc OA</cell></row><row><cell>AdaAggre (w. softmax)</cell><cell>56.44 66.17 83.06</cell></row><row><cell cols="2">AdaAggre (w.o. softmax) 55.01 64.12 82.67</cell></row><row><cell>MaxPool + Concat</cell><cell>63.36 72.61 86.13</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on Xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Hao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04952</idno>
		<title level="m">Convolutional neural networks on 3D surfaces using parallel frames</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PointRCNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SEGCloud: Semantic segmentation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PointConv: Deep convolutional networks on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointWeb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
