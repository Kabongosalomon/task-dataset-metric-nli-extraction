<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-Aware Trident Networks for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
							<email>chenyuntao2016@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<email>zhaoxiang.zhang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scale-Aware Trident Networks for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP.</p><p>Codes are available at https://git.io/fj5vR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref> have achieved great success in object detection. Typically, these CNN-based methods can be roughly divided into two types: one-stage methods such as YOLO <ref type="bibr" target="#b31">[32]</ref> or SSD <ref type="bibr" target="#b28">[29]</ref> which directly utilizes feedforward CNN to predict the bounding boxes of interest, while two-stage methods such as Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> or R-FCN <ref type="bibr" target="#b9">[10]</ref> first generate proposals, and then exploit the extracted region features from CNN for further refinement. However, a central issue for both methods is the handling of scale variation. The scales of object instances could vary in a wide range, which impedes the detectors, especially those very small or very large ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Equal Contribution</head><p>To remedy the large scale variation, an intuitive way is to leverage multi-scale image pyramids <ref type="bibr" target="#b0">[1]</ref>, which is popular in both hand-crafted feature based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> and current deep CNN based methods <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). Strong evidence <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref> shows that deep detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b9">10]</ref> could benefit from multi-scale training and testing. To avoid training objects with extreme scales (small/large objects in smaller/larger scales), SNIP <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> proposes a scale normalization method that selectively trains the objects of appropriate sizes in each image scale. Nevertheless, the increase of inference time makes the image pyramid methods less favorable for practical applications.</p><p>Another line of efforts aims to employ in-network feature pyramids to approximate image pyramids with less computation cost. The idea is first demonstrated in <ref type="bibr" target="#b12">[13]</ref>, where a fast feature pyramid is constructed for object detection by interpolating some feature channels from nearby scale levels. In the deep learning era, the approximation is even easier. SSD <ref type="bibr" target="#b28">[29]</ref> utilizes multi-scale feature maps from different layers and detects objects of different scales at each feature layer. To compensate for the absence of semantics in low-level features, FPN <ref type="bibr" target="#b24">[25]</ref>  <ref type="figure" target="#fig_0">(Figure 1(b)</ref>) further augments a top-down pathway and lateral connections to incorporate strong semantic information in high-level features. However, region features of objects with different scales are extracted from different levels of FPN backbone, which in turn are generated with different sets of parameters. This makes feature pyramids an unsatisfactory alternative for image pyramids.</p><p>Both the image pyramid and the feature pyramid methods share the same motivation that models should have different receptive fields for objects of different scales. Despite the inefficiency, the image pyramid fully utilizes the representational power of the model to transform objects of all scales equally. In contrast, the feature pyramid generates multi-level features thus sacrificing the feature consistency across different scales. This leads to a decrease in effective training data and a higher risk of overfitting for each scale. The goal of this work is to get the best of two worlds by creating features with a uniform representational power for all scales efficiently. In this paper, instead of feeding in multi-scale inputs like the image pyramid, we propose a novel network structure to adapt the network for different scales. In particular, we create multiple scale-specific feature maps with the proposed trident blocks as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(c). With the help of dilated convolutions <ref type="bibr" target="#b40">[41]</ref>, different branches of trident blocks have the same network structure and share the same parameters yet have different receptive fields. Furthermore, to avoid training objects with extreme scales, we leverage a scale-aware training scheme to make each branch specific to a given scale range matching its receptive field. Finally, thanks to weight sharing through the whole multibranch network, we could approximate the full TridentNet with only one major branch during inference. This approximation only brings marginal performance degradation. As a result, it could achieve significant improvement over the single-scale baseline without any compromise of inference speed. This property makes TridentNet more desirable over other methods for practical uses.</p><p>To summarize, our contributions are listed as follows:</p><p>• We present our investigation results about the effect of the receptive field in scale variation. To our best knowledge, we are the first to design controlled experiments to explore the receptive field on the object detection task.</p><p>• We propose a novel Trident Network to deal with scale variation problem for object detection. Through multibranch structure and scale-aware training, TridentNet could generate scale-specific feature maps with a uniform representational power.</p><p>• We propose a fast approximation, TridentNet Fast, with only one major branch via our weight-sharing trident-block design, thus introducing no additional parameters and computational cost during inference.</p><p>• We validate the effectiveness of our approach on the standard COCO benchmark with thorough ablation studies. Compared with the state-of-the-art methods, our proposed method achieves an mAP of 48.4 using a single model with ResNet-101 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Object Detectors. Deep learning based object detection methods have shown dramatic improvements in both accuracy and speed recently. As one of the predominant detectors, two-stage detection methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref> first generate a set of region proposals and then refine them by CNN networks. In <ref type="bibr" target="#b15">[16]</ref>, R-CNN generates region proposals by Selective Search <ref type="bibr" target="#b39">[40]</ref> and then classifies and refines the cropped proposal regions from the original image by a standard CNN independently and sequentially. To reduce the redundant computation of feature extraction in R-CNN, SPPNet <ref type="bibr" target="#b17">[18]</ref> and Fast R-CNN <ref type="bibr" target="#b14">[15]</ref> extract the feature of the whole image once, and then generate region features through spatial pyramid pooling and RoIPooling layers, respectively. The RoIPooling layer is further improved by RoIAlign layer <ref type="bibr" target="#b16">[17]</ref> to address the coarse spatial quantization problem. Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> first proposes a unified end-to-end framework for object detection. It introduces a region proposal network (RPN) which shares the same backbone network with the detection network to replace the original standalone time-consuming region proposal methods. To further improve the efficiency of Faster R-CNN, R-FCN <ref type="bibr" target="#b9">[10]</ref> constructs position-sensitive score maps through a fully convolutional network to avoid the RoI-wise head network. To avoid additional large score maps in R-FCN, Light-Head R-CNN <ref type="bibr" target="#b22">[23]</ref> uses a thin feature map and a cheap R-CNN subnet to build a two-stage detector more efficiently.</p><p>On the other hand, one-stage methods which are popularized by YOLO <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and SSD <ref type="bibr" target="#b28">[29]</ref>, aim to be more efficient by directly classifying the pre-defined anchors and further refining them using CNNs without the proposal generation step. Based on the multi-layer prediction module in SSD, DSSD <ref type="bibr" target="#b13">[14]</ref> introduces additional context informa-tion with deconvolutional operators to improve the accuracy. RetinaNet <ref type="bibr" target="#b25">[26]</ref> proposes a new focal loss to address the extreme foreground-background class imbalance which stands out as a central issue in one-stage detectors. Inheriting the merits of two-stage approaches, RefineDet <ref type="bibr" target="#b41">[42]</ref> proposes an anchor refinement module to first filter the negative anchor boxes and coarsely adjust the anchor boxes for the next detection module.</p><p>Methods for handling scale variation. As the most challenging problem in object detection, large scale variation across object instances hampers the accuracy of detectors. The multi-scale image pyramid <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref> is a common scheme to improve the detection methods, especially for objects of small and large scales. Based on the image pyramid strategy, SNIP <ref type="bibr" target="#b37">[38]</ref> proposes a scale normalization method to train objects that fall into the desired scale range for each resolution during multi-scale training. To perform multi-scale training more efficiently, SNIPER <ref type="bibr" target="#b38">[39]</ref> only selects context regions around the ground-truth instances and sampled background regions for each scale during training. However, SNIP and SNIPER still suffer from the inevitable increase of inference time.</p><p>Instead of taking multiple images as input, some methods utilize multi-level features of different spatial resolutions to alleviate scale variation. Methods like Hyper-Net <ref type="bibr" target="#b21">[22]</ref> and ION <ref type="bibr" target="#b1">[2]</ref> concatenate low-level and high-level features from different layers to generate better feature maps for prediction. Since the features from different layers usually have different resolutions, specific normalization or transformation operators need to be designed before fusing multi-level features. Instead, SSD <ref type="bibr" target="#b28">[29]</ref> and MS-CNN <ref type="bibr" target="#b3">[4]</ref> perform object detection at multiple layers for objects of different scales without feature fusion. TDM <ref type="bibr" target="#b36">[37]</ref> and FPN <ref type="bibr" target="#b24">[25]</ref> further introduce a top-down pathway and lateral connections to enhance the semantic representation of low-level features at bottom layers. PANet <ref type="bibr" target="#b27">[28]</ref> enhances the feature hierarchies in FPN by additional bottom-up path augmentation and proposes adaptive feature pooling to aggregate features from all levels for better prediction. Rather than using features from different layers, our proposed Tri-dentNet generates scale-specific features through multiple parallel branches, thus endowing our network the same representational power for objects of different scales.</p><p>Dilated convolution. Dilated convolution <ref type="bibr" target="#b40">[41]</ref> (aka Atrous convolution <ref type="bibr" target="#b19">[20]</ref>) enlarges the convolution kernel with original weights by performing convolution at sparsely sampled locations, thus increasing the receptive field size without additional cost. Dilated convolution has been widely used in semantic segmentation to incorporate largescale context information <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7]</ref>. In the object detection field, DetNet <ref type="bibr" target="#b23">[24]</ref> designs a specific detection back-bone network to maintain the spatial resolution and enlarge the receptive field using dilated convolution. Deformable convolution <ref type="bibr" target="#b10">[11]</ref> further generalizes dilated convolution by learning the sampling location adaptively. In our work, we employ dilated convolution in our multi-branch architecture with different dilation rates to adapt the receptive fields for objects of different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Investigation of Receptive Field</head><p>There are several design factors of the backbone network that may affect the performance of object detectors including downsample rate, network depth, and the receptive field. Several works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> have already discussed their impacts. The effects of the first two factors are straightforward: deeper network with low downsample rate may increase the complexity, but benefit the detection task in general. Nevertheless, as far as we know, there is no previous work that studies the impact of the receptive field in isolation.</p><p>To investigate the effect of the receptive field on the detection of objects with different scales, we replace some convolutions in the backbone network with their dilated variants <ref type="bibr" target="#b40">[41]</ref>. We use different dilation rates to control the receptive field of a network.</p><p>Dilated convolution with a dilation rate d s inserts d s − 1 zeros between consecutive filter values, enlarging the kernel size without increasing the number of parameters and computations. Specifically, a dilated 3×3 convolution could have the same receptive field as the convolution with the kernel size of 3 + 2(d s − 1). Suppose the total stride of current feature map is s , then a dilated convolution of rate d s could increase the receptive field of the network by 2(d s −1)s. Thus if we modify n conv layers with d s dilation rate, the receptive field could be increased by 2(d s − 1)sn.</p><p>We conduct our pilot experiment using a Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> detector with the ResNet-C4 backbone on the COCO <ref type="bibr" target="#b26">[27]</ref> dataset. The results are reported in the COCOstyle mmAP on all objects and objects of small, medium and large sizes, respectively. We use ResNet-50 and ResNet-101 as the backbone networks and vary the dilation rate d s of the 3×3 convolutions from 1 to 3 for the residual blocks in the conv 4 stage. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results. We can find that as the receptive field increases (larger dilation rate), the performance of the detector on small objects drops consistently on both ResNet-50 and ResNet-101. While for large objects, the detector benefits from the increasing receptive fields. The above findings suggest that:</p><p>1. The performance on objects of different scales are influenced by the receptive field of a network. The most suitable receptive field is strongly correlated with the scale of objects.  2. Although ResNet-101 has a large enough theoretical receptive field to cover large objects (greater than 96×96 resolution) in COCO, the performance of large objects could still be improved when enlarging the dilation rate. This finding shares the same spirit in <ref type="bibr" target="#b30">[31]</ref> that the effective receptive field is smaller than the theoretical receptive field. We hypothesize that the effective receptive field of detection networks needs to balance between small and large objects. Increasing dilation rates enlarges the effective receptive field by emphasizing large objects, thus compromising the performance of small objects.</p><p>The aforementioned experiments motivate us to adapt the receptive field for objects of different scales as detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Trident Network</head><p>In this section, we describe our scale-aware Trident Network (TridentNet) for object detection. The proposed Tri-dentNet consists of weight sharing trident blocks and a deliberately designed scale-aware training scheme. Finally, We also present the inference details of TridentNet, including a fast inference approximation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Structure</head><p>Our goal is to inherit the merits of different receptive field sizes and avoid their drawbacks for detection networks. We propose a novel Trident architecture for this goal as shown in <ref type="figure">Figure 2</ref>. In particular, our method takes a singlescale image as input, and then creates scale-specific feature maps through parallel branches where convolutions share the same parameters but with different dilation rates.</p><p>Multi-branch Block We construct TridentNets by replacing some convolution blocks with the proposed trident blocks in the backbone network of a detector. A trident block consists of multiple parallel branches in which each shares the same structure with the original convolution block except the dilation rate.</p><p>Taking ResNet as an example, for a single residual block in the bottleneck style <ref type="bibr" target="#b18">[19]</ref>, which consists of three convolutions with kernel size 1×1, 3×3 and 1×1, a corresponding trident block is constructed as multiple parallel residual blocks with different dilation rates for the 3×3 convs, as shown in <ref type="figure">Figure 3</ref>. Stacking trident blocks allows us to control receptive fields of different branches in an efficient way similar to the pilot experiment in Section 3. Typically, we replace the blocks in the last stage of the backbone network with trident blocks since larger strides lead to a larger difference in receptive fields as needed. Detailed design choices could be referred in Section 5.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output1</head><p>Output2 Output3</p><p>x N <ref type="figure">Figure 3</ref>: A trident block constructed from a bottleneck residual block.</p><p>Weight sharing among branches. An immediate problem of our multi-branch trident block is that it introduces several times parameters which may potentially incur overfitting. Fortunately, different branches share the same structure (except dilation rates) and thus make weight sharing straightforward. In this work, we share the weights of all branches and their associated RPN and R-CNN heads, and only vary the dilation rate of each branch. The advantages of weight sharing are three-fold. It reduces the number of parameters and makes TridentNet need no extra parameters compared with the original detector. It also echoes with our motivation that objects of different scales should go through a uniform transformation with the same representational power. A final point is that transformation parameters could be trained on more object samples from all branches. In other words, the same parameters are trained for different scale ranges under different receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scale-aware Training Scheme</head><p>The proposed TridentNet architecture generates scalespecific feature maps according to the pre-defined dilation rates. However, the degradation in <ref type="table" target="#tab_1">Table 1</ref> caused by scale mismatching (e.g. small objects on the branch with too large dilation) still exists for every single branch. Thus, it is natural to detect objects of different scales on different branches. Here, we propose a scale-aware training scheme to improve the scale awareness of every branch and avoid training objects of extreme scales on mismatched branches. Similar to SNIP <ref type="bibr" target="#b37">[38]</ref>, we define a valid range [l i , u i ] for each branch i. During training, we only select the proposals and ground truth boxes whose scales fall in the corresponding valid range of each branch. Specifically, for an Region-of-Interest (RoI) with width w and height h on the input image(before resize), it is valid for branch i when:</p><formula xml:id="formula_0">l i ≤ √ wh ≤ u i .<label>(1)</label></formula><p>This scale-aware training scheme could be applied on both RPN and R-CNN. For RPN, we select ground truth boxes which are valid for each branch according to Eq. 1 during anchor label assignment. Similarly, we remove all invalid proposals for each branch during the training of R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference and Approximation</head><p>During inference, we generate detection results for all branches and then filter out the boxes which fall outside the valid range of each branch. We then use NMS or soft-NMS <ref type="bibr" target="#b2">[3]</ref> to combine the detection outputs of multiple branches and obtain the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast Inference Approximation A major drawback of</head><p>TridentNet is the slow inference speed due to its branching nature. Here we propose TridentNet Fast, a fast approximation of TridentNet with only one branch during inference. For a three-branch network as in <ref type="figure">Figure 2</ref>, we use the middle branch for inference since its valid range covers both large and small objects. In this way, TridentNet Fast incurs no additional time cost compared with a standard Faster R-CNN detector. Surprisingly, we find that this approximation only exhibits a slight performance drop compared with the original TridentNet. This may due to our weight-sharing strategy, through which multi-branch training is equivalent to within-network scale augmentation. Detailed ablation of TridentNet Fast could be found in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct experiments on the COCO dataset <ref type="bibr" target="#b26">[27]</ref>. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>, we train models on the union of 80k training images and 35k subset of validation images (trainval35k), and evaluate on a set of 5k validation images (minival). We also report the final results on a set of 20k test images (test-dev). We first describe the implementation details of TridentNets and training settings in Section 5.1. We then conduct thorough ablation experiments to validate the proposed method in Section 5.2. Finally, Section 5.4 compares the results of TridentNets with state-of-the-art methods on the test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We re-implement Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> as our baseline method in MXNet <ref type="bibr" target="#b7">[8]</ref>. Following other standard detectors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref>, the network backbones are pre-trained on the ImageNet <ref type="bibr" target="#b35">[36]</ref>. The stem, the first residual stage, and all BN parameters are freezeed. The input images are resized to a short side of 800. Random horizontal flip is adopted during training. By default, models are trained in a batch size of  For the evaluation, we report the standard COCO evaluation metric of Average Precision (AP) <ref type="bibr" target="#b26">[27]</ref> as well as AP 50 /AP 75 . We also report COCO-style AP s , AP m and AP l on objects of small (less than 32×32), medium (from 32×32 to 96×96) and large (greater than 96×96) sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>Components of TridentNet. First, we analyze the importance of each component in TridentNet. The baseline methods (Table 2(a)) are evaluated on ResNet-101 and ResNet-101-Deformable <ref type="bibr" target="#b10">[11]</ref> backbones. Then we gradually apply our multi-branch architecture, weight sharing design, and scale-aware training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Multi-branch. Based on the pilot experiment, Table 2(b) evaluates a straightforward way to get the best of multiple receptive fields. This multi-branch variant improves over the baselines on AP for both ResNet-101 (from 37.9 to 39.0) and ResNet-101-Deformable (from 39.9 to 40.5), especially for large objects (2.3/1.2 increase). This indicates that even the simplest multi-branch design could benefit from different receptive fields.</p><p>2. Scale-aware.  <ref type="figure">2(b)</ref>). It brings additional improvements (0.8/1.0 increase on ResNet-101/ResNet-101-Deformable) for small objects but drops in performance for large objects. We conjecture that the scaleaware training design prevents each branch from training objects of extreme scales, but may also bring about the over-fitting problem in each branch caused by reduced effective samples.</p><p>3. Weight-sharing. By applying weight sharing on multi-branch (Table 2(c)) and TridentNet (Table 2(e)), we could achieve consistent improvements on both two base networks. This demonstrates the effectiveness of weight-sharing. It reduces the number of parameters and improves the performance of detectors. With the help of weight-sharing (Table 2(e)), all branches share the same parameters which are fully trained on objects of all scales, thus alleviating the over-fitting issue in scale-aware training <ref type="figure">(Table 2(d)</ref>).</p><p>Finally, TridentNets achieve significant improvements (2.7/1.9 AP increase) on the two base networks. It also reveals that the proposed TridentNet structure is compatible with methods like deformable convolution <ref type="bibr" target="#b10">[11]</ref> which could adjust receptive field adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of branches.</head><p>We study the choice of the number of branches in TridentNets. <ref type="table" target="#tab_6">Table 3</ref> shows the results using one to four branches. Note that we do not add scaleaware training scheme here to avoid elaborately tuning valid ranges for different numbers of branches. The results in <ref type="table" target="#tab_6">Table 3</ref>    Stage of Trident blocks. We conduct ablation study on TridentNet to find the best stage to place trident blocks in ResNet. <ref type="table" target="#tab_7">Table 4</ref> shows the results of applying trident blocks in conv 2 , conv 3 and conv 4 stages, respectively. The corresponding total strides are 4, 8 and 16. Comparing with conv 4 stage, TridentNets on conv 2 and conv 3 stages achieve minor increase over the baseline. This is because the strides in conv 2 and conv 3 feature maps are not large enough to widen the discrepancy of receptive fields between three branches.</p><p>Number of trident blocks. As the conv 4 stage in ResNet has multiple residual blocks, we also conduct ablation study to explore how many trident blocks are needed for Trident-Net. Here we replace different numbers of residual blocks with trident blocks on conv 4 on ResNet-101. The results in <ref type="figure" target="#fig_2">Figure 4</ref> show that when the number of trident blocks grows beyond 10, the performance of TridentNet becomes stable. This indicates the robustness of TridentNet with respect to the number of trident blocks, when the discrepancy of receptive fields between branches is large enough.</p><p>Performance of each branch. In this section, we investigate the performance of each branch of our multi-branch TridentNet. We evaluate the performance of each branch independently without removing the detections out of the scale-aware range. <ref type="table" target="#tab_8">Table 5</ref> shows the results of every single branch and three branches combined. As expected, through   scale-aware training, branch-1 with the smallest receptive field achieves good results on small objects, branch-2 works well on objects of the medium scale while branch-3 with the largest receptive field is good at large objects. Finally, the three-branch method inherits the merits from three single branches and achieves the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fast Inference Approximation</head><p>To reduce the inference time of TridentNet, we propose TridentNet Fast which uses a single major branch to approximate the three-branch results during inference. As indicated in <ref type="table" target="#tab_8">Table 5</ref>, branch-2 emerges as a natural candidate for the major branch as its scale-aware range covers most objects. We investigate the effect of scale-aware ranges for scale-aware training in <ref type="table" target="#tab_9">Table 6</ref>. As shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with State-of-the-Arts</head><p>In this section, we evaluate TridentNet on COCO testdev set and compare with other state-of-the-art methods. Here we report the results of our methods under different settings in <ref type="table" target="#tab_11">Table 7</ref>.</p><p>TridentNet, which is to directly apply our method on Faster R-CNN with ResNet-101 backbone in the 2× training scheme, achieves 42.7 AP without bells and whistles.</p><p>To fairly compare with SNIP and SNIPER, we apply multi-scale training, soft-NMS, deformable convolutions, large-batch BN, and the 3× training scheme on Trident-Net and get TridentNet*. It gives an AP of 46.8, already surpassing image pyramid based SNIP and SNIPER in the single-scale testing setting. If we adopt the image pyramid for testing, it further improves the result of TridentNet* to 48.4 AP. To our best knowledge, for single models with ResNet-101 backbone, our result is the best entry among state-of-the-art methods. Furthermore, TridentNet* Fast + Image Pyramid achieves 47.6 AP.</p><p>Compare with other scale handling methods. In this section, we compare TridentNet with other popular scale handling methods like FPN <ref type="bibr" target="#b24">[25]</ref> and ASPP <ref type="bibr" target="#b6">[7]</ref>. FPN is the de facto model for handling scale variation in detection. ASPP is a special case of TridentNet with only one trident block and dilation rates of three branches set to <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18)</ref>, followed by a feature fusion operator. To fairly compare with FPN, we adopt a 2fc head instead of a conv5 head for models in this section.  nificantly over other methods on all scales. It shows the effectiveness of scale specific feature maps generated by TridentNet with the same set of parameters. Furthermore, TridentNet Fast achieves 41.0 AP which improves 1.2 AP over the baseline with no computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present a simple object detection method called Trident Network to build in-network scalespecific feature maps with the uniform representational power. A scale-aware training scheme is adopted for our multi-branch architecture to equip each branch with the specialized ability for corresponding scales. The fast inference method with the major branch makes TridentNet achieve significant improvements over baseline methods without any extra parameters and computations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Using multiple images of several scales as input, the image pyramid methods perform feature extraction and object detection independently for each scale. (b) The feature pyramid methods utilize the features from different layers of CNNs for different scales, which is computational friendly. This figure takes FPN [25] as an example. (c) Our proposed Trident Network generates scale-aware feature maps efficiently by trident blocks with different receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results on the COCO minival set using different number of trident blocks on ResNet-101. Method Branch No. AP AP50 APs APm AP l 37.8 58.4 18.0 45.3 53.4 Branch-3 31.9 48.8 7.1 37.9 56.1 3 Branches 40.6 61.8 23.0 45.5 55.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed TridentNet. The multiple branches in trident blocks share the same parameters with different dilation rates to generate scale-specific feature maps. Objects of specified scales are sampled for each branch during training. The final proposals or detections from multiple branches will be combined using Non-maximum Suppression (NMS). Here we only show the backbone network of TridentNet. The RPN and Fast R-CNN heads are shared among branches and ignored for simplicity.</figDesc><table><row><cell>Backbone</cell><cell>Dilation</cell><cell>AP</cell><cell>APs</cell><cell>APm</cell><cell>AP l</cell></row><row><cell></cell><cell>1</cell><cell cols="4">0.332 0.174 0.384 0.464</cell></row><row><cell>ResNet-50</cell><cell>2</cell><cell cols="4">0.342 0.168 0.386 0.486</cell></row><row><cell></cell><cell>3</cell><cell cols="4">0.341 0.162 0.383 0.492</cell></row><row><cell></cell><cell>1</cell><cell cols="4">0.379 0.200 0.430 0.528</cell></row><row><cell>ResNet-101</cell><cell>2</cell><cell cols="4">0.380 0.191 0.427 0.538</cell></row><row><cell></cell><cell>3</cell><cell cols="4">0.371 0.181 0.410 0.538</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Object detection results with different receptive fields using Faster R-CNN [35] evaluated on the COCO minival dataset [27].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>BackboneMethodMulti-branch Weight-sharing Scale-aware AP AP50 APs APm AP l</figDesc><table><row><cell></cell><cell>(a) Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.9 58.8 20.0 43.0 52.8</cell></row><row><cell></cell><cell>(b) Multi-branch</cell><cell></cell><cell></cell><cell></cell><cell>39.0 59.7 20.6 43.5 55.1</cell></row><row><cell>ResNet-101</cell><cell>(c) TridentNet w/o scale-aware</cell><cell></cell><cell></cell><cell></cell><cell>40.3 61.1 21.8 44.7 56.7</cell></row><row><cell></cell><cell>(d) TridentNet w/o sharing</cell><cell></cell><cell></cell><cell></cell><cell>39.3 60.4 21.4 43.8 54.2</cell></row><row><cell></cell><cell>(e) TridentNet</cell><cell></cell><cell></cell><cell></cell><cell>40.6 61.8 23.0 45.5 55.9</cell></row><row><cell></cell><cell>(a) Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.9 61.3 21.6 45.0 55.6</cell></row><row><cell></cell><cell>(b) Multi-branch</cell><cell></cell><cell></cell><cell></cell><cell>40.5 61.5 21.9 45.3 56.8</cell></row><row><cell>ResNet-101-Deformable</cell><cell>(c) TridentNet w/o scale-aware</cell><cell></cell><cell></cell><cell></cell><cell>41.4 62.8 23.4 45.9 57.4</cell></row><row><cell></cell><cell>(d) TridentNet w/o sharing</cell><cell></cell><cell></cell><cell></cell><cell>40.3 61.6 22.9 45.0 55.0</cell></row><row><cell></cell><cell>(e) TridentNet</cell><cell></cell><cell></cell><cell></cell><cell>41.8 62.9 23.6 46.8 57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the COCO minival set. Starting from our baseline, we gradually add multi-branch design, weight sharing among branches and scale-aware training scheme in our TridentNet for ablation studies.16 on 8 GPUs. Models are trained in 12 epochs by default, with the learning rate starting from 0.02 and decreased by a factor of 0.1 after the 8th and 10th epoch. The 2× or 3× training schemes means doubling or tripling the total training epochs and learning rate schedules accordingly. We adopt the output of conv 4 stage in ResNet<ref type="bibr" target="#b18">[19]</ref> as the backbone feature map and the conv 5 stage as the R-CNN head in both baseline and TridentNet. If not otherwise specified, we adopt three branches as our default TridentNet structure.</figDesc><table><row><cell>For each branch in TridentNet, the top 12000/500 propos-</cell></row><row><cell>als are kept before/after NMS and we sample 128 ROIs for</cell></row><row><cell>training. The dilation rates are set to 1, 2 and 3 in three</cell></row><row><cell>branches, respectively. When adopting scale-aware train-</cell></row><row><cell>ing scheme for TridentNet, we set the valid ranges of three</cell></row><row><cell>branches as [0, 90], [30, 160] and [90, ∞], respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(d) shows the ablation re-</cell></row><row><cell>sults of adding scale-aware training based on multi-</cell></row><row><cell>branch (Table</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on the COCO minival set using different number of branches on ResNet-50.</figDesc><table><row><cell>Stage</cell><cell>AP</cell><cell cols="2">AP50 APs APm AP l</cell></row><row><cell cols="2">Baseline 33.2</cell><cell>53.8</cell><cell>17.4 38.4 46.4</cell></row><row><cell>conv2</cell><cell>34.1</cell><cell>54.8</cell><cell>17.1 39.1 48.6</cell></row><row><cell>conv3</cell><cell>34.4</cell><cell>55.0</cell><cell>17.5 39.3 49.0</cell></row><row><cell>conv4</cell><cell>36.6</cell><cell>57.3</cell><cell>18.3 41.4 52.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on the COCO minival set by replacing conv blocks with trident blocks in different stages of ResNet-50.</figDesc><table><row><cell>over the single-branch method (baseline) with 2.7 to 3.4 AP</cell></row><row><cell>increase. As can be noticed, four branches do not bring fur-</cell></row><row><cell>ther improvement over three branches. Thus, considering</cell></row><row><cell>the complexity and performance, we choose three branches</cell></row><row><cell>as our default setting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of each branch in TridentNet evaluated on the COCO minival set. The dilation rates of three branches in Trident Network are set as 1, 2 and 3. The results are based on ResNet-101. ∞], [90, ∞] 39.3 60.1 19.1 44.6 56.4 (d) [0, ∞], [0, ∞], [0, ∞] 40.0 61.1 20.9 44.3 56.6</figDesc><table><row><cell>Scale-aware Ranges</cell><cell>AP AP50 APs APm AP l</cell></row><row><cell>(a) Baseline</cell><cell>37.9 58.8 20.0 43.0 52.8</cell></row><row><cell cols="2">(b) [0, 90], [30, 160], [90, ∞] 37.8 58.4 18.0 45.3 53.4</cell></row><row><cell>(c) [0, 90], [0,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results of TridentNet Fast under different scaleaware range schemes evaluated on the COCO minival set. All results are based on ResNet-101 and share the same hyper-parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>(c), by enlarging the scale-aware range of the major branch to incorporate objects of all scales, the performance of Trident</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>AP</cell><cell cols="3">AP50 AP75 APs APm AP l</cell></row><row><cell>Cascade R-CNN [5]</cell><cell>ResNet-101-FPN</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7 45.5 55.2</cell></row><row><cell>DCNv2 [44]</cell><cell>ResNet-101-DeformableV2</cell><cell>46.0</cell><cell>67.9</cell><cell>50.8</cell><cell>27.8 49.1 59.5</cell></row><row><cell>DCR [9]</cell><cell cols="2">ResNet-101-FPN-Deformable 43.1</cell><cell>66.1</cell><cell>47.3</cell><cell>25.8 45.9 55.3</cell></row><row><cell>SNIP [38]</cell><cell>ResNet-101-Deformable</cell><cell>44.4</cell><cell>66.2</cell><cell>44.9</cell><cell>27.3 47.4 56.9</cell></row><row><cell>SNIPER [39]</cell><cell>ResNet-101-Deformable</cell><cell>46.1</cell><cell>67.0</cell><cell>51.6</cell><cell>29.6 48.9 58.1</cell></row><row><cell>TridentNet</cell><cell>ResNet-101</cell><cell>42.7</cell><cell>63.6</cell><cell>46.5</cell><cell>23.9 46.6 56.6</cell></row><row><cell>TridentNet*</cell><cell>ResNet-101-Deformable</cell><cell>46.8</cell><cell>67.6</cell><cell>51.5</cell><cell>28.0 51.2 60.5</cell></row><row><cell cols="2">TridentNet* + Image Pyramid ResNet-101-Deformable</cell><cell>48.4</cell><cell>69.7</cell><cell>53.5</cell><cell>31.8 51.3 60.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of single-model results for different object detection methods evaluated on the COCO test-dev set.Fast improves by 1.5 AP over the default scale-aware range setting. Furthermore, extending scale-aware ranges for all three branches achieves the best performance of 40.0 AP, which is close to the original TridentNet result of 40.6 AP. We hypothesize this may due to the weight-sharing strategy. Since the weights of the major branch are shared on other branches, training all branches in the scale-agnostic scheme is equivalent to perform within-network multi-scale augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc>compares these methods under the same training setting. TridentNet improves sig-Method AP AP50 AP75 APs APm AP l 2fc Baseline 39.8 61.7 43.0 22.0 44.7 54.4 FPN † [25] 39.8 61.3 43.3 22.9 43.3 52.6 ASPP 39.7 60.4 42.7 21.7 44.5 53.9 TridentNet 42.0 63.5 45.5 24.9 47.0 56.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of detection results on the COCO minival set. Following FPN † , all methods are based on ResNet-101 with 2fc head using 2× training schedule.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">M</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun. R-Fcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Light-Head R-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
		<title level="m">defense of twostage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DetNet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Deformable convnets v2: More deformable, better results. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
