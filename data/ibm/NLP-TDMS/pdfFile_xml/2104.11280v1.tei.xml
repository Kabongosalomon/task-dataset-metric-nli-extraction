<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion Representations for Articulated Animation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
							<email>aliaksandr.siarohin@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Work done while at Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><forename type="middle">J</forename><surname>Woodford</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Work done while at Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
							<email>stulyakov@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motion Representations for Articulated Animation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose novel motion representations for animating articulated objects consisting of distinct parts. In a completely unsupervised manner, our method identifies object parts, tracks them in a driving video, and infers their motions by considering their principal axes. In contrast to the previous keypoint-based works, our method extracts meaningful and consistent regions, describing locations, shape, and pose. The regions correspond to semantically relevant and distinct object parts, that are more easily detected in frames of the driving video. To force decoupling of foreground from background, we model non-object related global motion with an additional affine transformation. To facilitate animation and prevent the leakage of the shape of the driving object, we disentangle shape and pose of objects in the region space. Our model 1 can animate a variety of objects, surpassing previous methods by a large margin on existing benchmarks. We present a challenging new benchmark with high-resolution videos and show that the improvement is particularly pronounced when articulated objects are considered, reaching 96.6% user preference vs. the state of the art. 3 E.g. a music video in which images are animated using prior work <ref type="bibr" target="#b29">[30]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Animation-bringing static objects to life-has broad applications across education and entertainment. Animated characters and objects, such as those in <ref type="figure" target="#fig_6">Fig. 1</ref>, increase the creativity and appeal of content, improve the clarity of material through storytelling, and enhance user experiences 2 .</p><p>Until very recently, animation techniques necessary for achieving such results required a trained professional, specialized hardware, software, and a great deal of effort. Quality results generally still do, but vision and graphics communities have attempted to address some of these limitations by training data-driven methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> on object classes for which prior knowledge of object shape and pose can be learned. This, however, requires ground truth pose and shape data to be available during training. <ref type="bibr" target="#b0">1</ref> Our source code is publicly available at https://github.com/snapresearch/articulated-animation. <ref type="bibr" target="#b1">2</ref> Visit our project website for more qualitative samples.</p><p>Source Animated <ref type="figure" target="#fig_6">Figure 1</ref>: Our method animates still source images via unsupervised region detection (inset).</p><p>Recent works have sought to avoid the need for ground truth data through unsupervised motion transfer <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Significant progress has been made on several key challenges, including training using image reconstruction as a loss <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, and disentangling motion from appearance <ref type="bibr" target="#b19">[20]</ref>. This has created the potential to animate a broader range of object categories, without any domain knowledge or labelled data, requiring only videos of objects in motion during training <ref type="bibr" target="#b29">[30]</ref>. However, two key problems remain open. The first is how to represent the parts of an articulated or non-rigid moving object, including their shapes and poses. The second is given the object parts, how to animate them using the sequence of motions in a driving video.</p><p>Initial attempts used end-to-end frameworks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31]</ref> to first extract unsupervised keypoints <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17]</ref>, then warp a feature embedding of a source image to align its keypoints with those of a driving video. Follow on work <ref type="bibr" target="#b29">[30]</ref> further modelled the motion around each keypoint with local, affine transformations, and introduced a generation module that both composites warped source image regions and inpaints occluded regions, to render the final image. This enabled a variety of creative applications, <ref type="bibr" target="#b2">3</ref> for example needing only one source face image to generate a near photo-realistic animation, driven by a video of a different face.</p><p>However, the resulting unsupervised keypoints are detected on the boundary of the objects. While points on edges are easier to identify, tracking such keypoints between frames is problematic, as any point on the boundary is a valid candidate, making it hard to establish correspondences between frames. A further problem is that the unsupervised keypoints do not correspond to semantically meaningful object parts, representing location and direction, but not shape. Due to this limitation, animating articulated objects, such as bodies, remains challenging. Furthermore, these methods assume static backgrounds, i.e. no camera motion, leading to leakage of background motion information into one or several of the detected keypoints. Finally, absolute motion transfer, as in <ref type="bibr" target="#b29">[30]</ref>, transfers the shape of the driving object into the generated sequence, decreasing the fidelity of the source identity. These remaining deficiencies limit the scope of previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> to more trivial object categories and motions, especially when objects are articulated.</p><p>This work introduces three contributions to address these challenges. First, we redefine the underlying motion representation, using regions from which first-order motion is measured, rather than regressed. This enables improved convergence, more stable, robust object and motion representations, and also empirically captures the shape of the underpinning object parts, leading to better motion segmentation. This motion representation is inspired by Hu moments <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref>(a) contains several examples of region vs. keypoint-based motion representation.</p><p>Secondly, we explicitly model background or camera motion between training frames by predicting the parameters of a global, affine transformation explaining non-object related motions. This enables the model to focus solely on the foreground object, making the identified points more stable, and further improves convergence. Finally, to prevent shape transfer and improve animation, we disentangle the shape and pose of objects in the space of unsupervised regions. Our framework is self-supervised, does not require any labels, and is optimized using reconstruction losses.</p><p>These contributions further improve unsupervised motion transfer methods, resulting in higher fidelity animation of articulated objects in particular. To create a more challenging benchmark for such objects, we present a newly collected dataset of TED talk speakers. Our framework scales better in the number of unsupervised regions, resulting in more detailed motion. Our method outperforms previous unsupervised animation methods on a variety of datasets, including talking faces, taichi videos and animated pixel art being preferred by 96.6% of independent raters when compared with the state of the art <ref type="bibr" target="#b29">[30]</ref> on our most challenging benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image animation methods can be separated into supervised, which require knowledge about the animated object during training, and unsupervised, which do not. Such knowledge typically includes landmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref>, semantic segmentations <ref type="bibr" target="#b23">[24]</ref>, and parametric 3D models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref>. As a result, supervised methods are limited to a small number of object categories for which a lot of labelled data is available, such as faces and human bodies. Early face reenactment work <ref type="bibr" target="#b34">[35]</ref> fitted a 3D morphable model to an image, animating and rendering it back using graphical techniques. Further works used neural networks to get higher quality rendering <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, sometimes requiring multiple images per identity <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>. A body of works treats animation as an image-to-image <ref type="bibr" target="#b32">[33]</ref> or videoto-video <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref> translation problem. Apart from some exceptions <ref type="bibr" target="#b37">[38]</ref>, these works further constrain the problem to animating a single instance of an object, such as a single face <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref> or a single human body <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>, requiring retraining <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref> or fine-tuning <ref type="bibr" target="#b43">[44]</ref> for each new instance. Despite promising results, generalizing these methods beyond a limited range of object categories remains challenging. Additionally, they tend to transfer not only the motion but also the shape of the driving object <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Unsupervised methods address some of these limitations. They do not require any labelled data regarding the shape or landmarks of the animated object. Video-generation-based animation methods predict future frames of a video, given the first frame and an animation class label, such as "make a happy face", "do jumping jack", or "play golf" <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7]</ref>. Recently Menapace et al. <ref type="bibr" target="#b20">[21]</ref> introduce playable video generation, where action could be selected at each timestamp. A further group of works re-target animation from a driving video to a source frame. X2Face <ref type="bibr" target="#b41">[42]</ref> builds a canonical representation of an input face, and generates a warp field conditioned on the driving video. Monkey-Net <ref type="bibr" target="#b30">[31]</ref> learns a set of unsupervised keypoints to generate animations. Followup work substantially improves the quality of animation by considering a first order motion model (FOMM) <ref type="bibr" target="#b29">[30]</ref> for each keypoint, represented by regressing a local, affine transformation. Both of these works apply to a wider range of objects including faces, bodies, robots, and pixel art animations. Empirically, these methods extract keypoints on the boundary of the animated objects. Articulated objects such as human bodies are therefore challenging, as internal motion, for example, an arm moving across the body, is not well modeled, producing unconvincing animations.</p><p>This work presents an unsupervised method. We argue that the limitations of previous such methods in animating articulated objects is due to an inability of their internal representations to capture complete object parts, their shape and pose. X2Face <ref type="bibr" target="#b41">[42]</ref> assumes an object can be represented with a single RGB texture, while other methods find keypoints on edges <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>. Our new region motion representation resembles the construction of a motion history image whose shape is analyzed using principal components (namely Hu moments <ref type="bibr" target="#b7">[8]</ref>). In <ref type="bibr" target="#b7">[8]</ref> the authors construct motion descrip-tors by computing temporal image differences, aggregate them into motion history images, and use Hu moments to build a motion recognition system. In this manner, blob statistics are used to discriminate between different actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose three contributions over FOMM <ref type="bibr" target="#b29">[30]</ref>, namely our PCA-based motion estimation (Sec. 3.2), background motion representation (Sec. 3.3) and animation via disentanglement (Sec. 3.6). To make the manuscript self-contained, we first give the necessary technical background on the original first order motion representation <ref type="bibr" target="#b29">[30]</ref> (Sec. 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">First Order Motion Model</head><p>FOMM <ref type="bibr" target="#b29">[30]</ref> consists of the two main parts: motion estimation and image generation, where motion estimation further contains coarse motion estimation and dense motion prediction. Coarse motion is modelled as sparse motions between separate object parts, while dense motion produces an optical flow along with the confidence map for the entire image. We denote by S and D the source and the driving frames extracted from the same video respectively.</p><p>The first step is to estimate coarse motions from S and D. Motions for each object part are represented by affine transformations, A k X←R ∈ R 2×3 , to an abstract, common reference frame, R; X is either S or D. Motions are estimated for K distinct parts. An encoder-decoder keypoint predictor network outputs K heatmaps, M 1 , .., M K for the input image, followed by softmax, s.t. M k ∈ [0, 1] H×W , where H and W are the height and width of the image respectively, and z∈Z M k (z) = 1, where z is a pixel location (x, y coordinates) in the image, the set of all pixel locations being Z, and M k (z) is the k-th heatmap weight at pixel z. Thus, the translation component of the affine transformation (which is the last column of A k X←R ) can be estimated using softargmax:</p><formula xml:id="formula_0">µ k = z∈Z M k (z)z.<label>(1)</label></formula><p>In FOMM <ref type="bibr" target="#b29">[30]</ref> the remaining affine parameters are regressed per pixel and form 4 additional channels P k ij ∈ R H×W , where i ∈ {0, 1}, j ∈ {0, 1} indexes of the affine matrix A k X←R . The latter is estimated using weighted pooling:</p><formula xml:id="formula_1">A k X←R [i, j] = z∈Z M k (z)P k ij (z).<label>(2)</label></formula><p>We refer to this way of computing motion as regressionbased, where affine parameters are predicted by a network and pooled to compute A k X←R . Motion between D and S for part k is then computed via the common reference frame:</p><formula xml:id="formula_2">A k S←D = A k S←R A k D←R 0 0 1 −1 .<label>(3)</label></formula><p>Given the coarse motion, the next step is to predict the optical flow and the confidence map. Since the transformations between D and S are known for each part, the task is to combine them to obtain a single optical flow field. To this end, the flow predictor selects the appropriate coarse transformation for each pixel, via a weighted sum of coarse motions. Formally, K + 1 assignment maps are output, one per region, plus background, which FOMM <ref type="bibr" target="#b29">[30]</ref> assumes is motionless, and softargmax is applied pixelwise across them, s.t. W k ∈ [0, 1] H×W , W 0 corresponds to the background, and </p><formula xml:id="formula_3">O(z) = W 0 (z)z + K k=1 W k (z)A k S←D z 1 .<label>(4)</label></formula><p>With such a model, animation becomes challenging when there is even slight background motion. The model automatically adapts by assigning several of the available keypoints to model background as shown in the first row of <ref type="figure" target="#fig_2">Fig. 3</ref>(a). A confidence map, C ∈ [0, 1] H×W , is also predicted using the same network, to handle parts missing in the source image. Finally S is passed through an encoder, followed by warping the resulting feature map using the optical flow (Eq. 4) and multiplied by the confidence map. A decoder then reconstructs the driving image D.</p><p>At test time FOMM <ref type="bibr" target="#b29">[30]</ref> has two modes of animating S: standard and relative. In both cases the input is the source image S and the driving video D 1 , D 2 , .., D t , .., D T . In the standard animation the motion between source and driving is computed frame-by-frame using Eq. (3). For relative animation, in order to generate a frame t the motion between D 1 and D t is computed first and then applied to S. Both of these modes are problematic when the object in question is articulated, as we show in Sec. 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">PCA-based motion estimation</head><p>Accurate motion estimation is the main requirement for high-quality image animation. As mentioned previously, FOMM regresses the affine parameters. This requires higher capacity networks, and generalizes poorly (see Sec. 4.1). We propose a different motion representation: all motions are measured directly from the heatmap M k . We compute the translation as before, while in-plane rotation and scaling in x-and y-directions are computed via a principal component analysis (PCA) of the heatmap M k . Formally, the transformation A k X←R ∈ R 2×3 , of the k th region from the reference frame to the image is computed as:</p><formula xml:id="formula_4">µ k = z∈Z M k (z)z,<label>(5)</label></formula><formula xml:id="formula_5">U k S k V k = z∈Z M k (z) z − µ k z − µ k T (SVD),<label>(6)</label></formula><formula xml:id="formula_6">A k X←R = U k S k 1 2 , µ k .<label>(7)</label></formula><p>Region predictor  <ref type="figure">Figure 2</ref>: Overview of our model. The region predictor returns heatmaps for each part in the source and the driving images. We then compute principal axes of each heatmap, to transform each region from the source to the driving frame through a whitened reference frame. Region and background transformations are combined by the pixel-wise flow prediction network. The target image is generated by warping the source image in a feature space using the pixel-wise flow, and inpainting newly introduced regions, as indicated by the confidence map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG motion predictor</head><p>Here the singular value decomposition (SVD) approach to computing PCA <ref type="bibr" target="#b36">[37]</ref> is used, Eq. (6) decomposing the covariance of the heatmap into unitary matrices U k and V k , and S k , the diagonal matrix of singular values. We call this approach PCA-based, in contrast to regression-based for Eq. (1). Despite using the same region representation and encoder here, the encoded regions differ significantly (see <ref type="figure" target="#fig_2">Fig. 3</ref>(a)), ours mapping to meaningful object parts such as the limbs of an articulated body, due to our novel foreground motion representation, described above. Note that in our PCA-based approach, shear is not captured, therefore our transform is not fully affine, with only five degrees of freedom instead of six. Nevertheless, as we later show empirically, it captures sufficient motion, with shear being a less significant component of the affine transform for this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Background motion estimation</head><p>Background occupies a large portion of image. Hence even small background motion between frames, e.g. due to camera motion, negatively affects the animation quality. FOMM <ref type="bibr" target="#b29">[30]</ref> does not treat background motion separately, therefore must model it using keypoints. This has two negative consequences: (i) additional network capacity is required, since several keypoints are used to model the background instead of the foreground; (ii) overfitting to the training set, since these keypoints focus on specific parts of the background, which may not appear in the test set. Hence, we additionally predict an affine background transformation, A 0 S←D , using an encoder network assuming S and D as input and predicting six real values, a 1 , .., a 6 , such that A 0 S←D = [ a1,a2,a3 a4,a5,a6 ]. Since our framework is unsupervised, the background network can include parts of the foreground into the background motion. In practice this does not happen, since it is easier for the network to use a more appropriate PCA-based motion representation for the foreground. It is also simpler for the network to use S and D to predict background movement, instead of encoding it in the heatmaps modelling the foreground. We verify this empirically, demonstrating that the proposed motion representations can separate background and foreground in a completely unsupervised manner (see <ref type="figure">Fig. 6</ref> and Sec. 4.5 for comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Image generation</head><p>Similarly to FOMM <ref type="bibr" target="#b29">[30]</ref>, we render the target image in two stages: a pixel-wise flow generator converts coarse motions to dense optical flow, then the encoded features of the source are warped according to the flow, followed by inpainting the missing regions. The input of the dense flow predictor is a H × W × (4K + 3) tensor, with four channels per region, three for the source image warped according to the region's affine transformation, and one for a heatmap of the region, which is a gaussian approximation of M k , and a further three channels for the source image warped according to the background's affine transformation. In contrast to FOMM, which uses constant variances, we estimate covariances from heatmaps. Our dense optical flow is given  </p><formula xml:id="formula_7">O(z) = K k=0 W k (z)A k S←D z 1 .<label>(8)</label></formula><p>The predicted optical flow and confidence map are used as per FOMM <ref type="bibr" target="#b29">[30]</ref>. However in contrast to FOMM <ref type="bibr" target="#b29">[30]</ref>, but similar to Monkey-Net <ref type="bibr" target="#b30">[31]</ref>, here deformable skip connections <ref type="bibr" target="#b32">[33]</ref> are used between the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>The proposed model is trained end-to-end using a reconstruction loss in the feature space of the pretrained VGG-19 network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>. We use a multi-resolution reconstruction loss from previous work <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>:</p><formula xml:id="formula_8">L rec (D, D) = l i V i (F l D ) − V i (F l D) ,<label>(9)</label></formula><p>whereD is the generated image, V i is the i th -layer of the VGG-19 pretrained network, and F l is a downsampling op-erator. Per FOMM <ref type="bibr" target="#b29">[30]</ref>, we also use an equivariance loss,</p><formula xml:id="formula_9">L eq = A k X←R −ÃA k X←R ,<label>(10)</label></formula><p>whereX is image X transformed byÃ, andÃ is some random geometric transformation. The final loss is the sum of terms, L = L rec + L eq .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Animation via disentanglement</head><p>Image animation using both standard and relative methods has limitations. The standard method directly transfers object shape from the driving frame into the generated video, while relative animation is only applicable to a limited set of inputs, e.g. it requires that objects be in the same pose in the source S and initial driving D 1 frames. To address this, we learn disentangled shape and pose encoders, as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. The pose encoder takes in the set of driving motions, {A k D←R } K k=1 , while the shape encoder takes in the set of source motions, {A k S←R } K k=1 . A decoder then uses the concatenated latent representations (each in R 64 ) of these two encoders, to produce a set of modified driving motions, {Ã k D←R } K k=1 encoding the motion of the former and the shape of the latter. These are then used to render the output.</p><p>The encoders and the decoder are implemented using fully connected layers, and trained separately from (and after) other blocks, using an L 1 reconstruction loss on the motion parameters. As with earlier training, source and driving frames come from the same video (i.e. the object has the same shape), therefore to ensure that shape comes from the correct branch, random horizontal and vertical scaling deformations are applied to the driving motions during training, as shown in <ref type="figure" target="#fig_2">Fig. 3(b</ref>). This forces shape information to come from the other (shape) branch. However, since the shape branch has a different pose, the pose must still come from the pose branch. Thus shape and pose are disentangled. The deformations are not applied at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We now discuss the datasets, metrics and experiments used to evaluate the proposed method. Later we compare with prior work, as well as ablate our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Toy Motion Representation Experiment</head><p>To demonstrate the benefit of the proposed PCA-based motion representation, we devise an experiment on rotated rectangles (see Sup. Mat.): the task is to predict the rotation angle of a rectangle in an image. To fully isolate our contribution, we consider a supervised task, where three different architectures learn to predict angles under the L 1 loss. The first, a Naive architecture, directly regresses the angle using an encoder-like architecture. The second is Regression-based, as in FOMM <ref type="bibr" target="#b29">[30]</ref>. The third uses our PCA-based approach (see Sup. Mat.). Test results are presented in <ref type="figure" target="#fig_5">Fig. 5</ref>, against training set size. The Naive baseline struggles to produce meaningful results for any size of training set, while Regression-based performance improves with more data. However, the PCA-based approach significantly improves accuracy over the Regression-based one, being over an order of magnitude better with a large number of samples. This shows that it is significantly easier for the network to infer geometric parameters of the image, such as angle, using our proposed PCA-based representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmarks</head><p>We evaluate our method on several benchmark datasets for animating human faces, bodies and animated cartoons. Each dataset has separate training and test videos. The datasets are as follows:</p><p>• Since video animation is a relatively new problem, there are not currently many effective ways of evaluating it. For quantitative metrics, prior works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref> use video reconstruction accuracy as a proxy for image animation quality. We adopt the same metrics here:</p><p>• L 1 error is the mean absolute difference between reconstructed and ground-truth video pixel values. • Average keypoint distance (AKD) and missing keypoint rate (MKR) evaluate the difference between poses of reconstructed and ground truth video. Landmarks are extracted from both videos using public, body <ref type="bibr" target="#b4">[5]</ref> (for TaiChiHD and TED-talks) and face <ref type="bibr" target="#b2">[3]</ref> (for VoxCeleb) detectors. AKD is then the average distance between corresponding landmarks, while MKR is the proportion of landmarks present in the ground-truth that are missing in the reconstructed video. • Average Euclidean distance (AED) evaluates how well identity is preserved in reconstructed video. Public reidentification networks for bodies <ref type="bibr" target="#b12">[13]</ref> (for TaiChiHD and TED-talks) and faces <ref type="bibr" target="#b0">[1]</ref> extract identity from reconstructed and ground truth frame pairs, then we compute the mean L 2 norm of their difference across all pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state of the art</head><p>We compare our method with the current state of the art for unsupervised animation, FOMM <ref type="bibr" target="#b29">[30]</ref>, on both reconstruction (the training task) and animation (the test-time task). We used an extended training schedule compared to <ref type="bibr" target="#b29">[30]</ref>, with 50% more iterations. To compare fairly with FOMM <ref type="bibr" target="#b29">[30]</ref>, we also re-trained it with the same training schedule. Also, for reference, we include comparisons with X2Face <ref type="bibr" target="#b41">[42]</ref> and Monkey-Net <ref type="bibr" target="#b30">[31]</ref> on video reconstruction.</p><p>Reconstruction quality Quantitative reconstruction results are reported in <ref type="table" target="#tab_3">Table 2</ref>. We first show that our method reaches state-of-the-art results on a dataset with non-articulated objects such as faces. Indeed, when compared with FOMM <ref type="bibr" target="#b29">[30]</ref> on VoxCeleb, our method shows on-par results. The situation changes, however, when articulated objects are considered, such as human bodies in    TaiChiHD and TED-talks datasets, on which our improved motion representations boost all the metrics. The advantage over the state of the art holds at different resolutions, for TaiChiHD (256), TaiChiHD (512) and TED-talks, as well as for different numbers of selected regions (discussed later). <ref type="figure" target="#fig_2">Fig. 3</ref>(a,c) &amp; 4 show selected and representative animations respectively, using our method and FOMM <ref type="bibr" target="#b29">[30]</ref>, on articulated bodies. Here for FOMM <ref type="bibr" target="#b29">[30]</ref> we use the standard method, while ours uses animation via disentanglement. The results show clear improvements, in most cases, in animation quality, especially of limbs. Animation quality was evaluated quantitatively through a user preference study similar to that of <ref type="bibr" target="#b29">[30]</ref>. AMT users were presented with the source image, driving video, and the output from our method and FOMM <ref type="bibr" target="#b29">[30]</ref>, and asked which of the two videos they preferred. 50 such videos were  <ref type="table">Table 3</ref>: User study: second column -the proportion (%) of users that prefer our method over FOMM <ref type="bibr" target="#b29">[30]</ref>; third column -the proportion (%) of users that prefer animation via disentanglement over standard animation for our model. evaluated, by 50 users each, for a total of 2500 preferences per study. The results further support the reconstruction scores in Tab. 2. When the animated object is not articulated (VoxCeleb), the method delivers results comparable to the previous work, e.g. 52% preference in favour of our method. However, when bodies are animated (TaiChiHD &amp; TEDtalks), FOMM <ref type="bibr" target="#b29">[30]</ref> fails to correctly detect and animate the articulated body parts such as hands. Our method renders them in the driving pose even for extreme cases, leading to a high preference in favor of it (see Tab. 3, middle column). Moreover since it is not possible to demonstrate the benefits of the animation via disentanglement using reconstruction metrics, we run an additional user study to compare our method with standard animation and animation via disentanglement. Since animation via disentanglement preserves shape of the object much better (see <ref type="figure" target="#fig_2">Fig. 3(c)</ref>), users prefer it more often. It especially pronoun in case of the TED-talks dataset, since shape of the objects differs more in that case (see Tab. 3, last column).   <ref type="figure">Figure 6</ref>: Qualitative co-part segmentation comparison with recent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animation quality</head><p>Finally, we applied animation from a TED-talks video to a photograph of George Washington, shown in <ref type="figure" target="#fig_6">Fig. 1</ref>, demonstrating animation of out-of-domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations</head><p>In order to understand how much benefit each of our contributions bring, we ran a number of ablation experiments, detailed in Tab. 4.</p><p>PCA-based vs. regression-based representations First we compare the PCA-based motion model with the previous, regression-based one <ref type="bibr" target="#b29">[30]</ref>. From the qualitative heatmap depictions in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, we observe that the regression-based method localizes one edge of each corresponding part, while our method predicts regions that roughly correspond to the segmentation of the object into its constituent, articulated parts. This meaningful segmentation arises completely unsupervised.</p><p>From Tab. 1 we note that adding the PCA-based representation alone (second row) had marginal impact on the L 1 score (dominated by the much larger background region), but it had a much larger impact on other metrics, which are more sensitive to object-part-related errors on articulated objects. This is corroborated by Tab. 4. Moreover, we observed that when our PCA-based formulation is not used, the network encodes some of the movement into the background branch, leading to significant degradation of keypoint quality, which in turn leads to degradation of the AKD and AED scores (No-pca, Tab. 4).</p><p>We intuit that PCA-based estimation both captures regions and improves performance because it is much easier for the convolutional network to assign pixels of an object part to the corresponding heatmap than to directly regress motion parameters to an abstract reference frame. This is borne out by our toy experiment (sec. 4.1). In order to esti-mate the heatmap, it need only learn all appearances of the corresponding object part, while regression-based networks must learn the joint space of all appearances of a part in all possible geometric configurations (e.g. rotated, scaled etc.).</p><p>One of the most important hyper-parameters of our model is the number of regions, K. The qualitative and quantitative ablations of this parameter are shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref> and Tab. 1 respectively. We can observe that, while the regressionbased representation fails when the number of keypoints grows to 20, our PCA-based representation scales well with the number of regions.</p><p>Modeling background motion Background motion modeling significantly lowers L 1 error (see Tab. 4, Full method vs. No bg Model). Since background constitutes a large portion of the image, and L 1 treats all pixels equally, this is to be expected. AED was also impacted, suggesting that the identity representation captures some background appearance. Indeed, we observe ( <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, second row) that having no background model causes a reduction in region segmentation quality. However, since AKD &amp; MKR metrics evaluate object pose only, they are not improved by background modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Co-part Segmentation</head><p>While designed for articulated animation, our method produces meaningful object parts. To evaluate this capability of our method, we compare it against two recent unsupervised co-part segmentation works: MSCS <ref type="bibr" target="#b31">[32]</ref> and SCOPS <ref type="bibr" target="#b13">[14]</ref>. Following MSCS, we compute foreground segmentation IoU scores on TaiChiHD. Despite not being optimized for this task, our method achieves superior performance reaching 0.81 IoU vs. 0.77 for MSCS <ref type="bibr" target="#b31">[32]</ref> and 0.55 for SCOPS <ref type="bibr" target="#b13">[14]</ref>. See <ref type="figure">Fig. 6</ref> for qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have argued that previous unsupervised animation frameworks' poor results on articulated objects are due to their representations. We propose a new, PCA-based, region motion representation, that we show both makes it easier for the network to learn region motion, and encourages it to learn semantically meaningful object parts. In addition, we propose a background motion estimation module to decouple foreground and background motion. Qualitative and quantitative results across a range of datasets and tasks demonstrate several key benefits: improved region distribution and stability, improved reconstruction accuracy and user perceived quality, and an ability to scale to more regions. We also introduce a new, more challenging dataset, TED-talks, for benchmarking future improvements on this task.</p><p>While we show some results on out of domain data <ref type="figure" target="#fig_6">(Fig. 1)</ref>, generalization remains a significant challenge to making this method broadly practical in articulated animation of inanimate objects.</p><p>In this supplementary material we report additional details on the toy experiment in Sec. A. In Sec. B we provide additional details for the co-part segmentation experiment. We provide additional implementation details in Sec. C. Additionally in Sec. D we visually demonstrate the ability of the model to control the background. Finally in Sec. E we describe the TED-talks data collection procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Toy Experiment Details</head><p>The rotated rectangles dataset consists of images of rectangles randomly rotated from 0 • to 90 • , along with labels that indicate the angle of rotation. The rectangles have different, random colors. Visual samples are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>We tested three different networks: Naive, Regressionbased and PCA-based. The Naive network directly predicts an angle from an image using an encoder and a fullyconnected layer. Regression-based is similar to FOMM <ref type="bibr" target="#b29">[30]</ref>; the angle is regressed per pixel an using hourglass network, and pooled according to heatmap weights predicted using the same hourglass network. PCA-based is our method described in Sec. 3.2 we predict the heatmap using an hourglass network, PCA is performed according to Eq. (6), and the angle is computed from matrix U as arctan(U 10 /U 00 ).</p><p>Each of the networks was trained, on subsets of the dataset of varying sizes, to minimize the L 1 loss between predicted and ground truth rotation angle. All models were trained for 100 epochs, with batch size 8. We used the Adam optimizer, with a learning rate of 10 −4 . We varied the size of the training set from 32 to 1024. Results, on a separate, fixed test set of size 128, were then computed, shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Co-part segmentation details</head><p>To perform co-part segmentation we use M k . A pixel z is assigned to a part that has the maximum response of M k in that pixel, e.g argmax k M k (z) Moreover since region predictor did not explicitly predict background region, we assign pixel z to the background iff k M k (z) &lt; 0.001. We demonstrate additional qualitative comparisons with MSCS <ref type="bibr" target="#b31">[32]</ref> and SCOPS <ref type="bibr" target="#b13">[14]</ref> in <ref type="figure">Fig. 9. Fig. 9</ref> shows that our method produces more meaningful co-part segmentations compared to SCOPS <ref type="bibr" target="#b13">[14]</ref>. Furthermore, our method separates the foreground object from the background more accurately than MSCS <ref type="bibr" target="#b31">[32]</ref>. <ref type="figure">Figure 8</ref>: Examples of cloth swap performed using our model. First column depicts sources from which cloth is taken, while the first row shows a driving video to which we put the cloth. Rest demonstrates images generated with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Driving Source</head><p>Similarly to MSCS <ref type="bibr" target="#b31">[32]</ref> we can exploit produced segmentations in order to perform a part swap. In <ref type="figure">Fig. 8</ref> we copy the cloth from the person in the source image on to the person in the driving video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours SCOPS <ref type="bibr" target="#b13">[14]</ref> MSCS [32] <ref type="figure">Figure 9</ref>: Additional qualitative co-part segmentation comparisons with recent methods. First column is an input. In next columns, for every method segmentation mask and image with overlayed segmentation are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>For a fair comparison, in order to highlight our contributions, we mostly follow the architecture design of FOMM <ref type="bibr" target="#b29">[30]</ref>. Similar to FOMM, our region predictor, background motion predictor and pixel-wise flow predictor operate on a quarter of the original resolution, e.g. 64 × 64 for 256 × 256 images, 96 × 96 for 384 × 384 and 128 × 128 for 512 × 512. We use the U-Net <ref type="bibr" target="#b27">[28]</ref> architecture with five "convolution -batch norm -ReLU -pooling" blocks in the encoder and five "upsample -convolution -batch norm -ReLU" blocks in the decoder for both the region predictor and the pixel-wise flow predictor. For the background motion predictor, we use only five block encoder part. Similarly to FOMM <ref type="bibr" target="#b29">[30]</ref>, we use the Johnson architecture <ref type="bibr" target="#b14">[15]</ref> for image generation, with two down-sampling blocks, six residual-blocks, and two up-sampling blocks. However, we add skip connections that are warped and weighted by the confidence map. Our method is trained using Adam <ref type="bibr" target="#b17">[18]</ref> optimizer with learning rate 2e − 4 and batch size 48, 20, 12 for 256 × 256, 384 × 384 and 512 × 512 resolutions respectively. During the training process, the networks observe 3M source-driving pairs, each pair selected at random from a random video chunk, and we drop the learning rate by a factor of 10 after 1.8M and 2.7M pairs. We use 4 Nvidia P100 GPUs for training.</p><p>The shape-pose disentanglement network consists of 2 identical encoders and 1 decoder. Each encoder consists of 3 "linear -batch norm -ReLU" blocks, with a number of hidden units equal to 256, 512, 1024, and another linear layer with number of units equal to 64. Decoder takes a concatenated input from encoders and applies 3 "linearbatch norm -ReLU" blocks, with sizes 1024, 512, 256. The network is trained on 1M source-driving pairs, organized in batches of 256 images. We use Adam optimizer with learning rate 1e − 3 and drop the learning rate at 660K and 880K pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Background movement</head><p>While the primary purpose of background modelling is to free up the capacity of the network to better handle the object. For animating articulated objects, background motion is usually unnecessary. Thus, though we estimate background motion, we set it to zero during animation. However, nothing in our framework prevents us from controlling camera motion. Below we show a still background, then move it left, right, and rotate counterclockwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. TED-talks dataset creation</head><p>In order to create the TED-talks dataset, we downloaded 3,035 YouTube videos, shared under the "CC BY -NC -ND 4.0 International" license, 4 using the "TED talks" query. From these initial candidates, we selected the videos in which the upper part of the person is visible for at least 64 frames, and the height of the person bounding box was at least 384 pixels. After that, we manually filtered out static videos and videos in which a person is doing something other than presenting. We ended up with 411 videos, and split these videos in 369 <ref type="bibr" target="#b3">4</ref> This license allows for non-commercial use. training and 42 testing videos. We then split each video into chunks without significant camera changes (i.e. with no cuts to another camera), and for which the presenter did not move too far from their starting position in the chunk. We cropped the a square region around the presenter, such that they had a consistent scale, and downscaled this region to 384 × 384 pixels. Chunks that lacked sufficient resolution to be downscaled, or had a length shorter than 64 frames, were removed. Both the distance moved and the region cropping were achieved using a bounding box estimator for humans <ref type="bibr" target="#b42">[43]</ref>. We obtained 1,177 training video chunks and 145 test videos chunks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>K</head><label></label><figDesc>k=0 W k (z) = 1, ∀z. Optical flow per pixel, O(z) ∈ R 2 ,is then computed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " P D y 2 6 g h N X p 9 M B w D V P s N U f o I I d Z k = " &gt; A A A C D X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 1 V w V R J R d F l x o 7 v 6 6 A O a E C b T S T t 0 k g k z E 6 W E / I A b f 8 W N C 0 X c u n f n 3 z h p I 2 j r g Q u H c + 7 l 3 n v 8 m F G p L O v L K M 3 N L y w u l Z c r K 6 t r 6 x v m 5 l Z L 8 k R g 0 s S c c d H x k S S M R q S p q G K k E w u C Q p + R t j 8 8 z / 3 2 H R G S 8 u h W j W L i h q g f 0 Y B i p L T k m X t n X u q E S A 3 8 I L 3 J o M N I o J A Q / B 7 + q N d Z 5 p l V q 2 a N A W e J X Z A q K N D w z E + n x 3 E S k k h h h q T s 2 l a s 3 B Q J R T E j W c V J J I k R H q I + 6 W o a o Z B I N x 1 / k 8 F 9 r f R g w I W u S M G x + n s i R a G U o 9 D X n f m J c t r L x f + 8 b q K C U z e l U Z w o E u H J o i B h U H G Y R w N 7 V B C s 2 E g T h A X V t 0 I 8 Q A J h p Q O s 6 B D s 6 Z d n S e u w Z h / X r K u j a v 2 y i K M M d s A u O A A 2 O A F 1 c A E a o A k w e A B P 4 A W 8 G o / G s / F m v E 9 a S 0 Y x s w 3 + w P j 4 B k V i n F k = &lt; / l a t e x i t &gt; AS R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H B Q K F V 5 + K o V M 1 a Z 1 I k v 7 k 0 q o k 8 w = " &gt; A A A C D X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 1 V w V R J R d F n R h e 6 q 2 A c 0 I U y m k 3 b o J B N m J k o J + Q E 3 / o o b F 4 q 4 d e / O v 3 H S R t D W A x c O 5 9 z L v f f 4 M a N S W d a X U Z q b X 1 h c K i 9 X V l b X 1 j f M z a 2 W 5 I n A p I k 5 4 6 Lj I 0 k Y j U h T U c V I J x Y E h T 4 j b X 9 4 n v v t O y I k 5 d G t G s X E D V E / o g H F S G n J M / f O v N Q J k R r 4 Q X q R Q Y e R Q C E h + D 3 8 U W + y z D O r V s 0 a A 8 4 S u y B V U K D h m Z 9 O j + M k J J H C D E n Z t a 1 Y u S k S i m J G s o q T S B I j P E R 9 0 t U 0 Q i G R b j r + J o P 7 W u n B g A t d k Y J j 9 f d E i k I p R 6 G v O / M T 5 b S X i / 9 5 3 U Q F p 2 5 K o z h R J M K T R U H C o O I w j w b 2 q C B Y s Z E m C A u q b 4 V 4 g A T C S g d Y 0 S H Y 0 y / P k t Z h z T 6 u W d d H 1 f p V E U c Z 7 I B dc A B s c A L q 4 B I 0 Q B N g 8 A C e w A t 4 N R 6 N Z + P N e J + 0 l o x i Z h v 8 g f H x D S 0 + n E o = &lt; / l a t e x i t &gt; AD R Target Decoder (a) Comparisons of motion/part representations (c) Comparisons of FOMM, standard and animation via disentanglement (b) Transformation-based shape and pose disentanglement &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v C W W A B m L P U S g k k d H r W T O / z y b p 9 o = " &gt; A A A C F X i c b V B N S 8 N A E N 3 4 b f 2 q e v S y W A Q P U h J R 9 F j R g 9 5 U b B W a U j b b S b u 4 + W B 3 o p S Q P + H F v + L F g y J e B W / + G z d t B G 1 9 M P B 4 b 4 a Z e V 4 s h U b b / r I m J q e m Z 2 b n 5 k s L i 0 v L K + X V t Y a O E s W h z i M Z q R u P a Z A i h D o K l H A T K 2 C B J + H a u z 3 O / e s 7 U F p E 4 R X 2 Y 2 g F r B s K X 3 C G R m q X d 4 7 a q R s w 7 H l + 6 q K Q H U h P s o y 6 E n x k S k X 3 9 M e 9 z L J 2 u W J X 7 Q H o O H E K U i E F z t v l T 7 c T 8 S S A E L l k W j c d O 8 Z W y h Q K L i E r u Y m G m P F b 1 o W m o S E L Q L f S w V c Z 3 T J K h / q R M h U i H a i / J 1 I W a N 0 P P N O Z n 6 h H v V z 8 z 2 s m 6 B + 2 U h H G C U L I h 4 v 8 R F K M a B 4 R 7 Q g F H G X f E M a V M L d S 3 m O K c T R B l k w I z u j L 4 6 S x W 3 X 2 q / b F X q V 2 V s Q x R z b I J t k m D j k g N X J K z k m d c P J A n s g L e b U e r W f r z X o f t k 5 Y x c w 6 + Q P r 4 x s C B q A A &lt; / l a t e x i t &gt; AD R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 V m m 9 r n i 3 W z t c h y 5 a Z 8 2 o u F 9 n O 0 = " &gt; A A A C F X i c b V D L S g N B E J z 1 b X x F P X o Z D I I H C b u i 6 D E + D h 5 V T B S y I c x O e p P B 2 Q c z v U p Y 9 i e 8 + C t e P C j i V f D m 3 z i b r K C J B Q 1 F V T f d X V 4 s h U b b / r I m J q e m Z 2 b n 5 k s L i 0 v L K + X V t Y a O E s W h z i M Z q R u P a Z A i h D o K l H A T K 2 C B J + H a u z 3 J / e s 7 U F p E 4 R X 2 Y 2 g F r B s K X 3 C G R m q X d 1 w U s g P p U d Z O 3 Y B h z / P T 0 4 y 6 E n x k S k X 3 9 E e 9 z L J 2 u W J X 7 Q H o O H E K U i E F z t v l T 7 c T 8 S S A E L l k W j c d O 8 Z W y h Q K L i E r u Y m G m P F b 1 o W m o S E L Q L f S w V c Z 3 T J K h / q R M h U i H a i / J 1 I W a N 0 P P N O Z n 6 h H v V z 8 z 2 s m 6 B + 2 U h H G C U L I h 4 v 8 R F K M a B 4 R 7 Q g F H G X f E M a V M L d S 3 m O K c T R B l k w I z u j L 4 6 S x W 3 X 2 q / b F X q V 2 X M Q x R z b I J t k m D j k g N X J G z k m d c P J A n s g L e b U e r W f r z X o f t k 5 Y x c w 6 + Q P r 4 x s C K p / 5 &lt; / l a t e x i t &gt;Ã D R Motion representation and disentanglement. (a) A comparison of part regions estimated by regression-based (FOMM [30]) and PCA-based (our method) motion representations. Each row shows a generated sequence along with the detected keypoints and region heatmaps (inset). (b) Framework for animating motion driving frame whilst retaining shape from the source frame, by disentangling shape and pose from motion representations. (c) Qualitative animation results of articulated objects, using FOMM [30], and our method using standard and disentangled motion transfer (sec. 3.] 0.062 (7.34, 0.036) 0.181 0.056 (6.53, 0.033) 0.172 0.062 (8.29, 0.049) 0.196 Ours w/o bg 0.061 (6.67, 0.030) 0.175 0.059 (5.55, 0.026) 0.165 0.057 (5.47, 0.026) 0.155 Ours 0.049 (6.04, 0.029) 0.162 0.047 (5.59, 0.027) 0.152 0.046 (5.17, 0.026) 0.141</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparisons. We show representative examples of articulated animation using our method and FOMM [30], on two datasets of articulated objects: TED-talks (left) and TaiChiHD (right). Zoom in for greater detail. (6.53, 0.033) 0.172 0.075 (17.12, 0.066) 0.203 0.033 (7.07, 0.014) 0.163 0.041 1.27 0.134 0.0223 Ours 0.047 (5.58, 0.027) 0.152 0.064 (13.86, 0.043) 0.172 0.026 (3.75, 0.007) 0.114 0.040 1.28 0.133 0.0206</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Mean test-time absolute rotation error, as a function of training set size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>L 1 (</head><label>1</label><figDesc>AKD, MKR) AED No pca or bg model 0.060 (6.14, 0.033) 0.163 No pca 0.056 (9.58, 0.034) 0.206 No bg model 0.059 (5.55, 0.026) 0.165 Full method 0.048 (5.59, 0.027) 0.152</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Examples of synthetic rectangle dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Visualizations of background movement. From top to bottom we show driving frame, still background, background that moves left, moves right and rotates counterclockwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The reference frame in both is used only as an intermediate coordinate frame between the source and driving image coordinate frames. However, here (in contrast to FOMM) it is not in fact abstract, corresponding to the coordinate frame where the heatmap is whitened (i.e. has zero mean and identity covariance); seeFig. 2. A k S←D is computed per Eq. (3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparing our model with FOMM [30] on TaiChiHD (256), for K = 5, 10 and 20. (Best result in bold.) by (cf. Eq. (4)):</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>VoxCeleb [23] consists of interview videos of different celebrities. We extract square, face regions and downscale them to 256 × 256, following FOMM [30]. The number of frames per video ranges from 64 to 1024. • TaiChiHD [30] consists of cropped videos of full human bodies performing Tai Chi actions. We evaluate on two resolutions of the dataset: 256 × 256 (from FOMM [30]), and a new, 512 × 512 subset, removing videos lacking sufficient resolution to support that size. • MGif [31] is a dataset of .gif files, that depicts 2D cartoon animals. The dataset was collected using google searches. • TED-talks is a new dataset, collected for this paper in order to demonstrate the generalization properties of our model. We cropped the upper part of the human body from the videos, downscaling to 384 × 384. The number of frames per video ranges from 64 to 1024.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Video reconstruction: comparison with the state of the art on five different datasets. For all methods we use K = 10 regions. (Best result in bold.)</figDesc><table><row><cell>2 5</cell><cell>2 6</cell><cell>2 7 Training set size 2 8</cell><cell>2 9</cell><cell>2 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on TaiChiHD (256) dataset with K = 10. (Best result in bold.)</figDesc><table><row><cell>Input</cell><cell>Our method</cell><cell>MSCS</cell><cell>SCOPS</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recycle-gan: Unsupervised video retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The representation and recognition of human movement using temporal templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangled and controllable face image generation via 3d imitative-contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08379</idno>
		<title level="m">Controllable characters extracted from real-world videos</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d guided fine-grained face manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marionette: Few-shot face reenactment preserving identity of unseen targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scops: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised keypoint learning for guiding classconditional video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference</title>
		<meeting>the Neural Information Processing Systems Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised part-based disentangling of object shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Playable video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">pagan: real-time avatars using dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ganimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Make a face: Towards arbitrary high fidelity face manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human motion transfer from poses in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03142</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference</title>
		<meeting>the Neural Information Processing Systems Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motion-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deformable gans for posebased human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Stéphane Lathuilière, and Nicu Sebe</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual generator generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Singular value decomposition and principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis M</forename><surname>Rechtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A practical approach to microarray data analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="91" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Few-shot videoto-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference</title>
		<meeting>the Neural Information Processing Systems Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Conference</title>
		<meeting>the Neural Information Processing Systems Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Every smile is unique: Landmark-guided diverse smile generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 13</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
