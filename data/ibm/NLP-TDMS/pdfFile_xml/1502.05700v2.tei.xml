<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Bayesian Optimization Using Deep Neural Networks Prabhat PRABHAT@LBL.GOV</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015">2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
							<email>jsnoek@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
							<email>rippel@math.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
							<email>kswersky@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
							<email>rkiros@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
							<email>narayanan.sundaram@intel.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mostofa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Patwary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Department of Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="laboratory">Parallel Computing Lab NERSC</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
								<orgName type="institution" key="instit4">Intel Labs</orgName>
								<orgName type="institution" key="instit5">Lawrence Berkeley National Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Bayesian Optimization Using Deep Neural Networks Prabhat PRABHAT@LBL.GOV</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 32 nd International Conference on Machine Learning</title>
						<meeting>the 32 nd International Conference on Machine Learning <address><addrLine>Lille, France</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">37</biblScope>
							<date type="published" when="2015">2015</date>
						</imprint>
					</monogr>
					<note>Copy-right 2015 by the author(s).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.</p><p>In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.</p><p>In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the field of machine learning has seen unprecedented growth due to a new wealth of data, increases in computational power, new algorithms, and a plethora of exciting new applications. As researchers tackle more ambitious problems, the models they use are also becoming more sophisticated. However, the growing complexity of machine learning models inevitably comes with the introduction of additional hyperparameters. These range from design decisions such as the shape of a neural network architecture, to optimization parameters such as learning rates, to regularization hyperparameters such as weight decay. Proper setting of these hyperparameters is critical for performance on difficult problems.</p><p>There are many methods for optimizing over hyperparameter settings, ranging from simplistic procedures like grid or random search <ref type="bibr" target="#b1">(Bergstra &amp; Bengio, 2012)</ref>, to more sophisticated model-based approaches using random forests <ref type="bibr" target="#b24">(Hutter et al., 2011)</ref> or Gaussian processes <ref type="bibr" target="#b46">(Snoek et al., 2012)</ref>. Bayesian optimization is a natural framework for modelbased global optimization of noisy, expensive black-box functions. It offers a principled approach to modeling uncertainty, which allows exploration and exploitation to be naturally balanced during the search. Perhaps the most commonly used model for Bayesian optimization is the Gaussian process (GP) due to its simplicity and flexibility in terms of conditioning and inference.</p><p>However, a major drawback of GP-based Bayesian optimization is that inference time grows cubically in the number of observations, as it necessitates the inversion of a dense covariance matrix. For problems with a very small arXiv:1502.05700v2 [stat.ML] 13 Jul 2015 number of hyperparameters, this has not been an issue, as the minimum is often discovered before the cubic scaling renders further evaluations prohibitive. As the complexity of machine learning models grows, however, the size of the search space grows as well, along with the number of hyperparameter configurations that need to be evaluated before a solution of sufficient quality is found. Fortunately, as models have grown in complexity, computation has become significantly more accessible and it is now possible to train many models in parallel. A natural solution to the hyperparameter search problem is to therefore combine large-scale parallelism with a scalable Bayesian optimization method. The cubic scaling of the GP, however, has made it infeasible to pursue this approach.</p><p>The goal of this work is to develop a method for scaling Bayesian optimization, while still maintaining its desirable flexibility and characterization of uncertainty. To that end, we propose the use of neural networks to learn an adaptive set of basis functions for Bayesian linear regression. We refer to this approach as Deep Networks for Global Optimization (DNGO). Unlike a standard Gaussian process, DNGO scales linearly with the number of function evaluationswhich, in the case of hyperparameter optimization, corresponds to the number of models trained-and is amenable to stochastic gradient training. Although it may seem that we are merely moving the problem of setting the hyperparameters of the model being tuned to setting them for the tuner itself, we show that for a suitable set of design choices it is possible to create a robust, scalable, and effective Bayesian optimization system that generalizes across many global optimization problems.</p><p>We demonstrate the effectiveness of DNGO on a number of difficult problems, including benchmark problems for Bayesian optimization, convolutional neural networks for object recognition, and multi-modal neural language models for image caption generation. We find hyperparameter settings that achieve competitive with state-of-the-art results of 6.37% and 27.4% on CIFAR-10 and CIFAR-100 respectively, and BLEU scores of 25.1 and 26.7 on the Microsoft COCO 2014 dataset using a single model and a 3model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bayesian Optimization</head><p>Bayesian optimization is a well-established strategy for the global optimization of noisy, expensive black-box functions <ref type="bibr" target="#b38">(Mockus et al., 1978)</ref>. For an in-depth review, see <ref type="bibr" target="#b34">Lizotte (2008)</ref>, <ref type="bibr" target="#b4">Brochu et al. (2010)</ref> and <ref type="bibr" target="#b42">Osborne et al. (2009)</ref>. Bayesian optimization relies on the construction of a probabilistic model that defines a distribution over objective functions from the input space to the objective of interest. Conditioned on a prior over the functional form and a set of N observations of input-target pairs D = {(x n , y n )} N n=1 , the relatively cheap posterior over functions is then queried to reason about where to seek the optimum of the expensive function of interest. The promise of a new experiment is quantified using an acquisition function, which, applied to the posterior mean and variance, expresses a trade-off between exploration and exploitation. Bayesian optimization proceeds by performing a proxy optimization over this acquisition function in order to determine the next input to evaluate.</p><p>Recent innovation has resulted in significant progress in Bayesian optimization, including elegant theoretical results <ref type="bibr" target="#b49">(Srinivas et al., 2010;</ref><ref type="bibr" target="#b5">Bull, 2011;</ref><ref type="bibr" target="#b10">de Freitas et al., 2012)</ref>, multitask and transfer optimization <ref type="bibr" target="#b28">(Krause &amp; Ong, 2011;</ref><ref type="bibr" target="#b50">Swersky et al., 2013;</ref><ref type="bibr" target="#b0">Bardenet et al., 2013)</ref> and the application to diverse tasks such as sensor set selection <ref type="bibr" target="#b14">(Garnett et al., 2010)</ref>, the tuning of adaptive Monte Carlo <ref type="bibr" target="#b36">(Mahendran et al., 2012)</ref> and robotic gait control <ref type="bibr" target="#b8">(Calandra et al., 2014b)</ref>.</p><p>Typically, GPs have been used to construct the distribution over functions used in Bayesian optimization, due to their flexibility, well-calibrated uncertainty, and analytic properties <ref type="bibr" target="#b25">(Jones, 2001;</ref><ref type="bibr" target="#b42">Osborne et al., 2009)</ref>. Recent work has sought to improve the performance of the GP approach through accommodating higher dimensional problems <ref type="bibr" target="#b53">(Wang et al., 2013;</ref><ref type="bibr" target="#b11">Djolonga et al., 2013)</ref>, input nonstationarities  and initialization through meta-learning <ref type="bibr" target="#b13">(Feurer et al., 2015)</ref>. Random forests, which scale linearly with the data, have also been used successfully for algorithm configuration by <ref type="bibr" target="#b24">Hutter et al. (2011)</ref> with empirical estimates of model uncertainty.</p><p>More specifically, Bayesian optimization seeks to solve the minimization problem</p><formula xml:id="formula_0">x = argmin x∈X f (x) ,<label>(1)</label></formula><p>where we take X to be a compact subset of R K . In our work, we build upon the standard GP-based approach of <ref type="bibr" target="#b25">Jones (2001)</ref> which uses a GP surrogate and the expected improvement acquisition function <ref type="bibr" target="#b38">(Mockus et al., 1978)</ref>. For the surrogate model hyperparameters Θ, let σ 2 (x; Θ) = Σ(x, x; Θ) be the marginal predictive variance of the probabilistic model, µ(x; D, Θ) be the predictive mean, and define</p><formula xml:id="formula_1">γ(x) = f (x best ) − µ(x; D, Θ) σ(x; D, Θ) ,<label>(2)</label></formula><p>where f (x best ) is the lowest observed value. The expected improvement criterion is defined as</p><formula xml:id="formula_2">a EI (x; D, Θ) = (3) σ(x; D, Θ) [γ(x)Φ(γ(x)) + N (γ(x); 0, 1)] .</formula><p>Here Φ(·) is the cumulative distribution function of a standard normal, and N (·; 0, 1) is the density of a standard normal. Note that numerous alternate acquisition functions and combinations thereof have been proposed <ref type="bibr" target="#b29">(Kushner, 1964;</ref><ref type="bibr" target="#b49">Srinivas et al., 2010;</ref><ref type="bibr" target="#b23">Hoffman et al., 2011)</ref>, which could be used without affecting the analytic properties of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bayesian Neural Networks</head><p>The application of Bayesian methods to neural networks has a rich history in machine learning <ref type="bibr" target="#b35">(MacKay, 1992;</ref><ref type="bibr" target="#b20">Hinton &amp; van Camp, 1993;</ref><ref type="bibr" target="#b6">Buntine &amp; Weigend, 1991;</ref><ref type="bibr" target="#b40">Neal, 1995;</ref><ref type="bibr" target="#b10">De Freitas, 2003)</ref>. The goal of Bayesian neural networks is to uncover the full posterior distribution over the network weights in order to capture uncertainty, to act as a regularizer, and to provide a framework for model comparison. The full posterior is, however, intractable for most forms of neural networks, necessitating expensive approximate inference or Markov chain Monte Carlo simulation. More recently, full or approximate Bayesian inference has been considered for small pieces of the overall architecture. For example, in similar spirit to this work, Lázaro-Gredilla &amp; Figueiras-Vidal (2010); <ref type="bibr" target="#b21">Hinton &amp; Salakhutdinov (2008)</ref> and <ref type="bibr" target="#b7">Calandra et al. (2014a)</ref> considered inference over just the last layer of a neural network. Alternatively, variational approaches are developed in <ref type="bibr" target="#b26">Kingma &amp; Welling (2014)</ref>; <ref type="bibr" target="#b43">Rezende et al. (2014)</ref> and <ref type="bibr" target="#b37">Mnih &amp; Gregor (2014)</ref>, where a neural network is used in a variational approximation to the posterior distribution over the latent variables of a directed generative neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive Basis Regression with Deep Neural Networks</head><p>A key limitation of GP-based Bayesian optimization is that the computational cost of the technique scales cubically in the number of observations, limiting the applicability of the approach to objectives that require a relatively small number of observations to optimize. In this work, we aim to replace the GP traditionally used in Bayesian optimization with a model that scales in a less dramatic fashion, but retains most of the GP's desirable properties such as flexibility and well-calibrated uncertainty. Bayesian neural networks are a natural consideration, not least because of the theoretical relationship between Gaussian processes and infinite Bayesian neural networks <ref type="bibr" target="#b40">(Neal, 1995;</ref><ref type="bibr" target="#b54">Williams, 1996)</ref>. However, deploying these at a large scale is very computationally expensive.</p><p>As such, we take a pragmatic approach and add a Bayesian linear regressor to the last hidden layer of a deep neural network, marginalizing only the output weights of the net while using a point estimate for the remaining parameters. This results in adaptive basis regression, a well-established statistical technique which scales linearly in the number of observations, and cubically in the basis function dimensionality. This allows us to explicitly trade off evaluation time and model capacity. As such, we form the basis using the very flexible and powerful non-linear functions defined by the neural network.</p><p>First of all, without loss of generality and assuming compact support for each input dimension, we scale the input space to the unit hypercube. We denote by φ(·) = [φ 1 (·), . . . , φ D (·)] T the vector of outputs from the last hidden layer of the network, trained on inputs and targets</p><formula xml:id="formula_3">D := {(x n , y n )} N n=1 ⊂ R K × R.</formula><p>We take these to be our set of basis functions. In addition, define Φ to be the design matrix arising from the data and this basis, where Φ nd = φ d (x n ) is the output design matrix, and y the stacked target vector.</p><p>These basis functions are parameterized via the weights and biases of the deep neural network, and these parameters are trained via backpropagation and stochastic gradient descent with momentum. In this training phase, a linear output layer is also fit. This procedure can be viewed as a maximum a posteriori (MAP) estimate of all parameters in the network. Once this "basis function neural network" has been trained, we replace the MAP-parameterized output layer with a Bayesian linear regressor that captures uncertainty in the weights. See Section 3.1.2 for a more elaborate explanation of this choice.</p><p>The predictive mean µ(x; Θ) and variance σ 2 (x; Θ) of the model are then given by (see <ref type="bibr" target="#b3">Bishop, 2006)</ref> </p><formula xml:id="formula_4">µ(x; D, Θ) = m T φ(x) + η(x) ,<label>(4)</label></formula><formula xml:id="formula_5">σ 2 (x; D, Θ) = φ(x) T K −1 φ(x) + 1 β (5) where m = βK −1 Φ Tỹ ∈ R D (6) K = βΦ T Φ + Iα ∈ R D×D .<label>(7)</label></formula><p>Here, η(x) is a prior mean function which is described in Section 3.1.3, andỹ = y − η(x). In addition, α, β ∈ Θ are regression model hyperparameters. We integrate out α and β using slice sampling <ref type="bibr" target="#b39">(Neal, 2000)</ref> according to the methodology of <ref type="bibr" target="#b46">Snoek et al. (2012)</ref> over the marginal likelihood, which is given by</p><formula xml:id="formula_6">log p(y | X, α, β) = D 2 log α + N 2 log β − N 2 log(2π) − β 2 ||ỹ − Φm|| 2 − α 2 m T m − 1 2 log |K| . (8)</formula><p>It is clear that the computational bottleneck of this procedure is the inversion of K. However, note that the size of this matrix grows with the output dimensionality D, rather than the number of observations N as in the GP case. This allows us to scale to significantly more observations than with the GP as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">NETWORK ARCHITECTURE</head><p>A natural concern with the use of deep networks is that they often require significant effort to tune and tailor to specific problems. One can consider adjusting the architecture and tuning the hyperparameters of the neural network as itself a difficult hyperparameter optimization problem. An additional challenge is that we aim to create an approach that generalizes across optimization problems. We found that design decisions such as the type of activation function used significantly altered the performance of the Bayesian optimization routine. For example, in <ref type="figure" target="#fig_1">Figure 2</ref> we see that the commonly used rectified linear (ReLU) function can lead to very poor estimates of uncertainty, which causes the Bayesian optimization routine to explore excessively. Since the bounded tanh function results in smooth functions with realistic variance, we use this nonlinearity in this work; however, if the smoothness assumption needs to be relaxed, a combination of rectified linear functions with a tanh function only on the last layer can also be used in order to bound the basis.</p><p>In order to tune any remaining hyperparameters, such as the width of the hidden layers and the amount of regularization, we used GP-based Bayesian optimization. For each of one to four layers we ran Bayesian optimization using the Spearmint  package to minimize the average relative loss on a series of benchmark global optimization problems. We tuned a global learning rate, momentum, layer sizes, 2 normalization penalties for each set of weights and dropout rates <ref type="bibr" target="#b22">(Hinton et al., 2012)</ref> for each layer. Interestingly, the optimal configuration featured no dropout and very modest 2 normalization. We suspect that dropout, despite having an approximate correction term, causes noise in the predicted mean resulting in a loss of precision. The optimizer instead preferred to restrict capacity via a small number of hidden units. Namely, the optimal architecture is a deep and narrow network with 3 hidden layers and approximately 50 hidden units per layer. We use the same architecture throughout our empirical evaluation, and this architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">MARGINAL LIKELIHOOD VS MAP ESTIMATE</head><p>The standard empirical Bayesian approach to adaptive basis regression is to maximize the marginal likelihood with respect to the parameters of the basis (see Equation <ref type="formula">8)</ref>, thus taking the model uncertainty into account. However, in the context of our method, this requires evaluating the gradient of the marginal likelihood, which requires inverting a D × D matrix on each update of stochastic gradient descent. As this makes the optimization of the net significantly slower, we take a pragmatic approach and optimize the basis using a point estimate and apply the Bayesian linear regression layer post-hoc. We found that both approaches gave qualitatively and empirically similar results, and as such we in practice employ the more efficient one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">QUADRATIC PRIOR</head><p>One of the advantages of Bayesian optimization is that it provides natural avenues for incorporating prior information about the objective function and search space. For example, when choosing the boundaries of the search space, a typical assumption has been that the optimal solution lies somewhere in the interior of the input space. However, by the curse of dimensionality, most of the volume of the space lies very close to its boundaries. Therefore, we select a mean function η(x) (see Equation 4) to reflect our subjective prior beliefs that the function is coarsely approximated by a convex quadratic function centered in the bounded search region, i.e.,</p><formula xml:id="formula_7">η(x) = λ + (x − c) T Λ(x − c)<label>(9)</label></formula><p>where c is the center of the quadratic, λ is an offset and Λ a diagonal scaling matrix. We place a Gaussian prior with mean 0.5 (the center of the unit hypercube) on c, horseshoe <ref type="bibr" target="#b9">(Carvalho et al., 2009</ref>) priors on the diagonal elements Λ kk ∀k ∈ {1, . . . , K} and integrate out b, λ and c using slice sampling over the marginal likelihood. The horseshoe is a so-called one-group prior for inducing sparsity and is a somewhat unusual choice for the weights of a regression model. Here we choose it because it 1) has support only on the positive reals, leading to convex functions, and 2) it has a large spike at zero with a heavy tail, resulting in strong shrinkage for small values while preserving large ones. This last effect is important for handling model misspecification as it allows the quadratic effect to disappear and become a simple offset if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Incorporating input space constraints</head><p>Many problems of interest have complex, possibly unknown bounds, or exhibit undefined behavior in some regions of the input space. These regions can be characterized as constraints on the search space. Recent work <ref type="bibr" target="#b15">(Gelbart et al., 2014;</ref><ref type="bibr" target="#b45">Snoek, 2013;</ref><ref type="bibr" target="#b18">Gramacy &amp; Lee, 2010)</ref> has developed approaches for modeling unknown constraints in GP-based Bayesian optimization by learning a constraint classifier and then discounting expected improvement by the probability of constraint violation.</p><p>More specifically, define c n ∈ {0, 1} to be a binary indicator of the validity of input x n . Also, denote the sets of valid and invalid inputs as V = {(x n , y n ) | c n = 1} and I = {(x n , y n ) | c n = 0}, respectively. Note that D := V ∪ I. Lastly, let Ψ be the collection of constraint hyperparameters. The modified expected improvement function can be written as</p><formula xml:id="formula_8">a CEI (x; D, Θ, Ψ) = a EI (x; V, Θ)P [c = 1 | x, D, Ψ] .</formula><p>In this work, to model the constraint surface, we similarly replace the Gaussian process with the adaptive basis model, integrating out the output layer weights:</p><formula xml:id="formula_9">P[c = 1 | x, D, Ψ] = w P [c = 1 | x, D, w, Ψ] P(w; Ψ)dw .<label>(10)</label></formula><p>In this case, we use a Laplace approximation to the posterior. For noisy constraints we perform Bayesian logistic regression, using a logistic likelihood function for P [c = 1 | x, D, w, Ψ]. For noiseless constraints, we replace the logistic function with a step function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parallel Bayesian Optimization</head><p>Obtaining a closed form expression for the joint acquisition function across multiple inputs is intractable in general <ref type="bibr" target="#b16">(Ginsbourger &amp; Riche, 2010)</ref>. However, a successful Monte Carlo strategy for parallelizing Bayesian optimization was developed in <ref type="bibr" target="#b46">Snoek et al. (2012)</ref>. The idea is to marginalize over the possible outcomes of currently running experiments when making a decision about a new experiment to run. Following this strategy, we use the posterior predictive distribution given by Equations 4 and 5 to generate a set of fantasy outcomes for each running experiment which we then use to augment the existing dataset. By averaging over sets of fantasies, we can perform approximate marginalization when computing EI for a candidate point. We note that this same idea works with the constraint network, where instead of computing marginalized EI, we would compute the marginalized probability of violating a constraint.</p><p>To that end, given currently running jobs with inputs {x j } J j=1 , the marginalized acquisition function a MCEI (·; D, Θ, Ψ) is given by</p><formula xml:id="formula_10">a MCEI (x; D, {x j } J j=1 , Θ, Ψ) = a CEI (x; D ∪ {(x j , y j )} J j=1 , Θ, Ψ) × P {c j , y j } J j=1 | D, {x} J j=1 dy 1 ...dy n dc 1 ...dc n .</formula><p>When this strategy is applied to a GP, the cost of computing EI for a candidate point becomes cubic in the size of the augmented dataset. This restricts both the number of running experiments that can be tolerated, as well as the number of fantasy sets used for marginalization. With DNGO it is possible to scale both of these up to accommodate a much higher degree of parallelism.</p><p>Finally, following the approach of <ref type="bibr" target="#b46">Snoek et al. (2012)</ref> we integrate out the hyperparameters of the model to obtain our final integrated acquisition function. For each iteration of the optimization routine we pick the next input, x * , to evaluate according to</p><formula xml:id="formula_11">x * = argmax x a MCEI (x; D, {x j } J j=1 ) ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_12">a MCEI (x; D, {x j } J j=1 ) = a MCEI (x; D, {x j } J j=1 , Θ, Ψ) dΘdΨ.<label>(12)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">HPOLib Benchmarks</head><p>In the literature, there exist several other methods for model-based optimization. Among these, the most popular variants in machine learning are the random forest-based SMAC procedure <ref type="bibr" target="#b24">(Hutter et al., 2011)</ref> and the tree Parzen estimator (TPE) <ref type="bibr" target="#b2">(Bergstra et al., 2011)</ref>. These are faster to fit than a Gaussian process and scale more gracefully with large datasets, but this comes at the cost of a more heuristic treatment of uncertainty. By contrast, DNGO provides a balance between scalability and the Bayesian marginalization of model parameters and hyperparameters.  <ref type="bibr" target="#b56">Zaremba et al. (2015)</ref>. The baseline LBL tuned by a human expert and the Soft and Hard Attention models are reported in <ref type="bibr" target="#b55">Xu et al. (2015)</ref>. We see that ensembling our top models resulting from the optimization further improves results significantly. We noticed that there were distinct multiple local optima in the hyperparameter space, which may explain the dramatic improvement from ensembling a small subset of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To demonstrate the effectiveness of our approach, we compare DNGO to these scalable model-based optimization variants, as well as the input-warped Gaussian process method of <ref type="bibr" target="#b47">Snoek et al. (2014)</ref> on the benchmark set of continuous problems from the HPOLib package <ref type="bibr" target="#b12">(Eggensperger et al., 2013)</ref>. As <ref type="table" target="#tab_0">Table 1</ref> shows, DNGO significantly outperforms SMAC and TPE, and is competitive with the Gaussian process approach. This shows that, despite vast improvements in scalability, DNGO retains the statistical efficiency of the Gaussian process method in terms of the number of evaluations required to find the minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Caption Generation</head><p>In this experiment, we explore the effectiveness of DNGO on a practical and expensive problem where highly parallel evaluation is necessary to make progress in a reasonable amount of time. We consider the task of image caption generation using multi-modal neural language models. Specifically, we optimize the hyperparameters of the logbilinear model (LBL) from <ref type="bibr" target="#b27">Kiros et al. (2014)</ref> to maximize (a) "A person riding a wave in the ocean." (b) "A bird sitting on top of a field." (c) "A horse is riding a horse." <ref type="figure" target="#fig_2">Figure 3</ref>. Sample test images and generated captions from the best LBL model on the COCO 2014 dataset. The first two captions sensibly describe the contents of their respective images, while the third is offensively inaccurate.</p><p>the BLEU score of a validation set from the recently released COCO dataset <ref type="bibr" target="#b33">(Lin et al., 2014)</ref>. In our experiments, each evaluation of this model took an average of 26.6 hours.</p><p>We optimize learning parameters such as learning rate, momentum and batch size; regularization parameters like dropout and weight decay for word and image representations; and architectural parameters such as the context size, whether to use the additive or multiplicative version, the size of the word embeddings and the multi-modal representation size 1 . The final parameter is the number of factors, which is only relevant for the multiplicative model. This adds an interesting challenge, since it is only relevant for half of the hyperparameter space. This gives a total of 11 hyperparameters. Even though this number seems small, this problem offers a number of challenges which render its optimization quite difficult. For example, in order to not lose any generality, we choose broad box constraints for the hyperparameters; this, however, renders most of the volume of the model space infeasible. In addition, quite a few of the hyperparameters are categorical, which introduces severe non-stationarities in the objective surface.</p><p>Nevertheless, one of the advantages of a scalable method is the ability to highly parallelize hyperparameter optimization. In this way, high quality settings can be found after only a few sequential steps. To test DNGO in this scenario, we optimize the log-bilinear model with up to 800 parallel evaluations.</p><p>Running between 300 and 800 experiments in parallel (determined by cluster availability), we proposed and evaluated approximately 2500 experiments-the equivalent of over 2700 CPU days-in less than one week. Using the BLEU-4 metric, we optimized the validation set performance and the best LBL model found by DNGO outperforms recently proposed models using LSTM recurrent neural networks <ref type="bibr" target="#b56">(Zaremba et al., 2015;</ref><ref type="bibr" target="#b55">Xu et al., 2015)</ref> on 1 Details are provided in the supplementary material. the test set. This is remarkable, as the LBL is a relatively simple approach. Ensembling this top model with the second and third best (under the validation metric) LBL models resulted in a test-set BLEU score 2 of 26.7, significantly outperforming the LSTM-based approaches. We noticed that there were distinct multiple local optima in the hyperparameter space, which may explain the dramatic improvement from ensembling a small number of models. We show qualitative examples of generated captions on test images in   <ref type="table">Table 4</ref>. We use our algorithm to optimize validation set error as a function of various hyperparameters of a convolutional neural network. We report the test errors of the models with the optimal hyperparameter configurations, as compared to current state-ofthe-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer type # Filters Window Stride</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Deep Convolutional Neural Networks</head><p>Finally, we use DNGO on a pair of highly competitive deep learning visual object recognition benchmark problems. We tune the hyperparameters of a deep convolutional neural network on the CIFAR-10 and CIFAR-100 datasets.</p><p>Our approach is to establish a single, generic architecture, and specialize it to various tasks via individualized hyperparameter tuning. As such, for both datasets, we employed the same generic architecture inspired by the configuration proposed in <ref type="bibr" target="#b48">Springenberg et al. (2014)</ref>, which was shown to attain strong classification results. This architecture is detailed in <ref type="table">Table 5</ref>.</p><p>For this architecture, we tuned the momentum, learning rate, 2 weight decay coefficients, dropout rates, standard deviations of the random i.i.d. Gaussian weight initializations, and corruption bounds for various data augmentations: global perturbations of hue, saturation and value, random scalings, input pixel dropout and random horizontal reflections. We optimized these over a validation set of 10,000 examples drawn from the training set, running each network for 200 epochs. See <ref type="figure" target="#fig_3">Figure 4</ref> for a visualization of the hyperparameter tuning procedure.</p><p>We performed the optimization on a cluster of Intel R Xeon Phi TM coprocessors, with 40 jobs running in parallel using a kernel library that has been highly optimized for efficient computation on the Intel R Xeon Phi TM coprocessor 3 . For the optimal hyperparameter configuration found, we ran a final experiment for 350 epochs on the entire training set, and report its result.</p><p>Our optimal models for CIFAR-10 and CIFAR-100 achieved test errors of 6.37% and 27.4% respectively. A comparison to published state-of-the-art results <ref type="bibr" target="#b17">(Goodfellow et al., 2013;</ref><ref type="bibr" target="#b52">Wan et al., 2013;</ref><ref type="bibr" target="#b32">Lin et al., 2013;</ref><ref type="bibr" target="#b31">Lee et al., 2014;</ref><ref type="bibr" target="#b48">Springenberg et al., 2014)</ref> is given in <ref type="table">Table 4</ref>. We see that the parallelized automated hyperparameter tuning procedure obtains models that are highly competitive with the state-of-the-art in just a few sequential steps.</p><p>A comprehensive overview of the setup, the architecture, the tuning and the optimum configuration can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced deep networks for global optimization, or DNGO, which enables efficient optimization of noisy, expensive black-box functions. While this model maintains desirable properties of the GP such as tractability and principled management of uncertainty, it greatly improves its scalability from cubic to linear as a function of the number of observations. We demonstrate that while this model allows efficient computation, its performance is nevertheless competitive with existing state-of-the-art approaches for Bayesian optimization. We demonstrate empirically that it is especially well suited to massively parallel hyperparameter optimization.</p><p>While adaptive basis regression with neural networks provides one approach to the enhancement of scalability, other models may also present promise. One promising line of work, for example by <ref type="bibr" target="#b41">Nickson et al. (2014)</ref>, is to introduce a similar methodology by instead employing the sparse Gaussian process as the underlying probabilistic model <ref type="bibr" target="#b44">(Snelson &amp; Ghahramani, 2005;</ref><ref type="bibr" target="#b51">Titsias, 2009;</ref><ref type="bibr" target="#b19">Hensman et al., 2013)</ref>. Horizontal reflections Each input is reflected horizontally with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>Pixel dropout Each input element is dropped independently and identically with some random probability D0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Initialization and training procedure</head><p>We initialize the weights of each convolution layer m with i.i.d zero-mean Gaussians with standard deviation σ √ Fm where Fm is the number of parameters per filter for that layer. We chose this parametrization to produce activations whose variances are invariant to filter dimensionality. We use the same standard deviation for all layers but the input, for which we dedicate its own hyperparameter σI as it oftentimes varies in scale from deeper layers in the network. We train the model using the standard stochastic gradient descent and momentum optimizer. We use minibatch size of 128, and tune the momentum and learning rate, which we parametrize as 1 − 0.1 M and 0.1 L respectively. We anneal the learning rate by a factor of 0.1 at epochs 130 and 190. We terminate the training after 200 epochs.</p><p>We regularize the weights of all layers with weight decay coef-ficient W . We apply dropout on the outputs of the max pooling layers, and tune these rates D1, D2 separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Testing procedure</head><p>We evaluate the performance of the learned model by averaging its log-probability predictions on 100 samples drawn from the input corruption distribution, with masks drawn from the unit dropout distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional figures for image caption generation</head><p>In <ref type="figure" target="#fig_4">Figures 5(a)</ref>  Context size The goal of the LBL is to predict the next word given a sequence of words. The context size dictates the number of words in this sequence.</p><p>Learning rate, momentum, batch size These are optimization parameters used during stochastic gradient learning of the LBL model parameters. The optimization over learning rate is carried out in log-space, but the proposed learning rate is exponentiated before being passed to the training procedure.</p><p>Hidden layer size This controls the size of the joint hidden representation for words and images.</p><p>Embedding size Words are represented by feature embeddings rather than one-hot vectors. This is the dimensionality of the embedding.</p><p>Dropout A regularization parameter that determines the amount of dropout to be added to the hidden layer.  <ref type="figure" target="#fig_4">Figure 5</ref>(a), these are represented as a planar histogram, where the shade of each bin indicates the total count within it. The current best validation score discovered is traced in black. <ref type="figure" target="#fig_4">Figure 5(b)</ref> shows a scatter plot of the validation score of all the experiments in the order in which they finished. This projection demonstrates the exploration-versus-exploitation paradigm of Bayesian Optimization, in which the algorithm trades off visiting unexplored parts of the space, and focusing on parts which show promise.</p><p>Context decay, Word decay L2 regularization on the input and output weights respectively. Like the learning rate, these are optimized in log-space as they vary over several orders of magnitude.</p><p>Factors  <ref type="table">Table 7</ref>. Specification of the hyperparametrization scheme, and optimal hyperparameter configurations found for the multimodal neural language model. For parameters marked log-space, the log is given to the Bayesian optimization routine and the result is exponentiated before being passed into the multimodal neural language model for training. Square brackets denote a range of parameters, while curly braces denote a set of options.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The time per suggested experiment for our method compared to the state-of-the-art GP based approach from<ref type="bibr" target="#b47">Snoek et al. (2014)</ref> on the six dimensional Hartmann function. We ran each algorithm on the same 32 core system with 80GB of RAM five times and plot the mean and standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A comparison of the predictive mean and uncertainty learned by the model when using 2(a) only tanh, 2(c) only rectified linear (ReLU) activation functions or 2(b) ReLU's but a tanh on the last hidden layer. The shaded regions correspond to standard deviation envelopes around the mean. The choice of activation function significantly modifies the basis functions learned by the model. Although the ReLU, which is the standard for deep neural networks, is highly flexible, we found that its unbounded activation can lead to extremely large uncertainty estimates. Subfigure 2(d) illustrates the overall architecture of the DNGO model. Dashed lines correspond to weights that are marginalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Further figures showing the BLEU score as a function of the iteration of Bayesian optimization are provided in the supplementary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Validation errors on CIFAR-100 corresponding to different hyperparameter configurations as evaluated over time. These are represented as a planar histogram, where the shade of each bin indicates the total count within it. The current best validation error discovered is traced in black. This projection demonstrates the exploration-versus-exploitation paradigm of Bayesian optimization, in which the algorithm trades off visiting unexplored parts of the space, and focusing on parts which show promise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Validation BLEU-4 Score on MS COCO corresponding to different hyperparameter configurations as evaluated over time. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of DNGO on global optimization benchmark problems versus scalable (TPE, SMAC) and non-scalable (Spearmint) Bayesian optimization methods. All problems are minimization problems. For each problem, each method was run 10 times to produce error bars.</figDesc><table><row><cell>Experiment</cell><cell># Evals</cell><cell>SMAC</cell><cell>TPE</cell><cell>Spearmint</cell><cell>DNGO</cell></row><row><cell>Branin (0.398)</cell><cell>200</cell><cell>0.655 ± 0.27</cell><cell>0.526 ± 0.13</cell><cell cols="2">0.398 ± 0.00 0.398 ± 0.00</cell></row><row><cell>Hartmann6 (-3.322)</cell><cell cols="5">200 −2.977 ± 0.11 −2.823 ± 0.18 −3.3166 ± 0.02 −3.319 ± 0.00</cell></row><row><cell>Logistic Regression</cell><cell>100</cell><cell>8.6 ± 0.9</cell><cell>8.2 ± 0.6</cell><cell>6.88 ± 0.0</cell><cell>6.89 ± 0.04</cell></row><row><cell>LDA (On grid)</cell><cell>50</cell><cell>1269.6 ± 2.9</cell><cell>1271.5 ± 3.5</cell><cell cols="2">1266.2 ± 0.1 1266.2 ± 0.0</cell></row><row><cell>SVM (On grid)</cell><cell>100</cell><cell>24.1 ± 0.1</cell><cell>24.2 ± 0.0</cell><cell>24.1 ± 0.1</cell><cell>24.1 ± 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This work was supported by the Director, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. This work used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. We would like to acknowledge the NERSC systems staff, in particular Helen He and Harvey Wasserman, for providing us with generous access to the Babbage Xeon Phi testbed.HSV We shift the hue, saturation and value fields of each input by global constants bH ∼ U (−BH , BH ), bS ∼ U (−BS, BS), bV ∼ U (−BV , BV ). Similarly, we globally stretch the saturation and value fields by global constants aS ∼ U ( 11+A S , 1 + AS), aV ∼ U ( 1 1+A V , 1 + AV ).Translations We crop each input to size 27 × 27, where the window is chosen randomly and uniformly.</figDesc><table><row><cell>Layer type</cell><cell cols="3"># Filters Window Stride</cell></row><row><cell>Convolution</cell><cell>96</cell><cell>3 × 3</cell></row><row><cell>Convolution</cell><cell>96</cell><cell>3 × 3</cell></row><row><cell>Max pooling</cell><cell></cell><cell>3 × 3</cell><cell>2</cell></row><row><cell>Convolution</cell><cell>192</cell><cell>3 × 3</cell></row><row><cell>Convolution</cell><cell>192</cell><cell>3 × 3</cell></row><row><cell>Convolution</cell><cell>192</cell><cell>3 × 3</cell></row><row><cell>Max pooling</cell><cell></cell><cell>3 × 3</cell><cell>2</cell></row><row><cell cols="4">The image caption generation computations in this paper were run Convolution 192 3 × 3</cell></row><row><cell cols="4">on the Odyssey cluster supported by the FAS Division of Science, Convolution 192 1 × 1</cell></row><row><cell cols="4">Research Computing Group at Harvard University. We would like to acknowledge the FASRC staff and in particular James Cuff for Convolution 10/100 1 × 1</cell></row><row><cell cols="2">providing generous access to Odyssey. Global averaging</cell><cell>6 × 6</cell></row><row><cell cols="4">Jasper Snoek is a fellow in the Harvard Center for Research on Softmax</cell></row><row><cell cols="4">Computation and Society. Kevin Swersky is the recipient of an</cell></row><row><cell cols="4">Ontario Graduate Scholarship (OGS). This work was partially</cell></row><row><cell cols="4">funded by NSF IIS-1421780, the Natural Sciences and Engineer-Table 5. Our convolutional neural network architecture. This</cell></row><row><cell cols="4">ing Research Council of Canada (NSERC) and the Canadian In-choice was chosen to be maximally generic. Each convolution</cell></row><row><cell cols="2">stitute for Advanced Research (CIFAR). layer is followed by a ReLU nonlinearity.</cell><cell></cell></row><row><cell cols="4">Scalings Each input is scaled by some factor</cell></row><row><cell>s ∼ U ( 1 1+S , 1 + S).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The rank of the weight tensor. Only relevant for the multiplicative model.HyperparameterNotation Support of prior CIFAR-10 Optimum CIFAR-100 Optimum Specification of the hyperparametrization scheme, and optimal hyperparameter configurations found.</figDesc><table><row><cell>Momentum</cell><cell>M</cell><cell>[0.5, 2]</cell><cell>1.6242</cell><cell>1.3339</cell></row><row><cell>Learning rate</cell><cell>L</cell><cell>[1, 4]</cell><cell>2.7773</cell><cell>2.1205</cell></row><row><cell>Initialization deviation</cell><cell>σ I</cell><cell>[0.5, 1.5]</cell><cell>0.83359</cell><cell>1.5570</cell></row><row><cell cols="2">Input initialization deviation σ</cell><cell>[0.01, 1]</cell><cell>0.025370</cell><cell>0.13556</cell></row><row><cell>Hue shift</cell><cell>B H</cell><cell>[0, 45]</cell><cell>31.992</cell><cell>19.282</cell></row><row><cell>Saturation scale</cell><cell>A S</cell><cell>[0, 0.5]</cell><cell>0.31640</cell><cell>0.30780</cell></row><row><cell>Saturation shift</cell><cell>B S</cell><cell>[0, 0.5]</cell><cell>0.10546</cell><cell>0.14695</cell></row><row><cell>Value scale</cell><cell>A S</cell><cell>[0, 0.5]</cell><cell>0.13671</cell><cell>0.13668</cell></row><row><cell>Value shift</cell><cell>B S</cell><cell>[0, 0.5]</cell><cell>0.24140</cell><cell>0.010960</cell></row><row><cell>Pixel dropout</cell><cell>D 0</cell><cell>[0, 0.3]</cell><cell>0.19921</cell><cell>0.00056598</cell></row><row><cell>Scaling</cell><cell>S</cell><cell>[0, 0.3]</cell><cell>0.24140</cell><cell>0.12463</cell></row><row><cell>L2 weight decay</cell><cell>W</cell><cell>[2, 5]</cell><cell>4.2734</cell><cell>3.1133</cell></row><row><cell>Dropout 1</cell><cell>D 1</cell><cell>[0, 0.7]</cell><cell>0.082031</cell><cell>0.081494</cell></row><row><cell>Dropout 2</cell><cell>D 2</cell><cell>[0, 0.7]</cell><cell>0.67265</cell><cell>0.38364</cell></row><row><cell cols="3">Hyperparameter Support of prior</cell><cell>Notes</cell><cell>COCO Optimum</cell></row><row><cell>Model</cell><cell cols="2">{additive,multiplicative}</cell><cell></cell><cell>additive</cell></row><row><cell>Context size</cell><cell>[3, 25]</cell><cell></cell><cell></cell><cell>5</cell></row><row><cell>Learning rate</cell><cell>[0.001, 10]</cell><cell></cell><cell>Log-space</cell><cell>0.43193</cell></row><row><cell>Momentum</cell><cell>[0, 0.9]</cell><cell></cell><cell></cell><cell>0.23269</cell></row><row><cell>Batch size</cell><cell>[20, 200]</cell><cell></cell><cell></cell><cell>40</cell></row><row><cell cols="2">Hidden layer size [100, 2000]</cell><cell></cell><cell></cell><cell>441</cell></row><row><cell>Embedding size</cell><cell cols="2">{50, 100, 200}</cell><cell></cell><cell>100</cell></row><row><cell>Dropout</cell><cell>[0, 0.7]</cell><cell></cell><cell></cell><cell>0.14847</cell></row><row><cell>Word decay</cell><cell>[10 −9 , 10 −3 ]</cell><cell></cell><cell>Log-space</cell><cell>2.98456 −7</cell></row><row><cell>Context decay</cell><cell>[10 −9 , 10 −3 ]</cell><cell></cell><cell>Log-space</cell><cell>1.09181 −8</cell></row><row><cell>Factors</cell><cell>[50, 200]</cell><cell></cell><cell cols="2">Multiplicative model only -</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have verified that our BLEU score evaluation is consistent across reported results. We used a beam search decoding for our test predictions with the LBL model.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional neural network experiment specifications</head><p>In this section we elaborate on the details of the network architecture, training and the meta-optimization. In the following subsections we elaborate on the hyperparametrization scheme. The priors on the hyperparameters as well as their optimal configurations for the two datasets can be found in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Architecture</head><p>The model architecture is specified in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Data augmentation</head><p>We corrupt each input in a number of ways. Below we describe our parametrization of these corruptions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Collaborative hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Bayesian interactive optimization approach to procedural animation design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convergence rates of efficient global optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="2879" to="2904" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bayesian back-propagation. Complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="603" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Manifold Gaussian processes for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.5876</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental evaluation of Bayesian optimization on bipedal locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seyfarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handling sparsity via the horseshoe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exponential regret bounds for Gaussian process bandits with deterministic observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Trinity College, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Bayesian methods for neural networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High dimensional Gaussian process bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards an empirical foundation for assessing Bayesian optimization of hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Optimization in Theory and Practice</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Initializing Bayesian hyperparameter optimization via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian optimization for sensor set selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Sensor Networks</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian optimization with unknown constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dealing with asynchronicity in parallel Gaussian process based global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginsbourger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Riche</surname></persName>
		</author>
		<ptr target="http://hal.archives-ouvertes.fr/hal-00507632" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimization under unknown constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1004.4027</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian processes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Keeping neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequential modelbased optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Learning and Intelligent Optimization</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A taxonomy of global optimization methods based on response surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contextual Gaussian process bandit optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new method for locating the maximum point of an arbitrary multipeak curve in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Marginalized neural network mixtures for large-scale regression. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Figueiras-Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1345" to="1351" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Practical Bayesian Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lizotte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Edmonton, Alberta</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive MCMC with Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hamze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The application of Bayesian methods for seeking the extremum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mockus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tiesis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zilinskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Towards Global Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="705" to="767" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated machine learning using stochastic algorithm tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaussian processes for global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning and Intelligent Optimization</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and variational inference in deep latent Gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse Gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bayesian Optimization and Semiparametric Models with Applications to Assistive Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Input warping for Bayesian optimization of non-stationary functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6806" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: no regret and experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian optimization in high dimensions via random embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Computing with infinite networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044v2</idno>
		<title level="m">Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
