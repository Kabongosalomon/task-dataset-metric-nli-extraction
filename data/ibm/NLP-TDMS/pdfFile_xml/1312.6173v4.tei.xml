<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Distributed Representations without Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>phil.blunsom@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Distributed Representations without Word Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applications such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are semantically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words are increasingly being used to achieve high levels of generalisation within language modelling tasks. Successful applications of this approach include word-sense disambiguation, word similarity and synonym detection (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>). Subsequent work has also attempted to learn distributed semantics of larger structures, allowing us to apply distributed representation to tasks such as sentiment analysis or paraphrase detection (i.a. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>). At the same time a second strand of work has focused on transferring linguistic knowledge across languages, and particularly from English into low-resource languages, by means of distributed representations at the word level <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Currently, work on compositional semantic representations focuses on monolingual data while the cross-lingual work focuses on word level representations only. However, it appears logical that these two strands of work should be combined as there exists a plethora of parallel corpora with aligned data at the sentence level or beyond which could be exploited in such work. Further, sentence aligned data provides a plausible concept of semantic similarity, which can be harder to define at the word level. Consider the case of alignment between a German compound noun (e.g. "Schwerlastverkehr") and its English equivalent ("heavy goods vehicle traffic"). Semantic alignment at the phrase level here appears far more plausible than aligning individual tokens for semantic transfer.</p><p>Using this rationale, and building on both work related to learning cross-lingual embeddings as well as to compositional semantic representations, we introduce a model that learns cross-lingual embeddings at the sentence level. In the following section we will briefly discuss prior work in these two fields before going on to describe the bilingual training signal that we developed for learning multilingual compositional embeddings. Subsequently, we will describe our model in greater detail as well as its training procedure and experimental setup. Finally, we perform a number of evaluations and demonstrate that our training signal allows a very simple compositional vector model to outperform the state of the art on a task designed to evaluate its ability to transfer semantic information across languages. Unlike other work in this area, our model does not require word aligned data. In fact, while we evaluate our model on sentence aligned data in this paper, there is no theoretical requirement for this and technically our algorithm could also be applied to document-level parallel data or even comparable data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models of Compositional Distributed Semantics</head><p>In the case of representing individual words as vectors, the distributional account of semantics provides a plausible explanation of what is encoded in a word vector. This follows the idea that the meaning of a word can be determined by "the company it keeps" <ref type="bibr" target="#b10">[11]</ref>, that is by the context it appears in. Such context can easily be encoded in vectors using collocational methods, and is also underlying other methods of learning word embeddings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>For a number of important problems, semantic representations of individual words do not suffice, but instead a semantic representation of a larger structure-e.g. a phrase or a sentence-is required. This was highlighted in <ref type="bibr" target="#b9">[10]</ref>, who proposed a mechanism for modifying a word's representation based on its individual context. The distributional account of semantics can, due to sparsity, not be applied to such larger linguistic units. A notable exception perhaps is Baroni and Zamparelli <ref type="bibr" target="#b0">[1]</ref>, who learned distributional representations for adjective noun pairs using a collocational approach on a corpus of unprecedented size. The bigram representations learned from that corpus were subsequently used to learn lexicalised composition functions for the constituent words.</p><p>Most alternative attempts to extract such higher-level semantic representations have focused on learning composition functions that represent the semantics of a larger structure as a function of the representations of its parts. <ref type="bibr" target="#b20">[21]</ref> provides an evaluation of a number of simple composition functions applied to bigrams. Applied recursively, such approaches can then easily be reconciled with the co-occurrence based word level representations. There are a number of proposals motivating such recursive or deep composition models. Notably, <ref type="bibr" target="#b2">[3]</ref> propose a tensor-based model for semantic composition and, similarly, <ref type="bibr" target="#b3">[4]</ref> develop a framework for semantic composition by combining distributional theory with pregroup grammars. The latter framework was empirically evaluated and supported by the results in <ref type="bibr" target="#b11">[12]</ref>. More recently, various forms of recursive neural networks have successfully been used for semantic composition and related tasks such as sentiment analysis. Such models include recursive autoencoders <ref type="bibr" target="#b23">[24]</ref>, matrix-vector recursive neural networks <ref type="bibr" target="#b24">[25]</ref>, untied recursive neural networks <ref type="bibr" target="#b13">[14]</ref> or convolutional networks <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Embeddings</head><p>Much research has been devoted to the task of inducing distributed semantic representations for single languages. In particular English, with its large number of annotated resources, has enjoyed most attention. Recently, progress has been made at representation learning for languages with fewer available resources. Klementiev et al. <ref type="bibr" target="#b15">[16]</ref> described a form of multitask learning on word-aligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. <ref type="bibr" target="#b12">[13]</ref>, proposed a method for inducing cross-lingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. This approach has recently been extended by <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another. Is was demonstrated that this approach can be applied to improve tasks related to machine translation. Their CBOW model is also worth noting for its similarities to the composition function used in this paper. Using a slightly different approach, <ref type="bibr" target="#b28">[29]</ref>, also learned bilingual embeddings for machine translation. It is important to note that, unlike our proposed system, all of these methods require word aligned parallel data for training.</p><p>Two recent workshop papers deserve mention in this respect. Both Lauly et al. <ref type="bibr" target="#b16">[17]</ref> and Sarath Chandar et al. <ref type="bibr" target="#b22">[23]</ref> propose methods for learning word embeddings by exploiting bilingual data, not unlike the method proposed in this paper. Instead of the noise-contrastive method developed in this paper, both groups of authors make use of autoencoders to encode monolingual representations and to support the bilingual transfer.</p><p>So far almost all of this work has been focused on learning multilingual representations at the word level. As distributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to also attempt to induce these using cross-lingual data. This paper provides a first step in that direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>Language acquisition in humans is widely seen as grounded in sensory-motor experience <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>. Based on this idea, there have been some attempts at using multi-modal data for learning better vector representations of words (e.g. <ref type="bibr" target="#b25">[26]</ref>). Such methods, however, are not easily scalable across languages or to large amounts of data for which no secondary or tertiary representation might exist.</p><p>We abstract the underlying principle one step further and attempt to learn semantics from multilingual data. The idea is that, given enough parallel data, a shared representation would be forced to capture the common elements between sentences from different languages. What two parallel sentences have in common, of course, is the semantics of those two sentences. Using this data, we propose a novel method for learning vector representations at the word level and beyond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bilingual Signal</head><p>Exploiting the semantic similarity of parallel sentences across languages, we can define a simple bilingual (and trivially multilingual) error function as follows: Given a compositional sentence model (CVM) M A , which maps a sentence to a vector, we can train a second CVM M B using a corpus C A,B of parallel data from the language pair A, B. For each pair of parallel sentences (a, b) ∈ C A,B , we attempt to minimize</p><formula xml:id="formula_0">E dist (a, b) = a root − b root 2 (1)</formula><p>where a root is the vector representing sentence a and b root the vector representing sentence b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The BICVM Model</head><p>A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents. We assume individual words to be represented by vectors (x ∈ R d ).</p><p>Previous methods employ binary parse trees on the data (e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>) and use weighted or multiplicative composition functions. Under such a setup, where each node in the tree is terminal or has two children (p → c 0 , c 1 ), a binary composition function could take the following form:</p><formula xml:id="formula_1">p = g (W e [c 0 ; c 1 ] + b e )<label>(2)</label></formula><p>where [c 0 ; c 1 ] is the concatenation of the two child vectors, W e ∈ R d×2d and b e ∈ R d the encoding matrix and bias, respectively, and g an element-wise activation function such as the hyperbolic tangent. For the purposes of evaluation the bilingual signal proposed above, we simplify this composition function by setting all weight matrices to the identity and all biases to zero. Thereby the CVM reduces to a simple additive composition function:</p><formula xml:id="formula_2">a root = |a| i=0 a i<label>(3)</label></formula><p>Of course, this is a very simplified CVM, as such a bag-of-words approach no longer accounts for word ordering and other effects which a more complex CVM might capture. However, for the purposes of this evaluation (and with the experimental evaluation in mind), such a simplistic composition function should be sufficient to evaluate the novel objective function proposed here. Using this additive CVM we want to optimize the bilingual error signal defined above (Eq. 1). For the moment, assume that M A is a perfectly trained CVM such that a root represents the semantics of the sentence a. Further, due to the use of parallel data, we know that a and b are semantically equivalent. Hence we transfer the semantic knowledge contained in M A onto M B , by learning θ M B to minimize:</p><formula xml:id="formula_3">E bi (C A,B ) = (a,b)∈C A,B E dist (a, b)<label>(4)</label></formula><p>Of course, this objective function assumes a fully trained model which we do not have at this stage. While this can be a useful objective for transferring linguistic knowledge into low-resource languages <ref type="bibr" target="#b15">[16]</ref>, this precondition is not helpful when there is no model to learn from in first place. We resolve this issue by jointly training both models M A and M B .</p><p>Applying E bi to parallel data ensures that both models learn a shared representation at the sentence level. As the parallel input sentences share the same meaning, it is reasonable to assume that minimizing E bi will force the model to learn their semantic representation. Let θ bi = θ M A ∪ θ M B . The joint objective function J(θ bi ) thus becomes:</p><formula xml:id="formula_4">J(θ bi ) = E bi (C A,B ) + λ 2 θ bi 2<label>(5)</label></formula><p>where λ θ bi 1 is the L 2 regularization term.</p><p>It is apparent that this joint objective J(θ bi ) is degenerate. The models could learn to reduce all embeddings and composition weights to zero and thereby minimize the objective function. We address this issue by employing a form of contrastive estimation penalizing small distances between non-parallel sentence pairs. For every pair of parallel sentences (a, b) we sample a number of additional sentences n ∈ C B , which-with high probability-are not exact translations of a. This is comparable to the second term of the loss function of a large margin nearest neighbour classifier (see Eq. 12 in <ref type="bibr" target="#b27">[28]</ref>):</p><formula xml:id="formula_5">E noise (a, b, n) = [1 + E dist (a, b) − E dist (a, n)] +<label>(6)</label></formula><p>where [x] + = max(x, 0) denotes the standard hinge loss. Thus, the final objective function to minimize for the BICVM model is:</p><formula xml:id="formula_6">J(θ bi ) = (a,b)∈C A,B k i=1 E noise (a, b, n i ) + λ 2 θ bi 2 (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Learning</head><p>Given the objective function as defined above, model learning can employ the same techniques as any monolingual CVM. In particular, as the objective function is differentiable, we can use standard gradient descent techniques such as stochastic gradient descent, L-BFGS or the adaptive gradient algorithm AdaGrad <ref type="bibr" target="#b7">[8]</ref>. Within each monolingual CVM, we use backpropagation through structure after applying the joint error to each sentence level node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Parameters</head><p>All model weights were randomly initialised using a Gaussian distribution. There are a number of parameters that can influence model training. We selected the following values for simplicity and comparability with prior work. In future work we will investigate the effect of these parameters in greater detail. L2 regularization (1), step-size (0.1), number of noise elements (50), margin size (50), embedding dimensionality (d=40). The noise elements samples were randomly drawn from the corpus at training time, individually for each training sample and epoch. We use the Europarl corpus (v7) <ref type="bibr" target="#b0">1</ref> for training the bilingual model. The corpus was pre-processed using the set of tools provided by cdec 2 <ref type="bibr" target="#b8">[9]</ref> for tokenizing and lowercasing the data. Further, all empty sentences as well as their translations were removed from the corpus.</p><p>We present results from two experiments. The BICVM model was trained on 500k sentence pairs of the English-German parallel section of the Europarl corpus. The BICVM+ model used this dataset in combination with another 500k parallel sentences from the English-French section of the corpus, resulting in 1 million English sentences, each paired up with either a German or a French sentence. Each language's vocabulary used distinct encodings to avoid potential overlap.</p><p>The motivation behind BICVM+ is to investigate whether we can learn better embeddings by introducing additional data in a different language. This is similar to prior work in machine translation where English was used as a pivot for translation between low-resource languages <ref type="bibr" target="#b4">[5]</ref>.</p><p>We use the adaptive gradient method, AdaGrad <ref type="bibr" target="#b7">[8]</ref>, for updating the weights of our models, and terminate training after 50 iterations. Earlier experiments indicated that the BICVM model converges faster than the BICVM+ model, but we report results on the same number of iterations for better comparability 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Lingual Document Classification</head><p>We evaluate our model using the cross-lingual document classification (CLDC) task of Klementiev et al. <ref type="bibr" target="#b15">[16]</ref>. This task involves learning language independent embeddings which are then used for document classification across the English-German language pair. For this, CLDC employs a particular kind of supervision, namely using supervised training data in one language and evaluating without supervision in another. Thus, CLDC is a good task for establishing whether our learned representations are semantically useful across multiple languages.</p><p>We follow the experimental setup described in <ref type="bibr" target="#b15">[16]</ref>, with the exception that we learn our embeddings using solely the Europarl data and only use the Reuters RCV1/RCV2 corpora during the classifier training and testing stages. Each document in the classification task is represented by the average Cross-lingual compositional representations (BICVM and BICVM+), cross-lingual representations using learned embeddings and an interaction matrix (I-Matrix) <ref type="bibr" target="#b15">[16]</ref> translated (MT) and glossed (Glossed) words, and the majority class baseline. The MT and Glossed results are also taken from Klementiev et al. <ref type="bibr" target="#b15">[16]</ref>.  <ref type="table" target="#tab_0">Table 1</ref> for model descriptions). The left chart shows results for these models when trained on English data and evaluated on German data, the right chart vice versa.</p><p>of the d-dimensional representations of all its sentences. We train the multiclass classifier using the same settings and implementation of the averaged perceptron classifier <ref type="bibr" target="#b5">[6]</ref> as used in <ref type="bibr" target="#b15">[16]</ref>.</p><p>We ran the CLDC experiments both by training on English and testing on German documents and vice versa. Using the data splits provided by <ref type="bibr" target="#b15">[16]</ref>, we used varying training data sizes from 100 to 10,000 documents for training the multiclass classifier. The results of this task across training sizes are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the results for training on 1,000 documents.</p><p>Both models, BICVM and BICVM+ outperform all prior work on this task. Further, the BICVM+ model outperforms the BICVM model, indicating the usefulness of adding training data even from a separate language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>While the CLDC experiment focused on establishing the semantic content of the sentence level representations, we also want to briefly investigate the induced word embeddings. In particular the BICVM+ model is interesting for that purpose, as it allows us to evaluate our approach of using English as a pivot language in a multilingual setup.</p><p>In <ref type="figure">Figure 3</ref> we show the t-SNE projections for a number of English, French and German words. Of particular interest should be the right chart, which highlights bilingual embeddings between French and German words. Even though the model did not use any parallel French-German data during training, it still managed to learn semantic word-word similarity across these two languages. <ref type="figure">Figure 3</ref>: The left scatter plot shows t-SNE projections for a weekdays in all three languages using the representations learned in the BICVM+ model. Even though the model did not use any parallel French-German data during training, it still learns semantic similarity between these two languages using English as a pivot. To highlight this, the right plot shows another set of words (months of the year) using only the German and French words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>With this paper we have proposed a novel method for inducing cross-lingual distributed representations for compositional semantics. Using a very simple method for semantic composition, we nevertheless managed to obtain state of the art results on the CLDC task, specifically designed to evaluate semantic transfer across languages. After extending our approach to include multilingual training data in the BICVM+ model, we were able to demonstrate that adding additional languages further improves the model. Furthermore, using some qualitative experiments and visualizations, we showed that our approach also allows us to learn semantically related embeddings across languages without any direct training data.</p><p>Our approach provides great flexibility in training data and requires little to no annotation. Having demonstrated the successful training of semantic representations using sentence aligned data, a plausible next step is to attempt training using document-aligned data or even corpora of comparable documents. This may provide even greater possibilities for working with low-resource languages.</p><p>In the same vein, the success of our pivoting experiments suggest further work. Unlike other pivot approaches, it is easy to extend our model to have multiple pivot languages. Thus some pivots could preserve different aspects such as case, gender etc., and overcome other issues related to having a single pivot language.</p><p>As we have achieved the results in this paper with a relatively simple CVM, it would also be interesting to establish whether our objective function can be used in combination with more complex compositional vector models such as MV-RNN <ref type="bibr" target="#b24">[25]</ref> or tensor-based approaches, to see whether these can further improve results on both mono-and multilingual tasks when used in conjunction with our cross-lingual objective function. Related to this, we will also apply our model to a wider variety of tasks including machine translation and multilingual information extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Description of a bilingual model with parallel input sentences a and b. The objective function of this model is to minimize the distance between the sentence level encoding of the bitext. Principally any composition function can be used to generate the compositional sentence level representations. The composition function is represented by the CVM boxes in the diagram above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Classification accuracy for a number of models (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy for training on English and German with 1000 labeled examples.</figDesc><table><row><cell>Model</cell><cell cols="2">en → de de → en</cell></row><row><cell>Majority Class</cell><cell>46.8</cell><cell>46.8</cell></row><row><cell>Glossed</cell><cell>65.1</cell><cell>68.6</cell></row><row><cell>MT</cell><cell>68.1</cell><cell>67.4</cell></row><row><cell>I-Matrix</cell><cell>77.6</cell><cell>71.1</cell></row><row><cell>BICVM</cell><cell>83.7</cell><cell>71.4</cell></row><row><cell>BICVM+</cell><cell>86.2</cell><cell>76.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.statmt.org/europarl/ 2 https://github.com/redpony/cdec 3 These numbers were updated following comments in the ICLR open review process. Results for other dimensionalities and our source code for our model are available at http://www.karlmoritz.com.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Alexandre Klementiev and his co-authors for making their datasets and averaged perceptron implementation available, as well as answering a number of questions related to their work on this task. This work was supported by EPSRC grant EP/K036580/1 and a Xerox Foundation Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Precis of how children learn the meanings of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1095" to="1103" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining symbolic and distributional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Spring Symposium on Quantum Interaction</title>
		<meeting>AAAI Spring Symposium on Quantum Interaction</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lambek Festschrift. Linguistic Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine translation by triangulation: Making effective use of multi-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="728" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118694</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-EMNLP</title>
		<meeting>ACL-EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A synopsis of linguistic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
		<idno>1930-55. 1952-59:1-32</idno>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Role of Syntax in Vector Space Models of Compositional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multilingual word representations using a bag-of-words autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jančernocký</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grounded spoken language acquisition: Experiments in word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2003.811618</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="209" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Khapra Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
