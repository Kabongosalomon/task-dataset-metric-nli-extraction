<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Bukschat</surname></persName>
							<email>yannick.bukschat@stw.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Steinbeis Transferzentrum an der Hochschule Mannheim</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Vetter</surname></persName>
							<email>m.vetter@hs-mannheim.de</email>
							<affiliation key="aff1">
								<orgName type="department">ESM-Institut</orgName>
								<address>
									<settlement>Hochschule Mannheim</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-theart accuracy of 97.35% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting objects of interest in images is an important task in computer vision and a lot of works in this research field developed highly accurate methods to tackle this problem <ref type="bibr" target="#b26">[27]</ref>[8] <ref type="bibr" target="#b43">[45]</ref>[21] <ref type="bibr" target="#b31">[32]</ref>. More recently some works not only focused on the accuracy but also on the efficiency to make their methods applicable in real world scenarios with computational and runtime limitations <ref type="bibr" target="#b39">[41]</ref> <ref type="bibr" target="#b36">[38]</ref>. For example Tan et al. <ref type="bibr" target="#b36">[38]</ref> developed a highly scalable and efficient approach, called EfficientDet, that can easily be scaled over a high range of computational resources, speed and accu- racy, with a single hyperparameter. But for some tasks like robotic manipulation, autonomous vehicles and augmented reality, it is not enough to detect only the 2D bounding boxes of the objects in an image, but to also estimate their 6D poses. Most of the recent works achieving state-of-theart accuracy in the field of 6D object pose estimation with RGB input rely on an approach that detects 2D targets, e.g. keypoints, of the objects of interest in the image first and solve for their 6D poses with a PnP-algorithm afterwards <ref type="bibr" target="#b37">[39]</ref> <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b42">[44]</ref>[20] <ref type="bibr" target="#b33">[35]</ref>. While they achieve good 6D pose estimation accuracy and since some of them are also relatively fast in terms of single object pose estimation, the runtime linearly increases with the number of objects. This results from the need to compute the 6D pose via PnP for each object individually. Furthermore, some approaches use a pixel-wise RANSAC-based <ref type="bibr" target="#b9">[10]</ref> voting scheme to detect the needed keypoints, which also has to be performed for each object separately and therefore can be very time consuming <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b33">[35]</ref>. Moreover, some methods need a separate 2D object detector first to localize and crop the bounding boxes of the objects of interest. These cropped image patches subsequently serve as the input of the actual 6D pose estimation approach which means that the whole method needs to be applied for each detected object separately <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b19">[20]</ref>. For these reasons, those approaches are often not well suited for use cases with multiple objects and runtime limitations, which inhibit their deployment in many real world scenarios. In this work we propose a new approach which does not encounter these issues and still achieves state-of-the-art performance using RGB input on the widely-used benchmark dataset Linemod <ref type="bibr" target="#b14">[15]</ref>. To achieve this, we extend the stateof-the-art 2D object detection architecture family Efficient-Dets in an intuitive way to also predict the 6D poses of objects. Therefore, we add two extra subnetworks to predict the translation and rotation of objects, analogous to the classification and bounding box regression subnetworks. Since these subnets are relatively small and share the computation of the input feature maps with the already existing networks, we are able to get the full 6D pose very inexpensive without much additional computational cost. Through the seamless integration in the EfficientDet architecture, our approach is also capable of detecting multiple object categories as well as multiple object instances and can estimate their 6D poses -all within a single shot. Because we regress the 6D pose directly, we need no further post-processing steps like RANSAC and PnP. This makes the runtime of our method nearly independent from the number of objects per image.</p><p>A key element for our reported state-of-the-art accuracy, in terms of the ADD(-S) metric on the Linemod dataset, turned out to be our proposed 6D augmentation which boosts the performance of our approach enormously. This proposed augmentation technique allows direct 6D pose estimation methods like ours, to also use image rotation and scaling which otherwise would lead to a mismatch between image and annotated poses. Such image manipulations can help to significantly improve performance and generalization when dealing with small datasets like Linemod <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b43">[45]</ref>. 2D+PnP approaches are able to exploit those methods without much effort because the 2D targets can be relatively easy transformed accordingly to the image transformation. Using our proposed augmentation method can help to compensate for that previous advantage of 2D+PnP approaches which arguably could be a reason for the current dominance of those approaches in the field of 6D object pose estimation with RGB input <ref type="bibr" target="#b25">[26]</ref>[44] <ref type="bibr" target="#b33">[35]</ref>. Just like the original EfficientDets, our approach is also highly scalable via a single hyperparameter φ to adjust the network to a wide range of computational resources, speed and accuracy. Last but not least, because our method needs no further post-processing steps, as already mentioned, and as it is based on an architecture that inherently handles multiple object categories and instances, our approach is relatively easy to use and therefore makes it attractive for many real world scenarios. To sum it all up, our main contributions in this work are as follows:</p><p>• 6D Augmentation for direct 6D pose estimation approaches to improve performance and generalization, especially when dealing with small datasets.</p><p>• Extending the state-of-the-art 2D object detection family of EfficientDets with the additional ability of 6D object pose estimation while keeping their advantages like inherent single shot multi object and instance detection, high accuracy, scalability, efficiency and ease of use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we briefly summarize already existing works that are related to our topic. The deep learning based approaches in the research field of 6D pose estimation using RGB input can mostly be assigned to one of the following two categories -estimating the 6D pose directly or first detecting 2D targets in the given image and then solving a Perspective-n-Point (PnP) problem for the 6D pose. As our method is based on a 2D object detector, we also shortly summarize related work of this research field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Direct estimation of the 6D pose</head><p>Probably the most straight forward way to estimate an object's 6D pose is to directly regress it. PoseCNN <ref type="bibr" target="#b41">[43]</ref> follows this strategy as they internally decouple the translation and rotation estimation parts. They also propose a novel loss function to handle symmetric objects since, due to their ambiguities, the network can be penalized unnecessarily during training when not taking their symmetry into account. This loss function is called ShapeMatch-Loss and we base our own loss, described in subsection 3.4, on that function.</p><p>Another possibility is to discretize the continuous rotation space into bins and classify them. Kehl et al. <ref type="bibr" target="#b17">[18]</ref> and Sundermeyer et al. <ref type="bibr" target="#b34">[36]</ref> are using this approach. SSD-6D <ref type="bibr" target="#b17">[18]</ref> extends the 2D object detector SSD <ref type="bibr" target="#b22">[23]</ref> with that ability while AAE <ref type="bibr" target="#b34">[36]</ref> aims for learning an implicit rotation representation via auto encoders and assign that estimated rotation to a similar rotation vector in a codebook. However, due to the nature of the discretization process, the so obtained poses are very course and have to be further refined in order to get a relatively accurate 6D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">2D Detection and PnP</head><p>More recently the state-of-the-art accuracy regime of 6D object pose estimation using RGB input only is dominated by approaches that first detect 2D targets of the object in the given image and subsequently solve a Perspective-n-Point problem for their 6D pose <ref type="bibr" target="#b25">[26]</ref>  <ref type="bibr" target="#b37">[39]</ref> and dense 2D-3D correspondence methods <ref type="bibr" target="#b42">[44]</ref>[20] <ref type="bibr" target="#b24">[25]</ref>. The keypoint-based methods predict either the eight 2D projections of the cuboid corners of the 3D model as keypoints <ref type="bibr" target="#b27">[28]</ref>[40] <ref type="bibr" target="#b37">[39]</ref> or choose keypoints on the object's surface, often selected with the farthest point sampling algorithm <ref type="bibr" target="#b25">[26]</ref>[35] <ref type="bibr" target="#b3">[4]</ref>. Since the cuboid corners are often not on the object's surface, those keypoints are usually harder to predict than their surface counterparts, but instead only need the 3D cuboid of the object and not the complete 3D model. Because keypoints can also be invisible in the image due to occlusion or truncation, some methods perform a pixel-wise voting scheme where each pixel of the object predicts a vector pointing to the keypoint <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b33">[35]</ref>. The final keypoints are estimated using RANSAC <ref type="bibr" target="#b9">[10]</ref>, which makes it more robust to outliers when dealing with occlusion.</p><p>The dense 2D-3D correspondence methods predict the corresponding 3D model point for each 2D pixel of the object. These dense 2D-3D correspondences are either obtained using UV maps <ref type="bibr" target="#b42">[44]</ref> or regressing the coordinates in the object's 3D model space <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b19">[20]</ref>. The 6D poses are computed afterwards using PnP and RANSAC. DPOD <ref type="bibr" target="#b42">[44]</ref> uses an additional refinement network that is fed with the cropped image patch of the object and another image patch that has to be rendered separately using the predicted pose from the first stage and outputs the refined pose.</p><p>While those works often report fast inference times for single object pose estimation, due to their indirect pose estimation approach using intermediate representations and computing the 6D pose subsequently for each object inde-pendently, the runtime is highly dependent of the number of objects per image. Furthermore, some methods can't handle multiple objects well and need a separate trained model for each object <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b38">[40]</ref> or have problems with multiple instances in some cases and need additional modifications to handle these scenarios <ref type="bibr" target="#b42">[44]</ref>. There are also some methods that rely on an external 2D object detector first to detect the objects of interest in the input image and to operate on these detections separately <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b24">[25]</ref>. All these mentioned cases increase the complexity of the approaches and limit their applicability in some use cases, especially when multiple objects or instances are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">2D Object Detection</head><p>While the development from R-CNN <ref type="bibr" target="#b11">[12]</ref> over Fast-R-CNN <ref type="bibr" target="#b10">[11]</ref> to Faster-R-CNN <ref type="bibr" target="#b32">[33]</ref> led to substantial gains in accuracy and performance in the field of 2D object detection, those so-called two-stage approaches tend to be more complex and not as efficient as one-stage methods <ref type="bibr" target="#b36">[38]</ref>. Nevertheless, they usually achieved a higher accuracy under similar computational costs when compared to onestage methods <ref type="bibr" target="#b20">[21]</ref>. The difference between both is that one-stage detectors perform the task in a single shot, while two-stage approaches perform a region proposal step in the first stage and make the final object detection in the second step based on the region proposals. Since RetinaNet <ref type="bibr" target="#b20">[21]</ref> closed the accuracy gap, one-stage detectors gained more attention due to their simplicity and efficiency <ref type="bibr" target="#b36">[38]</ref>. A common method to push the detection performance further, is to use larger backbone networks, like deeper ResNet <ref type="bibr" target="#b12">[13]</ref> variants or AmoebaNet <ref type="bibr" target="#b29">[30]</ref>, or to increase the input resolution <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b43">[45]</ref>. Yet, with the gains in detection accuracy, the computational costs often significantly increase in parallel, which reduces their applicability to use cases without computational constraints. Therefore, Tan et al. <ref type="bibr" target="#b36">[38]</ref> focused not only on accuracy but also on efficiency and brought the idea of the scalable backbone architecture EfficientNet <ref type="bibr" target="#b35">[37]</ref> to 2D object detection. The resulting EfficientDet architecture family can be scaled easily with a single hyperparameter over a wide range of computational resources -from mobile size to a huge network achieving state-of-the-art result on COCO test-dev <ref type="bibr" target="#b21">[22]</ref>. To introduce those advantages also to the field of 6D object pose estimation, we therefore base our approach on this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section we describe our approach for 6D object pose estimation using RGB images as input. The complete 6D pose is composed of two parts -the 3D rotation R ∈ SO(3) of the object and the 3D translation t ∈ R 3 . This 6D pose represents the rigid transformation from the object coordinate system into the camera coordinate system. Because this overall task involves several subtasks like de- tecting objects in the 2D image first, handling multiple object categories and instances, etc. which are already solved in recent works from the relatively matured field of 2D object detection, we decided to base our work on such an 2D object detection approach and extend it with the ability to also predict the 6D pose of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extending the EfficientDet architecture</head><p>Our goal is to extend the EfficientDet architecture in an intuitive way and keep the computational overhead rather small. Therefore, we add two new subnetworks, analogous to the classification and bounding box regression subnetworks, but instead of predicting the class and bounding box offset for each anchor box, the new subnets predict the rotation R and translation t respectively. Since those subnets are small and share the input feature maps with the already existing classification and box subnets, the additional computational cost is minimal. Integrating the task of 6D pose estimation via those two subnetworks and using the anchor box mapping and non-maximum-suppression (NMS) of the base architecture to filter out background and multiple detections, we are able to create an architecture that can detect the • Class</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• 2D bounding box • Rotation</head><p>• Translation of one or more object instances and categories for a given RGB image in a single shot. To maintain the scalability of the underlying EfficientDet architecture, the size of the rotation and translation network is also controlled by the scaling hyperparameter φ. A high-level view of our architecture is presented in <ref type="figure" target="#fig_1">Figure 2</ref>. For further information about the base architecture we refer the reader to the EfficientDet publication <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rotation Network</head><p>We choose axis angle representation for the rotation because it needs fewer parameters than quaternions and Mahendran et al. <ref type="bibr" target="#b23">[24]</ref> found that it also performed slightly better in their experiments. Yet, this representation is not crucial for our approach and can also be switched if needed. So instead of a rotation matrix R ∈ SO(3), the subnetwork predicts one rotation vector r ∈ R 3 for each anchor box. The network architecture is similar to the classification and box network in EfficientDet <ref type="bibr" target="#b36">[38]</ref> but instead of using the output r init directly as the regressed rotation, we further add an iterative refinement module, inspired by Kanazawa et al. <ref type="bibr" target="#b16">[17]</ref>. This module takes the concatenation along the channel dimension of the current rotation r init and the output of the last convolution layer prior to the initial regression layer which outputs r init as the input and regresses ∆r so that the final rotation regression is</p><formula xml:id="formula_0">r = r init + ∆r<label>(1)</label></formula><p>The iterative refinement module consists of D iter depthwise separable convolution layer <ref type="bibr" target="#b4">[5]</ref>, each layer followed by group normalization <ref type="bibr" target="#b40">[42]</ref> and SiLU (swish-1) activation function <ref type="bibr" target="#b28">[29]</ref>[9] <ref type="bibr" target="#b13">[14]</ref>. The number of layers D iter , dependent by the scaling hyperparameter φ is described by the following equation  where denotes the floor function. These layers are followed by the output layer -a single depthwise separable convolution layer with linear activation function -which outputs ∆r.</p><formula xml:id="formula_1">D iter (φ) = 2 + φ/3<label>(2)</label></formula><p>This iterative refinement module is applied N iter times to the rotation r, initialized with the output of the base network r init and after each intermediate iteration step r is set to r init for the next step. N iter is also dependent on φ to preserve the scalability and is defined as follows</p><formula xml:id="formula_2">N iter (φ) = 1 + φ/3<label>(3)</label></formula><p>The number of channels for all layers are the same as in the class and box networks, except for the output layers, which are determined by the number of anchors and rotation parameters. Equation 2 and Equation 3 are based on the equation for the depth D box and D class of the box and class networks from EfficientDet <ref type="bibr" target="#b36">[38]</ref> but are not backed up with further experiments and could possibly be optimized. The architecture of the complete rotation network is presented in <ref type="figure">Figure 3</ref>, while the detailed topology of the refinement module is shown in <ref type="figure">Figure 4</ref>.</p><p>Even though our design of the rotation and translation network, described in subsection 3.3, is based on the box and class network from the vanilla EfficientDet, we replace batch normalization with group normalization to reduce the minimum needed batch size during training <ref type="bibr" target="#b40">[42]</ref>. With this replacement we are able to successfully train the rotation and translation network from scratch with a batch size of 1 which heavily reduces the needed amount of memory during training compared to the needed minimum batch size of 32 with batch normalization. We aim for 16 channels per group which works well according to Wu et al. <ref type="bibr" target="#b40">[42]</ref> and therefore calculating the number of groups N groups as follows</p><formula xml:id="formula_3">N groups (φ) = W bif pn (φ) 16<label>(4)</label></formula><p>where W bif pn denotes the number of channels in the Effi-cientDet BiFPN and prediction networks <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Translation Network</head><p>The network topology of the translation network is basically the same as for the rotation network described in subsection 3.2, with the difference of outputting a translation t ∈ R 3 for each anchor box. However, instead of directly regressing all components of the translation vector t = (t x , t y , t z ) T , we adopt the approach of PoseCNN <ref type="bibr" target="#b41">[43]</ref> and split the task into predicting the 2D center point c = (c x , c y ) T of the object in pixel coordinates and the distance t z separately. With the center point c, the distance t z and the intrinsic camera parameters, the missing components t x and t y of the translation t can be calculated using the following equations assuming a pinhole camera</p><formula xml:id="formula_4">t x = (c x − p x ) · t z f x (5) t y = (c y − p y ) · t z f y<label>(6)</label></formula><p>where p = (p x , p y ) T is the principal point and f x and f y are the focal lengths. For each anchor box we predict the offset in pixels from the center of this anchor box to the center point c of the corresponding object. This is equivalent to predicting the offset to the center point from the current point in the given feature map, as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. To maintain the relative spatial relations, the offset is normalized with the stride of the input feature map from every level of the feature pyramid. Using the • predicted relative offsets,</p><p>• the coordinate maps X and Y of the feature maps where every point contains its own x and y coordinate respectively</p><p>• and the strides, the absolute coordinates of the center point c can be calculated. Our intention here is that it might be easier for the network to predict the relative offset at each point in the feature maps instead of directly regressing the absolute coordinates c x and c y due to the translational invariance of the convolution. We also verified this assumption experimentally.</p><p>The above described calculations of the translation t from the 2D center point c and the depth t z , as well as the absolute center point coordinates c x and c y from their predicted relative offsets are both implemented in separate TensorFlow <ref type="bibr" target="#b0">[1]</ref> layers to avoid extra post-processing steps and to enable GPU or TPU acceleration, while keeping the architecture as simple as possible. As mentioned earlier, the calculation of t also needs the intrinsic camera parameters which is the reason why there is another input layer needed for the translation network. This input layer provides a vector a ∈ R 6 for each input image which contains the focal lengths f x and f y of the pinhole camera, the principal point coordinates p x and p y and finally an optional translation scaling factor s translation and the image scale s image . The translation scaling factor s translation can be used to adjust the translation unit, e.g. from mm to m. The image scale s image is the scaling factor from the original image size to the input image size which is needed to rescale the predicted center point c to the original image resolution to apply Equation 5 and Equation 6 for recovering t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transformation Loss</head><p>The loss function we use is based on the PoseLoss and ShapeMatch-Loss from PoseCNN <ref type="bibr" target="#b41">[43]</ref> but instead of considering only the rotation, our approach takes also the translation into account. For asymmetric objects our loss L asym is defined as follows</p><formula xml:id="formula_5">L asym = 1 m x∈M (Rot(r, x) +t) −(Rot(r, x) + t) 2 ,<label>(7)</label></formula><p>whereby Rot(r, x) and Rot(r, x) respectively indicate the rotation of x with the ground truth rotation r and the estimated rotationr by applying the Rodrigues' rotation formula <ref type="bibr" target="#b6">[7]</ref>[34]. Furthermore, M denotes the set of the object's 3D model points and m is the number of points. The loss function basically performs the transformation of the object of interest with the ground truth 6D pose and the estimated 6D pose and then calculates the mean point distances between the transformed model points which is identical to the ADD metric described in subsection 4.2. This approach has the advantage that the model is directly optimized on the metric with which the performance is measured. It also eliminates the need of an extra hyperparameter to balance the partial losses when the rotation and translation losses are calculated independently from each other.</p><p>To also handle symmetric objects, the corresponding loss L sym is given by the following equation</p><formula xml:id="formula_6">L sym = 1 m x1∈M min x2∈M (Rot(r, x 1 ) +t) −(Rot(r, x 2 ) + t) 2<label>(8)</label></formula><p>which is similar to L asym but instead of strictly calculating the distance between the matching points of the two transformed point sets, the minimal distance for each point to any point in the other transformed point set is taken into account. This helps to avoid unnecessary penalization during training when dealing with symmetric objects as described by Xiang et al. <ref type="bibr" target="#b41">[43]</ref>.</p><p>The complete transformation loss function L trans is defined as follows</p><formula xml:id="formula_7">L trans = L sym if symmetric, L asym if asymmetric.<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">6D Augmentation</head><p>The Linemod <ref type="bibr" target="#b14">[15]</ref> and Occlusion <ref type="bibr" target="#b1">[2]</ref> datasets used in this work are very limited in the amount of annotated data. Linemod roughly consists of about 1200 annotated examples per object and Occlusion is a subset of Linemod where all objects of a single scene are annotated so the amount of data is equally small. This makes it especially hard for large neural networks to converge to more general solutions. Data augmentation can help a lot in such scenarios <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b43">[45]</ref> and methods which rely on any 2D detection and PnP approach have a great advantage here. Such methods can easily use image manipulation techniques like rotation, scaling, shearing etc. because the 2D targets, e.g. keypoints, can be relatively easy transformed according to the image transformation. Approaches that directly predict the 6D pose of an object are limited in this aspect because some image transformations, like rotation for example, lead to a mismatch between image and ground truth 6D pose. To overcome this issue, we developed a 6D augmentation that is able to rotate and scale an image randomly and transform the ground truth 6D poses so they still match to the augmented image. As can be seen in <ref type="figure" target="#fig_4">Figure 6</ref>  θ ∈ [0 • , 360 • ), the 3D rotation R and translation t of the 6D pose also have to be rotated with θ around the z-axis. This rotation around the z-axis can be described with the rotation vector ∆r in axis angle representation as follows ∆r = (0, 0, θ 180 · π ) T .</p><p>Using the rotation matrix ∆R obtained from ∆r, the augmented rotation matrix R aug and translation t aug can be computed with the following equations</p><formula xml:id="formula_9">R aug = ∆R · R (11) t aug = ∆R · t<label>(12)</label></formula><p>To handle image scaling as an additional augmentation technique as well, we need to adjust the t z component of the translation t = (t x , t y , t z ) T . Rescaling the image with a factor f scale , the augmented translation t aug can be calculated as follows</p><formula xml:id="formula_10">t aug = (t x , t y , t z f scale ) T .<label>(13)</label></formula><p>It has to be mentioned that the scaling augmentation introduces an error if the object of interest is not in the image center. When rescaling the image, the 2D projection of the object remains the same. It only becomes bigger or smaller. However, when moving the object along the z-axis in reality, the view from the camera to the 3D object would change and so the projection onto the 2D image plane. Nevertheless, the benefits from the additional data obtained with this augmentation technique strongly outweigh its introduced error as shown in subsection 4.7. <ref type="figure" target="#fig_5">Figure 7</ref> contains some examples where the top left image is the original image with the ground truth 6D pose and the other images are augmented with the method described in this subsection. In this work we use for all experiments a random angle θ, uniformly sampled from the interval [0 • , 360 • ) and a random scaling factor f scale , uniformly sampled from [0.7, 1.3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Color space Augmentation</head><p>We also use several augmentation techniques in the color space that can be applied without further need to adjust the annotated 6D poses. For this task we adopt the RandAugment <ref type="bibr" target="#b5">[6]</ref> method which is a learned augmentation that is able to boost performance and enhance generalization among several datasets and models. It consists of multiple augmentation methods, like adjusting the contrast and brightness of the input image, and can be tuned with two parameters -the number n of applied image transformations and the strength m of these transformations. As mentioned earlier, some image transformations like rotation and shearing lead to a mismatch between the input image and the ground truth 6D poses, so we remove those augmentation techniques from the RandAugment method. We further add gaussian noise to the selection. To maintain the approach of setting the augmentation strength with the parameter m, the channel-wise additive gaussian noise is sampled from a normal distribution with the range [0, m 100 · 255]. For all our experiments we choose n randomly sampled from an integer uniform distribution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> and m from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we describe the experiments we did, our experimental setup with implementation details as well as the evaluation metrics we use. In case of the Linemod experiment, we also compare our results to current state-ofthe-art methods. Please note that our approach can be scaled from φ = 0 to φ = 7 in integer steps but due to computational constraints, we only use φ = 0 and φ = 3 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our approach on two popular benchmark datasets which are described in this subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Linemod</head><p>The Linemod <ref type="bibr" target="#b14">[15]</ref> dataset is a popular and widely-used benchmark dataset for 6D object pose estimation. It consists of 13 different objects (actually 15 but only 13 are used in most other works <ref type="bibr" target="#b37">[39]</ref>[25] <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b42">[44]</ref>[20] <ref type="bibr" target="#b33">[35]</ref>) which are placed in 13 cluttered scenes. For each scene only one object is annotated with it's 6D pose although other objects are visible at the same time. So despite of our approach being able to detect multiple objects and to estimate their poses, we had to train one model for each object. There are about 1200 annotated examples per object and we use the same train and test split as other works <ref type="bibr" target="#b2">[3]</ref>[26] <ref type="bibr" target="#b37">[39]</ref> for fair comparison. This split selects training images so the object poses had a minimum angular distance of 15 • , which results in about 15% training images and 85% test images. Furthermore, we do not use any synthetically rendered images for training. We compare our results with state-of-the-art methods in subsection 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Occlusion</head><p>The Occlusion dataset is a subset of Linemod and consists of a single scene of Linemod where eight other objects visible in this scene are additionally annotated. These objects are partially heavily occluded which makes it challenging to estimate their 6D poses. We use this dataset to evaluate our method's ability for multi object 6D pose estimation. Therefore, we trained a single model on the Occlusion dataset. We use the same train and test split as for the corresponding Linemod scene. The results of this experiment are presented in subsection 4.5. Please note that the evaluation convention in other works <ref type="bibr" target="#b41">[43]</ref> <ref type="bibr" target="#b25">[26]</ref> is to use the Linemod dataset for training and the complete Occlusion data as the test set, so this experiment is not comparable with those works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><p>We evaluate our approach with the commonly used ADD(-S) metric <ref type="bibr" target="#b15">[16]</ref>. This metric calculates the average point distances between the 3D model point set M transformed with the ground truth rotation R and translation t and the model point set transformed with the estimated rota-tionR and translationt. It also differs between asymmetric and symmetric objects. For asymmetric objects the ADD metric is defined as follows</p><formula xml:id="formula_11">ADD = 1 m x∈M (Rx + t) − (Rx +t) 2 .<label>(14)</label></formula><p>An estimated 6D pose is considered correct if the average point distance is smaller than 10% of the object's diameter. Symmetric objects are evaluated using the ADD-S metric which is given by the following equation</p><formula xml:id="formula_12">ADD-S = 1 m x1∈M min x2∈M (Rx + t) −(Rx +t) 2 .<label>(15)</label></formula><p>Method YOLO6D <ref type="bibr" target="#b37">[39]</ref> Pix2Pose <ref type="bibr" target="#b24">[25]</ref> PVNet <ref type="bibr" target="#b25">[26]</ref> DPOD <ref type="bibr" target="#b42">[44]</ref> DPOD+ <ref type="bibr" target="#b42">[44]</ref> CDPN <ref type="bibr" target="#b19">[20]</ref> Hybrid-Pose <ref type="bibr" target="#b33">[35]</ref> Ours φ = 0  <ref type="table">Table 1</ref>. Quantitative evaluation and comparison on the Linemod dataset in terms of the ADD(-S) metric. Symmetric objects are marked with * and approaches marked with + are using an additional refinement method.</p><p>Finally, the ADD(-S) metric is defined as</p><formula xml:id="formula_13">ADD(-S) = ADD if asymmetric, ADD-S if symmetric.<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We use the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with an initial learning rate of 1e-4 for all our experiments and a batch size of 1. We also use gradient norm clipping with a threshold of 0.001 to increase training stability. The learning rate is reduced with a factor of 0.5 if the average point distance does not decrease within the last 25 evaluations on the test set. The minimum learning rate is set to 1e-7. Since the training set of Linemod and Occlusion is very small (roughly 180 examples per object), as mentioned in subsubsection 4.1.1 and subsubsection 4.1.2, we evaluate our model only every 10 epochs to measure training progression. Our model is trained for 5000 epochs. The complete loss function L is composed of three parts -the classification loss L class , the bounding box regression loss L bbox and the transformation loss L trans . To balance the influence of these partial losses on the training procedure, we introduce a hyperparameter λ for each partial loss, so the final loss L is defined as follows L = λ class · L class + λ bbox · L bbox + λ trans · L trans <ref type="bibr" target="#b16">(17)</ref> We found that λ class = λ bbox = 1 and λ trans = 0.02 performs well in our experiments. To calculate the transformation loss L trans , described in subsection 3.4, we use m = 500 points of the 3D object model point set M. We use our 6D and color space augmentation by default with the parameters mentioned in subsection 3.5 and subsection 3.6 respectively but randomly skip augmentation with a probability of 0.02 to also include examples from the original image domain in our training process. We initialize the neural network, except the rotation and translation network, with COCO <ref type="bibr" target="#b21">[22]</ref> pretrained weights from the vanilla EfficientDet <ref type="bibr" target="#b36">[38]</ref>. Because of our small batch size, we freeze all batch norm layers during training and use the population statistics learned from COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison on Linemod</head><p>In <ref type="table">Table 1</ref> we compare our results with current state-ofthe-art methods using RGB input on the Linemod dataset in terms of the ADD(-S) metric. Our approach outperforms all other methods without further refinement steps by a large margin. Even DPOD+ which uses an additional refinement network and reported the best results on Linemod so far using only RGB input data, is outperformed considerably by our method, roughly halving the remaining error. Note again that, in contrast to all other recent works in <ref type="table">Table 1</ref>   <ref type="bibr" target="#b33">[35]</ref>, our approach detects and estimates objects with their 6D poses in a single shot without the need of further post-processing steps like RANSAC-based voting or PnP. This fact demonstrates the current domination of 2D+PnP approaches in the high accuracy regime on Linemod using only RGB input. Since a crucial part of our reported performance on Linemod is our proposed 6D augmentation, as can be seen in subsection 4.7, the question arises if the previous superiority of 2D+PnP approaches over direct 6D pose estimation comes from the broader use of some augmentation techniques like rotation, which better enriches the small Linemod dataset. To the best of our knowledge, our approach is the first holistic method achieving competitive performance on Linemod with current state-of-the-art approaches like PVNet <ref type="bibr" target="#b25">[26]</ref>, DPOD <ref type="bibr" target="#b42">[44]</ref> and HybridPose <ref type="bibr" target="#b33">[35]</ref>. We therefore demonstrate that single shot direct 6D object pose estimation approaches are able to compete in terms of accuracy with 2D+PnP approaches and even with additional refinement methods. <ref type="figure" target="#fig_6">Figure 8</ref> shows some qualitative results of our method. Interestingly, the performance of our φ = 0 and φ = 3 models are nearly the same, despite of their different capacities. This suggests that the capacity of our φ = 0 model is already enough for the single object 6D pose estimation task on Linemod and that the bottleneck seems to be the small amount of data. Additionally, the small φ = 0 model may not suffer from overfitting as much as the larger models which could be an explanation why the φ = 0 model performs slightly better on some objects. The advantage of the larger φ = 3 model is much more pronounced at multi object 6D pose estimation as we demonstrate in subsection 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Multi object pose estimation</head><p>To validate that our approach is really capable of handling multiple objects in practice, we also trained a single model on Occlusion. Because of the reasons explained in subsubsection 4.1.1, we could not use the Linemod data of the objects for training like other works did <ref type="bibr" target="#b25">[26]</ref>[43] and had to train our model on the Occlusion dataset. Therefore, we used the train and test split of the corresponding Linemod scene. Thus due to the different train and test data of this experiment, the reported results are not comparable to the results of other works <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b41">[43]</ref>. Training parameters remain the same as described in subsection 4.3. The results in <ref type="table">Table 2</ref> suggest that our method is indeed able to detect and estimate the 6D poses of multiple objects in a single shot. <ref type="figure" target="#fig_0">Figure 1</ref>     <ref type="table">Table 2</ref>. Quantitative evaluation in terms of the ADD(-S) metric for the task of multi object 6D pose estimation using a single model on the Occlusion dataset. Symmetric objects are marked with * with ground truth and estimated 6D poses of the Occlusion test set for qualitative evaluation. Interestingly the performance difference in terms of the ADD(-S) metric between the φ = 0 and φ = 3 model is quiet significant, unlike the Linemod experiment in subsection 4.4. We argue that the larger number of objects benefits more from the higher capacity of the φ = 3 model. On top of that, the objects in this dataset often deal with severe occlusions which makes the 6D pose estimation task at the same time more challenging than on Linemod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Runtime analysis</head><p>In this subsection we examine the average runtime of our apprach in several scenarios and compare it with the vanilla EfficientDet <ref type="bibr" target="#b36">[38]</ref>. The experiments were performed using the φ = 0 and φ = 3 model to study the influence of the scaling hyperparameter φ. For each model we measured the runtime for single and multi object 6D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Vanilla EfficientDet <ref type="bibr" target="#b36">[38]</ref> Model  <ref type="table">Table 3</ref>. Runtime analysis and comparison of our method performing single and multiple object pose estimation while using different scales. For single object 6D pose estimation the Linemod dataset is used while for multi object pose estimation the Occlusion dataset is used which contains usually eight annotated objects per image. We further compare our method's runtime with the vanilla EfficientDet <ref type="bibr" target="#b36">[38]</ref> to measure the influence of our 6D pose estimation extension.</p><formula xml:id="formula_14">φ = 0 φ = 3 φ = 0 φ = 3</formula><p>To examine the single object task, we use the Linemod test dataset and for the latter the Occlusion test dataset because it typically contains eight annotated objects per image. All experiments were performed using a batch size of 1. We measured the time needed to</p><p>• preprocess the input data (Preprocessing),</p><p>• the pure network inference time (Network)</p><p>• and finally the complete end-to-end time including the data preprocessing, network inference with nonmaximum-suppression and post-processing steps like rescaling the 2D bounding boxes to the original image resolution (end-to-end).</p><p>To make a fair comparison with the vanilla EfficientDet, we use the same implementation on which our EfficientPose implementation is based on and also use the same weights so that the 2D detection remains identical. The results of these experiments are reported in <ref type="table">Table 3</ref>. For a more fine grained evaluation, we performed a separate experiment in which we measured the runtime w.r.t. the number of objects per image. We used the Occlusion test set and cut out objects using the ground truth segmentation mask if necessary to match the target number of objects per image. Using this method we then iteratively measured the end-to-end runtime of the complete occlusion test set from a single object up to eight objects. To ensure a correct measuring, we filtered out images in which our model did not detect the estimated number of objects. The results of this experiment are visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. All experiments are run on the same machine with an i7-6700K CPU and a 2080 Ti GPU using Tensorflow 1.15.0, CUDA 10.0 and CuDNN 7.6.5.</p><p>Our φ = 0 model runs end-to-end with an average 27.45 FPS at the single object 6D pose estimation task which makes it suitable for real time applications. Even more promising is the average end-to-end runtime of 26.22 FPS when performing multi object 6D pose estimation on the Occlusion test dataset which typically contains eight objects per image. Using the much larger φ = 3 model, our method still runs end-to-end at over 9 FPS while the difference between single and multi object 6D pose estimation nearly vanishes with 9.43 vs. 9.34 FPS. <ref type="figure" target="#fig_0">Figure 1</ref> also demonstrates that the runtime of our approach is nearly independent from the number of objects per image. These results show the advantage of our method in multi object 6D pose estimation compared to the 2D detection approaches solving a PnP problem to obtain the 6D poses afterwards, which linearly increases the runtime with the number of objects. This makes our single shot approach very attractive for many real world scenarios, no matter if there are one or more objects. When comparing the runtimes of the vanilla EfficientDet and our approach with roughly 35 vs. 27 FPS using φ = 0 and 12 vs. 9 FPS with the φ = 3 model, our extension of the EfficientDet architecture as described in subsection 3.1 seems computationally very efficient considering this rather small drop in frame rate in exchange for the additional ability of full 6D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation study</head><p>To demonstrate the importance of our proposed 6D augmentation, described in subsection 3.5, we trained a φ = 0 model with and without the 6D augmentation. To gain further insights into the influence of the rotation and scaling part respectively, we also performed experiments in which only one part of the augmentation is used. The color space augmentation is applied in all the experiments to isolate the effect of the 6D augmentation. Due to computational constraints, we performed these experiments only on the driller object from Linemod.</p><p>As can be seen from the results in <ref type="table">Table 4</ref>, the 6D augmentation is a key element in our approach and boosts the performance significantly from 72.15% without 6D augmentation to 99.9% in terms of ADD metric. Furthermore, the results from the experiments using only one part of the  <ref type="table">Table 4</ref>. Ablation study to evaluate the influence of our proposed 6D augmentation and it's individual parts. The reported results are in terms of the ADD(-S) metric and are obtained using our φ = 0 model, trained on the driller object of the Linemod dataset.</p><p>6D augmentation (only scale or only rotation) show very similar improvements which suggests that they contribute equally to the overall effectiveness of the 6D augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we introduce EfficientPose, a highly scalable end-to-end 6D object pose estimation approach that is based on the state-of-the-art 2D object detection architecture family EfficientDet <ref type="bibr" target="#b36">[38]</ref>. We extend the architecture in an intuitive and efficient way to maintain the advantages of the base network and to keep the additional computational costs low while performing not only 2D object detection but also 6D object pose estimation of multiple objects and instances -all within a single shot. Our approach achieves a new state-of-the-art result on the widely-used benchmark dataset Linemod while still running end-to-end at over 27 FPS. We thus state that holistic approaches for direct 6D object pose estimation can compete in terms of accuracy with 2D+PnP methods under similar training data conditionsa gap that we close with our proposed 6D augmentation. Moreover, in contrast to 2D+PnP approaches, the runtime of our method is also nearly independent from the number of objects which makes it suitable for real world scenarios like robotic grasping or autonomous driving, where multiple objects are involved and real-time constraints are given.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top: Example prediction for qualitative evaluation of our φ = 0 model performing single shot 6D multi object pose estimation on the Occlusion test set while running end-to-end at over 26 FPS. Green 3D bounding boxes visualize ground truth poses while our estimated poses are represented by the other colors. Bottom: Average end-to-end runtimes in FPS of our φ = 0 and φ = 3 model on the Occlusion test set w.r.t. the number of objects per image. Shaded areas represent the standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic representation of our EfficientPose architecture including the EfficientNet<ref type="bibr" target="#b35">[37]</ref> backbone, the bidirectional feature pyramid network (BiFPN) and the prediction subnetworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Rotation network architecture with the initial regression and iterative refinement module. Each conv block consists of a depthwise separable convolution layer followed by group normalization and SiLU activation. Architecture of the rotation refinement module. Each conv block consists of a depthwise separable convolution layer followed by group normalization and SiLU activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the 2D center point estimation process. The target for each point in the feature map is the offset from the current location to the object's center point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>, when performing a 2D rotation of the image around the principal point with an angle Schematic figure of a pinhole camera illustrating the projection of an object's 3D center point onto the 2D image plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Some examples of our proposed 6D augmentation. The image in the top left is the original image with the projected object cuboid, transformed with the ground truth 6D pose. The other images are obtained through augmenting the image and the 6D poses separately from each other and then transforming the object's cuboid with the augmented 6D poses and finally project each cuboid onto the corresponding augmented image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Some example predictions for qualitative evaluation of our φ = 0 model on the Linemod test dataset. Green 3D bounding boxes visualize ground truth poses while our estimated poses are represented by blue boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and Figure 9 are showing some examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative evaluation of our single φ = 0 model's ability for estimating 6D poses of multiple objects in a single shot. Green 3D bounding boxes visualize ground truth poses while our estimated poses are represented by the other colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Inital regression Iterative refinement conv conv ...</head><label></label><figDesc></figDesc><table><row><cell>conv</cell><cell>Refinement Module</cell><cell>+</cell><cell>Refinement Module</cell><cell>+</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell></row></table><note>...</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>26.95 FPS 122.40 123.14 41.02 41.22 123.92 116.82 38.93 37.11</figDesc><table><row><cell cols="2">Single or multiple objects Single</cell><cell>Multi</cell><cell>Single</cell><cell>Multi</cell><cell>Single</cell><cell>Multi</cell><cell>Single Multi</cell></row><row><cell cols="8">Preprocessing 25.69 Network ms 8.17 8.12 24.38 24.26 8.07 8.56 ms 28.18 29.96 81.60 82.69 19.26 21.42 51.71 53.97 FPS 35.49 33.38 12.26 12.09 51.91 46.69 19.34 18.53</cell></row><row><cell>End-to-end</cell><cell>ms 36.43 FPS 27.45</cell><cell cols="4">38.13 106.04 107.01 27.38 26.22 9.43 9.34 36.52</cell><cell>30.02 33.31</cell><cell>77.45 80.98 12.91 12.35</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2014</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end learnable geometric vision by backpropagating pnp optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jun</forename><surname>Chin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Euler-rodrigues formula variations, quaternion conjugation and intrinsic connections. Mechanism and Machine Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nassir Navab, and Vincent Lepetit. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2012</title>
		<editor>Kyoung Mu Lee, Yasuyuki Matsushita, James M. Rehg, and Zhanyi Hu</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ssd-6d: Making rgbbased 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7677" to="7686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haider</forename><surname>Siddharth Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>3d pose regression using convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Realtime seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cspnet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning data augmentation strategies for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
