<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Surprisingly Robust Trick for the Winograd Schema Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vid</forename><surname>Kocijan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Creţu</surname></persName>
							<email>a.cretu@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana-Maria</forename><surname>Camburu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yordan</forename><surname>Yordanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Surprisingly Robust Trick for the Winograd Schema Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when finetuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSClike dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-theart solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the "complex" subsets of WSC273, introduced by Trichelair et al. (2018).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Winograd Schema Challenge (WSC) <ref type="bibr" target="#b7">(Levesque et al., 2012</ref><ref type="bibr" target="#b6">(Levesque et al., , 2011</ref> was introduced for testing AI agents for commonsense knowledge. Here, we refer to the most popular collection of such sentences as WSC273, to avoid confusion with other slightly modified datasets, such as PDP60, <ref type="bibr" target="#b0">(Davis et al., 2017)</ref> and the Definite Pronoun Resolution dataset <ref type="bibr" target="#b14">(Rahman and Ng, 2012)</ref>, denoted WSCR in the sequel. WSC273 consists of 273 instances of the pronoun disambiguation problem (PDP) <ref type="bibr" target="#b10">(Morgenstern et al., 2016)</ref>. Each is a sentence (or two) with a pronoun referring to one of the two or more nouns; the goal is to predict the correct one. The task is challenging, since WSC examples are constructed to require human-like commonsense knowledge and reasoning. The best known solutions use deep learning with an accuracy of 63.7% <ref type="bibr" target="#b12">(Opitz and Frank, 2018;</ref><ref type="bibr" target="#b16">Trinh and Le, 2018)</ref>. The problem is difficult to solve not only because of the commonsense reasoning challenge, but also due to the small existing datasets making it difficult to train neural networks directly on the task.</p><p>Neural networks have proven highly effective in natural language processing (NLP) tasks, outperforming other machine learning methods and even matching human performance <ref type="bibr" target="#b3">(Hassan et al., 2018;</ref><ref type="bibr" target="#b11">Nangia and Bowman, 2018)</ref>. However, supervised models require many per-task annotated training examples for a good performance. For tasks with scarce data, transfer learning is often applied <ref type="bibr" target="#b4">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b5">Johnson and Zhang, 2017)</ref>, i.e., a model that is already trained on one NLP task is used as a starting point for other NLP tasks.</p><p>A common approach to transfer learning in NLP is to train a language model (LM) on large amounts of unsupervised text <ref type="bibr" target="#b4">(Howard and Ruder, 2018)</ref> and use it, with or without further fine-tuning, to solve other downstream tasks. Building on top of a LM has proven to be very successful, producing state-of-the-art (SOTA) results <ref type="bibr" target="#b8">(Liu et al., 2019;</ref><ref type="bibr" target="#b16">Trinh and Le, 2018)</ref> on benchmark datasets like GLUE <ref type="bibr" target="#b18">(Wang et al., 2019)</ref> or WSC273 <ref type="bibr" target="#b6">(Levesque et al., 2011)</ref>.</p><p>In this work, we first show that fine-tuning existing LMs on WSCR is a robust method of improving the capabilities of the LM to tackle WSC273 and WNLI. This is surprising, because previous attempts to generalize from the WSCR dataset to WSC273 did not achieve a major improvement <ref type="bibr" target="#b12">(Opitz and Frank, 2018)</ref>. Secondly, we introduce a method for generating large-scale WSC-like examples. We use this method to create a 2.4M dataset from English Wikipedia 1 , which we further use together with WSCR for fine-tuning the pre-trained BERT LM <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>. The dataset and the code have been made pub-licly available 2 . We achieve accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous best solutions by 8.8% and 9.6%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This section introduces the main LM used in our work, BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, followed by a detailed description of WSC and its relaxed form, the Definite Pronoun Resolution problem.</p><p>BERT. Our work uses the pre-trained Bidirectional Encoder Representations from Transformers (BERT) LM <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> based on the transformer architecture <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>. Due to its high performance on natural language understanding (NLU) benchmarks and the simplicity to adapt its objective function to our finetuning needs, we use BERT throughout this work.</p><p>BERT is originally trained on two tasks: masked token prediction, where the goal is to predict the missing tokens from the input sequence, and next sentence prediction, where the model is given two sequences and asked to predict whether the second sequence follows after the first one.</p><p>We focus on the first task to fine-tune BERT using WSC-like examples. We use masked token prediction on a set of sentences that follow the WSC structure, where we aim to determine which of the candidates is the correct replacement for the masked pronoun.</p><p>Winograd Schema Challenge. Having introduced the goal of the Winograd Schema Challenge in Section 1, we illustrate it with the following example:</p><p>The trophy didn't fit into the suitcase because it was too [large/small].</p><p>Question: What was too [large/small]? Answer: the trophy / the suitcase The pronoun "it" refers to a different noun, based on the word in the brackets. To correctly answer both versions, one must understand the meaning of the sentence and its relation to the changed word. More specifically, a text must meet the following criteria to be considered for a Winograd Schema <ref type="bibr" target="#b6">(Levesque et al., 2011):</ref> 1. Two parties must appear in the text. <ref type="bibr">2</ref> The code can be found at https://github.com/ vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: 9b34602b-c982-4b49-b4f4-6555b5a82c3d 2. A pronoun or a possessive adjective appears in the sentence and refers to one party. It would be grammatically correct if it referred to the other.</p><p>3. The question asks to determine what party the pronoun or the possessive adjective refers to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A "special word" appears in the sentence. When switched to an "alternative word", the sentence remains grammatically correct, but the referent of the pronoun changes.</p><p>Additionally, commonsense reasoning must be required to answer the question.</p><p>A detailed analysis by <ref type="bibr" target="#b15">Trichelair et al. (2018)</ref> shows that not all WSC273 examples are equally difficult. They introduce two complexity measures (associativity and switchability) and, based on them, refine evaluation metrics for WSC273.</p><p>In associative examples, one of the parties is more commonly associated with the rest of the question than the other one. Such examples are seen as "easier" than the rest and represent 13.5% of WSC273. The remaining 86.5% of WSC273 is called non-associative.</p><p>47% of the examples are "switchable", because the roles of the parties can be changed, and examples still make sense. A model is tested on the original, "unswitched" switchable subset and on the same subset with switched parties. The consistency between the two results is computed by comparing how often the model correctly changes the answer when the parties are switched.</p><p>Definite Pronoun Resolution. Since collecting examples that meet the criteria for WSC is hard, <ref type="bibr" target="#b14">Rahman and Ng (2012)</ref>  WNLI. One of the 9 GLUE benchmark tasks <ref type="bibr" target="#b18">(Wang et al., 2019)</ref>, WNLI is very similar to the WSC273 dataset, but is phrased as an entailment problem instead. A WSC schema is given as a premise. The hypothesis is constructed by extracting the sentence part where the pronoun is, and replacing the pronoun with one candidate. The label is 1, if the candidate is the correct replacement, and 0, otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>There have been several attempts at solving WSC273. Previous work is based on Google queries for knowledge ) (58%), sequence ranking <ref type="bibr" target="#b12">(Opitz and Frank, 2018</ref>) (63%), and using an ensemble of LMs (Trinh and Le, 2018) (63%).</p><p>A critical analysis <ref type="bibr" target="#b15">(Trichelair et al., 2018)</ref> showed that the main reason for success when using an ensemble of LMs <ref type="bibr" target="#b16">(Trinh and Le, 2018)</ref> was largely due to imperfections in WSC273, as discussed in Section 2.</p><p>The only dataset similar to WSC273 is an easier but larger (1886 examples) variation published by <ref type="bibr" target="#b14">Rahman and Ng (2012)</ref> and earlier introduced as WSCR. The sequence ranking approach uses WSCR for training and attempts to generalize to WSC273. The gap in performance scores between WSCR and WSC273 (76% vs. 63%) implies that examples in WSC273 are much harder. We note that <ref type="bibr" target="#b12">Opitz and Frank (2018)</ref> do not report removing the overlapping examples between WSCR and WSC273.</p><p>Another important NLU benchmark is GLUE <ref type="bibr" target="#b18">(Wang et al., 2019)</ref>, which gathers 9 tasks and is commonly used to evaluate LMs. The best score has seen a huge jump from 0.69 to over 0.82 in a single year. However, WNLI is a notoriously difficult task in GLUE and remains unsolved by the existing approaches. None of the models have beaten the majority baseline at 65.1, while human performance lies at 95.9 (Nangia and Bowman, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>WSC Approach. We approach WSC by finetuning the pre-trained BERT LM <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> on the WSCR training set and further on a very large Winograd-like dataset that we introduce. Below, we present our fine-tuning objective function and the introduced dataset.</p><p>Given a training sentence s, the pronoun to be resolved is masked out from the sentence, and the LM is used to predict the correct candidate in the place of the masked pronoun. Let c 1 and c 2 be the two candidates. BERT for Masked Token Predic-tion is used to find P(c 1 |s) and P(c 2 |s). If a candidate consists of several tokens, the corresponding number of [MASK] tokens is used in the masked sentence. Then, log P(c|s) is computed as the average of log-probabilities of each composing token. If c 1 is correct, and c 2 is not, the loss is:</p><formula xml:id="formula_0">L = − log P(c 1 |s) + (1) + α · max(0, log P(c 2 |s) − log P(c 1 |s) + β),</formula><p>where α and β are hyperparameters.</p><p>MaskedWiki Dataset. To get more data for fine-tuning, we automatically generate a largescale collection of sentences similar to WSC. More specifically, our procedure searches a large text corpus for sentences that contain (at least) two occurrences of the same noun. We mask the second occurrence of this noun with the [MASK] token. Several possible replacements for the masked token are given, for each noun in the sentence different from the replaced noun. We thus obtain examples that are structurally similar to those in WSC, although we cannot ensure that they fulfill all the requirements (see Section 2).</p><p>To generate such sentences, we choose the English Wikipedia as source text corpus, as it is a large-scale and grammatically correct collection of text with diverse information. We use the Stanford POS tagger <ref type="bibr" target="#b9">(Manning et al., 2014)</ref> for finding nouns. We obtain a dataset with approximately 130M examples. We downsample the dataset uniformly at random to obtain a dataset of manageable size. After downsampling, the dataset consists of 2.4M examples. All experiments are conducted with this downsampled dataset only.</p><p>To determine the quality of the dataset, 200 random examples are manually categorized into 4 categories:</p><p>• Unsolvable: the masked word cannot be unambiguously selected with the given context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this work, we use the PyTorch implementation 3 of <ref type="bibr">Devlin et al.'s (2018)</ref> pre-trained model, BERT-large. To obtain BERT WIKI, we train on MaskedWiki starting from the pre-trained BERT.</p><p>The training procedure differs from the training of BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> in a few points. The model is trained with a single epoch of the MaskedWiki dataset, using batches of size 64 (distributed on 8 GPUs), Adam optimizer, a learning rate of 5.0 · 10 −6 , and hyperparameter val-ues α = 20 and β = 0.2 in the loss function <ref type="figure">(Eq. (1)</ref>). The values were selected from α ∈ {5, 10, 20} and β ∈ {0.1, 0.2, 0.4} and learning rate from {3 · 10 −5 , 1 · 10 −5 , 5 · 10 −6 , 3 · 10 −6 } using grid search. To speed up the hyperparameter search, the training (for hyperparameter search only) is done on a randomly selected subset of size 100, 000. The performance is then compared on the WSCR test set. Both BERT and BERT WIKI are fine-tuned on the WSCR training dataset to create BERT WSCR and BERT WIKI WSCR.</p><p>The WSCR test set was used as the validation set. The fine-tuning procedure was the same as the training procedure on MaskedWiki, except that 30 epochs were used. The model was validated after every epoch, and the model with highest performance on the validation set was retained. The hyperparameters α and β and learning rate were selected with grid search from the same sets as for MaskedWiki training.</p><p>For comparison, experiments are also conducted on two other LMs, BERT-base (BERT with less parameters) and General Pre-trained Transformer (GPT) by <ref type="bibr" target="#b13">Radford et al. (2018)</ref>. The training on BERT-base was conducted in the same way as for the other models. When using GPT, the probability of a word belonging to the sentence P(c|s) is computed as partial loss in the same way as by <ref type="bibr" target="#b16">Trinh and Le (2018)</ref>.</p><p>Due to WSC's "special word" property, examples come in pairs. A pair of examples only differs in a single word (but the correct answers are different). The model BERT WIKI WSCR no pairs is the BERT WIKI model, fine-tuned on WSCR, where only a single example from each pair is retained. The size of WSCR is thus halved. The model BERT WIKI WSCR pairs is obtained by fine-tuning BERT WIKI on half of the WSCR dataset. This time, all examples in the subset come in pairs, just like in the unreduced WSCR dataset.</p><p>We evaluate all models on WSC273 and the WNLI test dataset, as well as the various subsets of WSC273, as described in Section 2. The results are reported in <ref type="table" target="#tab_2">Table 1</ref> and will be discussed next.</p><p>Discussion. Firstly, we note that models that are fine-tuned on the WSCR dataset consistently outperform their non-fine-tuned counterparts. The BERT WIKI WSCR model outperforms other language models on 5 out of 6 sets that they are com-  <ref type="formula">(2018)</ref>, the accuracy is more consistent between associative and non-associative subsets and less affected by the switched parties. However, it remains fairly inconsistent, which is a general property of LMs. Secondly, the results of BERT WIKI seem to indicate that this dataset alone does not help BERT. However, when additionally fine-tuned to WSCR, the accuracy consistently improves.</p><p>Finally, the results of BERT WIKI no pairs and BERT WIKI pairs show that the existence of WSC-like pairs in the training data affects the performance of the trained model. MaskedWiki does not contain such pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Outlook</head><p>This work achieves new SOTA results on the WSC273 and WNLI datasets by fine-tuning the BERT language model on the WSCR dataset and a newly introduced MaskedWiki dataset. The previous SOTA results on WSC273 and WNLI are improved by 8.8% and 9.6%, respectively. To our knowledge, this is the first model that beats the majority baseline on WNLI.</p><p>We show that by fine-tuning on WSC-like data, the language model's performance on WSC consistently improves. The consistent improvement of several language models indicates the robustness of this method. This is particularly surprising, because previous work <ref type="bibr" target="#b12">(Opitz and Frank, 2018)</ref> implies that generalizing to WSC273 is hard.</p><p>In future work, other uses and the statistical significance of MaskedWiki's impact and its applications to different tasks will be investigated. Furthermore, to further improve the results on WSC273, data-filtering procedures may be introduced to find harder WSC-like examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>relax the criteria and construct the Definite Pronoun Resolution (DPR) dataset, following the structure of WSC, but also accepting easier examples. The dataset, referred throughout the paper as WSCR, is split into a training set with 1322 examples and test set with 564 examples. Six examples in the WSCR training set reappear in WSC273. We remove these examples from WSCR. We use the WSCR training and test sets for fine-tuning the LMs and for validation, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>45% were hard, 45.5% were easy, and 1% fell into the noise category.</figDesc><table><row><cell>choice. Example: The syllables are pro-</cell></row><row><cell>nounced strongly by Gaga in syncopation</cell></row><row><cell>while her vibrato complemented Bennett's</cell></row><row><cell>characteristic jazz vocals and swing . Olivier</cell></row><row><cell>added , " [MASK] 's voice , when stripped</cell></row><row><cell>of its bells and whistles, showcases a time-</cell></row><row><cell>lessness that lends itself well to the genre . "</cell></row><row><cell>[Gaga/syncopation]</cell></row><row><cell>• Noise: The example is a result of a parsing</cell></row><row><cell>error.</cell></row><row><cell>In the analyzed subset, 8.5% of examples were un-</cell></row><row><cell>solvable, WNLI Approach. Models are additionally</cell></row><row><cell>tested on the test set of the WNLI dataset. To use</cell></row><row><cell>the same evaluation approach as for the WSC273</cell></row><row><cell>dataset, we transform the examples in WNLI</cell></row><row><cell>from the premise-hypothesis format into the</cell></row><row><cell>masked words format. Since each hypothesis is</cell></row><row><cell>just a substring of the premise with the pronoun</cell></row><row><cell>replaced for the candidate, finding the replaced</cell></row><row><cell>pronoun and one candidate can be done by finding</cell></row><row><cell>the hypothesis as a substring of the premise.</cell></row><row><cell>All other nouns in the sentence are treated as</cell></row><row><cell>alternative candidates. The Stanford POS-tagger</cell></row><row><cell>(Manning et al., 2014) is used to find the nouns in</cell></row><row><cell>the sentence. The probability for each candidate</cell></row><row><cell>is computed to determine whether the candidate</cell></row><row><cell>in the hypothesis is the best match. Only the test</cell></row><row><cell>set of the WNLI dataset is used, because it does</cell></row><row><cell>not overlap with WSC273. We do not train or</cell></row><row><cell>validate on the WNLI training and validation sets,</cell></row><row><cell>because some of the examples share the premise.</cell></row><row><cell>Indeed, when upper rephrasing of the examples is</cell></row><row><cell>used, the training, validation, and test sets start to</cell></row><row><cell>overlap.</cell></row><row><cell>Example: Palmer and Crenshaw both used</cell></row><row><cell>Wilson 8802 putters , with [MASK] 's receiv-</cell></row><row><cell>ing the moniker " Little Ben " due to his pro-</cell></row><row><cell>ficiency with it . [Palmer/Crenshaw]</cell></row><row><cell>• Hard: the answer is not trivial to figure out.</cell></row><row><cell>Example: At the time of Plath 's suicide , As-</cell></row><row><cell>sia was pregnant with Hughes 's child , but</cell></row><row><cell>she had an abortion soon after [MASK] 's</cell></row><row><cell>death . [Plath/Assia]</cell></row><row><cell>• Easy: The alternative sentence is grammati-</cell></row><row><cell>cally incorrect or is very visibly an inferior</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on WSC273 and its subsets. The comparison between each language model and its WSCR-tuned model is given. For each column, the better result of the two is in bold. The best result in the column overall is underlined. Results for the LM ensemble and Knowledge Hunter are taken from<ref type="bibr" target="#b15">Trichelair et al. (2018)</ref>. All models consistently improve their accuracy when fine-tuned on the WSCR dataset.</figDesc><table><row><cell></cell><cell cols="7">WSC273 non-assoc. assoc. unswitched switched consist. WNLI</cell></row><row><cell>BERT WIKI</cell><cell>0.619</cell><cell>0.597</cell><cell>0.757</cell><cell>0.573</cell><cell>0.603</cell><cell cols="2">0.389 0.712</cell></row><row><cell>BERT WIKI WSCR</cell><cell>0.725</cell><cell>0.720</cell><cell>0.757</cell><cell>0.732</cell><cell cols="3">0.710 0.550 0.747</cell></row><row><cell>BERT</cell><cell>0.619</cell><cell>0.602</cell><cell>0.730</cell><cell>0.595</cell><cell>0.573</cell><cell cols="2">0.458 0.658</cell></row><row><cell>BERT WSCR</cell><cell>0.714</cell><cell>0.699</cell><cell>0.811</cell><cell>0.695</cell><cell cols="3">0.702 0.550 0.719</cell></row><row><cell>BERT-base</cell><cell>0.564</cell><cell>0.551</cell><cell>0.649</cell><cell>0.527</cell><cell>0.565</cell><cell cols="2">0.443 0.630</cell></row><row><cell>BERT-base WSCR</cell><cell>0.623</cell><cell>0.606</cell><cell>0.730</cell><cell>0.611</cell><cell cols="3">0.634 0.443 0.705</cell></row><row><cell>GPT</cell><cell>0.553</cell><cell>0.525</cell><cell>0.730</cell><cell>0.595</cell><cell>0.519</cell><cell>0.466</cell><cell>-</cell></row><row><cell>GPT WSCR</cell><cell>0.674</cell><cell>0.653</cell><cell>0.811</cell><cell>0.664</cell><cell cols="2">0.580 0.641</cell><cell>-</cell></row><row><cell cols="2">BERT WIKI WSCR no pairs 0.663</cell><cell>0.669</cell><cell>0.622</cell><cell>0.672</cell><cell>0.641</cell><cell>0.511</cell><cell>-</cell></row><row><cell>BERT WIKI WSCR pairs</cell><cell>0.703</cell><cell>0.695</cell><cell>0.757</cell><cell>0.718</cell><cell cols="2">0.710 0.565</cell><cell>-</cell></row><row><cell>LM ensemble</cell><cell>0.637</cell><cell>0.606</cell><cell>0.838</cell><cell>0.634</cell><cell>0.534</cell><cell>0.443</cell><cell>-</cell></row><row><cell>Knowledge Hunter</cell><cell>0.571</cell><cell>0.583</cell><cell>0.5</cell><cell>0.588</cell><cell>0.588</cell><cell>0.901</cell><cell>-</cell></row></table><note>pared on. In comparison to the LM ensemble by Trinh and Le</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://dumps.wikimedia.org/enwiki/ dump id: enwiki-20181201</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/huggingface/ pytorch-pretrained-BERT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Alan Turing Institute under the UK EPSRC grant EP/N510129/1, by the EPSRC grant EP/R013667/1, by the EPSRC studentship OUCS/EPSRC-NPIF/VK/ 1123106, and by an EPSRC Vacation Bursary. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (EP/P020275/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ortiz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The first Winograd Schema Challenge at IJCAI-16. AI Magazine</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="97" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A knowledge hunting framework for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noelia</forename><forename type="middle">De</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01375</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05567</idno>
		<title level="m">Achieving human parity on automatic Chinese to English news translation. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-tuned language models for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1052</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ernest Davis, and Leora Morgenstern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
	<note>The Winograd Schema Challenge</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Winograd Schema Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KR</title>
		<meeting>KR</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Planning, executing and evaluating the Winograd Schema Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Ortiz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A conservative human baseline estimate for GLUE: People still (mostly) beat machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Addressing the Winograd Schema Challenge as a sequence ranking task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Language Cognition and Computational Models</title>
		<meeting>the First International Workshop on Language Cognition and Computational Models</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="41" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resolving complex cases of definite pronouns: The Winograd Schema Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the evaluation of common-sense reasoning in natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Trichelair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01778</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Simple Method for Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
