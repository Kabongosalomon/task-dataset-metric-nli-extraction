<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07-25">25 Jul 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
							<email>zwcai@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SVCL</orgName>
								<orgName type="institution" key="instit2">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
							<email>qfan@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
							<email>rsferis@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SVCL</orgName>
								<orgName type="institution" key="instit2">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-07-25">25 Jul 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>object detection</term>
					<term>multi-scale</term>
					<term>unified neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classical object detectors, based on the sliding window paradigm, search for objects at multiple scales and aspect ratios. While real-time detectors are available for certain classes of objects, e.g. faces or pedestrians <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, it has proven difficult to build detectors of multiple object classes under this paradigm. Recently, there has been interest in detectors derived from deep convolutional neural networks (CNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. While these have shown much greater ability to address the multiclass problem, less progress has been made towards the detection of objects at multiple scales. The R-CNN <ref type="bibr" target="#b2">[3]</ref> samples object proposals at multiple scales, using a preliminary attention stage <ref type="bibr" target="#b7">[8]</ref>, and then warps these proposals to the size (e.g. 224×224) supported by the CNN. This is, however, very inefficient from a computational standpoint. The development of an effective and computationally efficient region proposal mechanism is still an open problem. The more recent Faster-RCNN <ref type="bibr" target="#b8">[9]</ref> addresses the issue with a region proposal network (RPN), which enables end-to-end training. However, the RPN generates proposals of multiple scales by sliding a fixed set of filters over a fixed set of convolutional feature maps. This creates an inconsistency between the sizes of objects, which are variable, and filter receptive fields, which are fixed. As shown in <ref type="figure">Fig. 1</ref>, a fixed receptive field cannot cover the multiple scales at which objects <ref type="figure">Fig. 1</ref>. In natural images, objects can appear at very different scales, as illustrated by the yellow bounding boxes. A single receptive field, such as that of the RPN <ref type="bibr" target="#b8">[9]</ref> (shown in the shaded area), cannot match this variability.</p><p>appear in natural scenes. This compromises detection performance, which tends to be particularly poor for small objects, like that in the center of <ref type="figure">Fig. 1</ref>. In fact, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref> handle such objects by upsampling the input image both at training and testing time. This increases the memory and computation costs of the detector.</p><p>This work proposes a unified multi-scale deep CNN, denoted the multi-scale CNN (MS-CNN), for fast object detection. Similar to <ref type="bibr" target="#b8">[9]</ref>, this network consists of two sub-networks: an object proposal network and an accurate detection network. Both of them are learned end-to-end and share computations. However, to ease the inconsistency between the sizes of objects and receptive fields, object detection is performed with multiple output layers, each focusing on objects within certain scale ranges (see <ref type="figure" target="#fig_1">Fig. 3</ref>). The intuition is that lower network layers, such as "conv-3," have smaller receptive fields, better matched to detect small objects. Conversely, higher layers, such as "conv-5," are best suited for the detection of large objects. The complimentary detectors at different output layers are combined to form a strong multi-scale detector. This is shown to produce accurate object proposals on detection benchmarks with large variation of scale, such as KITTI <ref type="bibr" target="#b9">[10]</ref>, achieving a recall of over 95% for only 100 proposals.</p><p>A second contribution of this work is the use of feature upsampling as an alternative to input upsampling. This is achieved by introducing a deconvolutional layer that increases the resolution of feature maps (see <ref type="figure" target="#fig_3">Fig. 4</ref>), enabling small objects to produce larger regions of strong response. This is shown to reduce memory and computation costs. While deconvolution has been explored for segmentation <ref type="bibr" target="#b10">[11]</ref> and edge detection <ref type="bibr" target="#b11">[12]</ref>, it is, as far as we know, for the first time used to speed up and improve detection. When combined with efficient context encoding and hard negative mining, it results in a detector that advances the state-of-the-art detection on the KITTI <ref type="bibr" target="#b9">[10]</ref> and Caltech <ref type="bibr" target="#b12">[13]</ref> benchmarks. Without image upsampling, the MS-CNN achieves speeds of 10 fps on KITTI (1250×375) and 15 fps on Caltech (640×480) images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One of the earliest methods to achieve real-time detection with high accuracy was the cascaded detector of <ref type="bibr" target="#b0">[1]</ref>. This architecture has been widely used to implement sliding window detectors for faces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, pedestrians <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> and cars <ref type="bibr" target="#b15">[16]</ref>.</p><p>Two main streams of research have been pursued to improve its speed: fast feature extraction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and cascade learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, a set of efficient Haar features was proposed with recourse to integral images. The aggregate feature channels (ACF) of <ref type="bibr" target="#b1">[2]</ref> made it possible to compute HOG features at about 100 fps. On the learning front, <ref type="bibr" target="#b13">[14]</ref> proposed the soft-cascade, a method to transform a classifier learned with boosting into a cascade with certain guarantees in terms of false positive and detection rate. <ref type="bibr" target="#b16">[17]</ref> introduced a Lagrangian formulation to learn cascades that achieve the optimal trade-off between accuracy and computational complexity. <ref type="bibr" target="#b14">[15]</ref> extended this formulation for cascades of highly heterogeneous features, ranging from ACF set to deep CNNs, with widely different complexity. The main current limitation of detector cascades is the difficulty of implementing multiclass detectors under this architecture.</p><p>In an attempt to leverage the success of deep neural networks for object classification, <ref type="bibr" target="#b2">[3]</ref> proposed the R-CNN detector. This combines an object proposal mechanism <ref type="bibr" target="#b7">[8]</ref> and a CNN classifier <ref type="bibr" target="#b17">[18]</ref>. While the R-CNN surpassed previous detectors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> by a large margin, its speed is limited by the need for object proposal generation and repeated CNN evaluation. <ref type="bibr" target="#b5">[6]</ref> has shown that this could be ameliorated with recourse to spatial pyramid pooling (SPP), which allows the computation of CNN features once per image, increasing the detection speed by an order of magnitude. Building on SPP, the Fast-RCNN <ref type="bibr" target="#b3">[4]</ref> introduced the ideas of back-propagation through the ROI pooling layer and multi-task learning of a classifier and a bounding box regressor. However, it still depends on bottomup proposal generation. More recently, the Faster-RCNN <ref type="bibr" target="#b8">[9]</ref> has addressed the generation of object proposals and classifier within a single neural network, leading to a significant speedup for proposal detection. Another interesting work is YOLO <ref type="bibr" target="#b20">[21]</ref>, which outputs object detections within a 7×7 grid. This network runs at ∼40 fps, but with some compromise of detection accuracy.</p><p>For object recognition, it has been shown beneficial to combine multiple losses, defined on intermediate layers of a single network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. GoogLeNet <ref type="bibr" target="#b21">[22]</ref> proposed the use of three weighted classification losses, applied at layers of intermediate heights, showing that this type of regularization is useful for very deep models. The deeply supervised network architecture of <ref type="bibr" target="#b22">[23]</ref> extended this idea to a larger number of layers. The fact that higher layers convey more semantic information motivated <ref type="bibr" target="#b10">[11]</ref> to combine features from intermediate layers, leading to more accurate semantic segmentation. A similar idea was shown useful for edge detection in <ref type="bibr" target="#b11">[12]</ref>. Similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, the proposed MS-CNN is learned with losses that account for intermediate layer outputs. However, the aim is not to simply regularize the learning, as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, or provide detailed information for higher outputs, as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Instead, the goal is to produce a strong individual object detector at each intermediate output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-scale Object Proposal Network</head><p>In this section, we introduce the proposed network for the generation of object proposals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-scale Detection</head><p>The coverage of many object scales is a critical problem for object detection. Since a detector is basically a dot-product between a learned template and an image region, the template has to be matched to the spatial support of the object to recognize. There are two main strategies to achieve this goal. The first is to learn a single classifier and rescale the image multiple times, so that the classifier can match all possible object sizes. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> (a), this strategy requires feature computation at multiple image scales. While it usually produces the most accurate detection, it tends to be very costly. An alternative approach is to apply multiple classifiers to a single input image. This strategy, illustrated in <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>, avoids the repeated computation of feature maps and tends to be efficient. However, it requires an individual classifier for each object scale and usually fails to produce good detectors. Several approaches have been proposed to achieve a good trade-off between accuracy and complexity. For example, the strategy of <ref type="figure" target="#fig_0">Fig. 2</ref> (c) is to rescale the input a few times and learn a small number of model templates <ref type="bibr" target="#b23">[24]</ref>. Another possibility is the feature approximation of <ref type="bibr" target="#b1">[2]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>, this consists of rescaling the input a small number of times and interpolating the missing feature maps. This has been shown to achieve considerable speed-ups for a very modest loss of classification accuracy <ref type="bibr" target="#b1">[2]</ref>.</p><p>The implementation of multi-scale strategies on CNN-based detectors is slightly different from those discussed above, due to the complexity of CNN features. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (e), the R-CNN of <ref type="bibr" target="#b2">[3]</ref> simply warps object proposal patches to the natural scale of the CNN. This is somewhat similar to <ref type="figure" target="#fig_0">Fig. 2</ref> (a), but features are computed for patches rather than the entire image. The multi-scale mechanism of the RPN <ref type="bibr" target="#b8">[9]</ref>, shown in <ref type="figure" target="#fig_0">Fig. 2 (f)</ref>, is similar to that of <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>. However, multiple sets of templates of the same size are applied to all feature maps. This can lead to a severe scale inconsistency for template matching. As shown in <ref type="figure">Fig. 1</ref>, the single scale of the feature maps, dictated by the (228×228) receptive field of the CNN, can be severely mismatched to small (e.g. 32×32) or large (e.g. 640×640) objects. This compromises object detection performance.</p><p>Inspired by previous evidence on the benefits of the strategy of <ref type="figure" target="#fig_0">Fig. 2</ref> (c) over that of <ref type="figure" target="#fig_0">Fig. 2 (b</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>The detailed architecture of the MS-CNN proposal network is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The network detects objects through several detection branches. The results by all detection branches are simply declared as the final proposal detections. The network has a standard CNN trunk, depicted in the center of the figure, and a set of output branches, which emanate from different layers of the trunk. These branches consist of a single detection layer. Note that a buffer convolutional layer is introduced on the branch that emanates after layer "conv4-3". Since this branch is close to the lower layers of the trunk network, it affects their gradients more than the other detection branches. This can lead to some instability during learning. The buffer convolution prevents the gradients of the detection branch from being back-propagated directly to the trunk layers.</p><p>During training, the parameters W of the multi-scale proposal network are learned from a set of training samples</p><formula xml:id="formula_0">S = {(X i , Y i )} N i=1</formula><p>, where X i is a training image patch, and Y i = (y i , b i ) the combination of its class label y i ∈ {0, 1, 2, · · · , K} and bounding box coordinates</p><formula xml:id="formula_1">b i = (b x i , b y i , b w i , b h i )</formula><p>. This is achieved with a multi-task loss</p><formula xml:id="formula_2">L(W) = M m=1 i∈S m α m l m (X i , Y i |W),<label>(1)</label></formula><p>where M is the number of detection branches, α m the weight of loss l m , and S = {S 1 , S 2 , · · · , S M }, where S m contains the examples of scale m. Note that only a subset S m of the training samples, selected by scale, contributes to the loss of detection layer m. Inspired by the success of joint learning of classification and bounding box regression <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, the loss of each detection layer combines these two objectives</p><formula xml:id="formula_3">l(X, Y |W) = L cls (p(X), y) + λ[y ≥ 1]L loc (b,b),<label>(2)</label></formula><p>where p(X) = (p 0 (X), · · · , p K (X)) is the probability distribution over classes,</p><formula xml:id="formula_4">λ a trade-off coefficient, L cls (p(X), y) = − log p y (X) the cross-entropy loss,b = (b x ,b y ,b w ,b h ) the regressed bounding box, and L loc (b,b) = 1 4 j∈{x,y,w,h} smooth L1 (b j ,b j ),<label>(3)</label></formula><p>the smoothed bounding box regression loss of <ref type="bibr" target="#b3">[4]</ref>. The bounding box loss is only used for positive samples and the optimal parameters W * = arg min W L(W) are learned by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sampling</head><p>This section describes the assembly of training samples</p><formula xml:id="formula_5">S m = {S m + , S m − } for each detection layer m.</formula><p>In what follows, the superscript m is dropped for notional simplicity. An anchor is centered at the sliding window on layer m associated with width and height corresponding to filter size. More details can be found in <ref type="table" target="#tab_2">Table 1</ref>.</p><formula xml:id="formula_6">A sample X of anchor bounding box b is labeled as positive if o * ≥ 0.5, where o * = max i∈Sgt IoU (b, b i ).<label>(4)</label></formula><p>S gt is the ground truth and IoU the intersection over union between two bounding boxes. In this case, Y = (y i * , b i * ), where i * = arg max i∈Sgt IoU (b, b i ) and (X, Y ) are added to the positive set S + . All the positive samples in </p><formula xml:id="formula_7">S + = {(X i , Y i )|y i ≥ 1}</formula><formula xml:id="formula_8">= {(X i , Y i )|y i = 0}, such that |S − | = γ|S + |,</formula><p>we considered three sampling strategies: random, bootstrapping, and mixture. Random sampling consists of randomly selecting negative samples according to a uniform distribution. Since the distribution of hard and easy negatives is heavily asymmetric too, most randomly collected samples are easy negatives. It is well known that hard negatives mining helps boost performance, since hard negatives have the largest influence on the detection accuracy. Bootstrapping accounts for this, by ranking the negative samples according to their objectness scores, and then collecting top |S − | negatives. Mixture sampling combines the two, randomly sampling half of S − and sampling the other half by bootstrapping. In our experiments, mixture sampling has very similar performance to bootstrapping.</p><p>To guarantee that each detection layer only detects objects in a certain range of scales, the training set for the layer consists of the subset of S that covers the corresponding scale range. For example, the samples of smallest scale are used to train the detector of "det-8" in <ref type="figure" target="#fig_1">Fig. 3</ref>. It is possible that no positive training samples are available for a detection layer, resulting in |S − |/|S + | ≫ γ. This can make learning unstable. To address this problem, the cross-entropy terms of positives and negatives are weighted as follows</p><formula xml:id="formula_9">L cls = 1 1 + γ 1 |S + | i∈S+ − log p yi (X i ) + γ 1 + γ 1 |S − | i∈S− − log p 0 (X i ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Data Augmentation In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, it is argued that multi-scale training is not needed, since deep neural networks are adept at learning scale invariance. This, however, is not true for datasets such as Caltech <ref type="bibr" target="#b12">[13]</ref> and KITTI <ref type="bibr" target="#b9">[10]</ref>, where object scales can span multiple octaves. In KITTI, many objects are quite small. Without rescaling, the cardinalities of the sets S + = {S 1 + , S 2 + , · · · , S M + } are wildly varying. In general, the set of training examples of largest object size is very small. To ease this imbalance, the original images are randomly resized to multiple scales.</p><p>Fine-tuning Training the Fast-RCNN <ref type="bibr" target="#b3">[4]</ref> and RPN <ref type="bibr" target="#b8">[9]</ref> networks requires large amounts of memory and a small mini-batch, due to the large size of the input (i.e. 1000×600). This leads to a very heavy training procedure. In fact, many background regions that are useless for training take substantially amounts of memory. Thus, we randomly crop a small patch (e.g. 448×448) around objects from the whole image. This drastically reduces the memory requirements, enabling four images to fit into the typical GPU memory of 12G.</p><p>Learning is initialized with the popular VGG-Net <ref type="bibr" target="#b24">[25]</ref>. Since bootstrapping and the multi-task loss can make training unstable in the early iterations, a twostage procedure is adopted. The first stage uses random sampling and a small trade-off coefficient λ (e.g. 0.05). 10,000 iterations are run with a learning rate of 0.00005. The resulting model is used to initialize the second stage, where random sampling is switched to bootstrapping and λ = 1. We set α i = 0.9 for "det-8" and α i = 1 for the other layers. Another 25,000 iterations are run with an initial learning rate of 0.00005, which decays 10 times after every 10,000 iterations. This two-stage learning procedure enables stable multi-task training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Object Detection Network</head><p>Although the proposal network could work as a detector itself, it is not strong, since its sliding windows do not cover objects well. To increase detection accu- racy, a detection network is added. Following <ref type="bibr" target="#b3">[4]</ref>, a ROI pooling layer is first used to extract features of a fixed dimension (e.g. 7×7×512). The features are then fed to a fully connected layer and output layers, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. A deconvolution layer, described in Section 4.1, is added to double the resolution of the feature maps. The multi-task loss of (1) is extended to</p><formula xml:id="formula_10">L(W, W d ) = M m=1 i∈S m α m l m (X i , Y i |W) + i∈S M +1 α M+1 l M+1 (X i , Y i |W, W d ),<label>(6)</label></formula><p>where l M+1 and S M+1 are the loss and training samples for the detection sub-network. S M+1 is collected as in <ref type="bibr" target="#b3">[4]</ref>. As in (2), l M+1 combines a crossentropy loss for classification and a smoothed L 1 loss for bounding box regression. The detection sub-network shares some of the proposal sub-network parameters W and adds some parameters W d . The parameters are optimized jointly, i.e. (W * , W * d ) = arg min L(W, W d ). In the proposed implementation, ROI pooling is applied to the top of the "conv4-3" layer, instead of the "conv5-3" layer of <ref type="bibr" target="#b3">[4]</ref>, since "conv4-3" feature maps performed better in our experiments. One possible explanation is that "conv4-3" corresponds to higher resolution and is better suited for location-aware bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN Feature Map Approximation</head><p>Input size has a critical role in CNN-based object detection accuracy. Simply forwarding object patches, at the original scale, through the CNN impairs performance (especially for small ones), since the pre-trained CNN models have a natural scale (e.g. 224×224). While the R-CNN naturally solves this problem through warping <ref type="bibr" target="#b2">[3]</ref>, it is not explicitly addressed by the Fast-RCNN <ref type="bibr" target="#b3">[4]</ref> or Faster-RCNN <ref type="bibr" target="#b8">[9]</ref>. To bridge the scale gap, these methods simply upsample input images (by ∼2 times). For datasets, such as KITTI <ref type="bibr" target="#b9">[10]</ref>, containing large amounts of small objects, this has limited effectiveness. Input upsampling also has three side effects: large memory requirements, slow training and slow testing. It should be noted that input upsampling does not enrich the image details.</p><p>Instead, it is needed because the higher convolutional layers respond very weakly to small objects. For example, a 32×32 object is mapped into a 4×4 patch of the "conv4-3" layer and a 2×2 patch of the "conv5-3" layer. This provides limited information for 7×7 ROI pooling.</p><p>To address this problem, we consider an efficient way to increase the resolution of feature maps. This consists of upsampling feature maps (instead of the input) using a deconvolution layer, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. This strategy is similar to that of <ref type="bibr" target="#b1">[2]</ref>, shown in <ref type="figure" target="#fig_0">Fig. 2 (d)</ref>, where input rescaling is replaced by feature rescaling. In <ref type="bibr" target="#b1">[2]</ref>, a feature approximator is learned by least squares. In the CNN world, a better solution is to use a deconvolution layer, similar to that of <ref type="bibr" target="#b10">[11]</ref>. Unlike input upsampling, feature upsampling does not incur in extra costs for memory and computation. Our experiments show that the addition of a deconvolution layer significantly boosts detection performance, especially for small objects. To the best of our knowledge, this is the first application of deconvolution to jointly improve the speed and accuracy of an object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context Embedding</head><p>Context has been shown useful for object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref> and segmentation <ref type="bibr" target="#b26">[27]</ref>. Context information has been modeled by a recurrent neural network in <ref type="bibr" target="#b25">[26]</ref> and acquired from multiple regions around the object location in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. In this work, we focus on context from multiple regions. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, features from an object (green cube) and a context (blue cube) region are stacked together immediately after ROI pooling. The context region is 1.5 times larger than the object region. An extra convolutional layer without padding is used to reduce the number of model parameters. It helps compress redundant context and object information, without loss of accuracy, and guarantees that the number of model parameters is approximately the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Learning is initialized with the model generated by the first learning stage of the proposal network, described in Section 3.4. The learning rate is set to 0.0005, and reduced by a factor of 10 times after every 10,000 iterations. Learning stops after 25,000 iterations. The joint optimization of (6) is solved by back-propagation throughout the unified network. Bootstrapping is used and λ = 1. Following <ref type="bibr" target="#b3">[4]</ref>, the parameters of layers"conv1-1" to "conv2-2" are fixed during learning, for faster training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>The performance of the MS-CNN detector was evaluated on the KITTI <ref type="bibr" target="#b9">[10]</ref> and Caltech Pedestrian <ref type="bibr" target="#b12">[13]</ref> benchmarks. These were chosen because, unlike VOC <ref type="bibr" target="#b27">[28]</ref> and ImageNet <ref type="bibr" target="#b28">[29]</ref>, they contain many small objects. Typical image sizes  <ref type="table" target="#tab_2">car  filter  5x5  7x7  5x5  7x7  5x5  7x7  5x5  7x7 4096  anchor 40x40 56x56 80x80 112x112 160x160 224x224 320x320   ped/cyc  filter  5x3  7x5  5x3  7x5  5x3  7x5  5x3  7x5 2048  anchor 40x28 56x36 80x56 112x72 160x112 224x144 320x224   caltech  filter  5x3  7x5  5x3  7x5  5x3  7x5  5x3</ref> 8x4 2048 anchor 40x20 56x28 80x40 112x56 160x80 224x112 320x160 are 1250×375 on KITTI and 640×480 on Caltech. KITTI contains three object classes: car, pedestrian and cyclist, and three levels of evaluation: easy, moderate and hard. The "moderate" level is the most commonly used. In total, 7,481 images are available for training/validation, and 7,518 for testing. Since no ground truth is available for the test set, we followed <ref type="bibr" target="#b4">[5]</ref>, splitting the trainval set into training and validation sets. In all ablation experiments, the training set was used for learning and the validation set for evaluation. Following <ref type="bibr" target="#b4">[5]</ref>, a model was trained for car detection and another for pedestrian/cyclist detection. One pedestrian model was learned on Caltech. The model configurations for original input size are shown in <ref type="table" target="#tab_2">Table 1</ref>. The detector was implemented in C++ within the Caffe toolbox <ref type="bibr" target="#b29">[30]</ref>, and source code is available at https://github.com/zhaoweicai/mscnn. All times are reported for implementation on a single CPU core (2.40GHz) of an Intel Xeon E5-2630 server with 64GB of RAM. An NVIDIA Titan GPU was used for CNN computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Proposal Evaluation</head><p>We start with an evaluation of the proposal network. Following <ref type="bibr" target="#b30">[31]</ref>, oracle recall is used as performance metric. For consistency with the KITTI setup, a ground truth is recalled if its best matched proposal has IoU higher than 70% for cars, and 50% for pedestrians and cyclists. The roles of individual detection layers <ref type="table" target="#tab_3">Table 2</ref> shows the detection accuracy of the various detection layers as a function of object height in pixels. As expected, each layer has highest accuracy for the objects that match its scale. While the individual recall across scales is low, the combination of all detectors achieves high recall for all object scales. The effect of input size <ref type="figure" target="#fig_4">Fig. 5</ref> shows that the proposal network is fairly robust to the size of input images for cars and pedestrians. For cyclist, performance increases between heights 384 and 576, but there are no gains beyond this. These results show that the network can achieve good proposal generation performance without substantial input upsampling. Detection sub-network improves proposal sub-network <ref type="bibr" target="#b3">[4]</ref> has shown that multi-task learning can benefit both bounding box regression and classification. On the other hand <ref type="bibr" target="#b8">[9]</ref> showed that, even when features are shared between the two tasks, object detection does not improve object proposals too much. <ref type="figure" target="#fig_4">Fig. 5</ref> shows that, for the MS-CNN, detection can substantially benefit proposal generation, especially for pedestrians.</p><p>Comparison with the state-of-the-art <ref type="figure" target="#fig_6">Fig. 6</ref> compares the proposal generation network to BING <ref type="bibr" target="#b31">[32]</ref>, Selective Search <ref type="bibr" target="#b7">[8]</ref>, EdgeBoxes <ref type="bibr" target="#b32">[33]</ref>, MCG <ref type="bibr" target="#b33">[34]</ref>, 3DOP <ref type="bibr" target="#b4">[5]</ref> and RPN <ref type="bibr" target="#b8">[9]</ref>. The top row of the figure shows that the MS-CNN achieves a recall about 98% with only 100 proposals. This should be compared to the ∼2,000 proposals required by 3DOP and the ∼10,000 proposals required by EdgeBoxbes. While it is not surprising that the proposed network outperforms unsupervised proposal methods, such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, its large gains over supervised methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5]</ref>, that can even use 3D information, are significant. The closest performance is achieved by RPN (input upsampled twice), which has substantially weaker performance for pedestrians and cyclists. When the input is not upsampled, RPN misses even more objects, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. It is worth mentioning that the MS-CNN generates high quality proposals (high overlap with the ground truth) without any edge detection or segmentation. This is evidence for the effectiveness of bounding box regression networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection Evaluation</head><p>In this section we evaluate object detection performance. Since the performance of the cyclist detector has large variance on the validation set, due to the low number of cyclist occurrences, only car and pedestrian detection are considered in the ablation experiments.</p><p>The effect of input upsampling <ref type="table">Table 3</ref> shows that input upsampling can be a crucial factor for detection. A significant improvement is obtained by upsampling the inputs by 1.5∼2 times, but we saw little gains beyond a factor of    <ref type="table">Table 3</ref>. Results on the KITTI validation set. "hXXX" indicates an input of height "XXX", "2x" deconvolution, "ctx" context encoding, and "c" dimensionality reduction convolution. In columns "Time" and "# params", entries before the "/" are for car model and after for pedestrian/cyclist model. 2. This is smaller than the factor of 3.5 required by <ref type="bibr" target="#b4">[5]</ref>. Larger factors lead to (exponentially) slower detectors and larger memory requirements. Sampling strategy <ref type="table">Table 3</ref> compares sampling strategies: random ("h576random"), bootstrapping ("h576") and mixture ("h576-mixture"). For car, these three strategies are close to each other. For pedestrian, bootstrapping and mixture are close, but random is much worse. Note that random sampling has many more false positives than the other two. CNN feature approximation Three methods were attempted for learning the deconvolution layer for feature map approximation: 1) bilinearly interpolated weights; 2) weights initialized by bilinear interpolation and learned with backpropagation; 3) weights initialized with Gaussian noise and learned by backpropagation. We found the first method to work best, confirming the findings of   <ref type="figure">Fig. 7</ref>. Comparison to the state-of-the-art on KITTI benchmark test set (moderate).  <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. As shown in <ref type="table">Table 3</ref>, the deconvoltion layer helps in most cases. The gains are larger for smaller input images, which tend to have smaller objects. Note that the feature map approximation adds trivial computation and no parameters. Context embedding <ref type="table">Table 3</ref> shows that there is a gain in encoding context. However, the number of model parameters almost doubles. The dimensionality reduction convolution layer significantly reduces this problem, without impairment of accuracy or speed.</p><p>Object detection by the proposal network The proposal network can work as a detector, by switching the class-agnostic classification to class-specific. <ref type="table">Table 3</ref> shows that, although not as strong as the unified network, it achieves fairly good results, which are better than those of some detectors on the KITTI leaderboard 1 .</p><p>Comparison to the state-of-the-art The results of model "h768-ctx-c" were submitted to the KITTI leaderboard. A comparison to previous approaches is given in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="figure">Fig. 7</ref>. The MS-CNN set a new record for the detection of pedestrians and cyclists. The columns "Pedestrians-Mod" and "Cyclists-Mod" show substantial gains (6 and 7 points respectively) over 3DOP <ref type="bibr" target="#b4">[5]</ref>, and much better performance than the Faster-RCNN <ref type="bibr" target="#b8">[9]</ref>, Regionlets <ref type="bibr" target="#b19">[20]</ref>, etc. We also led a nontrivial margin over the very recent SDP+RPN <ref type="bibr" target="#b41">[42]</ref>, which used scale depen-   dent pooling. In terms of speed, the network is fairly fast. For the largest input size, the MS-CNN detector is about 8 times faster than 3DOP. On the original images (1250×375) detection speed reaches 10 fps. Pedestrian detection on Caltech The MS-CNN detector was also evaluated on the Caltech pedestrian benchmark. The model "h720-ctx" was compared to methods such as DeepParts <ref type="bibr" target="#b38">[39]</ref>, CompACT-Deep <ref type="bibr" target="#b14">[15]</ref>, CheckerBoard <ref type="bibr" target="#b39">[40]</ref>, LDCF <ref type="bibr" target="#b42">[43]</ref>, ACF <ref type="bibr" target="#b1">[2]</ref>, and SpatialPooling <ref type="bibr" target="#b40">[41]</ref> on three tasks: reasonable, medium and partial occlusion. As shown in <ref type="figure" target="#fig_9">Fig. 8</ref>, the MS-CNN has state-ofthe-art performance. <ref type="figure" target="#fig_9">Fig. 8 (b)</ref> and (c) show that it performs very well for small and occluded objects, outperforming DeepParts <ref type="bibr" target="#b38">[39]</ref>, which explicitly addresses occlusion. Moreover, it misses a very small number of pedestrians, due to the accuracy of the proposal network. The speed is approximately 8 fps (15 fps) on upsampled 960×720 (original 640×480) Caltech images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed a unified deep convolutional neural network, denoted the MS-CNN, for fast multi-scale object detection. The detection is preformed at various intermediate network layers, whose receptive fields match various object scales. This enables the detection of all object scales by feedforwarding a single input image through the network, which results in a very fast detector. CNN feature approximation was also explored, as an alternative to input upsampling. It was shown to result in significant savings in memory and computation. Overall, the MS-CNN detector achieves high detection rates at speeds of up to 15 fps.</p><p>Acknowledgement This work was partially funded by NSF grant IIS1208522 and a gift from KETI. We also thank NVIDIA for GPU donations through their academic program.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Different strategies for multi-scale detection. The length of model template represents the template size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Proposal sub-network of the MS-CNN. The bold cubes are the output tensors of the network. h × w is the filter size, c the number of classes, and b the number of bounding box coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(g). This can be seen as the deep CNN extension of Fig. 2 (c), but only uses a single scale of input. It differs from both Fig. 2 (e) and (f) in that it exploits feature maps of several resolutions to detect objects at different scales. This is accomplished by the application of a set of templates at intermediate network layers. This results in a set of variable receptive field sizes, which can cover a large range of object sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Object detection sub-network of the MS-CNN. "trunk CNN layers" are shared with proposal sub-network. W and H are the width and height of the input image. The green (blue) cubes represent object (context) region pooling. "class probability" and "bounding box" are the outputs of the detection sub-network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Proposal recall on the KITTI validation set (moderate). "hXXX" refers to input images of height "XXX". "mt" indicates multi-task learning of proposal and detection sub-networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>CyclistFig. 6 .</head><label>6</label><figDesc>Proposal performance comparison on KITTI validation set (moderate). The first row is proposal recall curves and the second row is recall v.s. IoU for 100 proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison to the state-of-the-art on Caltech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>contribute to the loss. Samples such that o * &lt; 0.2 are assigned to a preliminary negative training pool, and the remaining samples discarded. For a natural image, the distribution of objects and non-objects is heavily asymmetric. Sampling is used to compensate for this imbalance. To collect a final set of negative samples S −</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Parameter configurations of the different models.</figDesc><table><row><cell>det-8</cell><cell>det-16</cell><cell>det-32</cell><cell>det-64</cell><cell>ROI</cell><cell>FC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Detection recall of the various detection layers on KITTI validation set (car), as a function of object hight in pixels.</figDesc><table><row><cell></cell><cell cols="5">det-8 det-16 det-32 det-64 combined</cell></row><row><cell>25≤height&lt;50</cell><cell cols="3">0.9180 0.3071 0.0003</cell><cell>0</cell><cell>0.9360</cell></row><row><cell cols="4">50≤height&lt;100 0.5934 0.9660 0.4252</cell><cell>0</cell><cell>0.9814</cell></row><row><cell cols="5">100≤height&lt;200 0.0007 0.5997 0.9929 0.4582</cell><cell>0.9964</cell></row><row><cell>height≥200</cell><cell>0</cell><cell>0</cell><cell cols="2">0.9583 0.9792</cell><cell>0.9583</cell></row><row><cell>all scales</cell><cell cols="4">0.6486 0.5654 0.3149 0.0863</cell><cell>0.9611</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Results on the KITTI benchmark test set (only published works shown). 76.45 59.70 73.14 61.15 55.21 70.41 58.72 51.83 3DOP [5] 3s 93.04 88.64 79.10 81.78 67.47 64.70 78.39 68.94 61.37 SDP+RPN [42] 0.4s 90.14 88.85 78.38 80.09 70.16 64.82 81.37 73.74 65.31 MS-CNN 0.4s 90.03 89.02 76.11 83.92 73.70 68.31 84.06 75.46 66.07</figDesc><table><row><cell>Method</cell><cell>Time</cell><cell cols="8">Cars Easy Mod Hard Easy Mod Hard Easy Mod Hard Pedestrians Cyclists</cell></row><row><cell cols="2">LSVM-MDPM-sv [35] 10s</cell><cell cols="8">68.02 56.48 44.18 47.74 39.36 35.95 35.04 27.50 26.21</cell></row><row><cell>DPM-VOC-VP [36]</cell><cell>8s</cell><cell cols="8">74.95 64.71 48.76 59.48 44.86 40.37 42.43 31.08 28.23</cell></row><row><cell>SubCat [16]</cell><cell cols="7">0.7s 84.14 75.46 59.71 54.67 42.34 37.95</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DVP [37]</cell><cell>40s</cell><cell cols="3">87.46 75.77 65.38</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AOG [38]</cell><cell>3s</cell><cell cols="3">84.80 75.94 60.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster-RCNN [9]</cell><cell>2s</cell><cell cols="8">86.71 81.84 71.12 78.86 65.90 61.18 72.26 63.35 55.90</cell></row><row><cell>CompACT-Deep [15]</cell><cell>1s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">70.69 58.74 52.71</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepParts [39]</cell><cell>1s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">70.49 58.67 52.78</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FilteredICF [40]</cell><cell>2s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">67.65 56.75 51.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>pAUCEnsT [41]</cell><cell>60s</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">65.26 54.49 48.60 51.62 38.03 33.38</cell></row><row><cell>Regionlets [20]</cell><cell>1s</cell><cell>84.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cvlibs.net/datasets/kitti/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="580" to="587" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fast R-CNN. In: ICCV</title>
		<imprint>
			<biblScope unit="page" from="1440" to="1448" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to detect vehicles by clustering appearance patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2511" to="2521" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Boosting algorithms for detector cascade learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2569" to="2605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="1106" to="1114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2903" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">segdeepm: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="4703" to="4711" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: MM</title>
		<imprint>
			<biblScope unit="page" from="675" to="678" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BING: binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of objects and scene layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1467" to="1475" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-view and 3d deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2232" to="2245" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integrating context and occlusion for car detection by hierarchical and-or model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="page" from="652" to="667" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="1751" to="1760" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno>abs/1409.5209</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
