<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IterDet: Iterative Scheme for Object Detection in Crowded Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
							<email>d.rukhovich@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
							<email>k.sofiiuk@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danil</forename><surname>Galeev</surname></persName>
							<email>d.galeev@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
							<email>o.barinova@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IterDet: Iterative Scheme for Object Detection in Crowded Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based detectors tend to produce duplicate detections of the same objects. After that, the detections are filtered via a non-maximum suppression algorithm (NMS) so that there remains only one bounding box per object. This simple greedy scheme is sufficient for isolated objects. However, it often fails in crowded environments since boxes for different objects should be preserved and duplicate detections should be suppressed at the same time. In this work, we propose to obtain predictions following iterative scheme called IterDet. At each iteration, a new subset of objects is detected. Detected boxes from all the previous iterations are considered at the current iteration to ensure that the same object would not be detected twice. This iterative scheme can be applied to both one-stage and two-stage deep learning-based detectors with minor modifications. Through extensive evaluation on 4 diverse datasets with two different baseline detectors, we prove our iterative scheme to achieve significant improvement over the baseline. On CrowdHuman and WiderPerson datasets, we obtain state-ofthe-art results. The source code and the trained models are available at https: //github.com/saic-vul/iterdet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning-based methods of object detection have significantly evolved and achieved solid improvements in terms of speed and accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>All deep learning-based detectors densely sample and independently evaluate candidate object locations. Accordingly, for a single object, they yield multiple similar boxes of varying confidence. This redundant set of detected boxes is then filtered via non-maximum suppression (NMS) or similar techniques to produce exactly one bounding box per object. This greedy scheme is sufficient if instances of the same class do not overlap in the image.</p><p>However, this is not always the case. Another possible scenario for object detection is so-called crowded environments that contain multiple overlapping objects of the same class (e.g. people in the street or bacteria in microscopy images). Crowded environments provide a challenging task for object detectors due to several reasons. First, it is extremely difficult to distinguish whether two candidate boxes belong to the same object or correspond to two overlapping objects. Second, weak visual cues of heavily occluded instances can hardly provide sufficient information for accurate object detection. In several works, this problem has been addressed with various modifications of the NMS algorithm <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. By NMS, both duplicate detections of the same object should be removed and the hard-to-detect occluded objects should be kept at the same time. Therefore, there is a natural trade-off between precision and recall that imposes severe restrictions on all these approaches.</p><p>In this work, we describe a novel iterative scheme (IterDet) for object detection. Rather than detecting all objects in the image simultaneously, we propose to yield detections iteratively. At each iteration, a new subset of objects is detected. Object boxes that have been already found at the previous iterations are passed to the neural network at the current iteration, so duplicates can be avoided. The proposed iterative scheme can be applied to any one-stage or two-stage object detection method with only minor modifications. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the results of IterDet for Faster RCNN <ref type="bibr" target="#b15">[16]</ref> on a test image from CrowdHuman dataset <ref type="bibr" target="#b18">[19]</ref>. True positive boxes with scores above 0.1 are visualized, and false positives are omitted for visual clarity. At the second iteration, 9 additional objects (shown in yellow) out of 137 are detected, overtaking the baseline Faster RCNN by 5 true positives and 2.7% of average precision (AP). In the top-right corner of the images, there is an example of two strongly overlapping objects detected with IterDet yet missed by the baseline detector.</p><p>Recently, there have been introduced several neural architectures that handle image context thus being more suitable for crowded environments <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>. For instance, <ref type="bibr" target="#b21">[22]</ref> proposed to use a special Hungarian loss function to train a convolutional-recurrent model that yields strictly one detection per iteration. In comparison, our approach is more computationally efficient. Moreover, instead of storing information about previously detected objects via LSTM, we explicitly pass it to the network in a form of object masks. Our approach guarantees that no previously detected bounding boxes are accidentally forgotten. Furthermore, compared to <ref type="bibr" target="#b21">[22]</ref> it allows incorporating the history of detections into deeper layers of a neural network.</p><p>In PS-RCNN <ref type="bibr" target="#b3">[4]</ref>, objects are also detected iteratively: simple objects are supposed to be found on the first iteration while the second iteration is performed to explore more difficult cases. This iterative approach can be applied only for RCNN-based detectors. At the same time, our approach can be easily integrated into state-of-the-art object detection methods.</p><p>We perform extensive experiments with both one-stage (RetinaNet <ref type="bibr" target="#b11">[12]</ref>) and twostage (Faster RCNN <ref type="bibr" target="#b15">[16]</ref>) object detectors on four challenging datasets (AdaptIS ToyV1 and ToyV2 <ref type="bibr" target="#b20">[21]</ref>, CrowdHuman <ref type="bibr" target="#b18">[19]</ref>, and WiderPerson <ref type="bibr" target="#b25">[26]</ref>). To prove our ideas, we evaluate IterDet against baseline models and compare the obtained results with the results reported by competitors. On all datasets, IterDet outperforms baseline models and sets new state-of-the-art on CrowdHuman and WiderPerson datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Standard methods for object detection. Deep learning-based object detectors can be classified as two-stage and one-stage detectors. Two-stage detectors are based on proposal-driven mechanism <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. They consist of two subnetworks: the first one outputs a sparse set of candidate object locations and the second one classifies these object locations into one of the foreground classes or a background.</p><p>One-stage methods are applied over a regular, dense sampling of object locations, scales, and aspect ratios <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>. Being much faster on inference than their two-stage counterparts, recent one-stage methods achieve comparable accuracy on some datasets. Moreover, anchor-free one-stage methods <ref type="bibr" target="#b22">[23]</ref> are more agile and less limited compared to their predecessors. However, two-stage methods still demonstrate state-of-the-art accuracy on challenging datasets.</p><p>Overall, all detectors have certain pros and cons and are applicable under certain conditions. To cover all possible scenarios, we design our iterative scheme so it can be combined with both one-stage and two-stage object detectors.</p><p>For deep learning-based methods, the detection problem is formulated as a classification task. Namely, class probabilities are estimated independently for each location for multiple candidate locations across an image. Differently, in our iterative scheme, the history of detections from the previous iterations is passed to the detector at the following iterations, providing the context for resolving ambiguities.</p><p>Modifications of NMS algorithm. The standard NMS algorithm greedily selects detections with a higher score and removes the less confident neighbors. Thus, a wide suppression parameter improves the precision and the narrow suppression improves the recall. Consequently, crowded environments are the most challenging case for NMS since both wide and narrow suppression lead to errors. To address this, numerous modifications of the NMS algorithm have been proposed in the literature. Rothe et al. <ref type="bibr" target="#b16">[17]</ref> formulated NMS as a clustering problem. Hosang et al. <ref type="bibr" target="#b8">[9]</ref> suggested decreasing the confidence of detections that cover the already detected objects. In soft NMS <ref type="bibr" target="#b1">[2]</ref>, scores for object proposals depend on their overlap with a target object. In adaptive NMS <ref type="bibr" target="#b12">[13]</ref>, parameters of NMS are chosen according to the density of the objects estimated via an extra branch. Most recent R 2 NMS [11] simultaneously predicts the full and visible boxes of an object.</p><p>Differently from all the listed methods, our proposed scheme is iterative that gives more freedom and flexibility. More specifically, we might miss the more difficult objects at the first iteration, since these objects can be detected later on. Accordingly, we do not need to assure high recall at each iteration as we can set wider suppression parameters to favor precision.</p><p>Neural architectures for crowded environments. Several neural architectures for object detection in crowded environments have been described in the literature. Stewart et al. <ref type="bibr" target="#b21">[22]</ref> used a Hungarian loss function to train an LSTM-based model that yields a sequence of detections. LSTM was also used in <ref type="bibr" target="#b6">[7]</ref> for iterative proposal refinement in RPN-based detectors. However, performing the NMS step after all iterations negates all the benefits in case of crowded environments. Hu et al. <ref type="bibr" target="#b9">[10]</ref> proposed an object relation module that processes a set of objects based on their visual appearance and geometry. Ge et al. <ref type="bibr" target="#b3">[4]</ref> introduced a modification of two-stage detectors called PS-RCNN. First, it detects non-occluded objects with RCNN and then suppresses the detected instances with object-shaped masks. At the second step, another RCNN detects occluded objects.</p><p>Compared to the aforementioned methods, our iterative scheme does not imply changing neural architecture, therefore it is much easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>The proposed iterative scheme is shown in <ref type="figure">Figure 2</ref>. First, we introduce notation and describe the inference process. Then, we describe the modified training procedure. <ref type="figure">Fig. 2</ref>. Proposed iterative scheme. The unchanged meta-architecture of an arbitrary detector is marked with blue. The single convolution layer for the history map is marked green. Out of the 4 overlapping objects in the image, 2 are in the history, where they were either randomly sampled at the training step, or detected during previous iterations of the inference. The remaining 2 are predicted by the detector.</p><p>Inference process. A typical object detector D is an algorithm that maps image I ∈ R w×h×3 to a set of bounding boxes B = {(x k , y k , w k , h k )} n k=1 . Each box is represented by the coordinates of its top left corner (x, y), width w and height h. For a given set of boxes B, we define a history image H ∈ Z w×h of the same size as an input image. In H ∈ Z w×h , each pixel contains the number of already detected boxes that cover that pixel:</p><formula xml:id="formula_0">H xy = |B| k=1 1 x k ≤x≤x k +w k , y k ≤y≤y k +h k</formula><p>(1) <ref type="figure">Figure 2</ref> shows an example of the history, where its values are color-coded. We can make a detector D history-aware if we pass the history H along with the image I as its inputs.</p><p>Let us now introduce the iterative scheme IterDet(D ), that, given an image I, produces a set of bounding boxes B in an iterative manner. At the first iteration t = 1 history H 1 is empty and D maps an image I and H 1 to a set of bounding boxes B 1 . Second, B 1 is mapped to history H 2 . H 2 is then mapped to B 2 by D at iteration t = 2. This process stops when the limit of iterations is reached or when</p><formula xml:id="formula_1">|B m | = 0 at some iteration m. The final prediction of IterDet(D ) is B = m t=1 B t ,</formula><p>where m denotes the total number of iterations.</p><p>To implement the described scheme, two modifications should be implied: 1) an arbitrary detector D should be altered to become a history-aware detector D and 2) D should be forced to predict different sets of objects B t on each iteration t. Below, we explain these alterations in detail.</p><p>Architecture of a history-aware detector. State-of-the-art deep learning-based object detection pipelines start with passing an image to an already pre-trained backbone, e.g. ResNet <ref type="bibr" target="#b7">[8]</ref>, VGG <ref type="bibr" target="#b19">[20]</ref>, etc. to obtain multi-level image features. These features are then fed into trainable feature extractors, e.g. Region Proposal Network, Feature Pyramid Network, etc. Their outputs are further passed to the head module predicting bounding boxes. Finally, non-maximum suppression is applied. We aim to introduce minimal changes to the original network architectures and incorporate history in the deepest layers of the network.</p><p>The proposed architecture of the history-aware detector is simple yet efficient. The history is processed via one convolution layer which output sums up with the output of the first convolution layer of the backbone. This scheme can be applied to any backbone without hyperparameter tuning. For ResNet-like backbone, the image is passed through a convolution layer with 64 filters of size 7 and stride 2, Batch Normalization layer, and ReLU activation layer. We follow the design choices of ResNet and use a convolution layer with 64 filters of size 3 and stride 2.</p><p>Training procedure. Several iterative methods predict only one object per iteration <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. Our iterative scheme is also able to predict one object per iteration e.g. by selecting the most confident detection. However, in practice, such an approach would be inefficient since inference time is proportional to the number of objects in the image. Our experiments in Section 4 demonstrate that performing two iterations is enough to achieve the best accuracy. With increasing the number of iterations, the recall improves but the precision degrades, worsening mMR and AP metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and implementation details</head><p>We evaluate our proposed iterative scheme on four datasets containing images of various crowded environments: AdaptIS ToyV1 and ToyV2 <ref type="bibr" target="#b20">[21]</ref>, CrowdHuman <ref type="bibr" target="#b18">[19]</ref> and WiderPerson <ref type="bibr" target="#b25">[26]</ref>.</p><p>AdaptIS. AdaptIS Toy V1 and Toy V2 are two synthetic datasets originally used for instance segmentation task <ref type="bibr" target="#b20">[21]</ref>. Each image contains about 30 objects on average, with many of those severely overlapping. The datasets statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>. For Toy V1, training and validation splits contain 10000 and 2000 images of size 96×96 pixels respectively. Toy V2 is split into training, validation, and test subsets with 25000, 1000, and 1000 images of size 128 × 128 pixels respectively. For both Toy datasets, we chose AP as the main metric, and also provide recall values for consistency. We do not report the mMR metric: if the average number of false positives per image is less than 1 it turns zero, thus being not representative.</p><p>CrowdHuman. The recently introduced CrowdHuman dataset has the largest number of persons per image and the largest number of pairs of intersecting bounding boxes among all datasets for human detection, according to <ref type="bibr" target="#b18">[19]</ref>. It contains 15000, 4370, and 5000 images for training, validation, and testing, respectively. There are 23 people presenting on an average image, each annotated with 3 boxes: full-body, visible-body and head box. The most challenging and most frequently used in other works is full-body annotation, where the boxes not only overlap more strongly but also go beyond the edges of the image. We also conduct experiments on visible-body annotation, training models on the training part of the data, and benchmarking on the validation subset.</p><p>[19] also reports metrics for one-stage RetinaNet detector and the two-stage Faster RCNN detector, both using ResNet-50 as a backbone. The mMR metric is proposed as the major metric to evaluate detection quality. This metric is calculated as the logarithm of missing rate averaged over 9 points ranging from 10 −2 to 10 0 false positives per image. Besides, recall and average precision (AP) are reported.</p><p>WiderPerson. WiderPerson <ref type="bibr" target="#b25">[26]</ref> is another human detection dataset collected from various sources. There are 8000, 1000, and 4382 images in train, validation, and test subsets. It contains annotations for 5 classes: pedestrians, riders, partially visible persons, crowd, and ignored regions. Following <ref type="bibr" target="#b3">[4]</ref>, we merge the last four types into one category for both training and testing.</p><p>Implementation details. Our implementation of the proposed IterDet and all baseline models is based on the MMDetection framework <ref type="bibr" target="#b2">[3]</ref>. This framework is built on top of the PyTorch library <ref type="bibr" target="#b14">[15]</ref> and contains implementations of numerous object detection models. For our experiments, we use RetinaNet and Faster RCNN based on ResNet-50 with default parameters. We use 8 GPUs with 2 images per each. The minor modifications are described below. First, we add a Batch Normalization layer after each convolution layer to the FPN of both detectors, which slightly improves performance. Secondly, we do not freeze the first block of ResNet as we add history together with the trainable convolution layer before this block. To simplify the hyperparameter tuning in IterDet experiments, we use Adam optimizer with an initial learning rate of 0.0001. For the baseline experiments, we use SGD optimizer with momentum 0.9, weight decay parameter 0.0001, and the initial learning rate 0.02. The training process finishes at the end of the 24th epoch, and the learning rate is multiplied by 0.1 after 16th and 22nd epochs.</p><p>Dataset-specific hyperparameters. To be consistent with the CrowdHuman benchmark on inference, the input image is re-scaled such that its shortest edge is 800 pixels, and the longest side is not beyond 1400 pixels. We do not use test-time augmentations. During training, we apply horizontal flips and zooming from 75% to 125%. When training on CrowdHuman, we use information about ignored regions when sampling negative examples. For experiments with full-body annotations on CrowdHuman, we follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref> using [1.0, 1.5, 2.0, 2.5, 3.0] anchor ratios and no clipping proposals. For AdaptIS Toy V1 and Toy V2 datasets, we upscale images to 384 × 384 pixels as it have been proposed in an original paper <ref type="bibr" target="#b20">[21]</ref>. The experimental protocol for the WiderPerson dataset is identical to the CrowdHuman dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and discussion</head><p>Results on AdaptIS datasets. <ref type="table">Table 2</ref> summarizes IterDet and baseline metrics on AdaptIS Toy V1 and Toy V2 datasets. For both datasets and detectors, IterDet substantially increases AP. For Faster RCNN, this increase expands 4% bringing the final AP up to 99%.</p><p>Results on CrowdHuman. Results on full-body and visible-body annotations of the CrowdHuman dataset are presented in <ref type="table">Tables 3 and 4</ref> respectively. We compare the proposed IterDet scheme to the methods that do not use additional data or annotations during training. According to <ref type="table">Table 3</ref>, we achieve a significant improvement in terms of all metrics for the most challenging full-body annotation. More specifically, IterDet improves recall by more than 5.5%, AP by 3.1% and mMR by 1.0% w.r.t. baseline. These results remain solid even when compared to the previous state-of-the-art approaches such as Adaptive NMS and PS-RCNN. In terms of mMR, IterDet outperforms all existing methods in all four scenarios: single-and two-stage detectors, visible-and full-body annotations. For the RetinaNet detector, the quality gap exceeds 6% for both types of annotations. Notice, that such an improvement of mMR value is achieved even after the first iteration. We attribute this to the regularization provided by history-aware training. Despite a slight degradation of mMR with an increasing number of iterations, the growth of AP always remains significant. For RetinaNet, we outperform the competitors by 3.9% AP for both types of annotations. Results on WiderPerson. The results on WiderPerson dataset are summarized in <ref type="table">Table 5</ref>. We refer to <ref type="bibr" target="#b25">[26]</ref> for results obtained on hard subset of annotations which contains all the boxes larger than 20 pixels in height. Following the protocol from [4], we do not limit height during testing which is an even more challenging task. For both detectors, we achieve significantly better results in terms of recall, AP, and mMR. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results of IterDet based on Faster RCNN on the four datasets. In all examples, there are strongly overlapping objects with IoU&gt;0.5 which are missed by the baseline detector but found by IterDet with 2 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detector Recall AP mMR Baseline <ref type="bibr">[</ref> Choice of the number of iterations. In some papers, e.g. <ref type="bibr" target="#b3">[4]</ref>, only one metric out of AP and mMR is used for evaluation since their optimal values are not achieved simultaneously. In this work, we report the values of both metrics for CrowdHuman and WiderPerson datasets. <ref type="table">Tables 3, 4</ref> and 5 demonstrate that the best value of mMR metric is achieved after the first iteration in the IterDet scheme. However, the optimal number of iterations in terms of AP metric is not as obvious. <ref type="figure" target="#fig_1">Figure 3</ref> depicts AP for a different number of iterations of the proposed iterative scheme based on Faster RCNN. For all datasets, we observe a noticeable improvement between the first and the second iteration. With increasing the number of iterations, AP does not improve. Moreover, for some datasets, a minor drop of AP can be observed.</p><p>We do not conduct experiments with a larger number of iterations due to the following reasons. First, IterDet already achieves state-of-the-art performance on Crowd-Human and WiderPersons datasets after only 2 iterations. Second, the inference time of the iterative scheme is proportional to the number of iterations, and for 3 iterations the inference would be 3 times slower which is not acceptable in practice.</p><p>Limiting detections per iteration. We also provide the results of an additional experiment to prove the iterative scheme works. For this purpose, the only change is the restriction of one detection per iteration. This can be achieved by changing the NMS step to selection of the bounding box with the highest probability. Note that in this formulation the training process is not changed, and during inference the detector stops when 0 objects are predicted on the next iteration. The computational time of the detector on an image becomes proportional to the number of objects on it, which of course is not acceptable in practice. However, in term of metrics the proposed iterative scheme performs well. Thus, the AP on Toy V2 reaches 98.39%, which is much higher than the baseline values from <ref type="table">Table 2</ref>. The intermediate steps of the iterative scheme with limited detections per iteration are given on <ref type="figure">Figure 5</ref>. For a test image from Toy V2 with 16 objects all of them are successfully detected in 16 iterations. <ref type="bibr" target="#b0">1</ref> 2 3 . . . <ref type="bibr">15 16 Fig. 5</ref>. An additional experiment with limited detections per iteration for an image from Toy V2 test split. First row -history maps with already detected objects. Second row -an object detected on the corresponding iteration. Resulting detections are on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present an iterative scheme (IterDet) of object detection designed for crowded environments. It can be applied to both two-stage and one-stage object detectors. On challenging AdaptIS ToyV1 and ToyV2 datasets with multiple overlapping objects Iter-Det achieves almost perfect accuracy. Through extensive evaluation on CrowdHuman and WiderPerson benchmarks, we show that the proposed iterative scheme outperforms existing methods when applied to either two-stage Faster RCNN or one-stage RetinaNet detector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>baseline recall: 78.8, AP: 76.81 IterDet 1 iter. recall: 75.9, AP: 74.28 IterDet 2 iter. recall: 82.5, AP: 79.59 The results of original Faster RCNN (left) and the proposed IterDet based on Faster RCNN (right) for the same image from CrowdHuman test set with visible-body annotations. The boxes found on the first and second iteration are marked in green and yellow, respectively. The metrics for baseline and IterDet after the first and the second iterations are listed below the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>AP for different number of iterations for IterDet based on Faster RCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>IterDet results on ToyV1, ToyV2 (first row), CrowdHuman (with visible-and full-body annotations, second row), and WiderPerson (third row). The boxes found on the first and second iterations are marked in green and yellow respectively. The scores thresholded for visualization are above 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>During training, we randomly split the set of ground truth bounding boxesB into two subsets B old and B new such that B old ∪ B new =B and B old ∩ B new = ∅. We map B old to a history H and force D to predict the bounding boxes B new that are missing in history. Thus, we optimize the losses of D by backpropagation of the error between the predicted boxes B and target boxes B new . We do not describe losses since we do not modify this part of baseline detectors. On the one hand, this method of training forces the model to exploit the history and predict only new objects at each iteration of inference. On the other hand, it provides an additional source of augmentations by sampling different combinations of B old and B new . Average number of objects and pair-wise overlap between two instances on the four datasets used in our experiments.</figDesc><table><row><cell cols="4">Toy V1 Toy V2 CrowdHuman WiderPerson</cell></row><row><cell cols="2">object/image 14.88 31.25</cell><cell>22.64</cell><cell>29.51</cell></row><row><cell>pair/image</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IoU &gt; 0.3 3.67</cell><cell>7.12</cell><cell>9.02</cell><cell>9.21</cell></row><row><cell>IoU &gt; 0.4 1.95</cell><cell>3.22</cell><cell>4.89</cell><cell>4.78</cell></row><row><cell>IoU &gt; 0.5 0.95</cell><cell>1.25</cell><cell>2.40</cell><cell>2.15</cell></row><row><cell>IoU &gt; 0.6 0.38</cell><cell>0.45</cell><cell>1.01</cell><cell>0.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Experimental results on AdaptIS Toy V1 and Toy V2 dataset. Experimental results on CrowdHuman dataset with full-body annotations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Toy V1</cell><cell>Toy V2</cell></row><row><cell>Method</cell><cell cols="2">Detector</cell><cell>Recall AP Recall AP</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell>95.46 94.46 96.27 95.62</cell></row><row><cell>IterDet, 1 iter.</cell><cell cols="2">RetinaNet</cell><cell>95.21 95.31 96.27 94.17</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>99.56 97.71 99.35 97.27</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell>94.05 93.96 94.88 94.81</cell></row><row><cell>IterDet, 1 iter.</cell><cell cols="2">Faster RCNN</cell><cell>94.34 94.27 94.97 94.89</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>99.60 99.25 99.29 99.00</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Detector Recall AP mMR</cell></row><row><cell cols="2">Baseline [19]</cell><cell></cell><cell>93.80 80.83 63.33</cell></row><row><cell cols="2">IterDet, 1 iter.</cell><cell cols="2">RetinaNet</cell><cell>79.68 76.78 53.03</cell></row><row><cell cols="2">IterDet, 2 iter.</cell><cell></cell><cell>91.49 84.77 56.21</cell></row><row><cell cols="2">Baseline [19]</cell><cell></cell><cell>90.24 84.95 50.49</cell></row><row><cell cols="2">Soft NMS [2,13]</cell><cell></cell><cell>91.73 83.92 51.97</cell></row><row><cell cols="2">Adaptive NMS [13]</cell><cell></cell><cell>91.27 84.71 49.73</cell></row><row><cell cols="2">Repulsion Loss [25,4]</cell><cell cols="2">Faster RCNN</cell><cell>90.74 85.71 -</cell></row><row><cell cols="2">PS-RCNN [4]</cell><cell></cell><cell>93.77 86.05 -</cell></row><row><cell cols="2">IterDet, 1 iter.</cell><cell></cell><cell>88.94 84.43 49.12</cell></row><row><cell cols="2">IterDet, 2 iter.</cell><cell></cell><cell>95.80 88.08 49.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Experimental results on CrowdHuman dataset with visible-body annotations. Experimental results on WiderPerson dataset.</figDesc><table><row><cell>19]</cell><cell></cell><cell></cell><cell>90.96 77.19 65.47</cell></row><row><cell cols="2">Feature NMS [18] IterDet, 1 iter.</cell><cell>RetinaNet</cell><cell>-68.65 75.35 86.91 81.24 58.78</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>89.63 82.32 59.19</cell></row><row><cell>Baseline [19]</cell><cell></cell><cell></cell><cell>91.51 85.60 55.94</cell></row><row><cell>IterDet, 1 iter.</cell><cell></cell><cell cols="2">Faster RCNN</cell><cell>87.59 83.28 55.54</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>91.63 85.33 55.61</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Detector Recall AP mMR</cell></row><row><cell>Baseline [26]</cell><cell></cell><cell></cell><cell>-</cell><cell>-48.32</cell></row><row><cell>IterDet, 1 iter.</cell><cell></cell><cell>RetinaNet</cell><cell>90.38 87.17 43.23</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>95.35 90.23 43.88</cell></row><row><cell>Baseline [26]</cell><cell></cell><cell></cell><cell>-</cell><cell>-46.06</cell></row><row><cell>Baseline [4]</cell><cell></cell><cell></cell><cell>93.60 88.89 -</cell></row><row><cell>PS-RCNN [4]</cell><cell cols="2">Faster RCNN</cell><cell>94.71 89.96 -</cell></row><row><cell>IterDet, 1 iter.</cell><cell></cell><cell></cell><cell>92.67 89.49 40.35</cell></row><row><cell>IterDet, 2 iter.</cell><cell></cell><cell></cell><cell>97.15 91.95 40.78</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ps-rcnn: Detecting secondary human instances in a crowd via primary object suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07080</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5227" to="5236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving multi-stage object detection via iterative proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC. p</title>
		<imprint>
			<biblScope unit="page">223</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nms by representative region: Towards crowded pedestrian detection by proposal pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12729</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>high-performance deep learning library</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-maximum suppression for object detection by passing messages between windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="290" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Salscheider</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07662</idno>
		<title level="m">Featurenms: Non-maximum suppression by learning feature embeddings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7355" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6877" to="6885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xinlong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chunhua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Widerperson: A diverse dataset for dense pedestrian detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
