<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Human Pose and Mesh Reconstruction with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Microsoft</roleName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
						</author>
						<title level="a" type="main">End-to-End Human Pose and Mesh Reconstruction with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D human pose and mesh vertices from a single image. Our method uses a transformer encoder to jointly model vertex-vertex and vertex-joint interactions, and outputs 3D joint coordinates and mesh vertices simultaneously. Compared to existing techniques that regress pose and shape parameters, METRO does not rely on any parametric mesh models like SMPL, thus it can be easily extended to other objects such as hands. We further relax the mesh topology and allow the transformer self-attention mechanism to freely attend between any two vertices, making it possible to learn non-local relationships among mesh vertices and joints. With the proposed masked vertex modeling, our method is more robust and effective in handling challenging situations like partial occlusions. METRO generates new state-of-the-art results for human mesh reconstruction on the public Human3.6M and 3DPW datasets. Moreover, we demonstrate the generalizability of METRO to 3D hand reconstruction in the wild, outperforming existing state-of-the-art methods on FreiHAND dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose and mesh reconstruction from a single image has attracted a lot of attention because it has many applications including virtual reality, sports motion analysis, neurodegenerative condition diagnosis, etc. It is a challenging problem due to complex articulated motion and occlusions.</p><p>Recent work in this area can be roughly divided into two categories. Methods in the first category use a parametric model like SMPL <ref type="bibr" target="#b28">[29]</ref> and learn to predict shape and pose coefficients <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23]</ref>. Great success has been achieved with this approach. The strong prior encoded in the parametric model increases its robustness to environment variations. A drawback of this approach is that the pose and shape spaces are constrained by the limited exemplars that are used to construct the parametric model. To overcome this limitation, methods in the second category do not use any parametric models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>. These methods either use a graph convolutional neural network to model neighborhood vertex-vertex interactions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>, or use 1D heatmap to regress vertex coordinates <ref type="bibr" target="#b31">[32]</ref>. One limitation with these approaches is that they are not efficient in modeling non-local vertex-vertex interactions. Researchers have shown that there are strong correlations between non-local vertices which may belong to different parts of the body (e.g. hand and foot) <ref type="bibr" target="#b54">[55]</ref>. In computer graphics and robotics, inverse kinematics techniques <ref type="bibr" target="#b1">[2]</ref> have been developed to estimate the internal joint positions of an articulated figure given the position of an end effector such as a hand tip. We believe that learning the correlations among body joints and mesh vertices including both short range and long range ones is valuable for handling challenging poses and occlusions in body shape reconstruction. In this paper, we propose a simple yet effective framework to model global vertex-vertex interactions. The main ingredient of our framework is a transformer.</p><p>Recent studies show that transformer <ref type="bibr" target="#b52">[53]</ref> significantly improves the performance on various tasks in natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. The success is mainly attributed to the self-attention mechanism of a transformer, which is particularly effective in modeling the dependencies (or interactions) without regard to their distance in both inputs and outputs. Given the dependencies, transformer is able to soft-search the relevant tokens and performs prediction based on the important features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>In this work, we propose METRO, a multi-layer Transformer encoder with progressive dimensionality reduction, to reconstruct 3D body joints and mesh vertices from a given input image, simultaneously. We design the Masked Vertex Modeling objective with a transformer encoder architecture to enhance the interactions among joints and vertices. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, METRO learns to discover both short-and long-range interactions among body joints and mesh vertices, which helps to better reconstruct the 3D human body shape with large pose variations and occlusions.</p><p>Experimental results on multiple public datasets demonstrate that METRO is effective in learning vertex-vertex and vertex-joint interactions, and consequently outperforms the prior works on human mesh reconstruction by a large margin. To the best of our knowledge, METRO is the first approach that leverages a transformer encoder architecture to jointly learn 3D human pose and mesh reconstruction from a single input image. Moreover, METRO is a general framework which can be easily applied to predict a different 3D mesh, for example, to reconstruct a 3D hand from an input image.</p><p>In summary, we make the following contributions.</p><p>• We introduce a new transformer-based method, named METRO, for 3D human pose and mesh reconstruction from a single image.</p><p>• We design the Masked Vertex Modeling objective with a multi-layer transformer encoder to model both vertex-vertex and vertex-joint interactions for better reconstruction.</p><p>• METRO achieves new state-of-the-art performance on the large-scale benchmark Human3.6M and the challenging 3DPW dataset.</p><p>• METRO is a versatile framework that can be easily realized to predict a different type of 3D mesh, such as 3D hand as demonstrated in the experiments. METRO achieves the first place on FreiHAND leaderboard at the time of paper submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Human Mesh Reconstruction (HMR): HMR is a task of reconstructing 3D human body shape, which is an active research topic in recent years. While pioneer works have demonstrated impressive reconstruction using various sensors, such as depth sensors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref> or inertial measurement units <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54]</ref>, researchers are exploring to use a monocular camera setting that is more efficient and convenient. However, HMR from a single image is difficult due to complex pose variations, occlusions, and limited 3D training data. Prior studies propose to adopt the pre-trained parametric human models, i.e., SMPL <ref type="bibr" target="#b28">[29]</ref>, STAR <ref type="bibr" target="#b34">[35]</ref>, MANO <ref type="bibr" target="#b42">[43]</ref>, and estimate the pose and shape coefficients of the parametric model for HMR. Since it is challenging to regress the pose and shape coefficients directly from an input image, recent works further propose to leverage various human body priors such as human skeletons <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref> or segmentation maps <ref type="bibr" target="#b33">[34]</ref>, and explore different optimization strategies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14]</ref> and temporal information <ref type="bibr" target="#b22">[23]</ref> to improve reconstruction.</p><p>On the other hand, instead of adopting a parametric human model, researchers have also proposed approaches to directly regress 3D human body shape from an input image. For example, researchers have explored to represent human body using a 3D mesh <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>, a volumetric space <ref type="bibr" target="#b51">[52]</ref>, or an occupancy field <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Each of the prior works addresses a specific output representation for their target application. Among the literature, the relevant study is GraphCMR <ref type="bibr" target="#b24">[25]</ref>, which aims to regress 3D mesh vertices using graph convolutional neural networks (GCNNs). Moreover, recent proposed Pose2Mesh <ref type="bibr" target="#b7">[8]</ref> is a cascaded model using GCNNs. Pose2Mesh reconstructs human mesh based on the given human pose representations.</p><p>While GCNN-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> are designed to model neighborhood vertex-vertex interactions based on a pre-specified mesh topology, it is less efficient in modeling longer range interactions. In contrast, METRO models global interactions among joints and mesh vertices without being limited by any mesh topology. In addition, our method learns with self-attention mechanism, which is different from prior studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentions and Transformers:</head><p>Recent studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref> have shown that attention mechanisms improve the performance on various language tasks. Their key insight is to learn the attentions to soft-search relevant inputs that are important for predicting an output <ref type="bibr" target="#b3">[4]</ref>. Vaswani et al. <ref type="bibr" target="#b52">[53]</ref> further propose a transformer architecture based solely on attention mechanisms. Transformer is highly parallelized using multi-head self-attention for efficient training and inference, and leads to superior performance in language modeling at scale, as explored in BERT <ref type="bibr" target="#b8">[9]</ref> and GPT <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref>. Inspired by the recent success in neural language field, there is a growing interest in exploring the use of transformer architecture for various vision tasks, such as learning the pixel distributions for image generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> and classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, or to simplify object detection as a set prediction problem <ref type="bibr" target="#b5">[6]</ref>. However, 3D human reconstruction has not been explored along this direction.</p><p>In this study, we present a multi-layer transformer archi- <ref type="figure">Figure 2</ref>: Overview of the proposed framework. Given an input image, we extract an image feature vector using a convolutional neural network (CNN). We perform position encoding by adding a template human mesh to the image feature vector by concatenating the image feature with the 3D coordinates (x i , y i , z i ) of every body joint i, and 3D coordinates (x j , y j , z j ) of every vertex j. Given a set of joint queries and vertex queries, we perform self-attentions through multiple layers of a transformer encoder, and regress the 3D coordinates of body joints and mesh vertices in parallel. We use a progressive dimensionality reduction architecture (right) to gradually reduce the hidden embedding dimensions from layer to layer. Each token in the final layer outputs 3D coordinates of a joint or mesh vertex. Each encoder block has 4 layers and 4 attention heads. H denotes the dimension of an image feature vector.</p><p>tecture with progressive dimensionality reduction to regress the 3D coordinates of the joints and vertices. <ref type="figure">Figure 2</ref> is an overview of our proposed framework. It takes an image of size 224 × 224 as input, and predicts a set of body joints J and mesh vertices V . The proposed framework consists of two modules: Convolutional Neural Network, and Multi-Layer Transformer Encoder. First, we use a CNN to extract an image feature vector from an input image. Next, Multi-Layer Transformer Encoder takes as input the feature vector and outputs the 3D coordinates of the body joint and mesh vertex in parallel. We describe each module in details as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Neural Network</head><p>In the first module of our framework, we employ a Convolutional Neural Network (CNN) for feature extraction. The CNN is pre-trained on ImageNet classification task <ref type="bibr" target="#b44">[45]</ref>. Specifically, we extract a feature vector X from the last hidden layer. The extracted feature vector X is typically of dimension 2048. We input the feature vector X to the transformer for the regression task.</p><p>With this generic design, it allows an end-to-end training for human pose and mesh reconstruction. Moreover, transformer can easily benefit from large-scale pre-trained CNNs, such as HRNets <ref type="bibr" target="#b55">[56]</ref>. In our experiments, we conduct analysis on the input features, and discover that highresolution image features are beneficial for transformer to regress 3D coordinates of body joints and mesh vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Layer Transformer Encoder with Progressive Dimensionality Reduction</head><p>Since we need to output 3D coordinates, we cannot directly apply the existing transformer encoder architecture <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref> because they use a constant dimensionality of the hidden embeddings for all the transformer layers. Inspired by <ref type="bibr" target="#b17">[18]</ref> which performs dimentionality reduction gradually with multiple blocks, we design a new architecture with a progressive dimensionality reduction scheme. As shown in <ref type="figure">Figure 2</ref> right, we use linear projections to reduce the dimensionality of the hidden embedding after each encoder layer. By adding multiple encoder layers, the model is viewed as performing self-attentions and dimensionality reduction in an alternating manner. The final output vectors of our transformer encoder are the 3D coordinates of the joints and mesh vertices.</p><p>As illustrated in <ref type="figure">Figure 2</ref> left, the input to the transformer encoder are the body joint and mesh vertex queries. In the same spirit as positional encoding <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13]</ref>, we use a template human mesh to preserve the positional information of each query in the input sequence. To be specific, we concatenate the image feature vector X ∈ R 2048×1 with the 3D coordinates (x i , y i , z i ) of every body joint i. This forms a set of joint queries</p><formula xml:id="formula_0">Q J = {q J 1 , q J 2 , . . . , q J n }, where q J i ∈ R 2051×1 .</formula><p>Similarly, we conduct the same positional encoding for every mesh vertex j, and form a set of vertex queries</p><formula xml:id="formula_1">Q V = {q V 1 , q V 2 , . . . , q V m }, where q V j ∈ R 2051×1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Masked Vertex Modeling</head><p>Prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">49]</ref> use the Masked Language Modeling (MLM) to learn the linguistic properties of a training corpus. However, MLM aims to recover the inputs, which cannot be directly applied to our regression task.</p><p>To fully activate the bi-directional attentions in our transformer encoder, we design a Masked Vertex Modeling (MVM) for our regression task. We mask some percentages of the input queries at random. Different from recovering the masked inputs like MLM <ref type="bibr" target="#b8">[9]</ref>, we instead ask the transformer to regress all the joints and vertices.</p><p>In order to predict an output corresponding to a missing query, the model will have to resort to other relevant queries. This is in spirit similar to simulating occlusions where partial body parts are invisible. As a result, MVM enforces transformer to regress 3D coordinates by taking other relevant vertices and joints into consideration, without regard to their distances and mesh topology. This facilitates both short-and long-range interactions among joints and vertices for better human body modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>To train the transformer encoder, we apply loss functions on top of the transformer outputs, and minimize the errors between predictions and ground truths. Given a dataset</p><formula xml:id="formula_2">D = {I i ,V i 3D ,J i 3D ,J i 2D } T i=1 ,</formula><p>where T is the total number of training images. I ∈ R w×h×3 denotes an RGB image.V 3D ∈ R M ×3 denotes the ground truth 3D coordinates of the mesh vertices and M is the number of vertices. J 3D ∈ R K×3 denotes the ground truth 3D coordinates of the body joints and K is the number of joints of a person. Similarly,J 2D ∈ R K×2 denotes the ground truth 2D coordinates of the body joints.</p><p>Let V 3D denote the output vertex locations, and J 3D is the output joint locations, we use L 1 loss to minimize the errors between predictions and ground truths:</p><formula xml:id="formula_3">L V = 1 M M i=1 V 3D −V 3D 1 ,<label>(1)</label></formula><formula xml:id="formula_4">L J = 1 K K i=1 J 3D −J 3D 1 .<label>(2)</label></formula><p>It is worth noting that, the 3D joints can also be calculated from the predicted mesh. Following the common practice in literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>, we use a pre-defined regression matrix G ∈ R K×M , and obtain the regressed 3D joints by J reg 3D = GV 3D . Similar to prior works, we use L 1 loss to optimize J reg 3D :</p><formula xml:id="formula_5">L reg J = 1 K K i=1 J reg 3D −J 3D 1 .<label>(3)</label></formula><p>2D re-projection has been commonly used to enhance the image-mesh alignment <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>. Also, it helps visualize the reconstruction in an image. Inspired by the prior works, we project the 3D joints to 2D space using the estimated camera parameters, and minimize the errors between the 2D projections and 2D ground truths:</p><formula xml:id="formula_6">L proj J = 1 K K i=1 J 2D −J 2D 1 ,<label>(4)</label></formula><p>where the camera parameters are learned by using a linear layer on top of the outputs of the transformer encoder.</p><p>To perform large-scale training, it is highly desirable to leverage both 2D and 3D training datasets for better generalization. As explored in literature <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>, we use a mix-training strategy that leverages different training datasets, with or without the paired image-mesh annotations. Our overall objective is written as:</p><formula xml:id="formula_7">L = α × (L V + L J + L reg J ) + β × L proj J ,<label>(5)</label></formula><p>where α and β are binary flags for each training sample, indicating the availability of 3D and 2D ground truths, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>Our method is able to process arbitrary sizes of mesh. However, due to memory constraints of current hardware, our transformer processes a coarse mesh: (1) We use a coarse template mesh (431 vertices) for positional encoding, and transformer outputs a coarse mesh; (2) We use learnable Multi-Layer Perceptrons (MLPs) to upsample the coarse mesh to the original mesh (6890 vertices for SMPL human mesh topology); (3) The transformer and MLPs are trained end-to-end; Please note that the coarse mesh is obtained by sub-sampling twice to 431 vertices with a sampling algorithm <ref type="bibr" target="#b41">[42]</ref>. As discussed in the literature <ref type="bibr" target="#b24">[25]</ref>, the implementation of learning a coarse mesh followed by upsampling is helpful to reduce computation. It also helps avoid redundancy in original mesh (due to spatial locality of vertices), which makes training more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We first show that our method outperforms the previous state-of-the-art human mesh reconstruction methods on Hu-man3.6M and 3DPW datasets. Then, we provide ablation study and insights for the non-local interactions and model design. Finally, we demonstrate the generalizability of our model on hand reconstruction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Following the literature <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32]</ref>, we conduct mix-training using 3D and 2D training data. We describe each dataset below.</p><p>Human3.6M <ref type="bibr" target="#b20">[21]</ref> is a large-scale dataset with 2D and 3D annotations. Each image has a subject performing a different action. Due to the license issue, the groundtruth 3D meshes are not available. Thus, we use the pseudo 3D meshes provided in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> for training. The pseudo labels are created by model fitting with SMPLify-X <ref type="bibr" target="#b37">[38]</ref>. For evaluation, we use the groundtruth 3D pose labels provided in Human3.6M for fair comparison. Following the common setting <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>, we train our models using subjects S1, S5, S6, S7 and S8. We test the models using subjects S9 and S11.</p><p>3DPW <ref type="bibr" target="#b53">[54]</ref> is an outdoor-image dataset with 2D and 3D annotations. The training set consists of 22K images, and the test set has 35K images. Following the previous stateof-the-arts <ref type="bibr" target="#b22">[23]</ref>, we use 3DPW training data when conducting experiments on 3DPW. <ref type="bibr" target="#b25">[26]</ref> is an outdoor-image dataset. Their 3D annotations are created by model fitting. The training set has 7K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UP-3D</head><p>MuCo-3DHP <ref type="bibr" target="#b30">[31]</ref> is a synthesized dataset based on MPI-INF-3DHP dataset <ref type="bibr" target="#b29">[30]</ref>. It composites the training data with a variety of real-world background images. It has 200K training images.</p><p>COCO <ref type="bibr" target="#b26">[27]</ref> is a large-scale dataset with 2D annotations. We also use the pseudo 3D mesh labels provided in <ref type="bibr" target="#b23">[24]</ref>, which are fitted with SMPLify-X <ref type="bibr" target="#b37">[38]</ref>.</p><p>MPII <ref type="bibr" target="#b0">[1]</ref> is an outdoor-image dataset with 2D pose labels. The training set consists of 14K images.</p><p>FreiHAND <ref type="bibr" target="#b60">[61]</ref> is a 3D hand dataset. The training set consists of 130K images, and the test set has 4K images. We demonstrate the generalizability of our model on this dataset. We use the provided set for training, and conduct evaluation on their online server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We report results using three standard metrics as below. The unit for the three metrics is millimetter (mm). MPJPE: Mean-Per-Joint-Position-Error (MPJPE) <ref type="bibr" target="#b20">[21]</ref> is a metric for evaluating human 3D pose <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref>. MPJPE measures the Euclidean distances between the ground truth joints and the predicted joints. PA-MPJPE: PA-MPJPE, or Reconstruction Error <ref type="bibr" target="#b59">[60]</ref>, is another metric for this task. It first performs a 3D alignment using Procrustes analysis (PA) <ref type="bibr" target="#b11">[12]</ref>, and then computes MPJPE. PA-MPJPE is commonly used for evaluating 3D reconstruction <ref type="bibr" target="#b59">[60]</ref> as it measures the errors of the reconstructed structure without regard to the scale and rigid pose (i.e., translations and rotations). MPVE: Mean-Per-Vertex-Error (MPVE) <ref type="bibr" target="#b38">[39]</ref> measures the Euclidean distances between the ground truth vertices and the predicted vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>We compare METRO with the previous state-of-the-art methods on 3DPW and Human3.6M datasets. Following the literature <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>, we conduct mix-training using 3D and 2D training data. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our method outperforms prior works on both datasets.</p><p>First of all, we are interested in how transformer works for in-the-wild reconstruction of 3DPW. As shown in the left three columns of <ref type="table" target="#tab_0">Table 1</ref>, our method outperforms VIBE <ref type="bibr" target="#b22">[23]</ref>, which was the state-of-the-art method on this dataset. It is worth noting that, VIBE is a video-based approach, whereas our method is an image-based approach.</p><p>In addition, we evaluate the performance on the in-door scenario of Human3.6M. We follow the setting in the prior arts <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>, and train our model without using 3DPW data. The results are shown in the right two columns of <ref type="table" target="#tab_0">Table 1</ref>. Our method achieves better reconstruction performance, especially on PA-MPJPE metric.</p><p>The two datasets Human3.6M and 3DPW have different Input R-Wrist R-Elbow L-Knee L-Ankle Head Output <ref type="figure">Figure 3</ref>: Qualitative results of our method. Given an input image (left), METRO takes non-local interactions among joints and vertices into consideration for human mesh reconstruction (right). We visualize the self-attentions between a specified joint and all other vertices, where brighter color indicates stronger attention. We observe that METRO discovers rich, inputdependent interactions among the joints and vertices.</p><p>challenges. The scenes in 3DPW have more severe occlusions. The scenes in Human3.6 are simpler and the challenge is more on how to accurately estimate body shape. The fact that METRO works well on both datasets demonstrates that it is both robust to occlusions and capable of accurate body shape regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effectiveness of Masked Vertex Modeling: Since we design a Masked Vertex Modeling objective for transformer, one interesting question is whether the objective is useful. <ref type="table" target="#tab_2">Table 2</ref> shows the ablation study on Human3.6M. We observe that Masked Vertex Modeling significantly improves the results. Moreover, we study how many percentage of query tokens should be masked. We vary the maximum masking percentage, and <ref type="table" target="#tab_3">Table 3</ref> shows the comparison. As we increase the number of masked queries for training, it improves the performance. However, the impact becomes less prominent if we mask more than 30% of input queries. This is because large numbers of missing queries would make the training more difficult.    <ref type="table">Table 4</ref>: Performance comparison with the state-of-the-art methods, evaluated on FreiHAND online server. METRO outperforms previous state-of-the-art approaches by a large margin. <ref type="figure">Figure 3</ref> shows the visualization of the self-attentions and mesh reconstruction. For each row in <ref type="figure">Figure 3</ref>, we show the input image, and the self-attentions between a specified joint and all the mesh vertices. The brighter color indicates stronger attention. At the first row, the subject is severely occluded and the right body parts are invisible. As we predict the location of right wrist, METRO attends to relevant non-local vertices, especially those on the head and left hand. At the bottom row, the subject is heavily bended. For the head position prediction, METRO attends to the feet and hands (6th column at the bottom row). It makes sense intuitively since the hand and foot positions provide strong cues to the body pose and subsequently the head position. Moreover, we observe the model performs self-attentions in condition to the input image. As shown in the second row of <ref type="figure">Figure 3</ref>, when predicting the location of right wrist, METRO focuses more on the right foot which is different from the attentions in the other three rows.</p><p>We further conduct quantitative analysis on the nonlocal interactions. We randomly sample 5000 images from 3DPW test set, and estimate an overall self-attention map. It is the average attention weight of all attention heads at the last transformer layer. We visualize the interactions among 14 body joints and 431 mesh vertices in <ref type="figure" target="#fig_2">Figure 4</ref>. Each pixel shows the intensity of self-attention, where darker color indicates stronger attention. Note that the first 14 columns are the body joints, and the rest of them represent the mesh vertices. We observe that METRO pays strong attentions to the vertices on the lower arms and the lower legs. This is consistent with the inverse kinematics literature <ref type="bibr" target="#b1">[2]</ref> where the interior joints of a linked figure can be estimated from the position of an end effector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Representations:</head><p>We study the behaviour of our transformer architecture by using different CNN backbones. We use ResNet50 <ref type="bibr" target="#b15">[16]</ref> and HRNet <ref type="bibr" target="#b55">[56]</ref> variations for this experiment. All backbones are pre-trained on the 1000class image classification task of ImageNet <ref type="bibr" target="#b44">[45]</ref>. For each backbone, we extract a global image feature vector X ∈ R 2048×1 , and feed it into the transformer. In <ref type="table">Table 5</ref>, we observe our transformer achieves competitive performance  <ref type="table">Table 5</ref>: Analysis on different backbones, evaluated on Hu-man3.6M. All backbones are pre-trained on ImageNet. We observe that increasing the number of filters in the high resolution feature maps of HRNet is beneficial to mesh regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Wrist Thumb Index Middle Ring Pinky Output <ref type="figure">Figure 5</ref>: Qualitative results of our method on FreiHAND test set. We visualize the self-attentions between a specified joint and all the mesh vertices, where brighter color indicates stronger attention. METRO is a versatile framework that can be easily extended to 3D hand reconstruction.</p><p>when using a ResNet50 backbone. As we increase the channels of the high-resolution feature maps in HRNet, we observe further improvement. Generalization to 3D Hand in-the-wild: METRO is capable of predicting arbitrary joints and vertices, without the dependencies on adjacency matrix and parametric coefficients. Thus, METRO is highly flexible and general for mesh reconstruction of other objects. To demonstrate this capability, we conduct experiment on FreiHAND <ref type="bibr" target="#b60">[61]</ref>. We train our model on FreiHAND from scratch, and evaluate results on FreiHAND online server. <ref type="table">Table 4</ref> shows the comparison with the prior works. METRO outperforms previous state-of-the-art methods by a large margin. Without using any external training data, METRO achieved the first place on FreiHAND leaderboard at the time of paper submission 1 . <ref type="figure">Figure 5</ref> shows our qualitative results with non-local interactions. In the appendix, we provide further analysis on the 3D hand joints, and show that the self-attentions learned in METRO are consistent with inverse kinematics <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a simple yet effective mesh transformer framework to reconstruct human pose and mesh from a single input image. We propose the Masked Vertex Modeling objective to learn non-local interactions among body joints and mesh vertices. Experimental results show that, our method advances the state-of-the-art performance on 3DPW, Human3.6M, and FreiHAND datasets.</p><p>A detailed analysis reveals that the performance improvements are mainly attributed to the input-dependent non-local interactions learned in METRO, which enables predictions based on important joints and vertices, regardless of the mesh topology. We further demonstrate the generalization capability of the proposed approach to 3D hand reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Reference</head><p>We would like to add additional references (HKMR <ref type="bibr" target="#b10">[11]</ref>, Arnab et al. <ref type="bibr" target="#b2">[3]</ref>, Zeng et al. <ref type="bibr" target="#b57">[58]</ref>, Zhang et al. <ref type="bibr" target="#b58">[59]</ref>, DenseRaC <ref type="bibr" target="#b56">[57]</ref>, HoloPose <ref type="bibr" target="#b14">[15]</ref>). Among the new references, HKMR <ref type="bibr" target="#b10">[11]</ref> regresses SMPL parameters by leveraging a pre-specified hierarchical kinematic structure that consists of a root chain and five child chains corresponding to 5 end effectors (head, left/right arms, left/right legs). Holo-Pose <ref type="bibr" target="#b14">[15]</ref> estimates rotation angles of body joints, and uses it as the prior to guide part-based human mesh reconstruction. Zeng et al. <ref type="bibr" target="#b57">[58]</ref> designs the continuous UV map to preserve neighboring relationships of the mesh vertices. Zhang et al. <ref type="bibr" target="#b58">[59]</ref> addresses the occlusion scenario by formulating the task as a UV-map inpainting problem. Since 3DPW is a relatively new benchmark, most literature reported results on Human3.6M, but not 3DPW. We have added their Hu-man3.6M results in <ref type="table" target="#tab_6">Table 6</ref>. As we can see, our method outperforms all of the prior works by a large margin.</p><p>Recently, researchers are exploring the transformer models for other 3D vision topics, such as multi-view human pose estimation <ref type="bibr" target="#b16">[17]</ref> and hand pose estimation based on point could <ref type="bibr" target="#b18">[19]</ref>. We encourage the readers to undertake these studies for further explorations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details and Computation Resource</head><p>We develop METRO using PyTorch and Huggingface transformer library. We conduct training on a machine equipped with 8 NVIDIA V100 GPUs (32GB RAM) and we use batch size 32. Each epoch takes 32 minutes and we train for 200 epochs. Overall, our training takes 5 days. We use the Adam optimizer and a step-wise learning rate decay. We set the initial learning rate as 1 × 10 −4 for both transformer and CNN backbone. The learning rate is decayed by a factor of 10 at the 100th epoch. Our multi-layer transformer encoder is randomly initialized, and the CNN backbone is initialized with ImageNet pre-trained weights. Following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, we apply standard data augmentation during training.</p><p>We evaluate the runtime speed of our model using a machine equipped with a single NVIDIA P100 GPU (16GB RAM). Our runtime speed is about 12 fps using batch size 1. The runtime speed can be accelerated to around 24 fps using batch size 32. <ref type="table" target="#tab_7">Table 7</ref> shows the details of each module in METRO.</p><p>For our masked vertex modeling, following BERT <ref type="bibr" target="#b8">[9]</ref>, we implement it by using a pre-defined special [MASK] token (2051-D floating value vector in our case) to replace the randomly selected input queries.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Progressive Dimensionality Reduction</head><p>Since we gradually reduce the hidden sizes in the transformer architecture, one interesting question is whether such a progressive dimensionality reduction scheme is useful. We have conducted an ablation study on different schemes, and <ref type="table" target="#tab_6">Table 6</ref> shows the comparison. In <ref type="table" target="#tab_6">Table 6</ref>, the row "(H+3)→3" corresponds to a baseline using one linear projection H + 3 to 3. The result is poor. Row "(H+3)→H/2→3" is another baseline which keeps a smaller dimension throughout the network. The result is also bad. Our finding is that large-step (steep) dimension reduction does not work well for 3D mesh regression. Our progressive scheme is inspired by <ref type="bibr" target="#b17">[18]</ref> which performed dimensionality reduction gradually with multiple blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Positional Encoding</head><p>Since our positional encoding is different from the conventional one, one may wonder what if we use sinusoidal functions <ref type="bibr" target="#b52">[53]</ref> but not a template mesh. We have compared our method with the conventional positional encoding which uses sinusoidal functions, and <ref type="table" target="#tab_8">Table 8</ref> shows the results. We see that using sinusoidal functions is slightly worse. This is probably because directly encoding coordinates makes it more efficient to learn 3D coordinate regres-sion. <ref type="figure">Figure 8</ref> shows a qualitative comparison with the previous image-based state-of-the-art methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref> in challenging scenarios. These methods only use a single frame as input. In the first row, the subject is heavily bending. Prior works have difficulty in reconstructing a correct body shape for the subject. In contrast, our method reconstructs a reasonable human mesh with correct pose. In the second row, the subject is occluded by the vehicle. We see that prior works are sensitive to the occlusions, and failed to generate correct human mesh. In contrast, our method performs more robustly in this occlusion scenario. In the bottom row, the subject is sitting on the chair. Our method reconstructed a better human mesh compared to the previous state-of-theart methods. <ref type="figure">Figure 9</ref> shows the qualitative results of our method on 3D hand reconstruction. Without making any modifications to the network architecture, our method works well for hands and is robust to occlusions. It demonstrates our method's advantage that it can be easily extended to other types of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Non-local Interactions of Hand Joints</head><p>We further conduct quantitative analysis on the non-local interactions among hand joints learned by our model. We randomly sample 1000 samples from FreiHAND test set, and estimate an overall self-attention map. <ref type="figure" target="#fig_0">Figure 10</ref> shows the interactions among 21 hand joints. There are 21 rows and 21 columns. Pixel (i, j) represents the amount of attention that hand joint i attends to joint j. A darker color indicates stronger attention. We can see that the wrist joint (column 0) receives strong attentions from all the joints. Intuitively wrist joint acts like a "root" of the hand's kinematics tree. In addition, columns 4, 8, 12, and 16 receive strong attentions from many other joints. These columns correspond to the tips of thumb, index, middle, and ring fingers, respectively. These finger tips are end effectors <ref type="bibr" target="#b1">[2]</ref> and they can be used to estimate the interior joint positions in inverse kinematics. On the other hand, the tip of pinky only receives  attentions from the joints on the ring finger. This is probably because pinky is not as active as the other fingers and its motion is more correlated to the ring finger compared to the other fingers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Test-Time Augmentation for FreiHAND</head><p>We have explored test-time augmentation in our Frei-HAND experiments. We do not use test-time augmentation in Human3.6M and 3DPW experiments. Given a test image, we apply different rotations and scaling to the test image. We then feed these transformed images to our model, and average the results to obtain the final output mesh. In order to compute an average 3D mesh, we perform 3D alignment (i.e., Procrustes analysis <ref type="bibr" target="#b11">[12]</ref>) to normalize the output meshes. In <ref type="table" target="#tab_10">Table 9</ref>, we empirically observed that such an implementation is helpful to improve 0.4 PA-MPVPE on FreiHAND test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Limitations</head><p>As METRO is a data-driven approach, it may not perform well when the testing sample is very different from the training data. We show some example failure cases in <ref type="figure">Figure 7</ref> where the test images are downloaded from the Internet. First, as shown in <ref type="figure">Figure 7</ref>(a), we observed that if the target body shape is very different from the existing training data (i.e., SMPL style data), our method may not faithfully reconstruct the muscles of the subject. Secondly, as shown in <ref type="figure">Figure 7</ref>(b), our model fails to reconstruct a correct mesh due to the fact that there is no glove data in the training set. Finally, the proposed method is a mesh-specific approach. If we were to apply our pre-trained right-hand model to the left-hand images, as can be seen in <ref type="figure">Figure 7</ref>(c), our model will not work well. How to develop a unified model for different 3D objects is an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Example1</head><p>Input Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Example2</head><p>Input Output (c) Example3</p><p>Input Output <ref type="figure">Figure 7</ref>: Failure cases. METRO may not perform well when the testing sample is very different from the training data. <ref type="figure">Figure 9</ref>: Qualitative results of our method on FreiHAND test set. Our method can be easily extended to reconstruct 3D hand mesh. <ref type="figure" target="#fig_0">Figure 10</ref>: Visualization of self-attentions among hand joints. There are 21 rows and 21 columns corresponding to 21 hand joints. Pixel (i, j) represents the amount of attention that joint i attends to joint j. A darker color indicates stronger attention. The definition of the 21 joints is shown in <ref type="figure" target="#fig_0">Figure 11</ref>. <ref type="figure" target="#fig_0">Figure 11</ref>: Definition of the hand joints. The illustration is adapted from <ref type="bibr" target="#b7">[8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>METRO learns non-local interactions among body joints and mesh vertices for human mesh reconstruction. Given an input image in (a), METRO predicts human mesh by taking non-local interactions into consideration. (b) illustrates the attentions between the occluded wrist joint and the mesh vertices where brighter color indicates stronger attention. (c) is the reconstructed mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3DPW</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of self-attentions among body joints and mesh vertices. The x-axis and y-axis correspond to the queries and the predicted outputs, respectively. The first 14 columns from the left correspond to the body joints. The rest of columns correspond to the mesh vertices. Each row shows the attention weight w i,j of the j-th query for the i-th output. Darker color indicates stronger attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Human3.6M Method MPVE ↓ MPJPE ↓ PA-MPJPE ↓ MPJPE ↓ PA-MPJPE ↓ Performance comparison with the state-of-the-art methods on 3DPW and Human3.6M datasets.</figDesc><table><row><cell>HMR [22]</cell><cell>−</cell><cell>−</cell><cell>81.3</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell>GraphCMR [25]</cell><cell>−</cell><cell>−</cell><cell>70.2</cell><cell>−</cell><cell>50.1</cell></row><row><cell>SPIN [24]</cell><cell>116.4</cell><cell>−</cell><cell>59.2</cell><cell>−</cell><cell>41.1</cell></row><row><cell>Pose2Mesh [8]</cell><cell>−</cell><cell>89.2</cell><cell>58.9</cell><cell>64.9</cell><cell>47.0</cell></row><row><cell>I2LMeshNet [32]</cell><cell>−</cell><cell>93.2</cell><cell>57.7</cell><cell>55.7</cell><cell>41.1</cell></row><row><cell>VIBE [23]</cell><cell>99.1</cell><cell>82.0</cell><cell>51.9</cell><cell>65.6</cell><cell>41.4</cell></row><row><cell>METRO (Ours)</cell><cell>88.2</cell><cell>77.1</cell><cell>47.9</cell><cell>54.0</cell><cell>36.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the Masked Vertex Modeling (MVM) objective, evaluated on Human3.6M.</figDesc><table><row><cell cols="2">Max Percentage 0% 10% 20% 30% 40% 50%</cell></row><row><cell>PA-MPJPE</cell><cell>39.1 37.6 37.5 36.7 38.2 37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the Masked Vertex Modeling objective using different percentages of masked queries, evaluated on Human3.6M. The variable n% indicates we mask randomly from 0% to n% of input queries. PA-MPJPE ↓ F@5 mm ↑ F@15 mm ↑</figDesc><table><row><cell>Hasson et al [22]</cell><cell>13.2</cell><cell>−</cell><cell>0.436</cell><cell>0.908</cell></row><row><cell>Boukhayma et al. [25]</cell><cell>13.0</cell><cell>−</cell><cell>0.435</cell><cell>0.898</cell></row><row><cell>FreiHAND [24]</cell><cell>10.7</cell><cell>−</cell><cell>0.529</cell><cell>0.935</cell></row><row><cell>Pose2Mesh [8]</cell><cell>7.8</cell><cell>7.7</cell><cell>0.674</cell><cell>0.969</cell></row><row><cell>I2LMeshNet [32]</cell><cell>7.6</cell><cell>7.4</cell><cell>0.681</cell><cell>0.973</cell></row><row><cell>METRO (Ours)</cell><cell>6.3</cell><cell>6.5</cell><cell>0.731</cell><cell>0.984</cell></row></table><note>Non-local Interactions: To further understand the effect of METRO in learning interactions among joints and mesh vertices, we conduct analysis on the self-attentions in our transformer.Method PA-MPVPE ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of different dimentionality reduction schemes, evaluated on Human3.6M validation set. Please note that all the transformer variants have the same total number of hidden layers (12 layers) for fair comparison. H=2048.</figDesc><table><row><cell></cell><cell cols="2">CNN (HRNet-W64) Transformer</cell></row><row><cell># Parameters</cell><cell>128M</cell><cell>102M</cell></row><row><cell>Inference time</cell><cell>52.05 ms</cell><cell>28.22 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Number of parameters and inference time per image. The runtime speed is estimated by using batch size 1.</figDesc><table><row><cell cols="2">Positional Encoding PA-MPJPE ↓</cell></row><row><cell>Sinusoidal [53]</cell><cell>37.5</cell></row><row><cell>Ours</cell><cell>36.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of different positional encoding schemes, evaluated on Human3.6M validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>MethodPA-MPVPE ↓ PA-MPJPE ↓ F@5 mm ↑ F@15 mm ↑</figDesc><table><row><cell>I2LMeshNet [32]</cell><cell>7.6</cell><cell>7.4</cell><cell>0.681</cell><cell>0.973</cell></row><row><cell>METRO</cell><cell>6.7</cell><cell>6.8</cell><cell>0.717</cell><cell>0.981</cell></row><row><cell>METRO + Test time augmentation</cell><cell>6.3</cell><cell>6.5</cell><cell>0.731</cell><cell>0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effectiveness of test-time augmentation on FreiHAND test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to the official FreiHAND leaderboard in November 2020: https://competitions.codalab.org/competitions/21238</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverse kinematics techniques in computer graphics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<idno>2020. 11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-coded: 3d correspondences by deep deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Handtransformer: Non-autoregressive structured modeling for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep inertial poser learning to reconstruct human pose from sparseinertial measurements in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<idno>185:1-185:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">STAR: A spare trained articulated human body regressor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3d human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d scene reconstruction with multi-layer depth and epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism quarterly</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3d pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Qualitative comparison between our method and other single-frame-based approaches. Our method is more robust to challenging poses and occlusions</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Input GraphCMR [25] I2L-M [32] Ours Figure</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
