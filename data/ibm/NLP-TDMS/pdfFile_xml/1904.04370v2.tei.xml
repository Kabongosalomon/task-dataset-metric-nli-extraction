<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Embeddings with Easy Positive Triplet Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">George Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">George Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">George Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Embeddings with Easy Positive Triplet Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Online Prod-ucts, In-Shop Clothes and Hotels-50K. Code is available at: https://github.com/littleredxh/EasyPositiveHardNegative</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. Substantial work has focused on loss functions and strategies to learn these embeddings by pushing images from the same class as close together in the embedding space as possible. In this paper, we propose an alternative, loosened embedding strategy that requires the embedding function only map each training image to the most similar examples from the same class, an approach we call "Easy Positive" mining. We provide a collection of experiments and visualizations that highlight that this Easy Positive mining leads to embeddings that are more flexible and generalize better to new unseen data. This simple mining strategy yields recall performance that exceeds state of the art approaches (including those with complicated loss functions and ensemble methods) on image retrieval datasets including CUB, Stanford</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. A number of approaches have been proposed for this problem, but many of them learn this embedding by considering triplets of images: an anchor image, a positive image from the same class, and a negative image from a different class. The network is trained to minimize a loss function that penalizes cases where the anchor-positive image distance is not substantially smaller than the anchor-negative image distance.</p><p>When these embedding spaces are learned for the purpose of classification, the evaluation for test data has several steps. First, images from all classes of interest are mapped into the embedding space. When a query arrives, it is mapped into the embedding space. The classification result is the class of whichever image the query was mapped closest to.</p><p>There have been a variety of triplet selection approaches  Our approach (c) considers the most similar positive and most similar negative example. The focus on the most similar positive example aligns with the embedding query criteria, where a query example need not be close to all possible examples from the class, but only the most visually similar example. This is motivated by the images in (d), which are all from the 'cardinal' class in the CUB <ref type="bibr" target="#b21">[22]</ref> dataset, but many of which have significantly different visual appearances.</p><p>proposed to maximize accuracy on this classification from embedding task. The general triplet selection model ( <ref type="figure" target="#fig_1">Figure 1a</ref>) chooses random anchor-positive pairs, while another approach <ref type="bibr" target="#b17">[18]</ref> considers all possible triplets in a batch <ref type="figure" target="#fig_1">(Figure 1b</ref>). These strategies align with the common goal of metric learning, which is to cluster all images from the same class as closely as possible in the embedding space. This does not align with the query criteria on some high intra-class variance datasets such as CUB-200 <ref type="bibr" target="#b21">[22]</ref>, where a query image need not be close to all examples in its class, but rather only close to some example of the class. This suggests that we should optimize the embedding so that images are close to some exemplar of their class, but perhaps not all exemplars. Thus we present the idea of "Easy Positive" triplet mining <ref type="figure" target="#fig_1">(Figure 1c)</ref>, where, for a given anchor image, we find the closest positive example, and optimize to make sure it is closer than negative examples. <ref type="figure" target="#fig_1">Figure 1d</ref> motivates this strategy, showing ten images from the 'cardinal' class in the CUB-200 dataset <ref type="bibr" target="#b21">[22]</ref>. Many of these images are significantly visually dissimilar, most obviously based on the differences in coloring, but also in background and orientation. Our approach allows these visually dissimilar members of the class to form manifold in the embedding instead of forcing the members to project to the same place in the embedding. Our specific contributions are:</p><p>• Introducing the idea of Easy Positive mining for metric learning and visualizations of the representational flexibility this supports; • Experimental comparisons of Easy Positive with other triplet selection or aggregation approaches; and • Results that demonstrate substantial improvement over other triplet based learning methods, and which improve the state of the art for CARS, CUB, SOP, Fashion, and Hotels-50K compared to all known methods including much more complicated approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>There is a large body of work in distance metric learning and its associated loss functions for deep learning, including contrastive loss <ref type="bibr" target="#b4">[5]</ref>, triplet loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18]</ref>, and quadruplet loss <ref type="bibr" target="#b0">[1]</ref>, as well as more complex variants of triplet loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref>, approaches that optimize to project all images from a class to a known location in the embedding space <ref type="bibr" target="#b9">[10]</ref>, and ensemble approaches that combine the output of multiple networks or approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper we specifically focus on triplet loss and the related N-pair loss <ref type="bibr" target="#b16">[17]</ref>, and different approaches for selecting examples within a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Triplet loss</head><p>Triplet loss is trained with triplets of images, (x a ,x p ,x n ), where x a is an anchor image, x p is a positive image of the same class as the anchor, and x n is a negative image of a different class, and the convolutional neural network, f(·), embeds the images on a unit sphere, (f(x a ),f(x p ),f(x n )). The target is to learn an embedding such that the anchor-positive pair are at least closer together than the anchor-negative pair by some margin:</p><formula xml:id="formula_0">d ap −d an &gt; margin, where d ap = f(x a )−f(x p ) 2 .</formula><p>The loss is then defined as:</p><formula xml:id="formula_1">L=max(0,d ap −d an +margin)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">N-pair and NCA Loss</head><p>N-pair loss <ref type="bibr" target="#b16">[17]</ref> adds more negative examples into triplets and turns the triplet into an N-tuplet, (x a ,x p ,x n1 ,...,x ni ). The convolution neural network embeds the images on a unit sphere,</p><formula xml:id="formula_2">(f(x a ), f(x p ), f(x n1 ), .</formula><p>.., f(x ni )). The authors additionally modify the standard triplet loss function from a margin based loss to an NCA loss <ref type="bibr" target="#b3">[4]</ref>, avoiding the selection of the margin hyper-parameter and more efficiently pushing positives and negative to be far apart. The loss is defined as:</p><formula xml:id="formula_3">L=−log exp(f(x a ) f(x p )) exp(f(x a ) f(x p ))+ i exp(f(x a ) f(x ni ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Triplet Mining</head><p>In order to construct a triplet for a particular anchor image x a , we must select a positive image, x p , from the same class and a negative image, x n , from a different class. In a dataset set with N training images, there are O(N 3 ) possible triplets, many of which do not help the training converge (e.g., triplets where d an &gt;&gt; d ap ). It is important for fast convergence of the triplet based networks to then construct only the most useful triplets. This triplet construction can either occur offline, selecting triplets from the entire training dataset after each training snapshot, or online, selecting triplets from within each batch in the training. The details of triplet mining will be discussed in further detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Drawbacks of Existing Approaches</head><p>One of the key problems with many existing metric learning approaches is that, when trained only with class labels for supervision, they learn an embedding that pushes together all examples of a class regardless of their semantic similarity. A natural way to embed those examples is to project them on several clusters or a manifold instead of a point to tolerant the semantic differences within a class, as seen in <ref type="figure" target="#fig_1">Figure 1d</ref>, or mislabeled data.</p><p>The most significant work to address this issue of overclustering in deep metric learning introduced the concept of 'magnet loss' <ref type="bibr" target="#b12">[13]</ref>, where classes are split into clusters using kmeans, and points within a cluster are pushed together, but clusters within a class are allowed to spread out. This approach, however, requires the computation and book-keeping of the clusters, and requires regular pauses during training to offline re-compute the clusters within a class using the current representation.</p><p>In this paper, we show that our simple, online triplet selection approach focusing on only the most similar positive example is tolerant to high intra-class variance, forming manifold embedding, avoiding over-clustering of the embedding space, and generalizes well to unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Strategies for Triplet Selection</head><p>In our training, each batch of N images contains n examples from c classes, randomly selected from the training data. Throughout the paper, we refer to the n examples per class as a group. We review different possible online mining strategies to select the most useful examples within a batch both from the same class and from different classes to an anchor image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hard Negative Mining</head><p>Hard negative examples are the most similar images which have a different label from the anchor image.</p><p>x hn = argmin</p><formula xml:id="formula_4">x:C(x) =C(xa) d(f(x a ),f(x))</formula><p>Many works have discussed the benefit of hard negative mining in constructing triplets that produce useful gradients and therefore help triplet loss networks converge quickly <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, the authors additionally propose the concept of Semi-Hard Negative mining, which chooses a anchor-negative pair that is farther than the anchor-positive pair, but within the margin, and so still contributes a positive loss. They demonstrate that using these Semi-Hard Negatives achieves superior performance to networks trained with random or hard negatives. In this paper, given the feature for an anchor image x a and its positive example x p , we define their Semi-Hard Negative:</p><formula xml:id="formula_5">x shn = argmin x: C(x) =C(xa) d(f(xa),f(x))&gt;d(f(xa),f(xp)) d(f(x a ),f(x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Easy Negative Mining</head><p>Easy negative examples are the least similar images which have the different label from the anchor image.</p><p>x en = argmax</p><formula xml:id="formula_6">x:C(x) =C(xa) d(f(x a ),f(x))</formula><p>This condition is not useful in triplet construction, as it will not produce useful gradients for updating the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hard Positive Mining</head><p>Hard positive examples are the least similar images which have the same label to as anchor image.</p><p>x hp = argmax</p><formula xml:id="formula_7">x:C(x)=C(xa) d(f(x a ),f(x))</formula><p>In <ref type="bibr" target="#b6">[7]</ref>, the authors show that hard positive examples increase clustering within a class. The authors also empirically demonstrate that hard positive mining is not universally suitable for all datasets. In Section 6, we show that the primary problem with hard positive mining is actually related to the number of images per class in the batch (if there is a large number of examples per class in a batch, the likelihood that the hardest anchor-positive pair in a batch are very dissimilar increases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Easy Positive Mining</head><p>Our solution to address the over-clustering of the embedding space and to keep intra-class variance in real data is to compute the loss using the easiest positive pair per class in the batch. The easy positive examples are the most similar images that have the same label as the anchor image:</p><formula xml:id="formula_8">x ep = argmin x:C(x)=C(xa) d(f(x a ),f(x))</formula><p>This selection will more likely to push two close positives together and less likely to push two far away positives together. Therefore, it can maintain the intra-class variance and allow classes to have manifold structure in the embedding space and and help reduce the over-clustering problem when an embedding must map dramatically different images to the same place. <ref type="figure" target="#fig_2">Figure 2</ref> shows that training data is less clustered using the easy-positive mining than when using existing approaches and <ref type="figure" target="#fig_6">Figure 6</ref> shows that training examples are embedded on flexible manifold instead of points.</p><p>Additionally, this approach seems to generalize better to unseen data than the other approaches. <ref type="figure" target="#fig_3">Figure 3</ref> shows where testing data embeds with respect to the closest points in the training data. Existing approaches tend to embed test data from new classes close to where training data was embedded, which Easy Positive approaches spread the new data out more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Easy Positive Triplet Loss</head><p>Given the Easy Positive mining described above, we can derive our loss function when selecting easy positive examples and different strategies for selecting negative examples. In our approach we follow the standard practice of mapping the output of our convolutional neural network onto a unit sphere. Then we can compute the similarity of a feature vector for an anchor image f(x a ) and its closest positive f(x ep ) as the dot product of these vectors:</p><formula xml:id="formula_9">f(x a ) f(x ep )</formula><p>Given an anchor image and its feature vector f(x a ), we find both the easy positive f(x EP ) in the batch and all possible negative examples, and define the Easy Positive (EP) loss as:</p><formula xml:id="formula_10">L EP =−log exp(f(x a ) f(x ep )) exp(f(x a ) f(x ep ))+ i exp(f(x a ) f(x ni ))</formula><p>We can also find the Easy Positive f( </p><formula xml:id="formula_11">L EP HN =−log exp(f(x a ) f(x ep )) exp(f(x a ) f(x ep ))+exp(f(x a ) f(x hn )) L EP SHN =−log exp(f(x a ) f(x ep )) exp(f(x a ) f(x ep ))+exp(f(x a ) f(x shn ))</formula><p>We can additionally compare our Easy Positive triplet losses with the hard positive mining approaches. In these cases, the loss functions, defined as Hard Positive (HP) and Hard Positive Hard Negative (HPHN) are computed as follows:</p><formula xml:id="formula_12">L HP =−log exp(f(x a ) f(x hp )) exp(f(x a ) f(x hp ))+ i exp(f(x a ) f(x ni )) L HP HN =−log exp(f(x a ) f(x hp )) exp(f(x a ) f(x hp ))+exp(f(x a ) f(x hn ))</formula><p>When the group size is 2, Easy Positive and Hard Positive become random Positive, making L EP and L HP equivalent to N-pair loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We calculate Recall@K to measure retrieval quality. To compute Recall@K, we first embed all query set and gallery set images to the unit hyper-sphere and calculate pair-wise cosine similarity between these two sets. For each query image, we retrieve the images with the K highest similarity scores from the gallery set. A recall score is 1 if at least one image of the K retrieved images have the same label as the query image, and 0 if none of the K retrieved images have the same label as the query image. Recall@K is the average of the recall score for all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>All tests are run on the PyTorch platform <ref type="bibr" target="#b11">[12]</ref>, using the GoogleNet <ref type="bibr" target="#b19">[20]</ref> and ResNet18 and ResNet50 <ref type="bibr" target="#b5">[6]</ref> architectures, pre-trained on ILSVRC 2012-CLS data <ref type="bibr" target="#b13">[14]</ref>. Training images are re-sized to 256 by 256 pixels. We adopt a standard data augmentation scheme (random horizontal flip and random crops padded by 10 pixels on each side). For pre-processing, we normalize the images using the channel means and standard deviations. All networks are trained using stochastic gradient descent (SGD) with 40 epoches. We set initial learning rate 0.0005 for CAR, SOP and In-shop cloth dataset and 0.0001 for CUB dataset, and divided by 10 after 20 and 30 epochs. The loss function is based on NCA which has a single parameter, temperature <ref type="bibr" target="#b24">[25]</ref>, and in all cases we set this parameter to 0.1.</p><p>On all datasets we train using a batch size of 128. Batches are constructed with a fixed number n examples per class by adding classes until the batch is full. When a class has fewer than n examples, we use all the examples from the class. If this leads to a case where the last class in the batch does not have space for n images, we just include enough images to fill the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>The CUB200 dataset <ref type="bibr" target="#b21">[22]</ref> contains 200 classes of birds with 11,788 images. We split the first 100 classes for training <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">864</ref> images) and the rest of the classes for testing (5,924 images). In the training set, the maximum, minimum, mean and standard deviation of the number of images in each class is 60, 41, 58.6 and 3.5.</p><p>The CAR196 dataset <ref type="bibr" target="#b8">[9]</ref> contains 196 classes of cars with 16,185 images. We use the standard split with the first 98 classes for training (8,054 images) and the rest of the classes for testing <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">131</ref> images). In the training set, the maximum, minimum, mean and standard deviation of the number of images in each class is 97, 59, 82.2 and 7.2.</p><p>The In-Shop Clothes Retrieval (In-Shop) dataset [26] contains 11,735 classes of clothing items with 54,642 images. Following the settings in <ref type="bibr" target="#b25">[26]</ref>, only 7,982 classes of clothing items with 52,712 images are used for training and testing. 3,997 classes are for training (25,882 images) and 3,985 classes are for testing (28,760 images). The test set are partitioned to query set and gallery set, where query set contains 14,218 images of 3,985 classes and gallery set contains 12,612 images of 3,985 classes. In the training set, the maximum, minimum, mean and standard deviation of the number of images in each class is 162, 1, 6.5 and 6.4.</p><p>The Stanford online products (SOP) dataset [18] contains 22,634 classes with 120,053 product images. We use 11,318 classes for training (59,551 images) and other 11,316 classes are for testing (60,502 images). In the training set, the maximum, minimum, mean and standard deviation of the number of images in each class is 12, 2, 5.3 and 3.0.</p><p>The Hotel-50K (Hotel) training dataset <ref type="bibr" target="#b18">[19]</ref> contains 50,000 hotel classes with 1,027,871 images of hotel rooms within each hotel. In the training set, the maximum, minimum, mean and standard deviation of image size in each class is 266, 2, 20.5, 13.6. The testing dataset consists of 17,954 images from 5,000 hotels represented in the training set.</p><p>In the CUB, CAR and SOP datasets, both the query set and gallery set refer to the testing set. During the query process, the top-K retrieved images exclude the query image itself. In the In-Shop dataset, the query set and gallery set are predefined by the original paper. In the Hotel dataset, the training set is used as the gallery for all query images in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We show three classes of experiments. The first gives an extensive comparison between different mining approaches on the CAR dataset, comparing different parameter settings and showing visualizations that offer intuitions about how different mining strategies affect the embedding. The second set of experiments shows comparisons between different algorithm performance for different network architectures. The third experiment compares our algorithm to a wider variety of algorithms on a larger collection of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparative Study Using the CAR Dataset</head><p>In the first set of experiments we explore different mining strategies using the CAR dataset <ref type="bibr" target="#b8">[9]</ref>, and specific explore the effect of the number of images per class in each batch. The CAR dataset has a large number of examples per class making it possible to test with large numbers of images per class.</p><p>Impact of Group Size. We train an embedding network on the CAR dataset, using a ResNet-18 architecture for 6 different mining strategies. In all cases, we train with a batch size of 128. <ref type="figure" target="#fig_5">Figure 4</ref> shows the impact of the group size, n (the number of examples per class that is included per batch) on the Recall@1 performance.</p><p>When n = 2, Easy Positive (EP), Hard Positive (HP) and Batch All (BA) have only 2 representatives per class, and each of these approaches use all negatives in the batch, so these three approaches are all the same in this case (and are the equivalent to the standard N-pair approach). Similarly, Hard Positive Hard Negative (HPHN) is the same as Easy Positive Hard Negative (EPHN) when there are only two examples per class.</p><p>When n increases, the number of examples per class grows, so the easiest positive image is likely to become more similar and the hardest positive is likely to become less similar. Our results show an increase in performance for all the Easy Positive approaches and a decrease for the hard positive approaches. When n &gt; 16 the performance drops for most methods; we believe this is because for large n there are relatively few different classes per batch, and when all negatives examples are drawn from fewer classes they have less variation.</p><p>The Hard Positive and Batch All approaches explicitly focus on creating constraints to pull together all examples in a group, which has the impact of forcing all the points in the class to be close. In contrast, EP, EPHN and EPSHN only constrain the most similar examples per group, and do not force all elements of the class into a cluster.</p><p>For the remainder of this section we drop results about about Hard Positive Hard Negative and just use the results from the group size of 16. We also include the N-pair approach, which is equivalent to the best version of Batch All and Hard Positive (group size=2).</p><p>Usage of the Embedding Space. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the impact of Easy Positive mining by showing the distribution of similarities between points from the same class. The points from the same class are much more spread out when the embedding is based on the Easy Positive mining, while the other approaches create tight clusters where all points from the same class are very similar.</p><p>For these six algorithms, <ref type="figure">Figure 5</ref> gives another visualization of how the points are distributed, showing for each data point the similarity to the most similar image from the same class and the <ref type="figure">Figure 5</ref>: A visual summary of the embedding structure for Batch All (BA), Hard Positive (HP), N-Pairs, and 3 approaches that focus on Easy Positives. Each figure shows the similarity (in embeddings space) of the closest same class image (on the x-axis) and the closest different class image (on the y-axis). It is especially interesting to note that N-Pairs is very good on training data (similarity to closest same class image is mostly greater than 0.9), but this does not generalize to embedding unseen classes. Embeddings trained with easy-positives are more spread out (the closest positives are not as close), but generalize better to new classes. most similar image from a different class. The structure of this plot is that if a point is below the y =x line, then to most similar image is from the same class, and would be classified correctly if it was a query image, and we have color coded those points blue.</p><p>The left side of this plot includes Batch All, N-pair and Hard Positive mining. These algorithms give tight clusters and this is visible in these plots for training data (the left-most column of the plot), where all points are very similar to images from the same class, or far to the right on the x-axis. In contrast, the Easy Positive approaches (the right-side of the figure) are more spread out.</p><p>On testing data, Batch All, N-pair and Hard Positive mining all have significantly larger distances to the closest example than in the training data (there points are less concentrated on the far right along the x-axis).</p><p>In contrast, while the Easy Positive approaches have lower similarities to the closest training points, the embedding generalizes better for new classes, giving distributions on test data of distances to nearby positive and negative examples that are similar to the training distributions; we believe this is why we get better results.</p><p>Visualization of Embedding Space. Finally, <ref type="figure" target="#fig_6">Figure 6</ref> shows a t-SNE embedding comparing N-pair with Easy Positive Semi-Hard Negative. Embedding the training points with N-pair leads to tight clusters, while the Easy Positive Semi-Hard Negative embeds classes to be more spread out and even sometimes disjoint (addressing the fact that sometimes classes have multiple modes, like the birds in <ref type="figure" target="#fig_1">Figure 1d</ref>). The embedding of test data points is also interesting. On the right side of <ref type="figure" target="#fig_6">Figure 6</ref>, we show the testing and training embedding at the same time; the red points are the embedded training data (that were shown on the left), and the blue points are the embedded testing data. On the EPSHN data the testing points are more spread out, and clusters are rarely mapped on top of the training clusters; while the N-pair training points are less spread out and often mapped exactly on top of training categories. While this does not necessarily impact testing accuracy, it indicates that the embedding has not learned a representation that generalizes to new classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison Across Architectures</head><p>In the second set of experiments, we train embedding networks on the CUB and CAR dataset and compare our Easy Positive Semi-Hard Negative approach with triplet, N-pair and Proxy loss, using an output embedding dimension size of 64. We compare the results of these approaches across the GoogleNet, Resnet18 and Resnet50 network architectures in <ref type="table" target="#tab_0">Table 1</ref>, demonstrating the superiority of our approach to these other comparably simple embedding approaches across all networks for CUB, and for both Resnet architectures for CAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison with State of the Art</head><p>In the third set of experiments, we compare the the best reported results for several more complex state of the art embedding approaches, including more complex triplet loss approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3]</ref> and ensemble based approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>. Our embeddings are trained with ResNet50 and an output embedding size of 512. For CUB and CAR, the optimal group size is 16, while the SOP, In-shop and hotel datasets have fewer examples per class, and therefore perform best with a group size of 4.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, the Easy Positive Semi-Hard Negative approach achieves a new record on the CUB dataset, which contains high intra-class variance as seen in <ref type="figure" target="#fig_1">Figure 1d</ref>, improving over even significantly more complex ensemble methods such as ABE <ref type="bibr">[</ref>   <ref type="table">Table 3</ref>: Retrieval performance on the Hotels-50K dataset <ref type="bibr" target="#b18">[19]</ref>, comparing to the author's original results trained with Resnet-50 and Batch All triplet loss. and DREML <ref type="bibr" target="#b22">[23]</ref>. On the CAR dataset, our result is comparable to the ensemble methods. We additionally achieve state of the art results for In-shop and Hotels-50K, and achieve the best reported Recall@1 for the SOP dataset. In the original Hotels-50K <ref type="bibr" target="#b18">[19]</ref> paper, the authors specifically cite high intra-class variance as a challenge of their dataset. Our EPSHN method doubles the accuracy of their original approach, which uses the Batch All triplet selection strategy and is trained with ResNet50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>The standard definition of distance metric learning is to create a function so that all images from class are mapped to similar locations and images from different classes are judged to be different. This criteria does not align well with data from natural classes; for example, <ref type="figure" target="#fig_1">Figure 1d</ref> shows the Cardinal category in the cub dataset that has at least two semantic clusters (colorful male birds and brown females). Even within those semantic classes, there may be value in explicitly matching to birds on a branch or birds on the ground. We posit that there is value in a distance metric learning approach that matches images to the most semantically similar examples, without needing all images to be similar. At query-time, recall accuracy typically depends only on the label of the most similar image, so a metric learning approach that optimizes for this condition fits better than a metric learning approach that requires all images in a class to be similar.</p><p>In this paper we show that the simple change of concentrating on easy positive examples within a batch improves performance across a wide range of datasets and outperforms all published results on large datasets (Stanford Online Products, In-shop Clothes, and Hotels-50K), including quite recent and interesting ensemble based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Ten images of the 'cardinal' class (id=17) from CUB-200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of different triplet selection strategies for an anchor image (blue circle) in a batch. Generic triplet loss (a) randomly selects a positive example (green circle, solid line) and a negative example (red square, dashed line). Batch All [18] (b) considers all possible positive and negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The distribution of similarities between all pairs from the same class in the training data. Batch All (BA), N-Pair, and Hard Positive (HP) all tightly cluster the training data, while the Easy Positive approaches cluster much less tightly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of similarities between points in the test dataset and their closest point in the training dataset. Batch All (BA), N-Pair, and Hard Positive (HP) all learn an embedding where testing points from new, unseen classes are mapped closely to training data, indicating that they have not learned a representation that differentiates well between training data and unseen testing data. By comparison, all of the Easy Positive approaches map test data much less closely particular training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x ep ), Hard Negative f(x hn ) and Semi-Hard Negative f(x shn ) examples for f(x a ) and define the Easy Positive Hard Negative (EPHN) loss and Easy Positive Semi-Hard Negative (EPSHN) loss as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of Recall@1 for different triplet mining strategies as a function of group-size, n (the number of images from the same class in a batch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>(a) N-pair training embedding (b) N-pair testing + training embedding (c) EPSHN training embedding (d) EPSHN testing + training embedding A t-SNE visualization of N-pair loss and EPSHN, showing the embedding of the training categories to highlight their structure (left) and a joint embedding of the training and the testing classes to highlight where test data is embedded compared to training data (right). The EPSHN approach only requires that nearby points are similar, so the classes are not embedding as tightly as clusters compared to the N-Pair approach. Additionally, with EPSHN, new data is better spread out and mapped less directly on top of a training category, compared the N-pair embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Retrieval Performance on the CUB and CAR dataset. DREML 576 63.9 75.0 83.1 86.0 91.7 95.0</figDesc><table><row><cell>8]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Retrieval Performance on the CUB, CAR, SOP and In-shop datasets comparing to the best reported results for more complex approaches and/or ensembles.</figDesc><table><row><cell></cell><cell>Hotels-50K</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">R@1 R@10 R@100</cell></row><row><cell>BATCH-ALL 256</cell><cell>8.1</cell><cell>17.6</cell><cell>34.8</cell></row><row><cell>EPSHN 256</cell><cell>16.3</cell><cell>30.5</cell><cell>49.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentionbased ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<title level="m">Metric learning with adaptive density discrimination. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hotels-50k: A global hotel recognition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Schölkopf, and J. C. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS- TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Heated-up softmax embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1809.04157</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q X W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
