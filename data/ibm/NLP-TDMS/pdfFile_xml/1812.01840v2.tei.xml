<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A ention Boosted Sequential Inference Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiyan</forename><surname>Jia</surname></persName>
							<email>cyjia@bjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology &amp; Beijing Key</orgName>
								<orgName type="laboratory">Lab of Tra c Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">School of Computer and Information Technology &amp; Beijing Key Lab of Tra c Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">School of Computer and Information Technology &amp; Beijing Key Lab of Tra c Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A ention Boosted Sequential Inference Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>natural language processing</term>
					<term>deep learning</term>
					<term>natural language inference</term>
					<term>Bi-LSTM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A ention mechanism has been proven e ective on natural language processing. is paper proposes an a ention boosted natural language inference model named aESIM by adding word a ention and adaptive direction-oriented a ention mechanisms to the traditional Bi-LSTM layer of natural language inference models, e.g. ESIM. is makes the inference model aESIM has the ability to e ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis. e empirical studies on the SNLI, MultiNLI and ora benchmarks manifest that aESIM is superior to the original ESIM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KEYWORDS</head><p>natural language processing, deep learning, natural language inference, Bi-LSTM</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural language inference (NLI) is an important and signicant task in natural language processing (NLP). It concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 1</ref> shows several samples of natural language inference from SNLI (Stanford Natural Language Inference) corpus <ref type="bibr" target="#b1">[2]</ref>.</p><p>In the literature, the task of NLI is usually viewed as a relation classi cation. It learns the relation between a premise Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. DAPA '19, Melbourne, Australia © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. . <ref type="bibr">DOI:</ref> and a hypothesis in a large training set, then predicts the relation between a new pair of premise and hypothesis. e existing methods of NLI can be roughly partitioned into two categories: feature-based models <ref type="bibr" target="#b1">[2]</ref> and neural network-based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Feature-based models represent a premise and a hypothesis by their unlexicalized and lexicalized features, such as n-gram length and the real-valued feature of length di erence, then train a classi er to perform relation classi cation. Recently, end-to-end neural network-based models have drawn worldwide a ention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation, natural language inference, etc. On the basis of their model structures, we can divide neural network-based models for NLI into two classes <ref type="bibr" target="#b0">[1]</ref>, sentence encoding models and sentence interaction-aggregation models. e architectures of the two types of models are shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Sentence encoding models <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> (their main architecture is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.a) independently encode a pair of sentences, a premise and a hypothesis using pre-trained word embedding vectors, then learn semantic relation between two sentences with a multi-layer perceptron (MLP). In these models, LSTM (Long Short-Term Memory networks) <ref type="bibr" target="#b8">[9]</ref>, its variants GRU (Gated Recurrent Units) <ref type="bibr" target="#b9">[10]</ref> and Bi-LSTM, are usually utilized to encode the sentences since they were capable of learning long-term dependencies inside sentences.  scheme and compared several sentence encoding architectures: LSTM or GRU, Bi-LSTM with mean/max pooling, selfa ention network and hierarchical convolutional networks <ref type="bibr" target="#b4">[5]</ref>. e experimental results demonstrated that the Bi-LSTM with max pooling achieved the best performance. Talman et al. designed a hierarchical Bi-LSTM max pooling (HBMP) model to encode sentences <ref type="bibr" target="#b5">[6]</ref>. is model applied parameters of one Bi-LSTM to initialize the next Bi-LSTM to convey information, which shown be er results than the model with a single Bi-LSTM. Besides LSTM, a ention mechanisms could also be used to boost the e ectiveness of sentence encoding. e model developed by Ghaeini et al. added selfa ention to LSTM model, and achieved be er performance <ref type="bibr" target="#b10">[11]</ref>.</p><p>Sentence interaction-aggregation models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> (their main architecture is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.b) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors, and then the matching results are aggregated into a vector to make the nal decision. Compared with sentence encoding model, sentence interaction-aggregation models aggregate word similarities between a pair of sentences, are capable of capturing the relevant information between two sentences, a premise and a hypothesis. Bahdanau et al. translated and aligned text simultaneously in machine translation task <ref type="bibr" target="#b14">[15]</ref>, innovatively introducing a ention mechanism to natural language process (NLP). He et al. designed a pairwise word interaction model (PWIM) <ref type="bibr" target="#b15">[16]</ref>, which made full use of word-level ne-grained information. Wang et al. put forward a bilateral multi-perspective matching (BiMPM) model <ref type="bibr" target="#b12">[13]</ref>, focusing on various matching strategies that could be seen as di erent types of a ention. e empirical studies of Lan et al. <ref type="bibr" target="#b0">[1]</ref> and Chen et al. <ref type="bibr" target="#b3">[4]</ref> concluded that sentence interation-aggregation models, especially ESIM (Enhanced Sequential Inference Model), a carefully designed sequential inference model based on chain LSTMs, outperformed all previous sentence encoding models.</p><p>Although ESIM has achieved excellent achievements, this model doesn't consider the a ention along the words in a sentence in its Bi-LSTM layer. Word a ention can characterize the di erent contribution of each word. erefore, it will be bene cial to put word a ention into the Bi-LTSM layer. Moreover, the orientation of the words represents the direction of the information ow, either forward or backward, should not be ignored. In traditional Bi-LSTM model, the forward and the backward vectors learnt by Bi-LSTM are simply jointed. It's necessary to consider whether each orientation (forward or backward) has di erent importance on word encoding, thus adaptively joint the two orientation vectors together with di erent weights. erefore, in this study, using ESIM model as the baseline, we add an a ention layer behind each Bi-LSTM layer, then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors. We name this a ention boosted Bi-LSTM as Bi-aLSTM, and denote the modi ed ESIM as aESIM. Experimental results on SNLI, MultiNLI <ref type="bibr" target="#b16">[17]</ref> and ora <ref type="bibr" target="#b12">[13]</ref> benchmarks have demonstrated be er performance of aESIM model than that of the baseline ESIM and the other state-ofthe-art models. We believe that the architecture of Bi-aLSTM has potentially to be used in other NLP tasks such as text classi cation, machine translation and so on.</p><p>is paper is organized as follows. We introduce the general frameworks of ESIM and aESIM in Section 2. We describe the datasets and the experiment se ings, and analyze our experimental results in Section 3. We then draw conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ATTENTION BOOSTED SEQUENTIAL INFERENCE MODEL</head><p>Supposed that we have two sentences p = (p 1 , · · · , p l p ) and q = (q 1 , · · · , q l q ), where p represents premise and q represents hypothesis. e goal is to predict the label meaning for their relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ESIM model</head><p>Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b8">[9]</ref> is composed of four main components: input encoding layer, local inference modeling layer, inference composition layer and classi cation layer.</p><p>In the input encoding layer, ESIM rst uses Bi-LSTM layer to encode input sentence pairs (Equations 1-2), which can be initialized using pre-trained word embeddings (e.g. Glove 840B vectors <ref type="bibr" target="#b17">[18]</ref>), where (p, i) is the word embedding vector of the i-th word in p, (q, i) is that of word in q.</p><formula xml:id="formula_0">p i = Bi-LST M(p, i), ∀i ∈ [1, · · · , l p ]<label>(1)</label></formula><formula xml:id="formula_1">q j = Bi-LST M(q, j), ∀j ∈ [1, · · · , l q ]</formula><p>(2) Secondly, ESIM implements the local inference layer for enhancing the sentence information. First it calculates a similarity matrix M based on p and q.</p><formula xml:id="formula_2">M = p T q<label>(3)</label></formula><p>It then gets the new expression for p and q with the equation below:</p><formula xml:id="formula_3">p i = l q j=1 exp(M i j ) l q k =1 exp(M ik ) q j , ∀i ∈ [1, · · · , l p ]<label>(4)</label></formula><formula xml:id="formula_4">q j = l p i=1 exp(M i j ) l p k =1 exp(M k j ) p i , ∀j ∈ [1, · · · , l q ]<label>(5)</label></formula><p>where p and q represent the weighted summation of p and q. It further enhances the local inference information collected as below. m p = [p; p; p − p; p p]</p><p>m q = [q; q; q − q; q q] (7) A er the enhancement of local inference, another Bi-LSTM layer is used to capture local inference information and their context for inference composition.</p><p>Instead of summation adopted by Parikh et al. <ref type="bibr" target="#b11">[12]</ref>, ESIM proposes to compute both max and average pooling and feeds the concatenate xed length vector to the nal classi er: a fully connected multi-layer perceptron. <ref type="figure" target="#fig_2">Figure 2</ref> shows a high-level view of the ESIM architecture, where the bo om LSTM1 layer of <ref type="figure" target="#fig_2">Figure 2</ref> is the input encoding layer, the middle part with LSTM2 layer is the local inference layer, the upper part is the inference composition layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">aESIM model</head><p>e overall architecture of our newly proposed a ention boosted sequential inference model (named aESIM) based on ESIM is similar to ESIM. In detail, aESIM also consists of four main parts: encoding layer, local inference modeling  <ref type="figure" target="#fig_2">Figure  2</ref>, the layers with red-do ed circles in ESIM will be replaced by the Bi-aLSTM layers shown in the right upper corner of the <ref type="figure" target="#fig_2">Figure 2</ref> and the details of Bi-aLSTM can be found in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Given the word vector x il , l ∈ [1,T ] of the l-th word in sentence i, which can be obtained by pre-trained word embeddings such as Glove 840B vectors <ref type="bibr" target="#b17">[18]</ref> in the rst Bi-aLSTM layer or obtained from the local inference modeling layer in the second Bi-aLSTM layer. We utilize a forward LSTM layer and a backward LSTM layer to collect both direction information</p><formula xml:id="formula_6">− → f and ← − f . − → f il = − −−− → LST M(x il ), l ∈ [0,T ] (8) ← − f il = ← −−− − LST M(x il )</formula><p>, l ∈ [0,T ] (9) As described in introduction section, in the following newly proposed Bi-aLSTM, we add word a ention and additive operation on both orientations of traditional Bi-LSTM layer.</p><p>Word attention layer It's obvious that not all words contribute equally to the representation of a sentence. A ention mechanism, which is introduced in [3], is extremely e ective to extract vital words from the whole sentence, and is particularly bene cial to generate the sentence vector. erefore, we use the following a ention mechanism a er we get </p><formula xml:id="formula_7">− → f and ← − f . Suppose f il ∈ { − → f il , ← − f il }, we then have u il = tanh(W f il + b)<label>(10)</label></formula><formula xml:id="formula_8">α il = exp(u T il u w ) l exp(u T il u w )<label>(11)</label></formula><formula xml:id="formula_9">s il = α il * f il<label>(12)</label></formula><p>where u il is obtained a er one-layer MLP for the input f il , α il is the importance of word l, is calculated by the So Max unit on the context vector u w of the sentence i which is randomly initialized and modi ed during the training, s il is the a ention enhanced vector through multiplying the weight α il and original vector f il , where s il ∈ { − → s il , ← − s il } correspond to the forward vector − → f il and the backward vector ← − f il , respectively.</p><p>Adaptive word direction layer In traditional Bi-LSTM model, the forward and the backward vectors of a word are considered to have equal importance on the word representation. e model simply connects the forward and backward vectors head and tail without weighing their importance. For a word in di erent direction or orientation, the former and the la er words are reversed. us, di erent direction vectors of a word make di erent contribution to the representation, especially the words in a long sentence. erefore, we propose a new adaptive direction layer to learn the contribution of di erent directions for a single word.</p><p>Formally, given two direction word vectors − → s il and ← − s il , the whole word vector can be expressed as:</p><formula xml:id="formula_10">s il = [(W F * − → s il + b F ) (W B * ← − s il + b B )]<label>(13)</label></formula><p>where,W * and b * denote weight matrix and the bias, denotes the nonlinear function, [ ] denotes the concentration. All the parameters can be learned during training. en we can get the whole sentence vector as below:</p><formula xml:id="formula_11">p i = Bi-aLST M(s il ), ∀i ∈ [1, · · · , l p ]<label>(14)</label></formula><formula xml:id="formula_12">q j = Bi-aLST M(s jl ), ∀j ∈ [1, · · · , l q ]<label>(15)</label></formula><p>is word and orientation enhanced Bi-LSTM is called Bi-aLSTM. Its whole architecture is shown in the <ref type="figure" target="#fig_3">Figure 3</ref>, is applied in ESIM model to replace the two Bi-LSTM layers for the task of natural language inference. Besides, this Bi-aLSTM can be used to other natural language processing tasks and our preliminary experiments have demonstrated that Bi-aLSTM is capable of improving the performance of Bi-LSTM models on sentimental classi cation task (for space limitation, this results will not be shown in the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT SETUP 3.1 Datasets</head><p>We evaluated our model on three datasets: the Stanford Natural Language Inference (SNLI) corpus, the Multi-Genre Natural Language Inference (MultiNLI) corpus, and ora duplicate question dataset. We selected these three relatively large corpora out of eight corpora in <ref type="bibr" target="#b0">[1]</ref> since deep learning models usually show be er generalization ability on large training sets and produce more convincing results than on small training sets.</p><p>SNLI e Stanford Natural Language Inference (SNLI) corpus contains 570,152 sentence pairs, including 549K training pairs, 10K validation pairs and 10K testing pairs. Each pair has one of relation classes (entailment, neutral, contradiction and '-'). e '-' class indicates there is no conclusion between the two sentences. Consequently, we remove all pairs with relation '-' during training, validating and testing processes.</p><p>MultiNLI is corpus is a crowd-sourced collection of 433K sentence pairs annotated with textual entailment information. e corpus is modeled on the SNLI corpus, but di ers in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generation evaluation.</p><p>ora e ora dataset contains 400,000 question pairs. e task of this corpus is to judge whether the two sentences means the same a air.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setting</head><p>We use the validation set to select models for testing. e hyper-parameters of aESIM model are listed as follows. We use the Adam method <ref type="bibr" target="#b18">[19]</ref> for optimization. e rst momentum is set to be 0.9 and the second 0.999. e initial learning rate is set to 0.0005, and the batch size is 128. e dimensions of all hidden states of Bi-aLSTM and word embedding are 300. We employ non-linearity function f = selu <ref type="bibr" target="#b19">[20]</ref> replacing recti ed linear unit ReLU on account of its faster convergence rate. Dropout rate is set to 0.2 during training. We use pre-trained 300-D Glove 840B vectors <ref type="bibr" target="#b17">[18]</ref> to initialize word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. All vectors are updated during training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment results</head><p>Except for comparing our method aESIM with ESIM, we listed the experimental results of methods with their references in <ref type="table" target="#tab_1">Table 2</ref> on SNIL. In <ref type="table" target="#tab_1">Table 2</ref>, the method in the rst block is a traditional feature engineering method, those in the second are the sentence vector-based models, those in the third are a ention-based models, and ESIM and our aESIM are shown in the fourth block. Where the results of ESIM and aESIM are implemented by ourselves on Keras, the results of the others are taken from their original publications. We then compare the baseline models, CBOW, Bi-LSTM with ESIM and our aESIM on MultiNLI corpus shown In <ref type="table">Table 3</ref>, where the results of the baselines are taken from <ref type="bibr" target="#b16">[17]</ref>. Finally,we compare several types of CNN and RNN models on roa corpus shown in <ref type="table">Table 4</ref>, the results of theses CNN and RNN models are taken from <ref type="bibr" target="#b12">[13]</ref>. e accuracy (ACC) of each method is measured by the commonly used precision score 1 , and the methods with the best accuracy are marked in bold.</p><p>According to the results in <ref type="table" target="#tab_1">Tables 2-4</ref>, aESIM model achieved 88.1% on SNLI corpus, elevating 0.8 percent higher than ESIM model. It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI. It also achieved 88.01% on ora. erefore, we concluded that aESIM with further word a ention and word orientation operation was superior to ESIM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention visualization</head><p>We selected three types of sentence pairs from a premise and its three hypothesis sentences in the test set of SNLI corpus as shown in <ref type="figure" target="#fig_5">Figure 4</ref>, where the premise sentence is 'A woman with a green headscarf, blue shirt and a very big grin', and three hypothesis sentences are 'the woman has 1 h ps://nlp.stanford.edu/projects/snli/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Acc Unlexicalized + Unigram and bigram features <ref type="bibr" target="#b1">[2]</ref> 78.2 300D LSTM encoders <ref type="bibr" target="#b1">[2]</ref> 80.6 300D NTI-SLSTM-LSTM encoders <ref type="bibr" target="#b20">[21]</ref> 83.4 4096D Bi-LSTM with max-pooling <ref type="bibr" target="#b4">[5]</ref> 84.5 300D Gumbel TreeLSTM encoders <ref type="bibr" target="#b21">[22]</ref> 85.6 512D Dynamic Meta-Embeddings <ref type="bibr" target="#b22">[23]</ref> 86.7 100D DF-LSTM17 <ref type="bibr" target="#b23">[24]</ref> 84.6 300D LSTMN with deep a ention fusion <ref type="bibr" target="#b8">[9]</ref> 85.7 BiMPM <ref type="bibr" target="#b12">[13]</ref> 87.5 ESIM 87.3 aESIM 88.1  <ref type="table">Table 3</ref>: e accuracy (%) of the methods on MultiNLI been shot', 'the woman is very happy' and 'the woman is young' with relation labels 'contradiction', 'entailment', and 'neutral', respectively. Each pair of sentences has their key word pairs: grin-shot, grin-happy and grin-young, which determines whether the premise can entail the hypothesis.   <ref type="table">Table 4</ref>: e accuracy (%) of the methods on ora</p><p>In each <ref type="figure">Figure,</ref> the brighter the color, the higher the weight is. We could conclude that our aESIM model had the higher weight than ESIM model on each key word pair, especially in <ref type="figure" target="#fig_5">Figure 4</ref>.b, where the similarity of 'happy' and 'grin' in aESIM model is much higher than that in ESIM model. erefore, our aESIM model was able to capture the most important word pair in each pair of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this study, we propose an improved version of ESIM named aESIM for NLI. It modi es the Bi-LSTM layer to collect more information. We evaluate our aESIM model on three NLI corpora. Experimental results show that aESIM model achieves be er performance than ESIM model. In the future, we will evaluate how a ention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For example, Conneau et al. proposed a generic NLI training arXiv:1812.01840v2 [cs.CL] 6 Dec 2018 (a) sentence encoding model (b) sentence interaction-aggregation model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Two types of neural network-based models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>ESIM and aESIM model architectures layer, decoding layer and classi cation layer. e only difference between ESIM and aESIM is that we substitute the two Bi-LSTM layers (LSTM1 and LSTM2) in ESIM with two Bi-aLSTM layers in aESIM. erefore, as illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>e structure of Bi-aLSTM including input layer, word attention layer and adaptive word direction layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Attention visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figures 4 .</head><label>4</label><figDesc>a-4.c are the visualization of the a ention layer between sentence pairs a er the Bi-LSTM layer in ESIM model and that a er Bi-aLSTM layer in aESIM model for contrasting ESIM and aESIM. By doing so, we could understand how the models judge the relation between two sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">e accuracy (%) of the methods on SNLI</cell></row><row><cell>Models</cell><cell>Matched</cell><cell>Accuracy (%) Mismatched</cell></row><row><cell>CBOW</cell><cell>64.8</cell><cell>64.5</cell></row><row><cell>Bi-LSTM</cell><cell>66.9</cell><cell>66.9</cell></row><row><cell>ESIM</cell><cell>73.4</cell><cell>73.5</cell></row><row><cell>aESIM</cell><cell>73.9</cell><cell>73.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT is work is supported in part by the National Nature Science Foundation of China (No. 61876016 and No. 61632004), the Fundamental Research Funds for the Central Universities (No. 2018JBZ006).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural network models for paraphrase identication, semantic textual similarity, natural language inference, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2018</title>
		<meeting>COLING 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical a ention networks for document classi cation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural language inference with hierarchical bilstm max pooling architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yli-Jyrä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08762</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distance-based self-a ention network for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02047</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reinforced self-a ention network: a hybrid of hard and so a ention for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05577</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A decomposable a ention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic sentence matching with densely-connected recurrent and co-a entive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfnormalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to compose task-speci c tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Association for the Advancement of Arti cial Intelligence (AAAI). and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<meeting>the 2018 Association for the Advancement of Arti cial Intelligence (AAAI). and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic meta-embeddings for improved sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep fusion lstms for text semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1034" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
