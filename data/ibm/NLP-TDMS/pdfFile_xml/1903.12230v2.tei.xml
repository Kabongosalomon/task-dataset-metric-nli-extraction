<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Transfer Examples for Partial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MOE; BNRist</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
							<email>youkaichao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MOE; BNRist</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MOE; BNRist</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">MOE; BNRist</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution" key="instit1">KLiss</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Big Data</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Transfer Examples for Partial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation is critical for learning in new and unseen environments. With domain adversarial training, deep networks can learn disentangled and transferable features that effectively diminish the dataset shift between the source and target domains for knowledge transfer. In the era of Big Data, large-scale labeled datasets are readily available, stimulating the interest in partial domain adaptation (PDA), which transfers a recognizer from a large labeled domain to a small unlabeled domain. It extends standard domain adaptation to the scenario where target labels are only a subset of source labels. Under the condition that target labels are unknown, the key challenges of PDA are how to transfer relevant examples in the shared classes to promote positive transfer and how to ignore irrelevant ones in the source domain to mitigate negative transfer. In this work, we propose a unified approach to PDA, Example Transfer Network (ETN), which jointly learns domain-invariant representations across domains and a progressive weighting scheme to quantify the transferability of source examples. A thorough evaluation on several benchmark datasets shows that ETN consistently achieves state-of-the-art results for various partial domain adaptation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have significantly advanced the state-of-the-art performance for various machine learning problems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> and applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. A common prerequisite of deep neural networks is the rich labeled data to train a high-capacity model to have sufficient generalization power. Such rich supervision is often prohibitive in real-world applications due to the huge cost of data annotation. Thus, to reduce the labeling cost, there is a strong need to develop versatile algorithms that can leverage rich labeled * Equal contribution, in alphabetic order data from a related source domain. However, this domain adaptation paradigm is hindered by the dataset shift underlying different domains, which forms a major bottleneck to adapting the category models to novel target tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>A major line of the existing domain adaptation methods bridge different domains by learning domain-invariant feature representations in the absence of target labels, i.e., unsupervised domain adaptation. Existing methods assume that the source and target domains share the same set of class labels <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref>, which is crucial for directly applying the source-trained classifier to the target domain. Recent studies in deep learning reveal that deep networks can disentangle explanatory factors of variations behind domains <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, thus learning more transferable features to improve domain adaptation significantly. These deep domain adaptation methods typically embed distribution matching modules, including moment matching <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and adversarial training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref>, into deep architectures for end-to-end learning of transferable representations.</p><p>Although existing methods can reduce the feature-level domain shift, they assume label spaces across domains are identical. In real-world applications, it is often formidable to find a relevant dataset with the label space identical to the target dataset of interest which is often unlabeled. A more practical scenario is Partial Domain Adaptation (PDA) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>, which assumes that the source label space is a superspace of the target label space, relaxing the constraint of identical label spaces. PDA enables knowledge transfer from a big domain of many labels to a small domain of few labels. With the emergence of Big Data, large-scale labeled datasets such as ImageNet-1K <ref type="bibr" target="#b30">[31]</ref> and Google Open Images <ref type="bibr" target="#b18">[19]</ref> are readily accessible to empower data-driven artificial intelligence. These repositories are almost universal to subsume categories of the target domain, making PDA feasible to many applications. PDA can also work in the regime where target data are in limited categories. For example, functions of protein are limited. A large database of known protein structures can be collected, which includes all functions. For a new species, proteins have different structures, but their functions are contained in the database. Predicting protein functions for new species falls into the PDA problem.</p><p>As a generalization to standard domain adaptation, partial domain adaptation is more challenging: the target labels are unknown at training, and there must be many "outlier" source classes that are useless for the target task. This technical challenge is intuitively illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, where the target classes (like purple '•' and orange ' ') will be forcefully aligned to the outlier source classes (like '+') by existing domain adaptation methods. As a result, negative transfer will happen because the learner migrates harmful knowledge from the source domain to the target domain. Negative transfer is the principal obstacle to the application of domain adaptation techniques <ref type="bibr" target="#b28">[29]</ref>.</p><p>Thus, matching the whole source and target domains as previous methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref> is not a safe solution to the PDA problem. We need to develop algorithms versatile enough to transfer useful examples from the many-class dataset (source domain) to the few-class dataset (target domain) while robust enough to irrelevant or outlier examples. Three approaches to partial domain adaptation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref> address the PDA by weighing each data point in the domain-adversarial networks, where a domain discriminator is learned to distinguish the source and target. While decreasing the impact of irrelevant examples on domain alignment, they do not undo the negative effect of the outlier classes on the source classifier. Moreover, they evaluate the transferability of source samples without considering the underlying discriminative and multimodal structures. As a result, it is still vulnerable that they may align the features of outlier source classes and target classes, giving way to negative transfer.</p><p>Towards a safe approach to partial domain adaptation, we present the Example Transfer Network (ETN), which improves the previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref> by learning to transfer useful examples. ETN automatically evaluates the transferability of source examples with a transferability quantifier based on their similarities to the target domain, which is used to weigh their contributions to both the source classifier and the domain discriminator. In particular, ETN improves the weight quality over previous work <ref type="bibr" target="#b42">[43]</ref> by further revealing the discriminative structure to the transferability quantifier. By this means, irrelevant source examples can be better detected and filtered out. Another key improvement of ETN over the previous methods is the capability to simultaneously confine the source classifier and the domain-adversarial network within the auto-discovered shared label space, thus promoting the positive transfer of relevant examples and mitigating negative transfer of irrelevant examples. Comprehensive experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets, including Office-31, Office-Home, ImageNet-1K, and Caltech-256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Domain Adaptation Domain adaptation, a special scenario of transfer learning <ref type="bibr" target="#b28">[29]</ref>, bridges domains of different distributions to mitigate the burden of annotating target data for machine learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref>, computer vision <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> and natural language processing <ref type="bibr" target="#b6">[7]</ref>. The main technical difficulty of domain adaptation is to formally reduce the distribution discrepancy across different domains. Deep networks can learn representations that suppress explanatory factors of variations behind data <ref type="bibr" target="#b2">[3]</ref> and manifest invariant factors across different populations. These invariant factors enable knowledge transfer across relevant domains <ref type="bibr" target="#b41">[42]</ref>. Deep networks have been extensively explored for domain adaptation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16]</ref>, yielding significant performance gains against shallow domain adaptation methods.</p><p>While deep representations can disentangle complex data distributions, recent advances show that they can only reduce, but not remove, the cross-domain discrepancy <ref type="bibr" target="#b37">[38]</ref>. Thus deep learning alone cannot bound the generalization risk for the target task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1]</ref>. Recent works bridge deep learning and domain adaptation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>. They extend deep networks to domain adaptation by adding adaptation layers through which high-order statistics of distributions are explicitly matched <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, or by adding a domain discriminator to distinguish features of the source and target domains, while the features are learned adversarially to deceive the discriminator in a minimax game <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>. Partial Domain Adaptation While the standard domain adaptation advances rapidly, it still needs the vanilla assumption that the source and target domains share the same label space. This assumption does not hold in partial domain adaptation (PDA), which transfers models from many-class domains to few-class domains. There are three valuable efforts towards the PDA problem. Selective Adversarial Network (SAN) <ref type="bibr" target="#b4">[5]</ref> adopts multiple adversarial networks with a weighting mechanism to select out source examples in the outlier classes. Partial Adversarial Domain Adaptation <ref type="bibr" target="#b5">[6]</ref> improves SAN by employing only one adversarial network and further adding the class-level weight to the source classifier. Importance Weighted Adversarial Nets (IWAN) <ref type="bibr" target="#b42">[43]</ref> uses the Sigmoid output of an auxiliary domain classifier (not involved in domain-adversarial training) to derive the probability of a source example belonging to the target domain, which is used to weigh source examples in the domain-adversarial network. These pioneering approaches achieve dramatical performance gains over standard methods in partial domain adaptation tasks.</p><p>These efforts mitigate negative transfer caused by outlier source classes and promote positive transfer among shared classes. However, as outlier classes are only selected out for the domain discriminators, the source classifier is still trained with all classes <ref type="bibr" target="#b4">[5]</ref>, whose performance for shared classes may be distracted by outlier classes. Further, the domain discriminator of IWAN <ref type="bibr" target="#b42">[43]</ref> for obtaining the importance weights distinguishes the source and target domains only based on the feature representations, without exploiting the discriminative information in the source domain. This will result in non-discriminative importance weights to distinguish shared classes from outlier classes. This paper proposes an Example Transfer Network (ETN) that down-weights the irrelevant examples of outlier classes further on the source classifier and adopts a discriminative domain discriminator to quantify the example transferability. Open-Set Domain Adaptation On par with domain adaptation, research has been dedicated to open set recognition, with the goal to reject outliers while correctly recognizing inliers during testing. Open Set SVM <ref type="bibr" target="#b17">[18]</ref> trains a probabilistic SVM and rejects unknown samples by a threshold. Open Set Neural Network <ref type="bibr" target="#b1">[2]</ref> generalizes deep neural networks to open set recognition by introducing an OpenMax layer, which estimates the probability of an input from an unknown class and rejects the unknown point by a threshold.</p><p>Open Set Domain Adaptation (OSDA) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> tackles the setting when the training and testing data are from different distributions and label spaces. OSDA methods often assume which classes are shared by the source and target domains are known at training. Unlike OSDA, in our scenario, target classes are entirely unknown at training. It is interesting to extend our work to the open set scenario under the generic assumption that all target classes are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Example Transfer Network</head><p>The scenario of partial domain adaptation (PDA) <ref type="bibr" target="#b4">[5]</ref> constitutes a source domain</p><formula xml:id="formula_0">D s = {(x s i , y s i )} ns i=1 of n s labeled examples associated with |C s | classes and a target domain D t = {x t j } nt j=1 of n t unlabeled examples drawn from |C t | classes. Note that in PDA the source domain label space C s is a superspace of the target domain label space C t i.e. C s ⊃ C t .</formula><p>The source and target domains are drawn from different probability distributions p and q respectively. Besides p = q as in standard domain adaptation, we further have p Ct = q in partial domain adaptation, where p Ct denotes the distribution of the source domain data in label space C t . The goal of PDA is to learn a deep network that enables end-to-end training of a transferable feature extractor G f and an adaptive classifier G y to sufficiently close the distribution discrepancy across domains and bound the target risk</p><formula xml:id="formula_1">Pr (x,y)∼q [G y (G f (x)) = y].</formula><p>We incur deteriorated performance when directly applying the source classifier G y trained with standard domain adaptation methods to the target domain. In partial domain adaptation, it is difficult to identify which part of the source label space C s is shared with the target label space C t because the target domain is fully unlabeled and C t is unknown at the training stage. Under this condition, most of existing deep domain adaptation methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref> are prone to negative transfer, a degenerated case where the classifier with adaptation performs even worse than the classifier without adaptation. The negative transfer happens since they assume that the source and target domains have identical label space and match whole distributions p and q even though p Cs\Ct and q are non-overlapping and cannot be matched in principle. Thus, decreasing the negative effect of the source examples in outlier label space C s \C t is the key to mitigating negative transfer in partial domain adaptation. Besides, we also need to reduce the distribution shift across p Ct and q to enhance positive transfer in the shared label space C t as before. Note that the irrelevant source examples may come from both outlier classes and shared classes, thus requiring a versatile algorithm to identify them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transferability Weighting Framework</head><p>The key technical problem of domain adaptation is to reduce the distribution shift between the source and target domains. Domain adversarial networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>  <ref type="figure">Figure 2</ref>. Example Transfer Network (ETN) for partial domain adaptation, where G f is the feature extractor, Gy is the source classifier, G d is domain discriminator (involved in adversarial training) for domain alignment;G d is the auxiliary domain discriminator (uninvolved in adversarial training) that quantifies the transferability w of each source example, andGy is the auxiliary label predictor encoding the discriminative information to the auxiliary domain discriminatorG d . Modules in blue are newly designed in this paper. Best viewed in color.</p><formula xml:id="formula_2">tackle this G ỹ G d G y G f G d w</formula><p>problem by learning transferable features in a two-player minimax game: the first player is a domain discriminator G d trained to distinguish the feature representations of the source domain from the target domain, and the second player is a feature extractor G f trained simultaneously to deceive the domain discriminator.</p><p>Specifically, the domain-invariant features f are learned in a minimax optimization procedure: the parameters θ f of the feature extractor G f are trained by maximizing the loss of domain discriminator G d , while the parameters θ d of the domain discriminator G d are trained by minimizing the loss of the domain discriminator G d . Note that our goal is to learn a source classifier that transfers to the target, hence the loss of the source classifier G y is also minimized. This leads to the optimization problem proposed in <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_3">E (θ f , θy, θ d ) =</formula><p>1 ns</p><formula xml:id="formula_4">x i ∈Ds Ly (Gy (G f (xi)) , yi) − 1 na x i ∈Da L d (G d (G f (xi)) , di),<label>(1)</label></formula><p>where D a = D s ∪ D t is the union of the source and target domains and n a = |D a |, d i is the domain label, L y and L d are the cross-entropy loss functions. While domain adversarial networks yield reliable results for standard domain adaptation, they will incur performance degeneration on the partial domain adaptation where C s ⊃ C t . This degeneration is caused by the outlier classes C s \C t in the source domain, which are undesirably matched to the target classes C t . Due to the domain gap, even the source examples in the shared label space D t may not transfer well to the target domain. As a consequence, we need to design a new framework for partial domain adaptation.</p><p>This paper presents a new transferability weighting framework to address the technical difficulties of partial domain adaptation. Denote by w(x s i ) the weight of each source example x s i , which quantifies the example's transferability. Then for a source example with a larger weight, we should increase its contribution to the final model to enhance positive transfer; otherwise, we should decrease its contribution to mitigating negative transfer. IWAN <ref type="bibr" target="#b42">[43]</ref>, a previous work for partial domain adaptation, reweighs the source examples in the loss of the domain discriminator G d . We further put the weights in the loss of the source classifier G y . This significantly enhances our ability to diminish the irrelevant source examples that deteriorate our final model. Furthermore, the unknownness of target labels can make the identification of shared classes difficult, making partial domain adaptation more difficult. We thus believe that the exploitation of unlabeled target examples by semi-supervised learning is also indispensable. We make use of the entropy minimization principle <ref type="bibr" target="#b13">[14]</ref>. Letŷ t j = G y (G f (x t j )) ∈ R |Cs| , the entropy loss to quantify the uncertainty of a target example's predicted label is H G y G f x t j = − |Cs| c=1ŷ t j,c logŷ t j,c . The transferability weighting framework is shown in <ref type="figure">Figure 2</ref>. By weighting the losses of the source classifier G y and the domain discriminator G d using the transferability w(x s i ) of each source example, and combining the entropy minimization criterion, we achieve the following objective:</p><formula xml:id="formula_5">EG y = 1 ns ns i=1 w (x s i ) L (Gy (G f (x s i ), y s i )) + γ nt n t j=1 H Gy G f x t j ,<label>(2)</label></formula><formula xml:id="formula_6">EG d = − 1 ns ns i=1 w (x s i ) log (G d (G f (x s i ))) − 1 nt n t j=1 log 1 − G d G f x t j ,<label>(3)</label></formula><p>where γ is a hyper-parameter to trade-off the labeled source examples and unlabeled target examples. The transferability weighting framework can be trained end-to-end by a minimax optimization procedure as follows, yielding a saddle point solution (θ f ,θ y ,θ d ):</p><formula xml:id="formula_7">(θ f ,θy) = arg min θ f ,θy EG y − EG d , (θ d ) = arg min θ d EG d .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Example Transferability Quantification</head><p>With the proposed transferability weighting framework in Equations <ref type="formula" target="#formula_5">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref>, the key technical problem is how to quantify the transferability of each source example w(x s i ). We introduce an auxiliary domain discriminatorG d , which is also trained to distinguish the representations of the source domain from the target domain, using the similar loss as Equation <ref type="formula" target="#formula_6">(3)</ref>  In partial domain adaptation, the source examples in C t differentiate from those in C s \C t mainly in that C t is shared with the target domain while C s \C t has no overlap with the target domain. Thus, it is natural to integrate discriminative information into our weight design to resolve the ambiguity between shared and outlier classes.</p><p>Inspired by AC-GANs <ref type="bibr" target="#b25">[26]</ref> that integrate the labeled information into the discriminator, we aim to integrate the label information into the auxiliary domain discriminator G d . However, we hope to develop a transferability measure w(x s i ) with both the discriminative information and domain information to generate clearly separable weights for source data in C t and C s \C t respectively. Thus, we add an auxiliary label predictorG y with leaky-softmax activation. WithinG y , the feature from feature extractor G f are transformed to |C s |dimension z. Then z will be passed through a leaky-softmax activation as follows,</p><formula xml:id="formula_8">σ (z) = exp (z) |C s | + |Cs| c=1 exp (z c ) ,<label>(5)</label></formula><p>where z c is the c-th dimension of z. The leaky-softmax has the property that the element-sum of its outputs is smaller than 1; when the logit z c of class c is very large, the probability to classify an example as class c is high. As the auxiliary label predictorG y is trained on source examples and labels, the source examples will have higher probability to be classified as a specific source class c, while the target examples will have smaller logits and uncertain predictions. Therefore, the element-sum of the leaky-softmax outputs are closer to 1 for source examples and closer to 0 for target examples. If we defineG d as</p><formula xml:id="formula_9">G d (G f (x i )) = |Cs| c=1G c y (G f (x i )),<label>(6)</label></formula><p>whereG c y (G f (x i )) is the probability of each example x i belonging to class c, thenG d (G f (x i )) can be seen as computing the probability of each example belonging to the source domain. For a source example, the smaller the value ofG d (G f (x i )) is, the more probable that it comes from the target domain, meaning that it is closer to the target domain and more likely to be in the shared label space C t . Thus, the output ofG d is suitable for transferability quantification.</p><p>We train the auxiliary label predictorG y with the leakysoftmax by a multitask loss over |C s | one-vs-rest binary classification tasks for the |C s |-class classification problem:</p><formula xml:id="formula_10">EG y = − λ n s ns i=1 |Cs| c=1 y s i,c logG c y (G f (x s i )) + 1 − y s i,c log 1 −G c y (G f (x s i )) ,<label>(7)</label></formula><p>where y s i,c denotes whether class c is the ground-truth label for source example x s i , and λ is a hyper-parameter. We also train the auxiliary domain discriminatorG d to distinguish the features of the source domain and the target domain as</p><formula xml:id="formula_11">EG d = − 1 n s ns i=1 log G d (G f (x s i )) − 1 n t nt j=1 log 1 −G d G f x t j .<label>(8)</label></formula><p>From Equations (6) to <ref type="bibr" target="#b7">(8)</ref>, we observe that the outputs of the auxiliary domain discriminatorG d depend on the outputs of the auxiliary label predictorG y . This guarantees thatG d is trained with both label and domain information, resolving the ambiguity between shared and outlier classes to better quantify the example transferability.</p><p>Finally, with the help of the auxiliary label predictorG y and the auxiliary domain discriminatorG d , we can derive more accurate and discriminative weights to quantify the transferability of each source example as</p><formula xml:id="formula_12">w (x s i ) = 1 −G d (G f (x s i )) .<label>(9)</label></formula><p>Since the outputs ofG d for source examples are closer to 1, implying very small weights, we normalize the weights in each mini-batch of batch size B as w (x) ←</p><formula xml:id="formula_13">w(x) 1 B B i=1 w(xi) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Minimax Optimization Problem</head><p>With the aforementioned derivation, we now formulate our final model, Example Transfer Network (ETN). We unify the transferability weighting framework in Equations (2)-(3) and the example transferability quantification in Equations (6)- <ref type="bibr" target="#b8">(9)</ref>. Denoting by θỹ the parameters of the auxiliary label predictorG y , the proposed ETN model can be solved by a minimax optimization problem that finds saddle-point solutionsθ f ,θ y ,θ d andθỹ to model parameters as follows, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments to evaluate our approach with state-of-the-art (partial) domain adaptation methods. Codes and datasets will be available at github.com/thuml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Office-31 <ref type="bibr" target="#b31">[32]</ref> is de facto for domain adaptation. It is relatively small with 4,652 images in 31 classes. Three domains, namely A, D, W, are collected by downloading from amazon.com (A), taking from DSLR (D) and from web camera (W). Following the protocol in <ref type="bibr" target="#b4">[5]</ref>, we select images from the 10 categories shared by Office-31 and Caltech-256 to build new target domain, creating six partial domain adaptation tasks: A→W, D→W, W→D, A→D, D→A and W→A. Note that there are 31 categories in the source domain and 10 categories in the target domain. Office-Home <ref type="bibr" target="#b39">[40]</ref> is a larger dataset, with 4 domains of distinct styles: Artistic, Clip Art, Product and Real-World. Each domain contains images of 65 object categories. Denoting them as Ar, Cl, Pr, Rw, we obtain twelve partial domain adaptation tasks: Ar→Cl, Ar→Pr, Ar→Rw, Cl→Ar, Cl→Pr, Cl→Rw, Pr→Ar, Pr→Cl, Pr→Rw, Rw→Ar, Rw→Cl, and Rw→Pr. For PDA, we use images from the first 25 classes in alphabetical order as the target domain and images from all 65 classes as the source domain. ImageNet-Caltech is a large dataset built with ImageNet-1K <ref type="bibr" target="#b30">[31]</ref> and Caltech-256. They share 84 classes, and thus we form two partial domain adaptation tasks: ImageNet (1000)→Caltech (84) and Caltech (256)→ImageNet (84).</p><p>As most networks are trained on the training set of ImageNet, we use images from ImageNet validation set as target domain for Caltech (256)→ImageNet (84) task.</p><p>We compare the proposed ETN model with state-of-theart deep learning and (partial) domain adaptation methods: ResNet-50 <ref type="bibr" target="#b14">[15]</ref>, Deep Adaptation Network (DAN) <ref type="bibr" target="#b20">[21]</ref>, Domain-Adversarial Neural Networks (DANN) <ref type="bibr" target="#b9">[10]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b36">[37]</ref>, Residual Transfer Networks (RTN) <ref type="bibr" target="#b21">[22]</ref>, Selective Adversarial Network (SAN) <ref type="bibr" target="#b4">[5]</ref>, Importance Weighted Adversarial Network (IWAN) <ref type="bibr" target="#b42">[43]</ref> and Partial Adversarial Domain Adaptation (PADA) <ref type="bibr" target="#b5">[6]</ref>.</p><p>Besides ResNet-50 <ref type="bibr" target="#b14">[15]</ref>, we also evaluate ETN and some methods based on VGG [34] on the Office-31 dataset. We perform ablation study to justify the example transfer mechanism, by evaluating two ETN variants: 1) ETN w/o classifier is the variant without weights on the source classifier; 2) ETN w/o auxiliary is the variant without the auxiliary label predictor on the auxiliary domain discriminator.</p><p>We implement all methods based on PyTorch, and finetune ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and VGG <ref type="bibr" target="#b33">[34]</ref> pre-trained on ImageNet. New layers are trained from scratch, and their learning rates are 10 times that of the fine-tuned layers. We use minibatch SGD with momentum of 0.9 and the learning rate decay strategy implemented in DANN <ref type="bibr" target="#b9">[10]</ref>: the learning rate is adjusted during SGD using η p = η0 (1+αp) β , where p is the training progress linearly changing from 0 to 1. The flip-coefficient of the gradient reversal layer is increased gradually from 0 to 1 as DANN <ref type="bibr" target="#b9">[10]</ref>. Hyper-parameters are optimized with importance weighted cross-validation <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The classification results based on ResNet-50 on the the twelve tasks of Office-Home, six tasks of Office-31 and the two large-scale tasks of ImageNet-Caltech are shown in <ref type="table" target="#tab_0">Tables 1 and 2</ref>. We also compare all methods on Office-31 with VGG backbone in <ref type="table" target="#tab_1">Table 3</ref>. ETN outperforms all other methods w.r.t average accuracy, showing that ETN performs well with different base networks on different datasets.</p><p>Specifically, we have several observations. 1) ADDA, DANN, and DAN outperform ResNet only on some tasks, implying that they suffer from the negative transfer issue. 2) RTN exploits the entropy minimization criterion to amend itself with semi-supervised learning. Thus, it has some improvement over ResNet but still suffers from negative transfer for some tasks. 3) Partial domain adaptation methods (SAN <ref type="bibr" target="#b4">[5]</ref> and IWAN <ref type="bibr" target="#b42">[43]</ref>) perform better than ResNet and other domain adaptation methods on most tasks, due to their weighting mechanism to mitigate negative transfer caused by outlier classes and promote positive transfer among shared classes. 4) ETN outperforms SAN and IWAN on most tasks, showing its power to discriminate the outlier classes from the shared classes accurately and to transfer relevant examples.   In particular, ETN outperforms SAN and IWAN by much larger margin on the large-scale ImageNet-Caltech dataset, indicating that ETN is robuster to outlier classes and performs better even on dataset with large number of outlier classes (916 in ImageNet→Caltech) relative to the shared classes (84 in ImageNet→Caltech). ETN has two advantages: learning discriminative weights and filtering outlier classes out from both source classifier and domain discriminator, which boost partial domain adaptation performance.</p><p>We inspect the efficacy of different modules by comparing in Tables 4 the results of ETN variants. 1) ETN outperforms ETN w/o classifier, proving that the weighting mechanism on the source classifier can reduce the negative influence of outlier-classes examples and focus the source classifier on the examples belonging to the target label space. 2) ETN also outperforms ETN w/o auxiliary by a larger margin, proving that the auxiliary classifier can inject label information into the domain discriminator to yield discriminative weights, which in turn enables ETN to filter out irrelevant examples.  formation in the target domain. We observe that features learned by DANN, IWAN, and SAN are not clustered as clearly as ETN, indicating that ETN can better discriminate target examples than the compared methods. Class Overlap: We conduct a wide range of partial domain adaptation with different numbers of target classes. <ref type="figure">Figure 4</ref> shows that when the number of target classes decreases fewer than 23, the performance of DANN degrades quickly, implying that negative transfer becomes severer when the label space overlap becomes smaller. The performance of SAN decreases slowly and stably, indicating that SAN potentially eliminates the influence of outlier classes. IWAN only performs better than DANN when the label space non-overlap is very large and negative transfer is very severe. ETN performs stably and consistently better than all compared methods, showing the advantage of ETN to partial domain adaptation. ETN also performs better than DANN in standard domain adaptation when the label spaces totally overlap, implying that the weighting mechanism will not degrade performance when there are no outlier classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>Convergence Performance: As shown in <ref type="figure">Figure 5</ref>, the test errors of all methods converge fast but baselines to high error rates. Only ETN converges to the lowest test error. Such phenomenon implies that ETN can be trained more efficiently and stably than previous domain adaptation methods.</p><p>Weight Visualization: We plot the approximate density function of the weights in Equation (9) generated by IWAN <ref type="bibr" target="#b42">[43]</ref> and ETN for all source examples in <ref type="figure" target="#fig_5">Figure 6</ref> on task Cl→Pr. The orange curve shows examples in shared classes C t and the blue curve shows outlier classes C s \C t . Compared to IWAN, our ETN approach assigns much larger weights to shared classes and much smaller weights to outlier classes. Most examples of outlier classes have nearly zero weights, explaining the strong performance of ETN on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented Example Transfer Network (ETN), a discriminative and robust approach to partial domain adaptation. It quantifies the transferability of source examples by integrating the discriminative information into the transferability quantifier and down-weights the negative influence of the outlier source examples upon both the source classifier and the domain discriminator. Based on the evaluation, our model performs strongly for partial domain adaptation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Partial domain adaptation (PDA) is a generalized setting of domain adaptation where the source label space subsumes the target label space. The technical challenge of PDA lies in an intrinsic negative transfer caused by the outlier source classes ('+' in this case), which cannot be forcefully transferred to the target domain. The proposed Example Transfer Network (ETN) designs a weighting scheme to quantify the transferability of source examples and only transfer source examples relevant to the target domain (purple circle and orange triangle), eliminating outlier source examples (in green shadow). Source and target domains are denoted by red and blue circles respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>but dropping w(x s i ). It is not involved in the adversarial training procedure, i.e., the features G f are not learned to confuseG d . Such an auxiliary domain discriminator can roughly quantify the transferability of the source examples, through the Sigmoid probability of classifying each source example x s i to the target domain. Such an auxiliary domain discriminatorG d discriminates source and target domains based on the assumption that source examples of shared classes C t are closer to the target domain than to those source examples in the outlier classes C s \C t , thus having higher probability to be predicted as from the target domain. However, the auxiliary domain discriminator only distinguishes the source and target examples based on domain information. There is potential small gap betweeñG d 's outputs for transferable and irrelevant source examples especially whenG d is trained well. So the model is still exposed to the risk of mixing up the transferable and irrelevant source examples, yielding unsatisfactory transferability measures w(x s i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>θ f ,θy) = arg min θ f ,θy EG y − EG d , (θ d ) = arg min θ d EG d , (θỹ) = arg min θỹ EG y + EG d . (10) ETN enhances partial domain adaptation by learning to transfer relevant examples and diminish outlier examples for both source classifier G y and domain discriminator G d . It exploits progressive weighting schemes w(x s i ) from the auxiliary domain discriminatorG d and auxiliary label predictor G y , well quantifying the transferability of source examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FeatureFigure 3 .</head><label>3</label><figDesc>Visualization: We plot in Figures 3 the t-SNE embeddings [8] of the features learned by DANN, SAN, IWAN and ETN on A (31 classes)→W (10 classes) with class in-Visualization of features learned by DANN, SAN, IWAN, and ETN (class information is denoted by different colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Accuracy by varying #target classes. Target test error w.r.t. to #iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Density function of the importance weights of source examples in the shared label space Ct and outlier label space Cs\Ct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification Accuracy (%) for Partial Domain Adaptation on Office-Home Dataset (ResNet-50) 32±0.49 73.90±0.38 90.45±0.36 61.78±0.56 74.95±0.67 67.64±0.29 71.34±0.46 71.30±0.46 60.13±0.50 65.72±0.48 DANN [10] 73.56±0.15 96.27±0.26 98.73±0.20 81.53±0.23 82.78±0.18 86.12±0.15 86.50±0.20 70.80±0.66 67.71±0.76 69.23±0.71 ADDA [37] 75.67± 0.17 95.38±0.23 99.85±0.12 83.41± 0.17 83.62±0.14 84.25±0.13 87.03±0.16 71.82±0.45 69.32±0.41 70.57±0.43 RTN [22] 78.98±0.55 93.22±0.52 85.35±0.47 77.07±0.49 89.25±0.39 89.46±0.37 85.56±0.47 75.50±0.29 66.21±0.31 70.85±0.30 IWAN [43] 89.15±0.37 99.32±0.32 99.36±0.24 90.45±0.36 95.62±0.29 94.26±0.25 94.69±0.31 78.06±0.40 73.33±0.46 75.70±0.43 SAN [5] 93.90±0.45 99.32±0.52 99.36±0.12 94.27±0.28 94.15±0.36 88.73±0.44 94.96±0.36 77.75±0.36 75.26±0.42 76.51±0.39 PADA [6] 86.54±0.31 99.32±0.45 100.00±.00 82.17±0.37 92.69±0.29 95.41±0.33 92.69±0.29 75.03±0.36 70.48±0.44 72.76±0.40 ETN 94.52±0.20 100.00±.00 100.00±.00 95.03±0.22 96.21±0.27 94.64±0.24 96.73±0.16 83.23±0.24 74.93±0.28 79.08±0.26</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Classification Accuracy (%) for Partial Domain Adaptation on Office-31 Dataset (VGG)</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>Office-31</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell>Avg</cell></row><row><cell>VGG [34]</cell><cell>60.34±0.84</cell><cell>97.97±0.63</cell><cell>99.36±0.36</cell><cell>76.43±0.48</cell><cell>72.96±0.56</cell><cell>79.12±0.54</cell><cell>81.03± 0.57</cell></row><row><cell>DAN [21]</cell><cell>58.78±0.43</cell><cell>85.86±0.32</cell><cell>92.78±0.28</cell><cell>54.76±0.44</cell><cell>55.42±0.56</cell><cell>67.29±0.20</cell><cell>69.15±0.37</cell></row><row><cell>DANN [10]</cell><cell>50.85±0.12</cell><cell>95.23±0.24</cell><cell>94.27±0.16</cell><cell>57.96±0.20</cell><cell>51.77±0.14</cell><cell>62.32±0.12</cell><cell>68.73±0.16</cell></row><row><cell>ADDA [37]</cell><cell>53.28±0.15</cell><cell>94.33±0.18</cell><cell>95.36±0.08</cell><cell>58.78±0.12</cell><cell>50.24±0.10</cell><cell>63.34±0.08</cell><cell>69.22±0.12</cell></row><row><cell>RTN [22]</cell><cell>69.35±0.42</cell><cell>98.42±0.48</cell><cell>99.59±0.32</cell><cell>75.43±0.38</cell><cell>81.45±0.32</cell><cell>82.98±0.36</cell><cell>84.54±0.38</cell></row><row><cell>IWAN [43]</cell><cell>82.90±0.31</cell><cell>79.75±0.26</cell><cell>88.53±0.16</cell><cell>90.95±0.33</cell><cell>89.57±0.24</cell><cell>93.36±0.22</cell><cell>87.51±0.25</cell></row><row><cell>SAN [5]</cell><cell>83.39±0.36</cell><cell>99.32±0.45</cell><cell>100.00±.00</cell><cell>90.70±0.20</cell><cell>87.16±0.23</cell><cell>91.85±0.35</cell><cell>92.07±0.27</cell></row><row><cell>PADA [6]</cell><cell>86.05±0.36</cell><cell>99.42±0.24</cell><cell>100.00±.00</cell><cell>81.73±0.34</cell><cell>93.00±0.24</cell><cell>95.26±0.27</cell><cell>92.54±0.24</cell></row><row><cell>ETN</cell><cell>85.66±0.16</cell><cell>100.00±.00</cell><cell>100.00±.00</cell><cell>89.43±0.17</cell><cell>95.93±0.23</cell><cell>92.28±0.20</cell><cell>96.74±0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Classification Accuracy (%) of ETN and Its Variants for Partial Domain Adaptation on Office-Home Dataset (ResNet-50)</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by National Key R&amp;D Program of China (No. 2016YFB1000701) and National Natural Science Foundation of China (61772299, 71690231, and 61672313).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="465" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>59:1-59:35</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiclass open set recognition using probability of inclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="393" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="164" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Learning Theory (COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556v6</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007-05" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flexible transfer learning under support and model shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
