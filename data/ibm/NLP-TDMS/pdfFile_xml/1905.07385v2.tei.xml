<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning on Visual-Symbolic Graphs for Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-30">30 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Mathematical Institute for Data Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learning on Visual-Symbolic Graphs for Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-30">30 Sep 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effrosyni Mavroudi [0000−0001−7552−8342]  , Benjamín Béjar Haro [0000−0001−9705−4483]  , and René Vidal [0000−0003−1838−0761]    Abstract. Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of video understanding has been moving towards increasing levels of complexity, from classifying a single action in short videos to detecting multiple complex activities performed by multiple actors interacting with objects in untrimmed videos. Therefore, there is a need to develop algorithms that can effectively model spatio-temporal visual and semantic context. One way of capturing such context is to use graph-based modeling, which has a rich history in computer vision. Traditional graph-based approaches to video understanding, e.g., using probabilistic graphical models <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b73">70,</ref><ref type="bibr" target="#b61">59]</ref>, focused mainly on modeling context at the level of symbols rather than visual signals or representations. However, recent advances have enabled representation learning on graph-structured data using deep architectures called Graph Neural Networks (GNNs), which learn how to refine node representations by aggregating messages from their neighbors <ref type="bibr" target="#b26">[25]</ref>.</p><p>Videos can be represented as visual spatio-temporal attributed graphs (visual st-graphs) whose nodes correspond to regions obtained by an object detector and whose edges capture interactions between such regions. <ref type="bibr">GNNs</ref>   (1) visual spatio-temporal interactions between actors and objects and (2) commonsense relationships between labels, such as co-occurrences. These cues can be encoded in a hybrid spatio-temporal visual and symbolic attributed graph. In this work, we perform representation learning on this hybrid graph to obtain context-aware representations of detected semantic entities, such as actors and objects, that can be used to solve downstream video understanding tasks, such as multi-label action recognition.</p><p>been designed for refining the local node/edge features, typically extracted by a convolutional neural network, based on the spatio-temporal context captured by the graph. Although representation learning on visual st-graphs has lead to significant advances in video understanding <ref type="bibr" target="#b67">[65,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b60">58,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b66">64,</ref><ref type="bibr" target="#b2">3]</ref>, there are four key limitations of state-of-the-art approaches that prevent them from fully exploiting the rich structure of these graphs. First, the visual st-graph is a heterogeneous graph that has distinct node types (actor, object, etc.) and distinct edge types (object-to-actor spatial, actor-to-actor temporal, etc.), with each type being associated with a feature of potentially different dimensionality and semantics, as shown in the example of <ref type="figure" target="#fig_0">Fig. 1</ref>. However, most GNNs assume nodes/edges of the same type. Therefore, recent attempts at explicitly modeling actors and objects have resorted to applying separate GNNs for each node/edge type <ref type="bibr" target="#b67">[65,</ref><ref type="bibr" target="#b12">12]</ref>. Second, most methods operate on a graph of fixed edge weights with dense connectivity between detected regions. In practice, only a few of the edges capture meaningful interactions. Third, current approaches do not incorporate edge features, such as geometric relations between regions, for updating the node representations. Finally, despite modeling local visual context, existing approaches do not reason at a global video level or exploit semantic label relationships, which have been shown to be beneficial in the image recognition domain <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In this work, in an effort to address these limitations, we propose a novel Graph Neural Network (GNN) model, called Visual Symbolic -Spatio Temporal -Message Passing Neural Network (VS-ST-MPNN), that performs represen-tation learning on visual st-graphs to obtain context-aware representations of detected actors and objects ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Our model handles heterogeneous graphs by employing learnable message functions that are specialized for each edge type. We also adapt the visual edge weights with an attention mechanism specialized for each type of interaction. For example, an actor node will separately attend to actor nodes at the previous frame and object nodes at the current frame. Furthermore, we use edge features to refine the actor and object representations and to compute the attention coefficients that determine the connection strength between regions. Intuitively, nodes which are close to each other or are interacting should be strongly connected. Finally, one of our key contributions is incorporating an attributed symbolic graph whose nodes correspond to semantic labels, such as actions, described by word embeddings and whose edges capture label relationships, such as co-occurrence. We fuse the information of the two graphs with learnable association weights between their nodes and learn global semantic interaction-aware features. Importantly, we do not require ground truth annotations of objects, tracks or semantic labels for each visual node.</p><p>In summary, the contributions of this work are three-fold. First, we model contextual cues for video understanding by combining a symbolic graph, capturing semantic label relationships, with a visual st-graph, encoding interactions between detected actors and objects. Second, we introduce a novel GNN that can perform joint representation learning on the heterogeneous visual-symbolic graph, in order to obtain visual and semantic context-aware representations of actors, objects and their interactions in a video, which can then be used to solve downstream recognition tasks. Finally, to demonstrate the effectiveness and generality of our method, we evaluate it on tasks such as multi-label temporal activity localization, object affordance detection and grounded video description on three challenging datasets and show that it achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Visual context for video understanding. Context and its role in vision has been studied for a long time <ref type="bibr" target="#b44">[43]</ref>. There are two major, complementary ways of utilizing context in video understanding tasks: (a) extracting global representations from whole frames by applying convolutional neural networks to short video segments <ref type="bibr" target="#b54">[52,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b69">67]</ref> followed by long-term temporal models <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b69">67]</ref> and (b) extracting mid-level representations based on semantic parts, such as body parts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">40]</ref>, latent attributes <ref type="bibr" target="#b37">[36]</ref>, secondary regions <ref type="bibr" target="#b15">[15]</ref>, human-object interactions <ref type="bibr" target="#b48">[46,</ref><ref type="bibr" target="#b71">69]</ref> and object-object interactions <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b69">67]</ref>. Our proposed method falls into the latter category, using GNNs to obtain representations of detected semantic entities based on interactions captured by visual and symbolic graphs. Graph neural networks for video understanding. The first approach applying a deep network on a visual graph for video understanding was the Structured Inference Machine <ref type="bibr" target="#b11">[11]</ref>, which introduced actor feature refinement with message passing, and trainable gating functions for filtering out spurious interactions, but only captured spatial relationships between actors. Another early approach was the S-RNN <ref type="bibr" target="#b22">[21]</ref>, which introduced the concept of weight-sharing between nodes or edges of the same type, but did not iteratively refine node representations. With the advent of GNNs, many researchers have explored modeling whole frames <ref type="bibr" target="#b68">[66]</ref>, tracklets <ref type="bibr" target="#b67">[65]</ref>, feature map columns <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b43">42]</ref> or object proposals <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b66">64]</ref> as graph nodes and using off-the-shelf GNNs, such as MPNNs <ref type="bibr" target="#b13">[13]</ref>, GCNs <ref type="bibr" target="#b26">[25]</ref> and Relation Networks <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b2">3]</ref> to refine the node or edge representations, obtaining significant performance gains. However, most of these GNNs are unable to handle edge features, directed edges and distinct node and edge types. Therefore, applying existing GNNs to visual st-graphs requires treating every node and edge in the same way <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14]</ref>, or focusing only on one edge type <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b21">20]</ref>, or using separate GNNs for each type of interaction <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b12">12]</ref>, hence completely ignoring or sub-optimally handling their rich graph structure. In contrast, our proposed method can be directly applied to any st-graph and supports message passing in heterogeneous graphs. The benefit of such fine-grained modeling has already been established in fields such as computational pharmacology and relational databases <ref type="bibr" target="#b74">[71,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b51">49]</ref>, but remains relatively unexplored in computer vision. Furthermore, similar to <ref type="bibr" target="#b49">[47,</ref><ref type="bibr" target="#b14">14]</ref>, our method iteratively adapts the visual edge weights, but employs an attention mechanism that is specialized for different edge types and takes edge features into account. Symbolic graphs. There is a long line of work on exploiting external knowledge encoded in label relation graphs for visual recognition tasks. Semantic label hierarchies, such as co-occurrence, have been leveraged for improving object recognition <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">10]</ref>, multi-label zero-shot learning <ref type="bibr" target="#b31">[30]</ref> and other imagebased visual tasks <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b50">48]</ref>. Much fewer papers utilize knowledge graphs for video understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b25">24]</ref>, possibly due to the limited number of semantic classes in traditional video datasets. However, most of these methods directly perform inference on the symbolic graph. For example, the SINN <ref type="bibr" target="#b25">[24]</ref> performs graphbased inference in a hierarchical label space for action recognition. Rather, we aim to use the semantics of labels to integrate prior knowledge about the inter-class relationships and facilitate the computation of semantic context-aware region features. In a similar vein, Liang et al. <ref type="bibr" target="#b35">[34]</ref> enhance feature maps extracted from images by using a symbolic graph, while <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b5">6]</ref> use a latent interaction graph. In contrast, we seek to improve the representation of visual st-graph nodes rather than enhance convolutional features on a regular grid. Fusing information from multiple graphs using GNNs is an exciting new line of research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b62">60,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b56">54]</ref>. Similar to our approach, Chen et al. <ref type="bibr" target="#b4">[5]</ref> combine a visual graph instantiated on objects with a symbolic graph and perform graph representation learning, while <ref type="bibr" target="#b23">[22]</ref> enforce the scalar edge weights between visual regions to be consistent with the edges of the symbolic graph. However, they operate on simple spatial graphs and assume access to semantic labels of regions during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe the overall architecture of our proposed VS-ST-MPNN model, shown in  jects and their interactions based on the contextual information captured in two graphs: a visual st-graph and a symbolic graph. The refinement is performed by a novel GNN, which is designed to exploit the rich structure of the visual st-graph by utilizing edge features and learning specialized attention-based neighborhood aggregation functions for different node and edge types. In addition, our model enables the fusion with the symbolic graph, by incorporating graph convolutions, to learn semantic relation-aware features, and soft-assignment weights, to connect visual and symbolic graph nodes without requiring access to ground-truth semantic labels of regions during training. Our model can be trained jointly with recognition networks to solve downstream video understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Context Module</head><p>Visual st-graph. Our input is a sequence of T frames with detected actor and object regions.</p><formula xml:id="formula_0">Let G v = (V v , E v ) be a spatio-temporal attributed directed graph, called the visual st-graph, where V v is a finite set of vertices and E v ⊆ V v × V v</formula><p>is a set of edges. Nodes correspond to actor and object detections, while edges model latent interactions. There are M actors and N objects per frame. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates a toy example with M = 1, N = 2 and T = 2. The graph is both nodeand edge-typed with N node types and E edge types, i.e., each node (edge) is associated with a single node (edge) type. Specifically, the node types are actor and object (N = 2) and the edge types are object-to-actor spatial (obj-act-s), actor-to-object spatial (act-obj-s), object-to-object spatial (obj-obj-s), actor-toactor temporal (act-act-t) and object-to-object temporal (obj-obj-t) (E = 5). The allowed spatio-temporal connections between nodes of the visual st-graph (E v ) are specified a priori and encode the family of spatio-temporal interactions captured by the model. For instance, we can constrain temporal edges to connect a node at frame t with another node of the same type at time t − 1. Each node and edge is described by an initial attribute, whose dimensionality may vary depending on the node/edge type. An actor/object appearance feature can be used as the initial attribute of node i (h </p><formula xml:id="formula_1">(0) i } i∈V v and {h (0) ij } (i,j)∈E v ,</formula><p>respectively, we introduce novel GNN propagation rules to perform representation learning on the visual stgraph with the goal of refining local node and edge features using spatio-temporal contextual cues. At each iteration our model: (1) refines the scalar visual edge weights using attention coefficients; (2) computes a message along each edge that depends on the edge type, the attention-based scalar edge weight, the features of the connected nodes and the edge feature; (3) updates the feature of every node by aggregating messages from incoming edges; and (4) updates the feature of every edge by using the message that was computed alongside it. Next, we describe each one of these steps in more detail.</p><p>-Attention mechanism: At each iteration l of the MPNN, we first refine the strength of region connections by computing attention coefficients, a ij , that capture the relevance of node j (message sender) for the update of node i (message receiver). In contrast to GAT <ref type="bibr" target="#b58">[56]</ref>, our model learns an attention mechanism specialized for each type of interaction and it utilizes edge features for its computation. The attention coefficients for the l-th iteration are computed as follows:</p><formula xml:id="formula_2">a (l) ij = exp γ (l) ij /    k∈N v ij (i) exp γ (l) ik    ,<label>(1)</label></formula><formula xml:id="formula_3">γ (l) ij = ρ (v ij a ) T W νi r h (l−1) i ; W νj s h (l−1) j ; βW ij e h (l−1) ij .<label>(2)</label></formula><p>Here, ij is the type of the edge from node j to node i, N v ij (i) is the set of visual nodes connected with node i via an incoming edge of type ij , h (l−1) i is the feature of the i-th node at the previous iteration, h (l−1) ij is the feature of the edge from j to i at the previous iteration, ν i is the type of node i and ρ is a non-linearity, such as Leaky-ReLU <ref type="bibr" target="#b19">[18]</ref>. The parameter β ∈ {0, 1} denotes whether edge features will be used for computing the attention coefficients (β = 1) or not (β = 0). W νi r , W νj s and W ij e are learnable projection weights and are shared between nodes (edges) of the same type. All projection matrices linearly transform the current node (edge) feature to a refined feature of fixed dimensionality d l . v ij a is a learnable attention vector. For improved readability we have dropped the layer index (l) from the attention and projection weights.</p><p>-Message computation: After computing the attention coefficients, we compute a message along each edge. The message from node j to node i is:</p><formula xml:id="formula_4">m (l) ij = a (l) ij λ v W νj s h (l−1) j + λ e W ij e h (l−1) ij ,<label>(3)</label></formula><p>where the parameters λ e , λ v ∈ {0, 1} denote whether the edge feature and the sender node feature will be used for the message computation, respectively.</p><p>-Node and edge update: Following the message computation, the node feature is updated using an aggregation of incoming messages from different edge types and a residual connection, while the edge feature is set to be equal to the message:</p><formula xml:id="formula_5">h (l) i = h (l−1) i + σ   j∈N v (i) m (l) ij   , h (l) ij = m (l) ij ,<label>(4)</label></formula><p>where N v (i) is the set of visual nodes that are connected with node i, σ(·) is a non-linearity, such as ReLU. After L layers of the spatio-temporal MPNN (or equivalently L rounds of node and edge updates), we obtain refined, visual context-aware node and edge features: h</p><formula xml:id="formula_6">(L) i ∈ R d L and h (L) ij ∈ R d L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Context Module</head><p>Symbolic graph. Let G s = (V s , E s ), be the input symbolic graph, where V s and E s denote the symbol set and edge set, respectively. The nodes of this graph correspond to semantic labels, such as action labels or object labels. Each symbolic node c is associated with a semantic attribute, such as the linguistic embedding of the label (s c ∈ R K ). Edges in the symbolic graph are associated with scalar weights, which encode label relationships, such as co-occurrence. These edge weights are summarized in the fixed adjacency matrix L s ∈ R |V s |×|V s | .</p><p>-Integration of visual evidence with the symbolic graph: As a first step, we update the attributes of the symbolic graph using visual evidence, i.e., the visual context-aware representations of the nodes of the visual st-graph. To achieve this, without requiring access to the ground-truth semantic labels of regions, we learn associations between the nodes of the visual st-graph and those of the symbolic graph. These associations are the edges of the bipartite graph G vs = (V vs , E vs ),</p><formula xml:id="formula_7">with V vs = V v ∪ V s and E vs ⊆ V v × V s .</formula><p>Although latent, we can specify a priori the allowed visual-to-symbolic node connections (edges). For example, when symbolic nodes correspond to action classes, we can remove edges between object and symbolic nodes. The learnable association weight φ vs c,i represents the confidence of assigning the feature from visual node i to the symbolic node c:</p><formula xml:id="formula_8">φ vs c,i = exp (w vs c ) T h (L) i c ∈N vs (i) exp (w vs c ) T h (L) i ,<label>(5)</label></formula><p>where w vs c ∈ R d L is a trainable weight vector and N vs (i) is the neighborhood of visual node i on the bipartite graph G vs . After computing the voting weights, each symbolic node is associated with a weighted sum of projected visual node features: -Semantic graph convolutions: We obtain semantic relation-aware features by applying a vanilla GCN <ref type="bibr" target="#b26">[25]</ref> to the nodes of the symbolic graph. More specifically, by iteratively applying the propagation rule S (r+1) = GCN(S (r) , L s ), where S (r) denotes the matrix of symbolic node embeddings at iteration r = 1, . . . , R, the GCN yields evolved symbolic node features S (R) ∈ R |V s |×Ds .</p><formula xml:id="formula_9">f c = σ( i φ vs c,i W vs p h (L) i ), where W vs p ∈ R Ds×d L is</formula><p>-Update of visual st-graph: The evolved symbolic node representations obtained after R iterations of graph convolutions on the symbolic graph can be mapped back to the visual st-graph, so that the representation of the visual nodes can be enriched by global semantic context. To achieve this we compute mapping weights (attention coefficients) from symbolic nodes to visual nodes:</p><formula xml:id="formula_10">φ sv i,c = exp (v sv a ) T s (R) c ; h (L) i c ∈N vs (i) exp (v sv a ) T s (R) c ; h (L) i ,<label>(6)</label></formula><p>where v sv a ∈ R d L +Ds is a learnable attention vector. The final visual node feature representation is then obtained using a residual connection:</p><formula xml:id="formula_11">h i = h (L) i + σ c ∈V s φ sv i,c W sv p s (R) c</formula><p>. These context-aware representations can be fed to recognition networks to solve downstream video understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the effectiveness and generality of our method, we conduct experiments on three challenging video understanding tasks that require reasoning about interactions between semantic entities and relationships between classes: a) sub-activity and object affordance classification (Sec. 4.1), b) multi-label temporal action localization (Sec. 4.2) and c) grounded video description (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on CAD-120</head><p>CAD-120. This dataset provides 120 RGB-D videos, with each video showing a daily activity comprised of a sequence of sub-activities (e.g., moving, drinking) <ref type="table">Table 1</ref>: Results on CAD-120 <ref type="bibr" target="#b28">[27]</ref> for sub-activity and object affordance detection, measured via F1-score. Our results are averaged over five random runs, with the standard deviation reported in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detection F1-score (%) Sub-activity Object affordance ATCRF <ref type="bibr" target="#b28">[27]</ref> 80.4 81.5 S-RNN <ref type="bibr" target="#b22">[21]</ref> 83. and object affordances (e.g., reachable, drinkable) <ref type="bibr" target="#b28">[27]</ref>. Given temporal segments, the task is to classify each actor in each segment into one of 10 sub-activity classes and each object into one of 12 affordance classes. Evaluation is performed with 4-fold, leave-one-subject-out, cross-validation using F1-scores averaged over all classes as an evaluation metric. With a visual st-graph provided by the dataset <ref type="bibr" target="#b28">[27]</ref> (including hand-crafted features of actors and objects and geometric relations), it is a particularly good test-bed for comparing different GNNs.</p><p>Implementation details. The visual st-graph provided with the dataset is instantiated on the actors and objects of each temporal segment of the input video and contains 5 edge types: obj-obj-s, obj-act-s, act-obj-s, act-act-t and obj-obj-t. We construct a symbolic graph that has nodes corresponding to the 10 sub-activity and 12 affordance classes, with edge weights capturing per-frame class co-occurrences in training data. The attribute of each symbolic node is obtained by using off-the-shelf word2vec <ref type="bibr" target="#b42">[41]</ref> class embeddings of size K = 300. Actor (object) nodes are connected to sub-activity (affordance) symbolic nodes. The following hyperparameters are used in the our model: L = 4, R = 1, λ v = 1, λ e = 1 and β = 1. All messages are of size 256. We use the sum of cross-entropy losses per node to jointly train our model and the sub-activity and affordance classifiers applied at each node of the st-graph. We train for 100 epochs with a batch size of 5 sequences and use the Adam learning rate scheduler with an initial learning rate of 0.001. Dropout with a rate of 0.5 is applied to all fully connected layers.</p><p>Comparison with the state of the art. <ref type="table">Table 1</ref> compares the sub-activity and affordance detection performance of our method with prior work. Our method obtains state-of-the-art results for sub-activity detection, with an average performance of 90.4% and a best of 91.3%, and the second best result on affordance detection (89.2%) -being only second to the S-RNN (multi-task) <ref type="bibr" target="#b22">[21]</ref>. The S-RNN was trained on the joint task of detection and anticipation and we outperform it by 8% in the sub-activity classification task. Even without using the symbolic graph, our method improves upon recent GNNs, which were applied on the same attributed visual st-graph, validating our novel layer propagation rules. Ablation analysis. In <ref type="figure">Fig. 3</ref>, we show the effect of attention, edge features and  <ref type="figure">Fig. 3</ref>: Effect of attention mechanism and node update type on CAD-120 detection performance. Using an attention mechanism outperforms using a fixed visual adjacency matrix. Updating nodes based on both neighboring node and incoming edge attributes (full ) is superior to updating them using just the nodes (nnode) or edges (relational ).</p><p>number of visual node updates on the recognition performance. First, we compare the performance of a model using a fixed binary adjacency matrix with that of a model using our attention mechanism. Clearly, attention benefits performance in both tasks. Second, we conclude that using the attributes of both the neighboring nodes and adjacent edges is better than using only those of the neighboring nodes, validating the usefulness of edge features. Finally, increasing the number of ST-MPNN layers improves performance, which saturates after 4-5 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Charades</head><p>Charades. Charades <ref type="bibr" target="#b53">[51]</ref> is a large dataset with 9848 RGB videos and temporal annotations for 157 action classes, many of which involve human-object interactions. Each video contains an average of 6.8 activity instances, many of which are co-occurring. Following <ref type="bibr" target="#b53">[51]</ref>, multi-label action temporal localization performance is measured in terms of mean Average Precision (mAP), evaluating per-frame predictions for 25 equidistant frames in each one of the 1.8k validation videos. Implementation details. To tackle the challenging problem of multi-label temporal action localization, we perform a late fusion of a global model, operating on whole frames, and a local model, operating on actors and objects. The global model is an I3D RGB model <ref type="bibr" target="#b3">[4]</ref> fine-tuned on Charades <ref type="bibr" target="#b46">[44]</ref>, combined with a two-layer biGRU of size 256, similar to existing baselines on this dataset. The proposed VS-ST-MPNN is used as the local model. To build the visual st-graph we detect actors and objects using a Faster-RCNN <ref type="bibr" target="#b17">[17]</ref> trained on the MS-COCO <ref type="bibr" target="#b36">[35]</ref> dataset. We rank detections based on their score and we keep the top-2 person detections and top-10 object detections per frame. We pool features from the Mixed 4f 3D feature map of the I3D for each detected region using RoIAlign <ref type="bibr" target="#b17">[17]</ref> and max-pooling in space. This yields an attribute of size 832 for the actor/object regions for the frames of the original video sampled at 1.5 FPS. <ref type="table">Table 2</ref>: Multi-label temporal action localization results on Charades <ref type="bibr" target="#b53">[51]</ref>. Performance is measured via per-frame mAP. R: RGB, F: optical flow. Our method yields a relative improvement of 6% over the state-of-the-art method by using only raw RGB frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Feat Input mAP (%) Predictive-corrective <ref type="bibr" target="#b9">[9]</ref> VGG R 8.9 Two-stream <ref type="bibr" target="#b52">[50]</ref> VGG R+F 8.94 Two-stream + LSTM <ref type="bibr" target="#b52">[50]</ref> VGG R+F 9.6 R-C3D <ref type="bibr" target="#b63">[61]</ref> VGG We use 3 types of visual edges: obj-act-s, act-obj-s and act-act-t and describe each edge with the relative position of the connected regions. Our symbolic graph has nodes corresponding to the 157 action classes and edge weights corresponding to per-frame label co-occurrences in training data. Only actor nodes are connected to symbolic nodes. Obtaining a linguistic attribute for each symbolic node is not trivial, since action names often contain multiple words. To circumvent that, each action class is separated into a verb and an object and the average of their word2vec <ref type="bibr" target="#b42">[41]</ref> embeddings (K = 300) is used as the initial node attribute. The hyperparameters are: L = 3, d L = 512, R = 1, D s = 256, λ v = 1, λ e = 1 and β = 1. For performing per-frame multi-label action classification, we average the learned actor node and edge representations at each frame, we input them to a two-layer biGRU of size 512, and we feed the resulting hidden states to binary action classifiers for per-frame multi-label action classification. We jointly train the VS-ST-MPNN and biGRU for 40 epochs with a binary cross-entropy loss applied per frame, using a batch size of 16 sequences. We also apply dropout with a rate of 0.5 on all fully connected layers and use the Adam scheduler, with an initial learning rate of 1e −4 .</p><p>Comparison with prior work. As shown in <ref type="table">Table 2</ref>, our framework outperforms all other methods on temporal action localization, with a mAP of 23.7% (averaged across 3 random runs) by using only raw RGB frames. It yields a relative improvement of 24% over the alternative graph-based approach <ref type="bibr" target="#b12">[12]</ref>, which uses both RGB and optical flow inputs, as well as additional actor embeddings trained at the ImSitu dataset <ref type="bibr" target="#b64">[62]</ref>. Impact of each graph. In <ref type="table" target="#tab_3">Table 3</ref>, we report the baseline result (10.7%) obtained by classifying activities based on local actor features (ID: 6). Refining these features by using our Visual Context Module improves performance by 3%. As shown quantitatively in the supplementary material, both our specialized attention mechanism and the usage of edge features improve the performance, outperforming a vanilla GNN. Representation learning on the hybrid graph yields a significant absolute improvement of 5% over the baseline. Additionally, modeling long-term temporal context and global context leads to the final stateof-the-art performance, indicating that the representations learned by our model are complementary to holistic scene cues and temporal dynamics. Per-class improvement analysis. To gain a better understanding of the benefits of representation learning on the visual graph, we highlight in <ref type="figure">Fig. 4</ref> the activity classes with the highest positive and negative difference in performance when adding obj-to-act-s messages. By harnessing visual human-object interaction cues, our model is able to better recognize actions such as Watching television. Impact of semantic context module. Comparing the models with IDs 3 and 4 in <ref type="table" target="#tab_3">Table 3</ref>, we observe that adding the semantic context module improves mAP by 2%. Notably, updating the visual nodes by attending over the initial symbolic node features (linguistic) instead of the evolved features did not improve performance in our experiments, showing the importance of semantic graph convolutions. The semantic module seems to particularly help with rare classes, such as Holding a vacuum, which has only 213 training examples (3% of available annotated segments), and classes with strong co-occurrences <ref type="figure" target="#fig_6">(Fig. 5</ref>). The t-SNE visualization shows that, although the visual context-aware actor embeddings are already capturing meaningful label relationships (e.g., open and hold book ), the integration of semantic relationships via the symbolic graph results in more tightly clustered embeddings and well-defined groups, facilitating action recognition. Model complexity. Since our visual st-graph is designed to capture only local spatio-temporal interactions, we can compute messages in parallel and process the entire Charades validation set (around 2K videos at 1.5FPS) in 2 minutes on a single Titan XP GPU, given initial features pooled from actor/object regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on ActivityNet Entities</head><p>ActivityNet Entities. The task in the recently released ActivityNet Entities <ref type="bibr" target="#b69">[67]</ref> dataset, containing 15k videos and more than 158k annotated bounding boxes, is to generate a sentence describing the event in a ground-truth video segment,  and to spatially localize all the generated nouns that belong to a vocabulary of 432 object classes. Following Zhou et al. <ref type="bibr" target="#b69">[67]</ref>, the quality of generated captions is measured using standard metrics, such as Bleu (B@1, B@4), METEOR (M), CIDEr (C), and SPICE (S), whereas the quality of object localization is evaluated on generated sentences using the F 1 all , F 1 loc metrics. Object localization results on the test set were obtained using the evaluation server 1 . The current state-ofthe-art grounded video description model (GVD) <ref type="bibr" target="#b69">[67]</ref> uses a hierarchical LSTM decoder that generates a sentence describing a video segment, given global video features as well as local region features of 100 region proposals from 10 equidistant frames. The region features are refined using a multi-head self-attention (MHA) mechanism. To validate the effectiveness of our model, we experiment with three variants of the GVD: (a) replace the MHA with our VS-ST-MPNN; (b) use <ref type="table">Table 4</ref>: Grounded video description results on ActivityNet Entities <ref type="bibr" target="#b69">[67]</ref>. MHA: multi-head self-attention. SCM-VG: our semantic context module with visual-to-symbolic node correspondences pre-trained on Visual Genome. the MHA along with our Semantic Context Module; (c) the same as before but with visual-to-symbolic node assignment weights initialized based on knowledge transfer from the Visual Genome dataset <ref type="bibr" target="#b29">[28]</ref>. We use a symbolic graph whose nodes correspond to object classes. As shown in <ref type="table">Table 4</ref>, replacing MHA with our visual module does not improve captioning, but it improves localization accuracy with a relative improvement of 4% (24.1 → 25.2). Adding our Semantic Context Module to GVD leads to an improvement across all captioning and localization metrics, which is even more pronounced in the test set (improving CIDEr from 45.5 to 47.7). Note that the initial region features already captured semantic information by including object class probabilities. Therefore, the improvement in captioning cannot be attributed solely to the inclusion of semantic context, but rather to our semantic reasoning framework. Finally, from the superior captioning performance of our third variant, we conclude that prior knowledge about correspondences between visual and symbolic nodes, if available, can possibly facilitate representation learning on the hybrid graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a novel deep learning framework for video understanding that performs joint representation learning on a hybrid graph, composed of a symbolic graph and a visual st-graph, for obtaining context-aware visual node and edge features. We obtained state-of-the-art performance on three challenging datasets, demonstrating the effectiveness and generality of our framework.</p><p>Acknowledgements: The authors thank Carolina Pacheco Oñate, Paris Giampouras and the anonymous reviewers for their valuable comments. This research was supported by the IARPA DIVA program via contract number D17PC00345.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Cues for video understanding:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our goal is to refine the features of detected actors, ob-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our VS-ST-MPNN model that performs representation learning on a hybrid visual-symbolic graph. Given an input video that is represented as a visual st-graph, with nodes corresponding to detected actors and objects and edges capturing latent interactions, our framework has two modules that integrate context in the local representations of its nodes and edges: (a) a Visual Context Module (Sec. 3.1) that performs L rounds of node and edge updates on the visual graph, with specialized neighborhood aggregation functions that depend on the type of an edge and (b) a Semantic Context Module (Sec. 3.2) that integrates visual evidence with semantic knowledge encoded in an external symbolic graph and learns global semantic interaction-aware features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(0) i ), while the relative spatial location of regions i and j can be used as the initial attribute of the edge from j to i (h(0) ij ). Visual ST-MPNN. Given the input visual st-graph G v with initial node and edge attributes/features, {h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a learnable projection weight matrix. The new representation of each node c is computed as the concatenation of the linguistic embedding and the visual feature: s (0) c = [s c ; f c ] ∈ R K+Ds .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative evaluation of the Semantic Context Module (SCM). (left) Classes with the highest positive and negative performance difference when adding the semantic module. (right) t-SNE visualization of actor node embeddings from Charades validation set obtained before and after adding the SCM. We show 1121 random samples per class for 5 selected action classes. (Best viewed zoomed in and in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have recently</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Edge</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>feature</cell><cell></cell><cell>Actor feature</cell><cell>Open Book Hold Book Standing</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Visual ST Graph</cell><cell>Object feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Hold</cell><cell>Hold</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>book</cell><cell>laptop</cell><cell></cell></row><row><cell></cell><cell>time</cell><cell>Sit in chair</cell><cell></cell><cell cols="2">Open book</cell></row><row><cell cols="2">Video with Detected</cell><cell></cell><cell></cell><cell>Read</cell><cell></cell><cell>Context-aware</cell></row><row><cell cols="2">Actors &amp; Objects</cell><cell></cell><cell></cell><cell>book</cell><cell></cell><cell>Actor Representation</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Symbolic Graph</cell><cell></cell></row><row><cell>label</cell><cell>linguistic</cell><cell>object-to-actor</cell><cell cols="2">actor-to-actor</cell><cell>actor-to-object</cell><cell>object-to-object</cell></row><row><cell>relation</cell><cell>embedding</cell><cell>spatial edge</cell><cell cols="2">temporal edge</cell><cell>spatial edge</cell><cell>temporal edge</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation analysis on Charades<ref type="bibr" target="#b53">[51]</ref>. Visual : Visual Context Module. Semantic: Semantic Context Module. Long Term: long-term temporal modeling.</figDesc><table><row><cell cols="5">ID Visual Semantic Long Term mAP (%)</cell><cell>mAP (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ Global model</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>18.6</cell><cell>23.4</cell></row><row><cell>2</cell><cell>-</cell><cell>-</cell><cell></cell><cell>15.2</cell><cell>22.2</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>-</cell><cell>15.3</cell><cell>22.0</cell></row><row><cell>4</cell><cell></cell><cell>-</cell><cell>-</cell><cell>13.7</cell><cell>21.8</cell></row><row><cell>5</cell><cell>-</cell><cell></cell><cell>-</cell><cell>11.7</cell><cell>21.8</cell></row><row><cell>6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.7</cell><cell>20.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Qualitative results on Charades. (left) The classes with the highest positive and negative performance difference after adding object-to-actor spatial messages. Incorporating spatial structure benefits actions that involve interactions with distant objects, such as watching television or cooking. (right) Action predictions of our model (ID: 3) for 9 frames of a sample Charades video.</figDesc><table><row><cell cols="2">Washing their hands Opening a refrigerator Someone is cooking something Watching television Working/Playing on a laptop Watching a laptop or something on a laptop Talking on a phone/camera Playing with a phone/camera Working on paper/notebook Holding a phone/camera Closing a refrigerator Someone is laughing Tidying something on the floor Someone is running somewhere Watching/Looking outside of a window Sitting on the floor</cell></row><row><cell></cell><cell>0</cell><cell>10</cell></row><row><cell></cell><cell cols="2">mAP Difference</cell></row><row><cell>Fig. 4: 0 Holding a vacuum Washing a window Closing a window Wash a dish/dishes Tidying something on the floor Holding a book Washing their hands Holding a laptop Tidying some clothes Holding a broom Washing a table Grasping onto a doorknob Closing a refrigerator Washing a cup/glass/bottle Someone is running somewhere</cell><cell>10</cell></row><row><cell cols="2">mAP Difference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>all F 1 loc Validation set GVD (MHA) [67] 23.9 2.59 11.2 47.5 15.1 7.11 24.1 GVD (VCM + SCM) (ours) 23.4 2.41 11.1 47.3 14.8 7.28 25.2 GVD (MHA + SCM) (ours) 23.8 2.67 11.3 48.6 15.2 7.35 25.3 GVD (MHA + SCM-VG) (ours) 23.9 2.78 11.3 49.1 15.1 7.15 24.0 .54 11.2 47.7 15.0 7.30 24.4 GVD (MHA + SCM-VG) (ours) 24.1 2.63 11.4 49.0 15.1 7.81 27.1</figDesc><table><row><cell cols="2">B@1 B@4 M S F 1 Test set C</cell><cell></cell></row><row><cell>Masked Transformer [68]</cell><cell>22.9 2.41 10.6 46.1 13.7 -</cell><cell>-</cell></row><row><cell>Bi-LSTM+TempoAttn [68]</cell><cell>22.8 2.17 10.2 42.2 11.8 -</cell><cell>-</cell></row><row><cell>GVD (MHA) [67]</cell><cell cols="2">23.6 2.35 11.0 45.5 14.9 7.59 25.0</cell></row><row><cell>GVD (VCM + SCM) (ours)</cell><cell>23.1 2.34 10.9 46.1 14.5 -</cell><cell>-</cell></row><row><cell>GVD (MHA + SCM) (ours)</cell><cell>23.6 2</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/20537</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video classification using semantic concept co-occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G3raphground: Graph-based language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4281" to="4290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="106" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.502" />
		<title level="m">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative Visual Reasoning Beyond Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00756</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00756" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7239" to="7248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graphbased global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-Based CNN Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.368</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.368" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2010.5540221</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2010.5540221" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictive-Corrective Networks for Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.223</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.223" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2067" to="2076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-Scale Object Classification Using Label Relation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.516</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.516" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4772" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stacked spatio-temporal graph convolutional networks for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Winter Applications of Computer Vision Conference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual Action Recognition with R*CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.129</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.129" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1080" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting edge features for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2018.2844175</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2844175" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic Graph Modules for Modeling Higher-Order Interactions in Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical relational networks for group activity recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
	<note>Structural-RNN: Deep Learning on Spatio-Temporal Graphs</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid Knowledge Routed Modules for Largescale Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1552" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2670560</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2670560" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured Label Inference for Visual Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I N</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2893215</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2893215" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust automatic traffic scene analysis in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913478446</idno>
		<ptr target="https://doi.org/10.1177/0278364913478446" />
	</analytic>
	<monogr>
		<title level="j">International Journal Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0981-7</idno>
		<ptr target="https://doi.org/10.1007/s11263-016-0981-7" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3273</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal Convolutional Networks for Action Segmentation and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.113</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.113" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-label Zero-Shot Learning with Structured Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00170</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00170" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, N., Garnett, R.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9225" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Symbolic Graph Reasoning Meets Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1853" to="1863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995353</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2011.5995353" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attend and Interact: Higher-Order Object Interactions for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00710</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00710" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic Hierarchies for Visual Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383272</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2007.383272" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Constructing Category Hierarchies for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="479" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Moving Poselets for Video Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mavroudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2017.20</idno>
		<ptr target="https://doi.org/10.1109/WACV.2017.20" />
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Applications of Computer Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent space-time graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolicioiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Duta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.tics.2007.09.009</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.tics.2007.09.009" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning Latent Super-Events to Detect Multiple Activities in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00556</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00556" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5304" to="5313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Explicit modeling of human-object interactions in realistic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning semantic relationships for better action retrieval in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rossenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298713</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298713" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Asynchronous Temporal Fields for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.599</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.599" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5650" to="5659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Val Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="413" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video event recognition with deep hierarchical context model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A graph-based framework to bridge movies and synopses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">R-C3d: Region Convolutional 3d Network for Temporal Activity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.617</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.617" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Heterogeneous graph learning for visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2769" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.200</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.200" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1819" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A structured model for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="831" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Interaction part mining: A midlevel approach for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3323" to="3331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2015.7298953</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298953" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Context-aware modeling and recognition of activities in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics p</title>
		<imprint>
			<biblScope unit="page">457466</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
