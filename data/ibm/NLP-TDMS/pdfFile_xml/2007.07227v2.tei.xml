<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">István</forename><surname>Sárándi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Timm</forename><surname>Linder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
						</author>
						<title level="a" type="main">MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D human pose estimation</term>
					<term>absolute human pose</term>
					<term>scale estimation</term>
					<term>truncation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heatmap representations have formed the basis of human pose estimation systems for many years, and their extension to 3D has been a fruitful line of recent research. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and Z to metric depth around the subject. To obtain metric-scale predictions, 2.5D methods need a separate post-processing step to resolve scale ambiguity. Further, they cannot localize body joints outside the image boundaries, leading to incomplete estimates for truncated images. To address these limitations, we propose metric-scale truncation-robust (MeTRo) volumetric heatmaps, whose dimensions are all defined in metric 3D space, instead of being aligned with image space. This reinterpretation of heatmap dimensions allows us to directly estimate complete, metric-scale poses without test-time knowledge of distance or relying on anthropometric heuristics, such as bone lengths. To further demonstrate the utility our representation, we present a differentiable combination of our 3D metric-scale heatmaps with 2D image-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We find that supervision via absolute pose loss is crucial for accurate non-root-relative localization. Using a ResNet-50 backbone without further learned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP and MuPoTS-3D. Our code is publicly available. 1 Index Terms-3D human pose estimation, absolute human pose, scale estimation, truncation ! • I. Sárándi and B. Leibe are with RWTH</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMAN pose estimation is a long-standing computer vision problem with wide applicability in humanrobot interaction <ref type="bibr" target="#b0">[1]</ref>, virtual reality <ref type="bibr" target="#b1">[2]</ref>, medicine <ref type="bibr" target="#b2">[3]</ref> and commerce <ref type="bibr" target="#b3">[4]</ref>, among others. Since the adoption of deep convolutional neural networks (CNN), and especially heatmap representations, we have witnessed rapid progress in pose estimation research <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. A particularly challenging task is monocular 3D pose estimation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, where a person's anatomical landmarks are sought in 3D space, i.e., in millimeters, instead of pixels, given only a single image. Reconstructing 3D from images is one of the major goals of computer vision research, but several geometric ambiguities make this challenging. First, different 3D articulations may share the same 2D projection and second, there is an ambiguity between object size and distance, as small objects near the camera appear the same as large ones far away.</p><p>There is no clear consensus on the most effective way to represent and tackle these problems. Heatmap estimation is a promising direction, because it makes direct use of the convolutional structure of CNNs by turning the coordinate estimation problem into a binary classification problem of © 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 1 https://vision.rwth-aachen. <ref type="bibr">de</ref>  <ref type="figure">Fig. 1</ref>. By defining heatmaps in the 3D metric space around the person (top row) we directly estimate scale-correct complete poses. This is in contrast to prior work (bottom row) that defines the X and Y heatmap axes in image space and requires further post-processing to obtain a metric-scale skeleton. The three columns show that this heatmap representation is nearly invariant w.r.t. image zooming. A knee heatmap is shown along with the soft-argmax decoded skeleton.</p><p>whether the joint is located at the given position or not. To estimate 3D pose, a successful line of works extends 2D joint heatmaps with a depth axis, resulting in a 2.5D volumetric representation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[14]</ref>, <ref type="bibr" target="#b22">[15]</ref>, <ref type="bibr" target="#b23">[16]</ref>.</p><p>Finding heatmap maxima gives the estimated pixel coordinates and root-relative metric depths per joint (a 2.5D pose). While these estimates can be accurate, 2.5D representations do not address the challenging ambiguity between person size and distance. To bridge the gap between a 2.5D and a 3D pose, a separate scale recovery step is needed in post-processing. Explicit anthropometric heuristics have been proposed as scale cues, e.g. bone length priors <ref type="bibr" target="#b12">[13]</ref> or a skeleton length prior <ref type="bibr" target="#b24">[17]</ref>, computed by averaging over the training poses. However, these simple heuristics have difficulties when the experimental subjects have diverse heights. A further limitation is that 2.5D formulations are constrained to the estimation of joints that lie within the image boundaries. This is problematic in practical applications, where the image crop may not include the whole person, e.g. due to occlusions or detector noise. While one could use an additional module to fill in missing joints, it is desirable to learn the complete skeleton estimation in a single unified stage.</p><p>Our goal in this paper is to tackle scale and distance estimation of 3D poses in a truncation-robust, simple and efficient manner, while also keeping the structural advantages of fully-convolutional heatmap estimation, as opposed to numerical coordinate regression (i.e. encoding position by activation location instead of activation value).</p><p>To this end, we propose training a fully-convolutional network to output our novel metric-scale truncation-robust (MeTRo) heatmaps as illustrated in <ref type="figure">Fig. 1</ref>. All dimensions of these heatmaps are defined to have a fixed metric extent in meters. This is an unconventional task definition for fullyconvolutional networks (FCN). FCNs are predominantly applied for pixel-wise prediction tasks, such as semantic segmentation, where the input and output are pixel-topixel aligned. In our proposed approach, the input pixel positions and the output metric positions only satisfy a looser form of spatial correspondence. Nevertheless, we show that somewhat surprisingly, such a mapping can still be learned effectively by a standard modern FCN backbone.</p><p>By skipping the 2.5D stage, the backbone FCN has to implicitly reason about out-of-image joints, discover scale cues and learn the geometric perspective back-projection in an end-to-end manner. Our MeTRo heatmaps can naturally encode body parts outside the image, since the prediction volume's bounds do not correspond to the image bounds. As there is no need to design an explicit scale recovery step, the pipeline becomes simpler and requires neither the focal length nor the root joint distance to be known at test time for root-relative prediction.</p><p>Employing the differentiable soft-argmax <ref type="bibr" target="#b21">[14]</ref>, <ref type="bibr" target="#b23">[16]</ref>, <ref type="bibr" target="#b25">[18]</ref>, <ref type="bibr" target="#b26">[19]</ref> layer, our method becomes end-to-end learned all the way from image to final 3D metric-scale prediction as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Soft-argmax also allows rapid training with lowresolution heatmaps. Without any additional learned decoder module, we perform dense prediction with reduced strides at test time for higher quality results. We find that the details of the striding mechanism are crucial and propose a centered striding method that distributes the output neuron receptive fields evenly over the image.</p><p>This paper presents an extension of our own previous work <ref type="bibr" target="#b27">[20]</ref>. While in <ref type="bibr" target="#b27">[20]</ref> we only considered single-person root-relative pose, here we show that MeTRo heatmaps are also effective for absolute (non-root-relative) 3D pose estimation. In multi-person scenes it is especially important to estimate absolute poses, in order to recover the spatial layout of the whole group. We combine 3D metric-scale root-relative heatmaps with 2D image-space heatmaps in a two-headed CNN architecture, and subsequently reconstruct the absolute 3D root position in a differentiable manner. While prior approaches have tackled the root reconstruction problem, to our knowledge we are the first to backpropagate gradients through this reconstruction, allowing us to endto-end supervise the absolute pose task. We evaluate our network in a top-down fashion combined with an off-theshelf person detector. We refer to this combined approach as MeTRAbs.</p><p>Recent approaches have achieved good generalization performance to in-the-wild images by using abundant and diverse images with 2D pose labels in the training procedure besides 3D data <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[14]</ref>, <ref type="bibr" target="#b23">[16]</ref>. Applying such weak supervision is challenging in our representation, since the MeTRo heatmap would require supervision with metric ground truth instead of the image-space ground truth supplied with 2D datasets. We tackle this by proposing a scale and translation agnostic loss for 2D-annotated examples using an alignment layer. Note that in contrast to 2.5D heatmaps, this alignment is only used for loss computation during training, and still allows MeTRo to infer joints outside the image boundaries.</p><p>Experimentally, we achieve state-of-the-art results on the two largest single-person 3D pose benchmarks, Human3.6M and MPI-INF-3DHP, as well as the popular multi-person dataset MuPoTS-3D. To isolate the effect of the representation, we perform direct comparisons with 2.5D heatmap learning using bone-length-based scale recovery <ref type="bibr" target="#b12">[13]</ref>, under otherwise equal training conditions. We find that scale cues can indeed be learned implicitly in this fashion and MeTRo outperforms the baseline on most test sequences. Furthermore, our approach achieved first place in the 2020 ECCV 3D Poses in the Wild <ref type="bibr" target="#b28">[21]</ref> Challenge.</p><p>In summary, we make the following contributions: 1) We propose a novel 3D heatmap representation for pose estimation, called MeTRo, whose dimensions are defined in a fixed metric scale, irrespective of the input image scale. We achieve state-of-the-art results on Human3.6M and MPI-INF-3DHP and demonstrate strong truncation-robustness. 2) We propose centered striding, an improvement to the usual CNN striding logic, enabling higher accuracy at a coarse (8×8) heatmap resolution. 3) For absolute pose estimation, we extend the MeTRo approach to MeTRAbs, by also estimating 2D image-space heatmaps from the same backbone and reconstructing the absolute pose. We achieve state-of-theart results on the MuPoTS-3D and 3DPW multi-person benchmarks using MeTRAbs in a top-down paradigm. 4) To our knowledge, we are first to use a monocular geometrybased differentiable absolute pose reconstruction module to supervise the network with the final absolute ground truth fully end-to-end. We show that this is crucial for good distance estimation and extensively evaluate strong and weak perspective-based reconstruction variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>3D human pose estimation has had a long research history starting with hand-crafted features and part-based models <ref type="bibr" target="#b29">[22]</ref>. Similar to other computer vision problems, the transition to deep convolutional networks has led to a dramatic performance increase in this task. For a thorough overview, see the recent survey by Chen et al. <ref type="bibr" target="#b30">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep 3D Human Pose Estimation</head><p>Much of the inspiration in recent 3D pose estimator design has come from lessons learned in 2D pose research. DeepPose, the first neural 2D pose estimator <ref type="bibr" target="#b31">[24]</ref> directly regressed 2D joint coordinates on the RGB input via convolutional and fully-connected layers. Later, top-performing 2D methods have transitioned to predicting body joint heatmaps by fully-convolutional networks (e.g., <ref type="bibr" target="#b4">[5]</ref>) as an intermediate representation. These heatmaps are spatially discretized arrays (one for each joint), in which higher values indicate higher confidence that the particular joint is located at the corresponding position.</p><p>One line of 3D pose research builds on top of 2D heatmaps and infers the 3D pose from them by exemplar-matching <ref type="bibr" target="#b32">[25]</ref>, regression <ref type="bibr" target="#b7">[8]</ref> or probabilistic inference <ref type="bibr" target="#b33">[26]</ref>. One downside of such approaches is that the image content only indirectly influences the 3D estimation, as it acts on the result of the 2D estimation stage. Furthermore, 2D-to-3D lifting is performed in a numerical coordinate representation, which does not benefit from the built-in convolutional structure of CNNs.</p><p>Nibali et al. <ref type="bibr" target="#b11">[12]</ref> predict three marginal heatmaps per body joint, for the XY, XZ and YZ planes, respectively. Pavlakos et al. <ref type="bibr" target="#b12">[13]</ref> propose extending 2D heatmaps with a root-relative metric depth axis. One can obtain the 2D pixel positions and root-relative depths of the joints by finding maxima in the heatmaps.</p><p>One downside of heatmap representations has been the requirement of a dense output, which can become especially costly in 3D. The recently proposed soft-argmax <ref type="bibr" target="#b25">[18]</ref>, <ref type="bibr" target="#b26">[19]</ref>, <ref type="bibr" target="#b34">[27]</ref> a.k.a. integral regression <ref type="bibr" target="#b21">[14]</ref> method greatly alleviates this problem. As opposed to hard-argmax, which simply finds the location of the highest heatmap activation, softargmax is computed as the weighted average of all voxel grid coordinates, using softmaxed heatmap activations as the weights. For example, a low resolution heatmap can encode a joint position lying halfway between two bin centers by outputting 0.5 for both bins. By virtue of being differentiable, unlike hard-argmax, it also obviates the need for explicit heatmap-level supervision (e.g., voxel-wise cross-entropy). Instead, the loss can be computed (and its gradients backpropagated) from the coordinates yielded by soft-argmax.</p><p>Besides 2D heatmaps, Mehta et al. <ref type="bibr" target="#b8">[9]</ref> estimate three further output channels per joint, the so-called location maps. These are read out at the position of the corresponding heatmap's peak to obtain the X, Y and Z coordinates on a metric scale. Note how in their approach the final 3D joint coordinates are generated in the form of activation values (of the location maps at the heatmap peaks), as opposed to high-activation locations. We can thus think of it a conceptual hybrid of direct numerical coordinate regression and heatmap estimation. A downside of this method is that it requires high-resolution location maps and cannot benefit from the soft-argmax approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scale and Distance Estimation</head><p>It is well-known that projecting a 3D world onto a 2D image plane results in ambiguity between size and distance (depth). However, the end goal for 3D scene understanding and 3D human pose estimation in particular is a metricspace output at the true scale. The ambiguity can only be resolved using semantic scale cues, i.e. prior knowledge of the usual size of humans and other objects appearing in the scene. Unfortunately, not all papers describe how this step is performed. Some authors report their results assuming an already known ground-truth root joint distance and focal length <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[14]</ref>, <ref type="bibr" target="#b35">[28]</ref>, <ref type="bibr" target="#b36">[29]</ref>. A simple anthropometric approach is used by Pavlakos et al. <ref type="bibr" target="#b12">[13]</ref> Given 2D pixel positions and root relative depth estimates from volumetric heatmaps, they optimize the absolute person distance such that the backprojected skeleton's bone lengths match the average over the training set in a least squares sense, assuming a full perspective model. We use this scale recovery approach as our main baseline comparison throughout the paper, described in more detail in Sec. 5. Sun et al. <ref type="bibr" target="#b24">[17]</ref> employ a similar idea, but use the overall skeleton length and a weak perspective model instead. Methods that are not based on volumetric heatmaps <ref type="bibr" target="#b37">[30]</ref>, <ref type="bibr" target="#b38">[31]</ref> can directly predict the metric-scale numerical coordinates. Some recent works have shown that direct regression of person height from an image is a challenging task <ref type="bibr" target="#b39">[32]</ref>, <ref type="bibr" target="#b40">[33]</ref>.</p><p>While monocular 3D pose estimation methods are typically only evaluated in a root-relative manner, some works have also explicitly tackled the absolute (non-root-relative) pose estimation task, where every joint position is predicted within the 3D camera coordinate frame. This is closely related to the above-discussed metric-scale prediction: if both the image-space pose and the metric-scale root-relative pose are known, one can reconstruct the absolute distance (assuming a calibrated camera). Mehta et al. <ref type="bibr" target="#b41">[34]</ref> and Dabral et al. <ref type="bibr" target="#b42">[35]</ref> reconstruct the root offset by assuming a weak perspective model. Mehta et al. <ref type="bibr" target="#b43">[36]</ref> assume the foot touches the known ground plane in the first frame. Moon et al. <ref type="bibr" target="#b44">[37]</ref> predict the metric area of the human bounding box as a numerical value via a separate deep network (RootNet), besides their root-relative 2.5D PoseNet. In contrast to Moon et al., we estimate the scaled pose fully-convolutionally and do not require multiple separate backbones. In our earlier work <ref type="bibr" target="#b45">[38]</ref>, we estimate the distance directly from the image crop, but that does not generalize well to novel environments. Dabral et al. <ref type="bibr" target="#b42">[35]</ref> propose to estimate the focal length jointly with the distance, implicitly relying on the perspective distortion of people far from the optical axis. As the authors note, this cannot work well when the camera is turned directly towards the target person. Véges et al. <ref type="bibr" target="#b46">[39]</ref> make use of a monocular depth prediction network pretrained on various indoor and outdoor datasets to help with absolute person distance estimation. Finally, some recent works also consider the depth relations among people: Jiang et al. <ref type="bibr" target="#b47">[40]</ref> optimize the depth ordering by occlusion cues, while Fieraru et al. <ref type="bibr" target="#b48">[41]</ref> explicitly localize contact points between people to help with coherent reconstruction. In contrast, we perform our estimation for each person independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Truncated Pose Estimation</head><p>Single-person 3D pose estimation benchmarks, such as Hu-man3.6M <ref type="bibr" target="#b49">[42]</ref>, <ref type="bibr" target="#b50">[43]</ref>, assume that the whole person is visible in the input image. In practical applications, however, bounding boxes are obtained using imperfect detectors, which can result in body truncation, especially in high-occlusion scenes. A possible remedy could be extending the detection crops by amodal completion <ref type="bibr" target="#b51">[44]</ref>, but this would result in a loss of image resolution. Generally, pose estimation performance under truncation has not been studied extensively in the  Overview of our approach. We predict volumetric heatmaps using an off-the-shelf fully-convolutional backbone. Applying soft-argmax on these heatmaps and scaling by an image-independent constant factor yields joint coordinates in metric space up to translation. We minimize the root-relative L 1 loss. Focusing on simplicity, no learnable parameters are introduced outside the standard backbone, except for a single 1×1 convolution. Optionally, if absolute (non-root-relative) pose estimation is required, our MeTRAbs extension also estimates classic 2D image-space heatmaps via another 1×1 convolutional head. We then reconstruct the absolute pose through a differentiable reconstruction module. This is based on a linear least squares formulation derived from the pinhole camera model. Supervision is applied both at the outputs of the individual prediction heads and at the final combined output. ( * For 2D-labeled examples, the root-relative loss is replaced by a scale and translation-invariant 2D loss and the absolute 3D loss is not used.) literature. Recent work by Park et al. <ref type="bibr" target="#b52">[45]</ref> uses cropping data augmentation to improve 2D pose estimation. Vosoughi et al. create randomly truncated crops from Human3.6M images, and show that current methods perform poorly on truncated person images, even when only considering the present (within-boundary) joints <ref type="bibr" target="#b53">[46]</ref>. They tackle the problem using direct numerical coordinate regression, similar to early 2D pose estimation methods <ref type="bibr" target="#b31">[24]</ref>. We show that our approach performs significantly better in the truncated setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SINGLE-PERSON ROOT-RELATIVE APPROACH</head><p>In this section we present our proposed approach for metricscale root-relative 3D human pose estimation. The input is an RGB image crop I ∈ R w×h×3 depicting a person. The desired output is a 3D skeleton, consisting of J joint coordinates (∆X j , ∆Y j , ∆Z j ) T J j=1 in millimeters, up to arbitrary translation (hence the ∆ symbols).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metric-Space Volumetric Heatmap Representation</head><p>As is common in heatmap-based approaches, we apply a fully-convolutional backbone network, with effective stride s to produce an array with d · J spatial output channels. Here d is the number of discretization bins along the depth axis of the prediction volume. We then split the array along the channel axis into J volumes, each of shape (w/s) × (h/s) × d. 3D spatial softmax is applied over each of them, resulting in volumetric heatmap activations V (j) ∈ R (w/s)×(h/s)×d . Up to this point the process is similar to other volumetric heatmap approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[14]</ref>. The difference lies in how the heatmap axes are interpreted to yield metric-scale coordinates. In particular, the 3D joint coordinates are decoded using softargmax with fixed scaling factors:</p><formula xml:id="formula_0">  ∆X j ∆Y j ∆Z j   = p,q,r V (j) p,q,r ·   p · s/w · W q · s/h · H r · 1/d · D   ,<label>(1)</label></formula><p>where the p, q, r are 0-based integer indices into the volumetric heatmap array and W, H, D are the fixed metric width, height and depth extents of the full prediction volume. We set these extents as 2.2 meters in our work, which allows capturing people of usual height even when stretched out. Depending on striding logic (see Sec. 3.3), Eq. 1 needs to be adjusted slightly, e.g. the volume size may change with denser striding <ref type="figure" target="#fig_2">(Fig. 3)</ref>. The final root-relative prediction is obtained by subtracting the predicted root coordinates from all joint positions. Supervision is applied on these rootrelative coordinates. This means that the position of the root joint prediction within the volume is not explicitly supervised and the network can place the skeleton anywhere within the prediction volume. The gradients are backpropagated through the root-joint-subtraction operation. No camera calibration-based back-projection, nor bone or skeleton sizebased rescaling is needed for this root-relative prediction. The network is trained to perform these operations implicitly within the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>In contrast to prior work that employs decoders with upsampling layers and multiple refinement stages, we show that the task can be tackled in a significantly simpler fashion. Indeed, we apply the widely used ResNet-50 <ref type="bibr" target="#b54">[47]</ref> backbone to directly predict spatial heatmaps, without any additional learnable layers, such as transposed convolutions. By default, ResNet-50 has an effective stride of 32, resulting in heatmaps of spatial size 8×8 from the input image of size 256×256 during training. The depth of the volumetric heatmap is set to 8. When testing on single-person datasets, we apply the trained network with an effective stride of 4, to obtain heatmaps with spatial size 64, which is the typical size used in prior work <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[14]</ref>. This is called dense prediction and is commonly used in image segmentation <ref type="bibr" target="#b55">[48]</ref>. In this technique, striding is removed from a given number of convolutional layers and the dilation rate of subsequent convolutions is increased correspondingly. As we will see, normal striding centered striding dense prediction increases the compute requirements but also improves accuracy, while still allowing real-time execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Centered Striding</head><p>When changing striding density at test time compared to training time, it is important to consider how the distribution of heatmap receptive field centers is affected. The left side of <ref type="figure" target="#fig_2">Fig. 3</ref> shows a 256×256 image processed with training stride 32 (+) and test stride 16 (×). The coverage changes significantly between training and test and is not symmetric over the image. While not an issue for pixel-labeling tasks, soft-argmax is a weighted vote-averaging scheme and introducing new voting positions in an uneven manner skews the prediction result. To tackle this issue, we propose centered striding, where the striding logic in the last convolutional layer of the backbone is "reversed", such that it outputs the bottom right result per each 2×2 block. The result is a more evenly distributed coverage over the image, with each original sampling position replaced with four new ones equally spaced around it. This benefit is evaluated in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scale and Translation Agnostic 2D Loss</head><p>Similar to recent approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[14]</ref>, <ref type="bibr" target="#b23">[16]</ref>, we train simultaneously on 3D-labeled data from motion capture studios and 2D-labeled, in-the-wild data from the MPII dataset <ref type="bibr" target="#b56">[49]</ref>, to incorporate more appearance variation in the training process. Half of each mini-batch is filled with examples of either kind. Supervision via 2D labels is straightforward when using 2.5D heatmaps, as the X and Y heatmap axes correspond to the space in which the 2D labels are defined. However, since our prediction volume is defined on a metric scale and is not aligned with image space, we propose a 2D loss computation method that is invariant to prediction scale and translation. To this end, we first orthographically project the predicted 3D skeleton onto the image plane by discarding the Z coordinate. Then we align the projected prediction to the 2D pixel-scale ground truth by translation and uniform scaling to the least-squares optimal fit before computing the loss. This alignment layer is differentiable and gradients can be backpropagated through it. We note that a similar scale-invariant loss has been used by Rhodin et al. to enforce multi-view consistency of 3D poses <ref type="bibr" target="#b57">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Truncated Pose Estimation</head><p>Our metric-space heatmap representation decouples the image boundary from the heatmap boundary. This enables the prediction of joint locations outside the image frame without additional design effort, the network is simply trained to output complete poses at a metric scale, regardless of how the input image is scaled or cropped. To evaluate this aspect, we follow Vosoughi et al. <ref type="bibr" target="#b53">[46]</ref> by randomly cropping H3.6M inputs, keeping at least 1/4 of the area of the person bounding square. Examples of such crops are in the second row of <ref type="figure">Fig. 7</ref>. We consider two scenarios. In the first one, the above described sampling of truncated crops is only performed at test time. In the second case, such crops are used for training as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training</head><p>Prior work has shown that the L 1 loss is preferable in softargmax-based pose estimation <ref type="bibr" target="#b21">[14]</ref>. To balance the losses computed on 3D and 2D-annotated examples, we use a fixed weighting factor λ = 0.1 tuned on a separate validation set of Human3.6M, yielding the overall loss as</p><formula xml:id="formula_1">L = L ann3D + λL ann2D .<label>(2)</label></formula><p>We initialize the network with ImageNet-pretrained weights and use the Adam optimizer with weight decay <ref type="bibr" target="#b58">[51]</ref> and a batch size of 64. We decay the learning rate exponentially by an overall factor of 100, in two parts: from 10 −4 to 3.33 × 10 −5 over 25 epochs and from 3.33 × 10 −6 to 10 −6 in 2 final cooldown epochs.</p><p>As usual in deep learning, several sources of randomness influence the exact results of an experiment: random weight initialization, data shuffling, data augmentation and hardware-level non-determinism of execution order. We control these (except the last) by consistently seeding the random number generators. To distinguish random fluctuations from algorithmic differences, we repeat our experiments with 5 different seeds and report the mean and standard deviation of the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Intuition</head><p>As described above, our network is trained to output complete skeletons at the same metric scale, regardless of image zooming and truncation. To gain more intuition, we illustrate in <ref type="figure" target="#fig_3">Fig. 4</ref> how this fully-convolutional model is able to achieve approximately image-scale-and truncationinvariant predictions. In particular, we can see that the soft-argmax output is not necessarily in the middle of the heatmap's most prominent peak. As soft-argmax yields the heatmap's center of mass, even distant heatmap values have an influence. Intuitively, this allows the network to move the prediction result towards different heatmap locations by adding counter-balancing correction weights, for example at the image sides or at the person center. Regarding truncation, the last row shows that the model can infer that the arms must lie above waist level, as there is no visual evidence of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input crop</head><p>Right wrist Metric-scale heatmaps (XY projection, side=2.2 m) Left ankle Right knee them in the image. To understand how a fully-convolutional network can "know" where the truncation happens, we refer to Islam et al.'s paper <ref type="bibr" target="#b59">[52]</ref>, showing that even fullyconvolutional networks can encode positional information as a result of the zero-paddings within convolutional layers. This means that the location of the top image border can be used as a cue for the network to shift the full skeleton downwards inside the heatmap volume, such that it fits. Note that the network is free to place the skeleton anywhere within the volume, since the root prediction is subtracted before computing the root-relative loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTI-PERSON ABSOLUTE POSE APPROACH</head><p>In this section, we propose MeTRAbs, a combination of MeTRo 3D heatmap estimation presented in Sec. 3 with traditional 2D pose heatmaps in a single end-to-end trained network for absolute 3D pose estimation. The main idea is that the MeTRo approach implicitly estimates the scale, which we then use to infer the distance. By applying this method within a top-down paradigm (detection, cropping, pose estimation), we obtain a fast and accurate way to tackle multi-person absolute 3D pose estimation.</p><p>Using our approach from Sec. 3, we estimate a complete metric-scale pose (∆X j , ∆Y j , ∆Z j ) T J j=1 up to translation (where J is the number of joints).</p><p>By additionally estimating the 2D, image-space pose (x j , y j ) T J j=1 , we obtain all the necessary information to recover the absolute 3D pose in the (calibrated) camera coordinate system, as we will see in the following. For absolute pose estimation we assume known camera intrinsics, since monocular focal length estimation <ref type="bibr" target="#b51">[44]</ref>, <ref type="bibr" target="#b60">[53]</ref> is a very challenging task (c.f . the "dolly zoom" effect <ref type="bibr" target="#b61">[54]</ref>). However, note that our method does not require the intrinsic calibration for root-relative estimation.</p><p>The absolute pose can be expressed as</p><formula xml:id="formula_2">(X 0 + ∆X j , Y 0 + ∆Y j , Z 0 + ∆Z j ) T J j=1</formula><p>with (X 0 , Y 0 , Z 0 ) being the absolute pose offset, which we aim to recover. For this, we first calculate the normalized image coordinates as (x j ,ỹ j ) T = K −1 (x j , y j ) T , where K is the intrinsic matrix.</p><p>Mehta et al. <ref type="bibr" target="#b41">[34]</ref> derive a formula to reconstruct the absolute root position using the weak perspective projection model. Véges et al. <ref type="bibr" target="#b46">[39]</ref>, while still operating in the weak perspective model, note that an approximation step involved in Mehta et al.'s algorithm leads to worse performance. Motivated by this, we derive a reconstruction method under the full perspective pinhole camera model and extensively compare it with Mehta et al.'s weak perspective method. In a full perspective model, a perfect estimate would satisfy</p><formula xml:id="formula_3">x j y j = (X 0 + ∆X j )/(Z 0 + ∆Z j ) (Y 0 + ∆Y j )/(Z 0 + ∆Z j ) ,<label>(3)</label></formula><p>which can be rearranged to</p><formula xml:id="formula_4">X 0 −x j Z 0 Y 0 −ỹ j Z 0 = x j ∆Z j − ∆X j y j ∆Z j − ∆Y j .<label>(4)</label></formula><p>Considering all joints, we obtain 2J linear equations in the three variables (X 0 , Y 0 , Z 0 ). Sincex,ỹ, X, Y and Z are estimates, the equation system is noisy and overdetermined. Hence we opt to solve it by linear least squares, with TensorFlow's differentiable solver based on Cholesky decomposition. This differentiability allows us to directly supervise the network with a loss L abs3D computed on the final absolute 3D pose output, which turns out to be crucial for accurate distance estimation.</p><p>For truncated images, Eq. 3 only holds for body joints inside the image frame, since the 2D heatmap method cannot estimate out-of-image joint locations. We therefore exclude joints from the optimization, which are predicted to lie closer to the image border than one stride length. After reconstructing the root joint position, we can obtain the absolute pose in two ways. Either as (∆X j + X 0 , ∆Y j + Y 0 , ∆Z j + Z 0 ) T (adding the reconstructed offset to the 3D head's root-relative output), or as (x j ,ỹ j , 1) T · (∆Z j + Z 0 ) (back-projecting the 2D head's output). For joints that lie within the image, we use the latter option, while for truncated ones we use the former. Both the individual prediction heads and the final absolute output are supervised with the L 1 loss. As in the root-relative MeTRo network, we apply weak supervision from 2D-labeled data for MeTRAbs as well, on both heads. Extending Eq. 2, the loss becomes </p><formula xml:id="formula_5">L =L abs3D ann3D + L head3D ann3D + L head2D ann3D + λ L head2D ann2D + L head3D ann2D ,<label>(5)</label></formula><p>where we again set λ = 0.1.</p><p>We found that the absolute loss can introduce numerical instabilities very early during training, since at this point the two prediction heads do not yet produce sufficiently compatible outputs, making the reconstruction problem illconditioned. Hence, we only turn on the absolute loss after 5000 update steps.</p><p>In a multi-person scenario, inference speed becomes a priority, since the model is evaluated on each person detection separately. To retain real-time performance, we do not apply dense prediction with MeTRAbs; the network is trained and tested with coarse, 8×8 heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">2.5D BASELINE</head><p>For comparison, we implement a 2.5D baseline derived from Pavlakos et al.'s work <ref type="bibr" target="#b12">[13]</ref>, which introduced volumetric heatmaps for 3D human pose estimation. Pavlakos et al. use a coarse-to-fine estimation scheme with a stacked hourglass architecture <ref type="bibr" target="#b4">[5]</ref> and no soft-argmax. To make the baseline directly comparable to our results, we instead use the architecture depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. This baseline directly estimates 2.5D heatmaps through a 1×1 convolution at the end of the backbone. We then use soft-argmax, and compute the L 1 loss on the resulting coordinates. This makes the baseline similar to Sun et al. <ref type="bibr" target="#b21">[14]</ref>, except Sun et al. used additional learned layers and did not perform scale recovery. As a testtime post-processing step, the baseline uses the bone-length optimization method from Pavlakos et al. <ref type="bibr" target="#b62">[55]</ref> to recover the root joint depth, which we briefly reiterate here. Given an assumed value for the root joint depth Z 0 and known camera intrinsics, the 2.5D pose can be back-projected into metric space and each bone's resulting length b i (Z 0 ) can be calculated. The optimal Z 0 is then the one that minimizes the squared bone length discrepancy, as compared to the average training bone lengths t i :</p><formula xml:id="formula_6">Z * 0 = arg min Z0 i∈bones (b i (Z 0 ) − t i ) 2 ,<label>(6)</label></formula><p>where we only use bones, whose both ends are predicted to lie within the image (further from the border than 1 stride length). This is a convex, nonlinear least-squares problem, and we solve it using the Levenberg-Marquardt algorithm initialized at Z 0 = 2 m. To reiterate, as in <ref type="bibr" target="#b12">[13]</ref>, the absolute pose is not supervised during the baseline's training and the convex optimization of Z 0 is not backpropagated through, for simplicity. We note, however, that the recent development of differentiable convex optimization layers <ref type="bibr" target="#b63">[56]</ref>, <ref type="bibr" target="#b64">[57]</ref> could, in principle, enable such a solution as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DATASETS, PREPROCESSING, EVALUATION</head><p>We conduct our single-person experiments on the largest available benchmarks: Human3.6M (shortened as H3.6M) <ref type="bibr" target="#b49">[42]</ref>, <ref type="bibr" target="#b50">[43]</ref> and MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b41">[34]</ref>. The extended approach described in Sec. 4 is evaluated in a multi-person context by training on MuCo-3DHP (MuCo) and testing on MuPoTS-3D (MuPoTS). H3.6M <ref type="bibr" target="#b49">[42]</ref>, <ref type="bibr" target="#b50">[43]</ref> was captured with 4 cameras in a motion capture studio. Two evaluation protocols are in wide use. In Protocol 1, the training subjects are 1, 5, 6, 7, 8, while 9 and 11 are used for testing. Prediction and ground-truth are aligned at the root joint, but no Procrustes alignment is performed. In Protocol 2, subjects 1, 5, 6, 7, 8, 9 are used in training and 11 in evaluation, with Procrustes alignment between prediction and ground truth. Every 64 th frame is evaluated. We use the provided bounding boxes. We downsample videos from 50 to 10 fps. To further reduce redundancy, training frames are only used if at least one body joint moves at least 100 mm since the previous kept frame.</p><p>MPII <ref type="bibr" target="#b56">[49]</ref> is a 2D-labeled dataset with 25k training images. We use this dataset for weak supervision, following the idea of Zhou et al. <ref type="bibr" target="#b9">[10]</ref>. Only arm and leg joints are used from MPII, as we found these to be the most consistently labeled across datasets. In single-person experiments we only use instances explicitly marked as "well-separated" from other people and take the provided person centers and sizes as the center and side length of the bounding box. In multiperson experiments, we use all person instances and the boxes are obtained with YOLOv3 <ref type="bibr" target="#b65">[58]</ref>.</p><p>3DHP <ref type="bibr" target="#b41">[34]</ref> shows 8 training subjects in a green-screen studio. Test frames come from 3 scenes, each with 2 subjects: green-screen studio, studio without green screen, and outdoor. The latter two make this benchmark more challenging than H3.6M. In this dataset, the hips are labeled closer to the legs than in MPII. Following <ref type="bibr" target="#b9">[10]</ref>, we move the hips towards the neck by a fifth of the pelvis-neck vector before comparing with MPII-annotated skeletons for 2D loss computation. 3DHP provides two ground truth variants: unnormalized metric-space poses and "universal" (height-normalized) ones. We evaluate on both. We use only the chest-height cameras as <ref type="bibr" target="#b41">[34]</ref>, and only examples where all joints are within the image. We generate 3DHP bounding boxes by combining the bounding box of labeled joints and the most confident person detection of YOLOv3. The same frame sampling strategy is used as described above for H3.6M. Since its publication, the official 3DHP ground truth has been changed twice, making not all published results comparable. In our experience the first update changes scores by 1-3%, while the second one only by 0.1%, which is within experimental fluctuation, making the two latest versions comparable.</p><p>MuCo <ref type="bibr" target="#b38">[31]</ref> is a synthetically composited multi-person dataset, derived from 3DHP by pasting persons over each other based on their root joint depth order. As <ref type="bibr" target="#b46">[39]</ref>, we generate 150k training images, each with 4 people. We run YOLOv3 on these images to get realistic bounding boxes.</p><p>MuPoTS <ref type="bibr" target="#b38">[31]</ref> is a mixed indoor and outdoor multiperson test set, compatible with MuCo, consisting of 20 sequences showing people performing various actions and interactions. Like 3DHP, MuPoTS also provides normalized and unnormalized skeletons.</p><p>We crop images to the person's bounding square and resize it to 256 × 256 px. Perspective effects must be taken into account when centering the image on the subject as this induces an implicit camera rotation <ref type="bibr" target="#b41">[34]</ref>. We compensate by transforming both the input image and the predicted pose according to the implied camera rotation. The indoor 3DHP and MuPoTS sequences are gamma-corrected with an exponent of 0.67.</p><p>We apply geometric augmentations (scaling, rotation, translation, horizontal flip) and color distortion (brightness, contrast, hue, saturation). For single-person datasets, synthetic occlusion is added with 70% probability, half of which are rectangles with uniform white noise as in <ref type="bibr" target="#b77">[70]</ref>, half are segmented non-person objects from Pascal VOC <ref type="bibr" target="#b78">[71]</ref> as in <ref type="bibr" target="#b45">[38]</ref>, <ref type="bibr" target="#b79">[72]</ref>. On MuCo, synthetic occlusion probability is reduced to 30% since some occlusion is already introduced from compositing person segments over each other. On 3DHP and MuCo, we also augment the background with 70% probability following <ref type="bibr" target="#b41">[34]</ref>. Backgrounds are taken from IN-RIA Holidays <ref type="bibr" target="#b80">[73]</ref>, excluding person images. All evaluation is done on a single crop, with no test-time augmentation.</p><p>We use the standard evaluation measures. The main one on 3DHP and MuPoTS is the percentage of correct keypoints (PCK), i.e. the fraction of joints predicted within 150 mm of the ground truth. The AUC is the area under the PCK curve as the threshold ranges from 0 to 150 mm. The measure on H3.6M is the mean per joint position error (MPJPE). 3DHP and MuPoTS evaluate 14 joints, excluding the root, while H3.6M uses 17, including the root. The official MuPoTS evaluation script rescales the bone lengths of the prediction to match the ground truth bone lengths before computing metrics, leading to some confusion and inconsistency between reported results. In <ref type="bibr" target="#b38">[31]</ref> rescaling was only used for evaluating LCR-Net <ref type="bibr" target="#b81">[74]</ref>, but it has since been adopted by other authors as well. For consistency and simplicity, we train MeTRAbs only with unnormalized skeletons. When evaluating on universal (normalized) skeletons, we use bone rescaling. On unnormalized skeletons, we do not use bone rescaling, in order to directly evaluate the raw metric-space outputs of the methods. Following Véges et al. <ref type="bibr" target="#b82">[75]</ref>, on MuPoTS we also evaluate absolute (i.e. non-root-relative) versions of these metrics, prefixed with "A-", e.g. A-PCK. For absolute MPJPE, Véges et al. <ref type="bibr" target="#b46">[39]</ref>, <ref type="bibr" target="#b82">[75]</ref> evaluate all 17 joints and 16 (no pelvis) for relative MPJPE (but use 14 for PCK and A-PCK). For consistency, we always use 14 joints on MuPoTS, except when marked otherwise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>On H3.6M without ground truth depth or scale information, we achieve 49.3 mm MPJPE, which is within the margin of error compared to the state-of-the-art by Xu et al. <ref type="bibr" target="#b72">[65]</ref> (49.2 mm), while using a considerably simpler approach (see Tab. 1). (In all tables, the number after "±" is the standard deviation of 5 repeated experiments with different random seeds, therefore the standard error of the mean is a fifth of this value.) Besides simplifying the prediction pipeline and allowing for truncation-robust prediction (see below), our metric heatmap representation also performs better than the 2.5D baseline with bone-length-based scale recovery under the exact same experimental conditions. Tab. 7 shows that training data augmentations improve performance by a large margin. On Protocol 2 (Tab. 2), the benefit of our method is masked by the use of Procrustes alignment, which explicitly ignores the quality of scale recovery. It is therefore unsurprising that our method performs about equally well as the 2.5D variant. On 3DHP, our method outperforms prior work by a large margin, including ones trained on more datasets as well (Tab. 3). Both with universal (height-normalized) skeletons and true metric-scale ones, the MeTRo representation outperforms the baseline on green-screen studio images, however, the outdoor scenes were recorded on an empty field without scale cues and the explicit bone-length-based scale recovery performs better there. Qualitative results are in <ref type="figure">Fig. 7</ref>.</p><p>We analyze scale recovery in more detail (Tab. 4). As expected, the idealized method with test-time access to the ground truth root joint depth performs best on both H3.6M and 3DHP. The proposed approach performs better than the 2.5D baseline using average bone lengths on H3.6M and comparably on 3DHP. On H3.6M, MeTRo closes most of this scale recovery gap between the 2.5D average bone length baseline and the idealized variant using the true root. Interestingly, our approach outperforms even the 2.5D variant using ground truth bone lengths for each test frame. On 3DHP, MeTRo's scale recovery performance is similar to the 2.5D baseline (equal PCK, better AUC, slightly worse MPJPE). Further, on this dataset, access to ground truth scale  information provides a larger improvement than on H3.6M, highlighting the importance of testing on many subjects.</p><p>When tested on truncated crops, our method by far outperforms prior approaches (Tab. 5). This is true even for our default training configuration, but performance improves substantially when training on truncated images as well. The method is robust to truncation of up to 7 or 8 joints (of the 17) before overall performance substantially degrades ( <ref type="figure" target="#fig_5">Fig. 6</ref>). Given the obvious ambiguity introduced by truncation, it is noteworthy that even truncated joints can be estimated with as little as about 100 mm average error. Qualitative examples are in the second row of <ref type="figure">Fig. 7</ref>, showing that our method can handle strongly truncated cases as well.</p><p>On the multi-person MuPoTS-3D, our MeTRAbs approach yields state-of-the-art results. For height-normalized skeletons with bone rescaling (standard setting in prior work, Tab. 11), MeTRAbs outperforms the 2.5D baseline, and the baseline already reaches state-of-the-art results. Our method performs particularly well on test sequence 2, with heavy occlusions (e.g. <ref type="figure" target="#fig_6">Fig. 8, left)</ref>. Removing the supervision with the absolute 3D loss worsens the absolute PCK of all poses from 38.4% to 35.0%. Surprisingly, the root-relative accuracy seems to improve when turning off the absolute loss. This is, however, hard to interpret, as Tab. 11 shows an artificial evaluation setting with normalized-height skeletons and bonerescaling, thereby removing some of the scale recovery aspect from the evaluation. When evaluating on unnormalized skeletons without bone rescaling (Tab. 8), it becomes clear that the absolute loss helps: absolute MPJPE improves from 328.8 mm to 248.2 mm, absolute PCK from 36.7% to 40.2%, with the root-relative metrics slightly improving as well. These are state-of-the-art results and improve over methods that are pre-or jointly trained on ground truth pixel-wise depth prediction datasets <ref type="bibr" target="#b46">[39]</ref>, <ref type="bibr" target="#b82">[75]</ref>. Further, we can see that the absolute PCK score has high variance and therefore small differences are not necessarily meaningful. The standard deviation across 5 repeat experiments is around 1.4-3.2%, and the absolute results for individual test sequences varies strongly as well across different configurations. This is because the test examples are strongly correlated since they come from video sequences. Lastly, we note that the detection rate is essentially the same for all of our configurations (Tab. 8), since we use the same detections, and the official evaluation script performs matching based on the 2D projection, which is very similar across these methods.</p><p>In <ref type="table" target="#tab_8">Table 9</ref> we evaluate whether using the full perspective pinhole camera model in the absolute pose reconstruction module brings benefits. In the last two rows, the absolute loss is not used at training time. In the other cases we backpropagate the absolute loss gradients either through the weak or full perspective reconstruction method. We find that training on MuCo with the full perspective model improves the absolute results, but when testing on MuPoTS, it is better to use the weak model. This may be explained by the fact that people in the MuCo dataset are closer to the camera than in MuPoTS, resulting in stronger perspective effects in MuCo. To verify this, we computed the ratio of the farthest and closest joint's depth max j Z j / min j Z j per pose. If this ratio is large, the weak perspective assumption is a bad approximation. The median and the 90th percentile of this  Another possible reason is that the model may output slightly perspective distorted results in the metric 3D head, which are better handled by the weak-perspective model in the next step, as opposed to training time, when the network learns to output the correct metric, perspectiveundistorted pose, for which the full perspective model works better afterwards. Nevertheless, as there is no clear overall winner between the weak and full perspective models, and changing the method across training and test is clearly not desirable, we use the more commonly applied weak perspective method for all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Inference Speed</head><p>Our method is capable of real-time inference. The rootrelative architecture can process 511 crops per second on    per frame, respectively. The above calculations assume the image crops are available instantly and the time cost of detection is excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">ECCV 3DPW Challenge</head><p>Finally, we note that our MeTRAbs method has won the 3D Poses in the Wild (3DPW) <ref type="bibr" target="#b28">[21]</ref> challenge, organized as a workshop event at the European Conference on Computer Vision, 2020. Tab. 10 compares results using the 3DPW protocol. For this, we train our network on a combination of the H3.6M, MuCo, SURREAL <ref type="bibr" target="#b88">[81]</ref>, SAIL-VOS <ref type="bibr" target="#b89">[82]</ref> and CMU-Panoptic <ref type="bibr" target="#b90">[83]</ref> datasets. We use ResNet-101 as the backbone and additionally apply upper body crop (truncation) augmentation at training time and 5-crop averaging at test time. When identity tracking is needed, we perform frame-toframe matching based on absolute pose distance. The listed methods are not directly comparable due to different training data. Even with this caveat, our top results show that our approach can scale with further training data and performs well even in challenging in-the-wild scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We proposed metric-scale truncation-robust (MeTRo) volumetric heatmaps for the tasks of root-relative and absolute 3D human pose estimation. These heatmaps directly represent the metric space around the person instead of being tied to the image space and can be predicted with any standard fullyconvolutional network. With a modified weak supervision scheme for 2D labels, careful stride alignment considerations and strong data augmentation, we achieved state-of-theart results on two important single-person benchmarks: Human3.6M and MPI-INF-3DHP. We showed that our approach can implicitly discover scale cues from the data, given its superior performance compared to a previous, fixed bone length based heuristic on most test scenarios. Future research should consider possibilities for learning similar scale cues from large-scale outdoor data as well. Another interesting future direction can be the evaluation on people with widely differing heights, if such data becomes available on a large scale. Further, we demonstrated the second benefit of the MeTRo representation, the prediction ("hallucination") of complete skeletons even when only a part of the body is contained in the image. For the multi-person absolute 3D pose estimation scenario, we developed a combination of MeTRo heatmaps with 2D heatmap prediction, referred to as MeTRAbs. We saw the importance of supervising the absolute pose prediction end-to-end by employing a differentiable combination of 2D and root-relative 3D poses. For this we tested two alternatives, based on weak and full perspective geometry, but neither performed clearly better than the other in our experiments. Applying MeTRAbs in the top-down multi-person paradigm, we achieved stateof-the-art results on the challenging MuPoTS-3D dataset while keeping the method real-time capable. Overall we can conclude that heatmap estimation is a versatile paradigm and it is possible to tackle absolute 3D human pose estimation through exclusively estimating heatmaps and encoding all quantities such as coordinates or sizes as activation locations, instead of as activation values. Comparison to prior work on the MuPoTS-3D benchmark for normalized skeletons with bone rescaling to the ground truth before computing the percentage of correct keypoints (PCK). (For the direct evaluation of the metric-space poses, see Tab. 8). S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg↑ H36M Ca me ra H3.6M (partial body)</p><p>Ca me ra</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DHP</head><p>Ca me ra MPII Ca me ra <ref type="figure">Fig. 7</ref>. Qualitative results on various datasets. Predictions are shown in color, ground truth in gray (except for MPII, where it is unavailable). Green spheres mark predictions within 150 mm of the ground truth, red cubes beyond that threshold. Note that our method performs well on truncated (partial body) images as well (second row). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our approach. We predict volumetric heatmaps using an off-the-shelf fully-convolutional backbone. Applying soft-argmax on these heatmaps and scaling by an image-independent constant factor yields joint coordinates in metric space up to translation. We minimize the root-relative L 1 loss. Focusing on simplicity, no learnable parameters are introduced outside the standard backbone, except for a single 1×1 convolution. Optionally, if absolute (non-root-relative) pose estimation is required, our MeTRAbs extension also estimates classic 2D image-space heatmaps via another 1×1 convolutional head. We then reconstruct the absolute pose through a differentiable reconstruction module. This is based on a linear least squares formulation derived from the pinhole camera model. Supervision is applied both at the outputs of the individual prediction heads and at the final combined output. ( * For 2D-labeled examples, the root-relative loss is replaced by a scale and translation-invariant 2D loss and the absolute 3D loss is not used.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Receptive field centers of the output neurons in a strided FCN operating on a 256×256 image (+: stride 32, ×: stride 16). left: normal striding logic, where the top left result is kept per 2×2 block. Note that denser striding skews the sample density towards the bottom and right in the border areas. right: by reversing the stride logic in the last strided layer (i.e., bottom right result taken, instead of top left), the samples are centered and the increased striding density is distributed evenly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>A closer look at scale and truncation robustness. We plot the projected metric-scale heatmaps for 3 joints with the full soft-argmax skeleton for reference. The predicted skeleton is approximately invariant to change in scale and truncation. Since the metric size of the person does not change with image scaling, the backbone learns to output heatmaps with a similar center of mass, regardless of image scale. Note that the heatmaps do not align with image space and this is intended by design. (The broad peaks are a result of training the model at low, 8×8 heatmap resolution.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Baseline architecture with 2.5D heatmaps for ablative comparison experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Analysis of robustness to truncation on H3.6M. Average performance remains relatively stable up to 7 truncated joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results on MuPoTS-3D (prediction in blue-yellow, ground truth in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>/metrabs</figDesc><table><row><cell>MeTRo heatmap</cell><cell>(proposed)</cell><cell>Zoom 0.5x</cell><cell>2.2 m Zoom 1x</cell><cell>2.2 m 2.2 m</cell><cell>Zoom 2x (truncation)</cell><cell>Ca me ra</cell></row><row><cell>2.5D heatmap</cell><cell>(baseline)</cell><cell></cell><cell>2.2 m</cell><cell>256 px 256 px</cell><cell></cell><cell>Ca me ra</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Evaluation on H3.6M Protocol 1 (subjects 9 and 11), using mean per joint position error (MPJPE) without Procrustes alignment. All methods use extra 2D-labeled pose data in training.Methods using ground-truth scale or depth information at test time</figDesc><table><row><cell></cell><cell cols="2">Dir. Dis.</cell><cell>Eat</cell><cell cols="4">Gre. Phn. Pose Pur.</cell><cell>Sit</cell><cell>SitD</cell><cell cols="6">Sm. Pht. Wait Walk WD WT</cell><cell>Avg ↓</cell></row><row><cell>Sun et al. [59]</cell><cell cols="3">52.8 54.8 54.2</cell><cell>54.3</cell><cell>61.8</cell><cell cols="3">53.1 53.6 71.7</cell><cell cols="3">86.7 61.5 67.2</cell><cell>53.4</cell><cell cols="3">47.1 61.6 53.4</cell><cell>59.1</cell></row><row><cell>Nibali et al. [12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.0</cell></row><row><cell>Luvizon et al. [16]</cell><cell cols="3">51.5 53.4 49.0</cell><cell>52.5</cell><cell>53.9</cell><cell cols="3">50.3 54.4 63.6</cell><cell cols="3">73.5 55.3 61.9</cell><cell>50.1</cell><cell cols="3">46.0 60.2 51.0</cell><cell>55.1</cell></row><row><cell>Luvizon et al. [60]</cell><cell cols="3">43.7 48.8 45.6</cell><cell>46.2</cell><cell>49.3</cell><cell cols="3">43.5 46.0 56.8</cell><cell cols="3">67.8 50.5 57.9</cell><cell>43.4</cell><cell cols="3">40.5 53.2 45.6</cell><cell>49.5</cell></row><row><cell>Sun et al. [14]</cell><cell cols="3">47.5 47.7 49.5</cell><cell>50.2</cell><cell>51.4</cell><cell cols="3">43.8 46.4 58.9</cell><cell cols="3">65.7 49.4 55.8</cell><cell>47.8</cell><cell cols="3">38.9 49.0 43.8</cell><cell>49.6</cell></row><row><cell>Chen et al. [29]</cell><cell cols="3">45.3 49.8 46.1</cell><cell>49.6</cell><cell>48.2</cell><cell cols="3">41.7 47.4 53.1</cell><cell cols="3">55.2 48.0 57.7</cell><cell>45.6</cell><cell cols="3">40.8 52.4 45.2</cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Methods using no ground truth scale or depth information at test time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Pavlakos et al. [13] 67.4 72.0 66.7</cell><cell>69.1</cell><cell>72.0</cell><cell cols="3">77.0 65.0 68.3</cell><cell cols="3">83.7 96.5 71.7</cell><cell>65.8</cell><cell cols="3">74.9 59.1 63.2</cell><cell>71.9</cell></row><row><cell>Zhou et al. [10]</cell><cell cols="3">54.8 60.7 58.2</cell><cell>71.4</cell><cell>62.0</cell><cell cols="6">53.8 55.6 75.2 111.6 64.2 65.5</cell><cell>66.0</cell><cell cols="3">51.4 63.2 55.3</cell><cell>64.9</cell></row><row><cell>Martinez et al. [8]</cell><cell cols="3">51.8 56.2 58.1</cell><cell>59.0</cell><cell>69.5</cell><cell cols="3">55.2 58.1 74.0</cell><cell cols="3">94.6 62.3 78.4</cell><cell>59.1</cell><cell cols="3">49.5 65.1 52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [61]</cell><cell cols="3">50.1 54.3 57.0</cell><cell>57.1</cell><cell>66.6</cell><cell cols="3">53.4 55.7 72.8</cell><cell cols="3">88.6 60.3 73.3</cell><cell>57.7</cell><cell cols="3">47.5 62.7 50.6</cell><cell>60.4</cell></row><row><cell>Yang et al. [62]</cell><cell cols="3">51.5 58.9 50.4</cell><cell>57.0</cell><cell>62.1</cell><cell cols="3">49.8 52.7 69.2</cell><cell cols="3">85.2 57.4 65.4</cell><cell>58.4</cell><cell cols="3">43.6 60.1 47.7</cell><cell>58.6</cell></row><row><cell cols="4">Pavlakos et al. [63] 48.5 54.4 54.4</cell><cell>52.0</cell><cell>59.4</cell><cell cols="3">49.9 52.9 65.8</cell><cell cols="3">71.1 56.6 65.3</cell><cell>52.9</cell><cell cols="3">44.7 60.9 47.8</cell><cell>56.2</cell></row><row><cell>Liu et al. [64]</cell><cell cols="3">47.0 53.1 50.3</cell><cell>48.8</cell><cell>56.0</cell><cell cols="3">48.1 47.6 65.9</cell><cell cols="3">72.6 52.3 61.4</cell><cell>49.1</cell><cell cols="3">39.3 54.2 40.6</cell><cell>52.4</cell></row><row><cell>Xu et al. [65]</cell><cell cols="3">40.6 47.1 45.7</cell><cell>46.6</cell><cell>50.7</cell><cell cols="3">45.0 47.7 56.3</cell><cell cols="3">63.9 49.4 63.1</cell><cell>46.5</cell><cell cols="3">38.1 51.9 42.3</cell><cell>49.2</cell></row><row><cell>Sharma et al. [66]</cell><cell cols="3">48.6 54.5 54.2</cell><cell>55.7</cell><cell>62.6</cell><cell cols="3">50.5 54.3 70.0</cell><cell cols="3">78.3 58.1 72.0</cell><cell>55.4</cell><cell cols="3">45.2 61.4 49.7</cell><cell>58.0</cell></row><row><cell>Cai et al. [67]</cell><cell cols="3">46.5 48.8 47.6</cell><cell>50.9</cell><cell>52.9</cell><cell cols="3">48.3 45.8 59.2</cell><cell cols="3">64.4 51.2 61.3</cell><cell>48.4</cell><cell cols="3">39.2 53.5 41.2</cell><cell>50.6</cell></row><row><cell>2.5D baseline</cell><cell cols="3">45.1 50.4 45.4</cell><cell>47.8</cell><cell>50.0</cell><cell cols="3">44.6 49.8 59.0</cell><cell cols="3">69.4 49.4 56.5</cell><cell>48.0</cell><cell cols="4">39.6 49.4 45.0 50.2±0.3</cell></row><row><cell>MeTRo (ours)</cell><cell cols="3">46.3 48.3 43.3</cell><cell>48.2</cell><cell>50.2</cell><cell cols="3">45.1 46.1 56.2</cell><cell cols="3">66.8 49.3 54.5</cell><cell>46.7</cell><cell cols="4">40.1 49.6 46.2 49.3±0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison of MPJPE with prior work on H3.6M under Protocol 2 (test subject 11 with Procrustes alignment to the ground truth).</figDesc><table><row><cell cols="11">Nie [68] Pavlakos [13] Sun [59] Martinez [8] Sun [14] Nibali [12] Habibie [69] Xu [65] Chen [29] 2.5D baseline MeTRo (ours)</cell></row><row><cell>P-MPJPE 79.5</cell><cell>51.9</cell><cell>48.3</cell><cell>47.7</cell><cell>40.6</cell><cell>40.4</cell><cell>49.2</cell><cell>38.9</cell><cell>33.7</cell><cell>34.5±0.4</cell><cell>34.7±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Comparison on MPI-INF-3DHP with prior methods. * Evaluated with the first version of the dataset, with some annotation difference. Dashes (-) reflect a lack of published information. Superscripts indicate the training data (first characters of 3DHP, H36M, MPII, LSP and COCO).</figDesc><table><row><cell></cell><cell>Stand/ walk</cell><cell>Exer-cise</cell><cell>Sit on chair</cell><cell>Cro./ reach</cell><cell>On floor</cell><cell cols="2">Sport Misc.</cell><cell>Green screen</cell><cell>No gr.sc.</cell><cell>Out-door</cell><cell></cell><cell>Total</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PCK↑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCK↑</cell><cell>AUC↑</cell><cell>MPJPE↓</cell></row><row><cell></cell><cell></cell><cell cols="8">Universal, height-normalized skeletons (simplified scale recovery task)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rogez et al. [74]  *</cell><cell>70.5</cell><cell>56.3</cell><cell>58.5</cell><cell>69.4</cell><cell>39.6</cell><cell>57.7</cell><cell>57.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.7</cell><cell>27.6</cell><cell>158.4</cell></row><row><cell>Zhou et al. H+M [10]  *</cell><cell>85.4</cell><cell>71.0</cell><cell>60.7</cell><cell>71.4</cell><cell>37.8</cell><cell>70.9</cell><cell>74.4</cell><cell>71.7</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell><cell>137.1</cell></row><row><cell>Zhou et al. H+M [76]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.6</cell><cell>71.3</cell><cell>80.3</cell><cell>75.3</cell><cell>38.0</cell><cell>-</cell></row><row><cell>Mehta et al. 3+M+L+H [9]  *</cell><cell>87.7</cell><cell>77.4</cell><cell>74.7</cell><cell>72.9</cell><cell>51.3</cell><cell>83.3</cell><cell>80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell>Mehta et al. 3+M+L+H [34]  *</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>84.6</cell><cell>72.4</cell><cell>69.7</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell>Mehta et al. 3+M+L+C [31]  *</cell><cell>83.8</cell><cell>75.0</cell><cell>77.8</cell><cell>77.5</cell><cell>55.1</cell><cell>80.4</cell><cell>72.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.2</cell><cell>37.8</cell><cell>122.2</cell></row><row><cell>Luo et al. 3+M+H [11], [77]</cell><cell>95.5</cell><cell>82.3</cell><cell>89.9</cell><cell>84.6</cell><cell>66.5</cell><cell>92.0</cell><cell>93.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.3</cell><cell>47.5</cell><cell>84.5</cell></row><row><cell>Nibali et al. 3+M [12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.6</cell><cell>48.8</cell><cell>87.6</cell></row><row><cell>2.5D baseline 3+M</cell><cell>95.1</cell><cell>90.7</cell><cell>86.8</cell><cell>92.4</cell><cell>74.2</cell><cell>94.1</cell><cell>91.7</cell><cell>92.1</cell><cell>89.0</cell><cell cols="4">87.7 89.9±0.2 52.8±0.4 79.7±0.6</cell></row><row><cell>MeTRo (ours) 3+M</cell><cell>95.0</cell><cell>91.8</cell><cell>90.2</cell><cell>92.1</cell><cell>73.4</cell><cell>95.1</cell><cell>91.8</cell><cell>93.4</cell><cell>90.3</cell><cell cols="4">86.5 90.6±0.4 56.2±0.5 74.9±1.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Metric-scale skeletons (full scale recovery task)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.5D baseline 3+M</cell><cell>93.1</cell><cell>89.3</cell><cell>83.6</cell><cell>93.1</cell><cell>73.7</cell><cell>93.2</cell><cell>91.1</cell><cell>89.0</cell><cell>87.9</cell><cell cols="4">89.4 88.7±0.6 48.6±1.3 87.1±2.2</cell></row><row><cell>MeTRo (ours) 3+M</cell><cell>94.0</cell><cell>89.2</cell><cell>87.1</cell><cell>89.1</cell><cell>68.9</cell><cell>92.6</cell><cell>90.3</cell><cell>90.1</cell><cell>87.8</cell><cell cols="4">85.7 88.2±0.5 48.7±0.7 88.4±1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 TABLE 5</head><label>45</label><figDesc>Comparison with baseline methods of scale recovery, with or without access to ground truth information. For both datasets, metric-scale skeletons are used with the same 17 joints for comparability. The first two comparison methods access the ground truth at test time. MPJPE scores on H3.6M under truncation, evaluating all or only the present joints. ( * Training was not performed with truncated crops.) Results of other methods are taken from [46]. ratio on MuCo is 1.22 and 1.41, while on MuPoTS it is only 1.16 and 1.26, respectively. This confirms that perspective effects are stronger in MuCo.</figDesc><table><row><cell></cell><cell></cell><cell>H3.6M</cell><cell></cell><cell>3DHP</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">PCK↑ AUC↑ MPJPE↓ PCK↑ AUC↑ MPJPE↓</cell></row><row><cell cols="3">2.5D GT root depth 96.6 68.8</cell><cell>49.0</cell><cell>90.8 56.1</cell><cell>74.2</cell></row><row><cell cols="3">2.5D GT bone length 96.4 67.0</cell><cell>51.9</cell><cell>90.3 56.1</cell><cell>74.6</cell></row><row><cell cols="3">2.5D avg train bones 96.6 68.1</cell><cell>50.2</cell><cell>89.6 52.1</cell><cell>80.6</cell></row><row><cell>MeTRo (ours)</cell><cell></cell><cell>97.0 68.6</cell><cell>49.3</cell><cell>89.6 52.6</cell><cell>81.1</cell></row><row><cell cols="2">Mehta  All joints 396.4</cell><cell>400.5</cell><cell>185.0</cell><cell>124.7</cell><cell>77.8</cell></row><row><cell>Present joints</cell><cell>338.0</cell><cell>332.5</cell><cell>173.6</cell><cell>76.8</cell><cell>59.8</cell></row></table><note>* [9] Zhou* [10] Vosoughi [46] MeTRo* MeTRo</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Test speed (crops per second, FPS) and error (H3.6M MPJPE) tradeoff with the two striding variants fromFig. 3.</figDesc><table><row><cell></cell><cell>Striding</cell><cell></cell><cell cols="2">Test stride</cell><cell></cell></row><row><cell></cell><cell>variant</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell></row><row><cell>MPJPE</cell><cell cols="5">normal strides 53.1 52.5 52.7 52.9 center-aligned 50.9 50.2 50.0 49.3</cell></row><row><cell>Speed</cell><cell>no batching</cell><cell>160</cell><cell>150</cell><cell>105</cell><cell>38</cell></row><row><cell>(crop per sec.)</cell><cell>batch size 8</cell><cell>511</cell><cell>475</cell><cell>292</cell><cell>92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Augmentation ablation on H36M.</figDesc><table><row><cell cols="3">Geometry Color Occlusion MPJPE</cell></row><row><cell>-</cell><cell>-</cell><cell>58.0</cell></row><row><cell></cell><cell>-</cell><cell>52.8</cell></row><row><cell></cell><cell></cell><cell>49.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Results on MuPoTS-3D. Detected, unnormalized poses, no bone rescaling. ( * Re-evaluated public results; joint count: † 17, ‡ 16, else 14) abs. loss 328.8 (327.8 † ) 108.4 (104.7 ‡ ) 36.7±3.2 80.9±0.4 94.1±0.1 an RTX 2080 Ti desktop GPU when operating on batches of 8 crops at stride 32 (Tab. 6). Varying the heatmap resolution using dense prediction provides diminishing returns in accuracy (Tab. 6), showing that soft-argmax can cope with heatmaps of very coarse resolution. By gathering all person instances of a frame in a batch, MeTRAbs can process 128, 118, 98, 67, 41 frames per second for 1, 2, 4, 8 and 16 people</figDesc><table><row><cell></cell><cell>A-MPJPE↓</cell><cell cols="4">MPJPE↓ A-PCK↑ PCK↑ Det.Rate↑</cell></row><row><cell>Rogez et al. [74]</cell><cell>-</cell><cell>146  ‡</cell><cell>-</cell><cell>-</cell><cell>86</cell></row><row><cell>Mehta et al. [31]</cell><cell>-</cell><cell>132  ‡</cell><cell>-</cell><cell>-</cell><cell>93</cell></row><row><cell>Baseline in [39]</cell><cell>320  †</cell><cell>122  ‡</cell><cell>-</cell><cell>-</cell><cell>91</cell></row><row><cell>Véges et al. [39]</cell><cell>292  †</cell><cell>120  ‡</cell><cell>-</cell><cell>-</cell><cell>91</cell></row><row><cell cols="3">Véges et al. [75]  *  257.2 (255  † ) 119.4 (108  ‡ )</cell><cell>38.1</cell><cell>75.4</cell><cell>93</cell></row><row><cell cols="6">2.5D baseline 317.6 (313.6  † ) 114.0 (110.0  ‡ ) 40.0±1.0 79.3±0.3 94.2±0.0</cell></row><row><cell>MeTRAbs</cell><cell cols="5">248.2 (246.9  † ) 108.2 (104.3  ‡ ) 40.2±1.9 81.1±0.4 94.1±0.1</cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Comparison of weak (W) and full (F) perspective-based absolute pose reconstruction. The two letters denote the training and the test time variant. (Unnormalized skeletons, without bone rescaling.)</figDesc><table><row><cell cols="3">Persp. assumption</cell><cell cols="2">All annotations</cell><cell cols="2">Matched annotations</cell></row><row><cell>Training</cell><cell>Test</cell><cell></cell><cell>A-PCK↑</cell><cell>PCK↑</cell><cell>A-PCK↑</cell><cell>PCK↑</cell></row><row><cell>F</cell><cell>F</cell><cell></cell><cell>37.2±1.7</cell><cell>76.2±0.5</cell><cell>39.3±1.7</cell><cell>79.9±0.5</cell></row><row><cell>F</cell><cell>W</cell><cell></cell><cell>39.4±1.6</cell><cell>76.2±0.5</cell><cell>41.6±1.6</cell><cell>80.0±0.5</cell></row><row><cell>W</cell><cell>F</cell><cell></cell><cell>35.6±1.8</cell><cell>77.1±0.4</cell><cell>37.6±1.8</cell><cell>81.0±0.5</cell></row><row><cell>W</cell><cell>W</cell><cell></cell><cell>38.1±1.8</cell><cell>77.2±0.4</cell><cell>40.2±1.9</cell><cell>81.1±0.4</cell></row><row><cell>-</cell><cell>F</cell><cell></cell><cell>33.0±3.3</cell><cell>77.0±0.4</cell><cell>34.9±3.3</cell><cell>80.8±0.4</cell></row><row><cell>-</cell><cell>W</cell><cell></cell><cell>34.8±3.1</cell><cell>77.0±0.4</cell><cell>36.7±3.2</cell><cell>80.9±0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 10</cell><cell></cell></row><row><cell cols="7">Results on the 3DPW challenge dataset. (PA=Procrustes analysis)</cell></row><row><cell></cell><cell cols="6">MPJPE↓ MPJPE-PA↓ PCK@50mm↑ AUC@200mm↑</cell></row><row><cell></cell><cell></cell><cell cols="4">Known association to GT identity</cell></row><row><cell>Sun et al. [78]</cell><cell></cell><cell>81.8</cell><cell>58.6</cell><cell></cell><cell>37.3</cell><cell>59.9</cell></row><row><cell cols="2">Kissos et al. [79]</cell><cell>83.2</cell><cell>59.7</cell><cell></cell><cell>42.4</cell><cell>62.3</cell></row><row><cell cols="3">MeTRAbs (ours) 68.8</cell><cell>49.7</cell><cell></cell><cell>48.8</cell><cell>66.8</cell></row><row><cell></cell><cell cols="5">Unknown association to GT identity</cell></row><row><cell cols="3">MeTRAbs (ours) 85.1</cell><cell>56.7</cell><cell></cell><cell>45.8</cell><cell>63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Yinglun Liu for help in evaluating on MuPoTS-3D. This work was funded, in parts, by a Bosch Research Foundation grant, the ERC Consolidator Grant project "DeeViSe" (ERC-CoG-2017-773161) and the EU H2020 projects ILIAD (H2020-ICT-2016b-732737) and CROWDBOT (H2020-ICT-2017-779942). Compute resources were granted by RWTH Aachen University under project "rwth0479".</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Timm Linder defended his PhD thesis on multimodal human detection, tracking and analysis for robots in crowded environments at the University of Freiburg, Germany in 2020. Since 2016, he is a research scientist in autonomous systems and robot perception at Bosch Corporate Research. His research interests include computer vision, in particular human detection, tracking and pose estimation, as well as 3D scene generation and sim-to-real transfer. He has co-authored peerreviewed publications at major international conferences and journals, served on different program committees in robotics and AI, and received an outstanding reviewer award at ICRA 2019.</p><p>Kai Oliver Arras is the head of robotics research and chief expert in robotics at Robert Bosch GmbH. Until 2015, he was assistant professor for social robotics and HRI at the University of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D human pose estimation in RGBD images for robotic task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welschehold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dornhege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3D people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MVOR: A multi-view RGB-D operating room dataset for 2D and 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI LABELS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OriNet: A fully convolutional network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3D human pose estimation with 2D marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coarseto-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabral</surname></persName>
		</author>
		<idno>35] 85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8 76.4 70.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benzine</surname></persName>
		</author>
		<idno>80] 78.1 62.5 55.5 63.8 70.2 50.8 73.8 65.3 55.1 79.3 70.4 72.3 65.4 55.3 65.2 81.3 77.2 75.9 74.2 71.6 67.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabral</surname></persName>
		</author>
		<idno>35] 85.8 73.6 61.1 55.7 77.9 53.3 75.1 65.5 54.2 81.3 82.2 71.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benzine</surname></persName>
		</author>
		<idno>80] 78.3 75.0 56.9 64.1 76.1 51.3 74.7 79.1 55.2 79.3 74.5 74.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benzine</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>80] 22.2 18.1 16.1 18.5 20.4 14.7 21.2 18.9 16.0 22.9 20.3 20.9 18.9 16.0 18.9 23.5 22.3 21.8 21.5 20.8 19.8</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Véges</surname></persName>
		</author>
		<idno>75] 50.4 33.4 52.8 27.5 53.7 31.4 22.6 33.5 38.3 56.5 24.4 35.5 45.5 34.9 49.3 23.2 32.0 30.7 26.3 43.8 37.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benzine</surname></persName>
		</author>
		<idno>80] 22.7 21</idno>
		<imprint/>
	</monogr>
	<note>2 17.1 18.6 22.0 14.8 21.5 22.9 16.0 22.9 21.5 21.6 20.3 20.0 19.4 18.9 23.8 22.6 22.9 25.8 20.9</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Véges</surname></persName>
		</author>
		<idno>75] 50.4 35.9 53.3 27.7</idno>
		<imprint/>
	</monogr>
	<note>2 31.4 23.1 39.3 38.3 56.5 25.2 35.8 49.3 42.5 49.4 24.1 32.1 33.1 29.3 59.2 39.6</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hand pose estimation via latent 2.5D heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An integral pose regression system for the ECCV2018 PoseTrack Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06079</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Numerical coordinate regression with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metric-scale truncation-robust heatmaps for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Autom. Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVIU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular human pose estimation: A survey of deep learning-based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Integral Human Pose Regression (code repository)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://github.com/JimmySuen/integral-human-pose" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>Online; accessed 28</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning depth-aware heatmaps for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">LCR-Net++: Multiperson 2D and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What face and body shapes can tell about height</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show me your face and I will tell you your height, weight and body mass index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-person 3D human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">XNect: Real-time multi-person 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the 2018 ECCV Pose-Track challenge on 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04987</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Véges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lőrincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Three-dimensional reconstruction of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Amodal completion and size constancy in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data augmentation method for improving the accuracy of human pose estimation with cropped images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep 3D human pose estimation under partial body presence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning monocular 3D human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode?&quot; in ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepfocal: A method for direct focal length estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baltenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The &quot;vertigo effect&quot; on your smartphone: Dolly zoom via single shot view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valtonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ornhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Heyden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Coarseto-fine volumetric prediction for single-image 3D human pose: Supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<title level="m">OptNnet: Differentiable optimization as a layer in neural networks,&quot; in ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Differentiable convex optimization layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Multi-task deep learning for real-time 3D human pose estimation and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Improving 3D human pose estimation via 3D part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2D features and intermediate 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">How robust is 3D human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Robotic Co-workers 4.0</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">LCR-Net: Localizationclassification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Multi-person absolute 3D human pose estimation with weak depth supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lorincz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03989</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">HEMlets pose: Learning part-centric heatmap triplets for accurate 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">OriNet-demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://github.com/chenxuluo/OriNet-demo" />
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">CenterHMR: a bottomup single-shot method for multi-person 3D mesh recovery from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12272</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Beyond weak perspective for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kissos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06549</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Singleshot 3D multi-person pose estimation in complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">SAIL-VOS: Semantic amodal instance level video object segmentation-a synthetic dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
