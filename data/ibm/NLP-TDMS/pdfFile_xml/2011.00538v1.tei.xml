<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Diacritization: Efficient Hierarchical Recurrence for Improved Arabic Diacritization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badr</forename><surname>Alkhamissi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The American University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country>AUC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">N</forename><surname>Elnokrashy</surname></persName>
							<email>muhammad.nael@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The American University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country>AUC</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Egypt Development Center (EGDC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Gabr</surname></persName>
							<email>mohamed.gabr@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Egypt Development Center (EGDC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Diacritization: Efficient Hierarchical Recurrence for Improved Arabic Diacritization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel architecture for labelling character sequences that achieves state-of-the-art results on the Tashkeela Arabic diacritization benchmark. The core is a two-level recurrence hierarchy that operates on the word and character levels separately-enabling faster training and inference than comparable traditional models. A cross-level attention module further connects the two, and opens the door for network interpretability. The task module is a softmax classifier that enumerates valid combinations of diacritics. This architecture can be extended with a recurrent decoder that optionally accepts priors from partially diacritized text, which improves results. We employ extra tricks such as sentence dropout and majority voting to further boost the final result. Our best model achieves a WER of 5.34%, outperforming the previous state-of-the-art with a 30.56% relative error reduction. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. Affiliation emails:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Arabic script (and similarly Hebrew, Aramaic, Pahlavi...) is an impure abjad. These writing systems represent short consonants and long vowels using full letter graphemes, but generally omit short vowels and consonant length from writing. This leaves the task of inferring the missing phonemes to the reader by using context from neighbouring words and knowledge of the language structure to determine the correct pronunciation and disambiguate the meaning of the text. Those sounds are represented by diacritical marks-small graphemes that appear usually above or below a basic letter in the abjad. <ref type="table" target="#tab_1">Table 2</ref> shows the diacritics considered in this work. Diacritics are usually utilized in specific domains where it is important to explicitly clear up ambiguities or where inferring the correct forms might be difficult for non-experts, such as religious texts, some literary works such as poetry, and language teaching books as novice readers have yet to build up the intuition for reading undiacritized text.</p><p>We focus in this work on diacritization of Arabic texts. However, our proposed architecture has no explicitly language-dependent components and should be adaptable for other character sequence labelling tasks. Although it is the first language of several million people, and is spoken in some of the fastest growing markets <ref type="bibr" target="#b20">(Tinsley and Board, 2013)</ref>, the Arabic language, like many others, lacks attention from the NLP community compared to established test bed languages such as English or Chinese, which both enjoy higher momentum and an abundance of established resources and techniques. The automatic restoration of diacritics to Arabic text is arguably one of the most important NLP tasks for the Arabic language. Besides direct applications like facilitating learning, diacritics are used to enhance language modeling, acoustic modeling for speech recognition, morphological analysis, machine translation, and text-to-speech systems (which need to restore the lost phonemes to render words properly) <ref type="bibr" target="#b24">(Zitouni and Sarikaya, 2009;</ref><ref type="bibr" target="#b1">Azmi, 2013)</ref>.</p><p>To illustrate this further, <ref type="table">Table 1</ref> shows the Arabic word Elm 1 in different diacritized forms with their corresponding English translations, showcasing the importance of diacritics in resolving ambiguity. Note that the MADA <ref type="bibr" target="#b12">(Habash et al., 2009)</ref> morphological analyzer produces at least 13 different forms for this undiacritized word <ref type="bibr" target="#b2">(Belinkov and Glass, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arabic (diacritized) Transliteration English Translation</head><p>Ealima He knew</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eulima</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It was known</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eal∼ama</head><p>He taught</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eilomu Knowledge</head><p>Ealamu Flag <ref type="table">Table 1</ref>: Subset of possible diacritized forms for Elm adapted from <ref type="bibr" target="#b2">(Belinkov and Glass, 2015)</ref> Table 2 shows the different diacritics commonly used in Arabic texts along with their phonemic symbols. They fit roughly into four kinds. (1) H . arakāt are diacritics for short vowels; we have three: fath . ah, kasrah, dammah. The symbols for those vowels have another form (usually a visual doubling) used at the end of a word to form a (2) tanwīn, or nunation, which is a VC sound of the h . arakah's vowel followed by the consonant "n" <ref type="bibr">({a, i, u}n)</ref>. <ref type="formula" target="#formula_3">(3)</ref> The shaddah is the gemination symbol used to indicate consonant doubling. It can be combined with one of the h . arakāt or tanwīn on the same character. Finally, (4) the sukūn is used to indicate that the current consonant is not followed by a vowel and instead forms a cluster with the next consonant. Diacritics which appear at the end of a word are referred to as case-endings (CE); most of which are specified by the syntactic role of the word. They are harder to infer than the core-word diacritics (CW) that specify lexical selection and appear on the rest of the word <ref type="bibr" target="#b14">(Mubarak et al., 2019)</ref>.  The paper is structured as follows: First we cover some of the approaches used in related works on restoring Arabic diacritics. Then we introduce our system and support it by comparing experimental results on an adapted version of the Tashkeela corpus <ref type="bibr" target="#b23">(Zerrouki and Balla, 2017)</ref> proposed by <ref type="bibr" target="#b8">(Fadel et al., 2019a)</ref> as a standard benchmark for Arabic diacritization systems. Each design decision will then be motivated by an ablation study. We analyze the learned attention model then discuss existing limitations in an error analysis. Finally, we offer directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The literature surrounding the automatic diacritization of Arabic text provides methods in two categories: classical rule-based solutions, and statistical modeling-based methods. Early approaches have worked on constructing a large set of language specific rules to restore the lost diacritics <ref type="bibr" target="#b12">(Habash et al., 2009;</ref><ref type="bibr" target="#b25">Zitouni et al., 2006;</ref><ref type="bibr" target="#b16">Pasha et al., 2014;</ref><ref type="bibr" target="#b5">Darwish et al., 2017)</ref>. Researchers have then shifted to rely more on learning-based methods that do not require extra expert systems such as morphological analyzers and part-of-speech taggers. <ref type="bibr" target="#b2">(Belinkov and Glass, 2015)</ref> have shown that recurrent neural networks are suitable candidate models for learning the task entirely from data and can be easily extended to other languages and dialects without the use of manually engineered features. Other methods such as hidden Markov models (HMMs) <ref type="bibr" target="#b7">(Elshafei et al., 2006)</ref>, conditional random fields (CRFs) <ref type="bibr" target="#b5">(Darwish et al., 2017)</ref>, maximum-entropy models <ref type="bibr" target="#b25">(Zitouni et al., 2006)</ref> and finite-state transducers <ref type="bibr" target="#b15">(Nelken and Shieber, 2005)</ref> have similarly been employed. However, more recent works have started to use deep (neural-based) architectures such as sequence-to-sequence transformers and recurrent cell-based models inspired by work in Neural Machine Translation <ref type="bibr" target="#b14">(Mubarak et al., 2019)</ref>. Solutions combining both rule-based and deep learning methods appear in recently published work <ref type="bibr" target="#b0">(Abbad and Xiong, 2020)</ref>. <ref type="bibr" target="#b22">(Zalmout and Habash, 2020)</ref> have shown that the diacritization task benefits from jointly modelling lexicalized and non-lexicalized morphological features instead of targeting only the diacritization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We report on the cleaned version of the Tashkeela corpus <ref type="bibr" target="#b8">(Fadel et al., 2019a</ref>)-a high quality, publicly available dataset. It is split into train (2,449k tokens), dev (119k tokens), and test (125k tokens) sets. In this work, we propose two models: The Two-Level Diacritizer (D2) and the Two-Level Diactritizer with Decoder (D3). D3 extends D2 by allowing partially diacritized text to be taken as input-a much needed feature for neural diacritizers <ref type="bibr" target="#b8">(Fadel et al., 2019a)</ref>. D2 outperforms fully character-based models in both task and runtime performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Two-Level (Hierarchical) Model</head><p>Restoring diacritics can be seen as a character sequence labeling task. Each label depends on word-and character-level context. This structure motivates the hierarchy in our two-level encoder architecturethe first encoder sees the sequence of words (where words are atoms) and provides word-level context for the second, character-level encoder. The character-level encoder is evaluated independently for each word in the sentence, enabling much faster training and inference compared to character-level recurrent cell-based models of similar structure to previous works <ref type="bibr" target="#b2">(Belinkov and Glass, 2015;</ref><ref type="bibr" target="#b14">Mubarak et al., 2019;</ref><ref type="bibr" target="#b22">Zalmout and Habash, 2020;</ref><ref type="bibr" target="#b9">Fadel et al., 2019b)</ref>. Let T s be the maximum number of words allowed in a sentence and T w the maximum number of characters allowed in a word. Then the overall character sequence length that a model in prior works would see is in the order of (T s · T w ). In contrast, our approach operates on a maximum sequence length of T s at the word level and T w at the character level. Because character-level recurrence is independent of characters outside the current word, the serial bottleneck complexity goes from</p><formula xml:id="formula_0">O w∈s |w| ≈ O(T s · T w ) down to O |s| + max w∈s {|w|} ≈ O(T s + T w )</formula><p>, assuming adequate parallelization across word units. See <ref type="table" target="#tab_3">Table 4</ref> for a speed comparison.</p><p>Let</p><formula xml:id="formula_1">s = {w i } Ts i=1 denote a sequence of words. w = {w i } Ts i=1</formula><p>are the corresponding fastText features pretrained on CommonCrawl Arabic data <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref>. Let w i = {c i,j } Tw j=1 denote the sequence of characters for the word at index i in sentence s. Each character is assigned a 32-dimensional learned embedding c i,j . Let f (w) i denote the feature vector from the word-level encoder which maps each word in s to a contextual word representation. Let g(·) i,j denote the character-level recurrence that outputs a contextual encoding of the character relative to its parent word and sentence. See <ref type="figure">Figure 1</ref> for an overview. Formally, the contextual embedding z i,j of c i,j is</p><formula xml:id="formula_2">g i,j = g ([c i,j ; f (w) i ]) (1) z i,j = g i,j ; f * i,j<label>(2)</label></formula><p>Where f * i,j is the attention view. Both f (·) and g(·) use Bidirectional LSTM (Bi-LSTM) layers <ref type="bibr" target="#b11">(Graves et al., 2005)</ref> trained with backpropagation through time. We note that any similar sequence modelling architecture would be applicable <ref type="bibr" target="#b4">(Chung et al., 2014;</ref><ref type="bibr" target="#b21">Vaswani et al., 2017)</ref> but leave that to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-level Attention Module</head><p>This module attends over the word-level encodings f (w) based on the character encodings g(·) of each character in a word. In other words, it uses the initial contextualization of the characters (g i,j ) to attend to all words in the sentence (except the current) to refine the character's representation. We use the attention formulation from <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref>. For each character c i,j , we calculate</p><formula xml:id="formula_3">f * i,j = AttendReduce (u = g i,j ; X = {...f (w) 0:i−1 , f (w) i+1:Ts ...})<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">AttendReduce (u; X) = W O Softmax t W Q (u) · W K (X) t √ d K · W V (X)<label>(4)</label></formula><p>where in eq. (4), W Q , W K , W V , and W O are independent linear layers. We tried to remove W O but faced lower performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Decoder</head><p>Used in D3, this component is a forward-only LSTM that takes as input a concatenation of the basic contextual character embedding z i,j and a one-hot representation of the output of the previous character from the classifier module. Formally: [z i,j ;ŷ i,j−1 ]. Theŷ i,j−1 signal passed to the decoder also encodes the Beginning-of-Word in addition to the previous-character diacritics. This allows the model to accept partially diacritized sentences such that the ground truth diacritic is injected in place ofŷ i,j−1 during inference. This feature is important as many Arabic texts contain sparse diacritics that act as hints to assist readers. Having this clean signal yields improvements as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Task Objective</head><p>The final classifier optimizes a Softmax objective over an enumeration of all valid diacritic combinations. Combined, we have 3 h . arakāt in 4 variants (with tanwīn, shadda, tanwīn and shadda, and neither), the sukūn, and the plain shadda. Thus 15 classes including the None (no diacritic) class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Parameters, Hyper-parameters, and Regularization</head><p>Optimization We use the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.002. The model is left to converge until the validation loss does not improve for 3 consecutive epochs where each epoch enumerates a randomly shuffled version of the training segments exactly once. The learning rate is reduced by half when the validation loss does not improve for one epoch. We train with a mini-batch size of 128 segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoders and Decoder</head><p>The word and character level encoders are each a 2-layer stacked Bi-LSTM with 256 and 512 hidden units, respectively. We apply feature-level dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref> with probability 0.2 to the input of the character level encoder. The decoder in D3 is a one-layer forward-only LSTM with 1024 hidden units. All recurrent cells use a vertical and recurrent dropout of 0.25 each. The recurrent dropout used is untied between time-steps, in contrast to <ref type="bibr" target="#b10">(Gal and Ghahramani, 2016)</ref>.</p><p>Context Window and Voting Similar to <ref type="bibr" target="#b14">(Mubarak et al., 2019)</ref>, we use a sliding context window of size T s on each sentence. A given sentence is split into several overlapping segments each of which is given separately to the model during training. This works well as the local context is often sufficient for correct inference. During inference, the same sequence of characters may appear in different contexts (different segments from one sentence) and potentially lead to different diacritized forms. To choose the final diacritic, we use a popularity voting mechanism and, in the case of a tie, choose one of the outputs at random. The values chosen for T s , T w and the stride are: T s = 10 with stride = 1 for training and validation (a small T s was observed to stabilize training and improve results); T s = 20 with stride = 2 for evaluation/testing; and T w = 13 for both training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Dropout</head><p>We randomly dropout 20% of the words given to the word-level encoder during training. The positions of the dropped out words are preserved, and their embedding vectors w i are replaced with zeros. This was observed to lead to better generalization in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">D3 Training</head><p>This model is not trained from scratch, but uses the weights of the encoders and character embeddings learned from D2. Those weights are kept frozen and only the decoder and classifier are trained.</p><p>Ramp-up of Teacher-forcing Signal We pass the ground truth of p% of the previous-character diacritics as input to the decoder at the current time-step. This value is ramped up from p = 0% (all characters receive previous diacritics as zeros; i.e. no signal) to p = 100% (all characters receive ground truth of previous diacritic as signal). This is done over a period of n = 10 epochs in increments of 10%. Then the model is left to converge using the same stopping criteria as in D2. We found this to be the best approach as otherwise the model overfits early on the teacher forcing signal given from the previous time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Source Code</head><p>The code is made open source and is available on GitHub 2 . We also provide an accompanying web application to demo the proposed models which can be found at this web address 3 . The system uses PyTorch for implementing the neural training and inference components <ref type="bibr" target="#b17">(Paszke et al., 2019)</ref>. The LSTM cell implementation used is due to (ElNokrashy, 2020) 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>We use the script provided by <ref type="bibr" target="#b8">(Fadel et al., 2019a)</ref> to evaluate our results. To be consistent with prior work, we report our results in terms of both word error-rate (WER) and diacritic error-rate (DER), with and without case-endings, as well as including and excluding characters with no diacritics. <ref type="table" target="#tab_2">Table 3</ref> shows our results on the Tashkeela benchmark in comparison with the more recent works. We outperform stateof-the-art by 30.56% relative (2.35% absolute) error reduction on "WER with case-ending".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DER/WER</head><p>Including 'no diacritic'</p><p>Excluding 'no diacritic' w/ case ending w/o case ending w/ case ending w/o case ending <ref type="bibr">(Barqawi, 2017)</ref> 3.73% / 11.19% 2.88% / 6.53% 4.36% / 10.89% 3.33% / 6.37% <ref type="bibr" target="#b9">(Fadel et al., 2019b)</ref> 2.60% / 7.69% 2.11% / 4.57% 3.00% / 7.39% 2.42% / 4.44% <ref type="bibr" target="#b0">(Abbad and Xiong, 2020)</ref> 3.39% / 9.94% 2.61% / 5.83% 3.34% / 7.98% 2.43% / 3.98% D2 (Ours) 1.85% / 5.53% 1.49% / 3.27% 2.11% / 5.26% 1.71% / 3.15% D3 (Ours) (@0% hints) 1.83% / 5.34% 1.48% / 3.11% 2.09% / 5.08% 1.69% / 3.00%   <ref type="table" target="#tab_3">Table 4</ref> compares our plain 2-level hierarchy design (without Attention) with a "Flat" model in task and runtime performance. The Flat model comprises a 4-layer stacked Bi-LSTM with similar implementation details as described in 3.3.1 for D2, including Sentence Dropout and the Voting mechanism. The Flat model sees each sentence as one sequence of characters.</p><p>Partially Diacritized Text <ref type="figure">Figure 2</ref> shows the results of DER including 'no diacritic' with and without case ending when the model is supplied with partially diacritized text as input. For each character in the sentence, with some probability, we may replace the predicted output of the previous time-step with the ground truth as input to the decoder in the current step. The reported output of the previous time-step is masked to force the provided hint to be the model's "prediction" even if the inferred were different (see <ref type="figure">Figure 2b</ref>)-in contrast to <ref type="figure">Figure 2a</ref> where the final predictions are the model's unmodified outputs. The results are averaged across five runs with different seeds (i.e. injecting the ground truth signal at different characters in the sentence). Error bars represent standard deviation. Many Arabic texts already come with some hints that can improve model performance. Here we show how a neural model could be trained to leverage that. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>We conduct an ablation study to measure the effect of components proposed for the final model. We train and evaluate the D2 model previously detailed but with the component(s) specified removed. <ref type="table">Table  5</ref> shows the results after removing the sentence dropout and cross-level attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DER/WER</head><p>Including 'no-diacritic' Excluding 'no-diacritic' w/ case ending w/o case ending w/ case ending w/o case ending <ref type="bibr" target="#b9">(Fadel et al., 2019b)</ref> 2.60% / 7.69% 2.11% / 4.57% 3.00% / 7.39% 2.42% / 4.44% D3 (@0% hints) 1.83% / 5.34% 1.48% / 3.11% 2.09% / 5.08% 1.69% / 3.00% D2 1.85% / 5.53% 1.49% / 3.27% 2.11% / 5.26% 1.71% / 3.15% D2 − {Attention 3.2.2} 1.94% / 5.80% 1.58% / 3.44% 2.23% / 5.52% 1.80% / 3.31% D2 − {SDO 3.3.1} 1.91% / 5.71% 1.54% / 3.36% 2.18% / 5.43% 1.75% / 3.23% D2 − {Attn, SDO} 1.93% / 5.78% 1.57% / 3.45% 2.21% / 5.49% 1.79% / 3.32% <ref type="table">Table 5</ref>: Ablation Study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention Analysis</head><p>The cross-level attention module allows us to gauge the contribution of each word to each output diacritic.</p><p>Here we examine some examples to see whether the model was able to learn such Arabic grammar rules as a human expert would use when annotating case endings. The examples presented in this section reflect patterns we have found repeated during our analysis. The pattern in <ref type="figure">Figure 3a</ref> is related to the h . arf-atf 6 rule (generally prepositions), which states that the word coming after it gets the same case as the main word of the phrase it is related to-the grammatical parent. We see indeed that the word coming after the ("w") h . arf-atf attends the most on the word that comes before it. This is similar to what an expert would do; look at the main word in the phrase preceding the "w" in order to determine the case and case-ending of what follows.</p><p>(a) Self attends on a local word with similar effect on Self (b) Self attends on a local word with similar role to Self <ref type="figure">Figure 4</ref>: Attention visualization of confusion of grammatical parents. <ref type="figure">Figure 3b</ref> shows another prevalent example where the word in question attends the most on the h . arfjarr 7 preceding it. However, in other cases where the same rule appears twice in a segment, we found that the model may choose to attend equally or more on the components of the first occurrence rather than the occurrence the current word is actually affected by-the grammatical parent. <ref type="figure">Figure 4a</ref> shows one example of this where the word ("zmylth") attends equally to two words that would affect it the same <ref type="bibr">("&lt;lY" and "mE")</ref>, but only the second should be affecting it. In <ref type="figure">Figure 4b</ref> we see that the second maf'ool-bih 8 (roughly an object of a verb) ("AlrmAyp") attends heavily on the first occurrence of a mf'ool-bih in the segment ("Altjdyf"), rather than the verb that should be affecting it ("tmArs"). This behavior of attending on a previous word with a similar role suggests that the attention mechanism is aware of grammatical rules; it is able to group words with the same role together.</p><p>Generally, we found that not all sentences yield interpretable attention weights. We leave the task of comprehensively studying the extent of agreement of the learned weights with Arabic grammatical rules to future work.  <ref type="figure" target="#fig_3">Figures 5a and 5b</ref> show the the confusion matrices (for visualization clarity we show errors only) of case ending and core words diacritics respectively. Many confusions are between the dammah and kasrah, and between dammah and fath . ah, and the confusion goes both ways. We analyze the errors between kasrah and dammah for case endings and try to correlate them with grammar rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error Analysis</head><p>The first error example is related to the start of a new sentence in Arabic grammar. The word "wa" composed of one letter can either mark a conjunction (e.g. in an enumeration), or mark the start of a new sentence, based on context. An example with such confusion is in the following sentence, with the ground truth: "wa yarud∼u Ealayohi &gt;an∼a faAqida AlT∼ahuwrayoni wa naHowahu layosa lahu SalaApN &lt;l∼aA &lt;*aA DaAqa Alowaqotu" 9 . We see one confusion example where "wa naHowah" was predicted as "wa naHowih". Grammatically, the "wa" relates the next word to a "sentence starter" word ("faAqida") in a case that would make a the correct diacritic. Instead, we observe it follows the word immediately before the "wa" ("AlT∼ahuwrayoni"), which is indeed in a grammatical case that would make i the correct diacritic for "naHow{a/i}h", were it the correct grammatical parent of this "wa".</p><p>The second example is related to the use of punctuation marks that signal an abrupt start of a new sentence or an end of one with unique context that may not be easily learnt. In the following sentence with the ground truth: "kaqaA}ilK : AloHar∼u &gt;awo Alobarodu Al$∼adiydu" 10 was predicted as 7 8 9 10 "kaqaA}ilK : AloHar∼i &gt;awo Alobarodi Al$∼adiydi...". The mark ":" here denotes the start of a new sentence, by convention, as we start a quotation. In speech, this would manifest as a brief pause or change in tone. Without ":", the word would be an ism-majrūr that takes the kasrah h . arakah (i) in this position, which the model mistakenly outputs. But because it starts a new sentence, the correct diacritic is a dammah h . arakah (u). Further, the predictions for the words following "&gt;awo" behave grammatically by following the case of the parent word ("AloHar∼i") (according to "wa"), but are incorrect because the error has propagated.</p><p>One other type of errors is related to inconsistencies in the corpus-the same word with the same role in the sentence is not diacritized the same way across the dataset. For instance, the word "&lt;lY", which is the second top word that causes a core word error as shown in <ref type="table" target="#tab_5">Table 6</ref>, appears multiple times in different forms: "&lt;ilY", "&lt;lY", and "&lt;ilaY"-all correct. There are other examples that show the need to clean the dataset (at least the test set) to evaluate the published models properly.  Looking at the top words that yield confusions in both core words and case ending diacritics, we find a notable intersection between <ref type="table" target="#tab_5">Table 6</ref> and the most frequent tokens in the Tashkeela corpus with an average max-normalized frequency of 0.24 (0.24 as frequent as the most frequent word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented a novel architecture that outperforms previously published results on the Tashkeela Arabic diacritization benchmark. Future work may include:</p><p>• Replacing the word-and character-level Bi-LSTM encoders with transformer-based encoders. • Using byte-pair-encoding (BPE) <ref type="bibr" target="#b18">(Sennrich et al., 2016)</ref> to better handle suffixes and prefixes as Arabic is a moderately fusional language. • Investigating more efficient use of injected hints to improve performance. • Training/Evaluating this design/model on Modern Standard Arabic and dialectical benchmarks. • Cleaning the testset of Tashkeela to remove any inconsistencies as described in the error analysis. • Finally, achieving more interpretable attention weights through multi-task training, training on larger datasets, or otherwise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: D3 Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) DER vs Percentage of Injected Hints (actual output) (b) DER vs Percentage of Injected Hints (hints covering output) Figure 2: Error Rate of Partially Diacritized Text 5 All models use the same custom LSTM implementation and are run on a single Nvidia GeForce RTX 2080 Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Ex. of case agreement by h . arf-atf (b) Ex. of CE of ism-majrūr depending on h . arf-jarr Figure 3: Attention visualization of words correctly attending to grammatical parents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) Confusion Matrix for Case Endings (Errors Only) (b) Confusion Matrix for Core Word Diacritics (Errors Only) Error Confusion Matrix for CE and Core Diacritics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Primary Arabic diacritics on letter</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the Tashkeela benchmark</figDesc><table><row><cell>Method</cell><cell cols="3">#Params T/epoch Convergence</cell><cell cols="2">Inference Full DER/WER</cell></row><row><cell cols="2">D2 -{Attn} 13.369M</cell><cell>28 mins</cell><cell cols="3">17 epochs 34,996 wps 1.94% / 5.80%</cell></row><row><cell>Flat</cell><cell cols="2">13.304M 121 mins</cell><cell>13 epochs</cell><cell>2,466 wps</cell><cell>2.20% / 6.39%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Speed Comparison 5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top 10 Words with CE and Core Word Errors</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/bkhmsi/deep-diacritization 3 https://deep-diacritization.herokuapp.com/ 4 https://github.com/munael/pt-rnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We offer special thanks to Khaled Essam, as well as Mohamed Afify and Ahmed Tawfik of Microsoft EGDC, for many helpful discussions, suggestions and comments on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-components system for automatic arabic diacritization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamza</forename><surname>Abbad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwu</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<editor>Joemon M. Jose, Emine Yilmaz, João Magalhães, Pablo Castells, Nicola Ferro, Mário J. Silva, and Flávio Martins</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="341" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of automatic arabic diacritization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aqil</forename><surname>Azmi</surname></persName>
		</author>
		<ptr target="https://github.com/Barqawiz/Shakkala" />
	</analytic>
	<monogr>
		<title level="m">Shakkala, Arabic text vocalization</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arabic diacritization with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2281" to="2285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Arabic diacritization: Stats, rules, and hacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Arabic Natural Language Processing Workshop</title>
		<meeting>the Third Arabic Natural Language Processing Workshop<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Extensible RNN cells for PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">N</forename><surname>Elnokrashy</surname></persName>
		</author>
		<ptr target="https://github.com/munael/pt-rnn" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical methods for automatic diacritization of arabic text. The Saudi 18th National Computer Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Elshafei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Husni</forename><surname>Al-Muhtaseb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansour</forename><surname>Alghamdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Riyadh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arabic text diacritization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Tuffaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Bara&amp;apos; Al-Jawarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 2nd International Conference on Computer Applications Information Security (ICCAIS)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Arabic text diacritization: State of the art results and a novel approach for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Tuffaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Bara&amp;apos; Al-Jawarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Asian Translation</title>
		<meeting>the 6th Workshop on Asian Translation<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="215" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bidirectional LSTM Networks for Improved Phoneme Classification and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks: Formal Models and Their Applications -ICANN 2005</title>
		<editor>Włodzisław Duch, Janusz Kacprzyk, Erkki Oja, and Sławomir Zadrożny</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR)</title>
		<meeting>the 2nd International Conference on Arabic Language Resources and Tools (MEDAR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980Comment</idno>
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published as a conference paper at</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Highly effective Arabic diacritization using sequence to sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2390" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arabic diacritization using weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rani</forename><surname>Nelken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages</title>
		<meeting>the ACL Workshop on Computational Approaches to Semitic Languages<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arfath</forename><surname>Pasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Pooleery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="1094" to="1101" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Junjie Bai, and Soumith Chintala</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Languages for the Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Tinsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Board</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>British Council</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint diacritization, lemmatization, normalization, and fine-grained morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Zalmout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="8297" to="8307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tashkeela: Novel corpus of arabic vocalized texts, data for autodiacritization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><surname>Zerrouki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Balla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data in Brief</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="147" to="151" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Arabic diacritic restoration approach based on maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum entropy based restoration of Arabic diacritics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
