<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
							<email>mansimov@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">New York University CIFAR Azrieli Global Scholar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En↔De and En↔Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conditional neural sequence modeling has become a de facto standard in a variety of tasks (see, e.g., <ref type="bibr">Cho et al., 2015, and references therein)</ref>. Much of this recent success is built on top of autoregressive sequence models in which the probability of a target sequence is factorized as a product of conditional probabilities of next symbols given all the preceding ones. Despite its success, neural autoregressive modeling has its weakness in decoding, i.e., finding the most likely sequence. Because of intractability, we must resort to suboptimal approximate decoding, and due to its sequential nature, decoding cannot be easily parallelized and results in a large latency (see, e.g., <ref type="bibr" target="#b5">Cho, 2016)</ref>. This has motivated the recent investigation into non-autoregressive neural sequence modeling by <ref type="bibr" target="#b11">Gu et al. (2017)</ref> in the context of machine translation and <ref type="bibr" target="#b30">Oord et al. (2017)</ref> in the context of speech synthesis.</p><p>In this paper, we propose a non-autoregressive neural sequence model based on iterative refinement, which is generally applicable to any sequence generation task beyond machine translation. The proposed model can be viewed as both * Equal Contribution a latent variable model and a conditional denoising autoencoder. We thus propose a learning algorithm that is hybrid of lowerbound maximization and reconstruction error minimization. We further design an iterative inference strategy with an adaptive number of steps to minimize the generation latency without sacrificing the generation quality.</p><p>We extensively evaluate the proposed conditional non-autoregressive sequence model and compare it against the autoregressive counterpart, using the state-of-the-art Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>, on machine translation and image caption generation. In the case of machine translation, the proposed deterministic nonautoregressive models are able to decode approximately 2 − 3× faster than beam search from the autoregressive counterparts on both GPU and CPU, while maintaining 90-95% of translation quality on IWSLT'16 En↔De, WMT'16 En↔Ro and WMT'14 En↔De. On image caption generation, we observe approximately 3× and 5× faster decoding on GPU and CPU, respectively, while maintaining 85% of caption quality. 1 2 Non-Autoregressive Sequence Models Sequence modeling in deep learning has largely focused on autoregressive modeling. That is, given a sequence Y = (y 1 , . . . , y T ), we use some form of a neural network to parametrize the conditional distribution over each variable y t given all the preceding variables, i.e., log p(y t |y &lt;t ) = f θ (y &lt;t ), where f θ is for instance a recurrent neural network. This approach has become a de facto standard in language modeling <ref type="bibr" target="#b27">(Mikolov et al., 2010)</ref>. When this is conditioned on an extra variable X, it becomes a conditional sequence model log p(Y |X) which serves as a basis on which many recent advances in, e.g., machine translation <ref type="bibr" target="#b2">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b35">Sutskever et al., 2014;</ref><ref type="bibr" target="#b19">Kalchbrenner and Blunsom, 2013)</ref> and speech recognition <ref type="bibr" target="#b7">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b4">Chiu et al., 2017)</ref> have been made.</p><p>Despite the recent success, autoregressive sequence modeling has a weakness due to its nature of sequential processing. This weakness shows itself especially when we try to decode the most likely sequence from a trained model, i.e.,</p><formula xml:id="formula_0">Y = arg max Y log p(Y |X).</formula><p>There is no known polynomial algorithm for solving it exactly, and practitioners have relied on approximate decoding algorithms (see, e.g., <ref type="bibr" target="#b5">Cho, 2016;</ref><ref type="bibr" target="#b15">Hoang et al., 2017)</ref>. Among these, beam search has become the method of choice, due to its superior performance over greedy decoding, which however comes with a substantial computational overhead <ref type="bibr" target="#b5">(Cho, 2016)</ref>.</p><p>As a solution to this issue of slow decoding, two recent works have attempted non-autoregressive sequence modeling. <ref type="bibr" target="#b11">Gu et al. (2017)</ref> have modified the Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> for non-autoregressive machine translation, and Oord et al. (2017) a convolutional network <ref type="bibr" target="#b29">(Oord et al., 2016)</ref> for non-autoregressive modeling of waveform. Non-autoregressive modeling factorizes the distribution over a target sequence given a source into a product of conditionally independent perstep distributions:</p><formula xml:id="formula_1">p(Y |X) = T t=1 p(y t |X),</formula><p>breaking the dependency among the target variables across time. This allows us to trivially find the most likely target sequence by taking arg max yt p(y t |X) for each t, effectively bypassing the computational overhead and suboptimality of decoding from an autoregressive sequence model. This desirable property of exact and parallel decoding however comes at the expense of potential performance degradation <ref type="bibr" target="#b17">(Kaiser and Bengio, 2016)</ref>. The potential modeling gap, which is the gap between the underlying, true model and the neural sequence model, could be larger with the non-autogressive model compared to the autoregressive one due to challenge of modeling the factorized conditional distribution above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Iterative Refinement for Deterministic</head><p>Non-Autoregressive Sequence Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent variable model</head><p>Similarly to two recent works <ref type="bibr" target="#b30">(Oord et al., 2017;</ref><ref type="bibr" target="#b11">Gu et al., 2017)</ref>, we introduce latent variables to implicitly capture the dependencies among target variables. We however remove any stochastic behavior by interpreting this latent variable model, introduced immediately below, as a process of iterative refinement.</p><p>Our goal is to capture the dependencies among target symbols given a source sentence without auto-regression by introducing L intermediate random variables and marginalizing them out:</p><formula xml:id="formula_2">p(Y |X) = Y 0 ,...,Y L T t=1 p(y t |Y L , X) (1) T t=1 p(y L t |Y L−1 , X) · · · T t=1 p(y 0 t |X) .</formula><p>Each product term inside the summation is modelled by a deep neural network that takes as input a source sentence and outputs the conditional distribution over the target vocabulary V for each t.</p><p>Deterministic Approximation The marginalization in Eq. (1) is intractable. In order to avoid this issue, we consider two approximation strategies; deterministic and stochastic approximation. Without loss of generality, let us consider the case of single intermediate latent variable, that is L = 1. In the deterministic case, we setŷ 0 t to the most likely value according to its distribution p(y 0 t |X), that isŷ 0 t = arg max y 0 t p(y 0 t |X). The entire lower bound can then be written as:</p><formula xml:id="formula_3">log p(Y |X) ≥ T t=1 log p(y t |Ŷ L , X) + · · · + T t=1 log p(y 1 t |Ŷ 0 , X) + T t=1 log p(ŷ 0 t |X) .</formula><p>Stochastic Approximation In the case of stochastic approximation, we instead sampleŷ 0 t from the distribution p(y 0 t |X). This results in the unbiased estimate of the marginal log-probability log p(Y |X). Other than the difference in whether most likely values or samples are used, the remaining steps are identical.</p><p>Latent Variables Although the intermediate random variables could be anonymous, we constrain them to be of the same type as the output Y is, in order to share an underlying neural network. This constraint allows us to view each conditional p(Y l |Ŷ l−1 , X) as a single-step of refinement of a rough target sequenceŶ l−1 . The entire chain of L conditionals is then the L-step iterative refinement. Furthermore, sharing the parameters across these refinement steps enables us to dynamically adapt the number of iterations per input X. This is important as it substantially reduces the amount of time required for decoding, as we see later in the experiments.</p><p>Training For each training pair (X, Y * ), we first approximate the marginal log-probability. We then minimize</p><formula xml:id="formula_4">J LVM (θ) = − L+1 l=0 T t=1 log p θ (y * t |Ŷ l−1 , X) ,<label>(2)</label></formula><p>whereŶ l−1 = (ŷ l−1 1 , . . . ,ŷ l−1 T ), and θ is a set of parameters. We initializeŷ 0 t (t-th target word in the first iteration) as x t , where t = (T /T ) · t. T and T are the lengths of the source X and target Y * , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Denoising Autoencoder</head><p>The proposed approach could instead be viewed as learning a conditional denoising autoencoder which is known to capture the gradient of the logdensity. That is, we implicitly learn to find a direction ∆ Y in the output space that maximizes the underlying true, data-generating distribution log P (Y |X). Because the output space is discrete, much of the theoretical analysis by <ref type="bibr" target="#b0">Alain and Bengio (2014)</ref> are not strictly applicable. We however find this view attractive as it serves as an alternative foundation for designing a learning algorithm.</p><p>Training We start with a corruption process C(Y |Y * ), which introduces noise to the correct output Y * . Given the reference translation Y * , we sampleỸ ∼ C(Y |Y * ) which becomes as an input to each conditional in Eq. (1). Then, the goal of learning is to maximize the log-probability of the original reference Y * given the corrupted version. That is, to minimize</p><formula xml:id="formula_5">J DAE (θ) = − T t=1 log p θ (y * t |Ỹ , X).<label>(3)</label></formula><p>Once this cost J DAE is minimized, we can recursively perform the maximum-a-posterior inference, i.e.,Ŷ = arg max Y log p θ (Y |X), to findŶ that (approximately) maximizes log p(Y |X).</p><p>Corruption Process C There is little consensus on the best corruption process for a sequence, especially of discrete tokens. In this work, we use a corruption process proposed by <ref type="bibr" target="#b13">Hill et al. (2016)</ref>, which has recently become more widely adopted (see, e.g., <ref type="bibr" target="#b1">Artetxe et al., 2017;</ref><ref type="bibr" target="#b25">Lample et al., 2017)</ref>. Each y * t in a reference target Y * = (y * 1 , . . . , y * T ) is corrupted with a probability β ∈ [0, 1]. If decided to corrupt, we either (1) replace y * t+1 with this token y * t , (2) replace y * t with a token uniformly selected from a vocabulary of all unique tokens at random, or (3) swap y * t and y * t+1 . This is done sequentially from y * 1 until y * T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Cost function Although it is possible to train the proposed non-autoregressive sequence model using either of the cost functions above (J LVM or J DAE ,) we propose to stochastically mix these two cost functions. We do so by randomly replacing each termŶ l−1 in Eq.</p><p>(2) withỸ in Eq. <ref type="formula" target="#formula_5">(3)</ref>:</p><formula xml:id="formula_6">J(θ) = − L+1 l=0 α l T t=1 log p θ (y * t |Ŷ l−1 , X) (4) +(1 − α l ) T t=1 log p θ (y * t |Ỹ , X) ,</formula><p>whereỸ ∼ C(Y |Y * ), and α l is a sample from a Bernoulli distribution with the probability p DAE . p DAE is a hyperparameter. As the first conditional p(Y 0 |X) in Eq. (1) does not take as input any target Y , we set α 0 = 1 always.</p><p>Distillation <ref type="bibr" target="#b11">Gu et al. (2017)</ref>, in the context of machine translation, and Oord et al. <ref type="formula" target="#formula_4">(2017)</ref>, in the context of speech generation, have recently discovered that it is important to use knowledge distillation <ref type="bibr" target="#b14">(Hinton et al., 2015;</ref><ref type="bibr" target="#b21">Kim and Rush, 2016)</ref> to successfully train a non-autoregressive sequence model. Following <ref type="bibr" target="#b11">Gu et al. (2017)</ref>, we also use knowledge distillation by replacing the reference target Y * of each training example (X, Y * ) with a target Y AR generated from a welltrained autoregressive counterpart. Other than this replacement, the cost function in Eq (4) and the model architecture remain unchanged.</p><p>Target Length Prediction One difference between the autoregressive and non-autoregressive models is that the former naturally models the length of a target sequence without any arbitrary upper-bound, while the latter does not. It is hence necessary to separately model p(T |X), where T is the length of a target sequence, although during training, we simply use the length of each reference target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference: Decoding</head><p>Inference in the proposed approach is entirely deterministic.</p><p>We start from the input X and first predict the length of the target sequenceT = arg max T log p(T |X). Then, given X andT we generate the initial target sequence byŷ 0 t = arg max yt log p(y 0 t |X), for t = 1, . . . , T We continue refining the target sequence byŷ l t = arg max yt log p(y l t |Ŷ l−1 , X), for t = 1, . . . , T .</p><p>Because these conditionals, except for the initial one, are modeled by a single, shared neural network, this refinement can be performed as many iterations as necessary until a predefined stopping criterion is met.</p><p>A criterion can be based either on the amount of change in a target sequence after each iteration (i.e., D(Ŷ l−1 ,Ŷ l ) ≤ ), or on the amount of change in the conditional log-probabilities (i.e., | log p(Ŷ l−1 |X) − log p(Ŷ l−1 |X)| ≤ ) or on the computational budget. In our experiments, we use the first criterion and use Jaccard distance as our distance function D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Non-Autoregressive Neural Machine Translation Schwenk (2012) proposed a continuousspace translation model to estimate the conditional distribution over a target phrase given a source phrase, while dropping the conditional dependencies among target tokens. The evaluation was however limited to reranking and to short phrase pairs (up to 7 words on each side) only. <ref type="bibr" target="#b17">Kaiser and Bengio (2016)</ref> investigated neural GPU <ref type="bibr" target="#b18">(Kaiser and Sutskever, 2015)</ref>, for machine translation. They evaluated both non-autoregressive and autoregressive approaches, and found that the non-autoregressive approach significantly lags behind the autoregressive variants. It however differs from our approach that each iteration does not output a refined version from the previous iteration. The recent paper by <ref type="bibr" target="#b11">Gu et al. (2017)</ref> is most relevant to the proposed work. They similarly introduced a sequence of discrete latent variables. They however use supervised learning for inference, using the word alignment tool <ref type="bibr" target="#b9">(Dyer et al., 2013)</ref>. To achieve the best result, <ref type="bibr" target="#b11">Gu et al. (2017)</ref> stochastically sample the latent variables and rerank the corresponding target sequences with an external, autoregressive model. This is in contrast to the proposed approach which is fully deterministic during decoding and does not rely on any extra reranking mechanism.</p><p>Parallel WaveNet Simultaneously with <ref type="bibr" target="#b11">Gu et al. (2017)</ref>, Oord et al. <ref type="formula" target="#formula_4">(2017)</ref> presented a nonautoregressive sequence model for speech generation. They use inverse autoregressive flow <ref type="bibr">(IAF, Kingma et al., 2016)</ref> to map a sequence of independent random variables to a target sequence. They apply the IAF multiple times, similarly to our iterative refinement strategy. Their approach is however restricted to continuous target variables, while the proposed approach in principle could be applied to both discrete and continuous variables. <ref type="bibr" target="#b28">Novak et al. (2016)</ref> proposed a convolutional neural network that iteratively predicts and applies token substitutions given a translation from a phasebased translation system. Unlike their system, our approach can edit an intermediate translation with a higher degree of freedom. QuickEdit (Grangier and Auli, 2017) and deliberation network <ref type="bibr" target="#b37">(Xia et al., 2017)</ref> incorporate the idea of refinement into neural machine translation. Both systems consist of two autoregressive decoders. The second decoder takes into account the translation generated by the first decoder. We extend these earlier efforts by incorporating more than one refinement steps without necessitating extra annotations. <ref type="bibr" target="#b3">Bordes et al. (2017)</ref> proposed an unconditional generative model for images based on iterative refinement. At each step l of iterative refinement, the model is trained to maximize the log-likelihood of target Y given the weighted mixture of generated samples from the previous iterationŶ l−1 and a corrupted targetỸ . That is, the corrupted version of target is "infused" into generated samples during training. In the domain of text, however, computing a weighted mixture of two sequences of discrete tokens is not well defined, and we propose to stochastically mix denoising and lowerbound maximization objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-Editing for Machine Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Infusion Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Network Architecture</head><p>We use three transformer-based network blocks to implement our model. The first block ("Encoder") encodes the input X, the second block ("Decoder 1") models the first conditional log p(Y 0 |X), and the final block ("Decoder 2") is shared across iterative refinement steps, modeling log p(Y l |Ŷ l−1 , X). These blocks are depicted side-by-side in <ref type="figure">Fig. 2</ref>. The encoder is identical to that from the original Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref>. We however use the decoders from <ref type="bibr" target="#b11">Gu et al. (2017)</ref> with additional positional attention and use the highway layer <ref type="bibr" target="#b34">(Srivastava et al., 2015)</ref> instead of the residual layer <ref type="bibr" target="#b12">(He et al., 2016)</ref>.</p><p>The original input X is padded or shortned to fit the length of the reference target sequence before being fed to Decoder 1. At each refinement step l, Decoder 2 takes as input the predicted target se-quenceŶ l−1 and the sequence of final activation vectors from the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setting</head><p>We evaluate the proposed approach on two sequence modeling tasks: machine translation and image caption generation. We compare the proposed non-autoregressive model against the autoregressive counterpart both in terms of generation quality, measured in terms of BLEU <ref type="bibr" target="#b31">(Papineni et al., 2002)</ref>, and generation efficiency, measured in terms of (source) tokens and images per second for translation and image captioning, respectively.</p><p>Machine Translation We choose three tasks of different sizes: IWSLT'16 En↔De (196k pairs), WMT'16 En↔Ro (610k pairs) and WMT'14 En↔De (4.5M pairs). We tokenize each sentence using a script from Moses <ref type="bibr" target="#b24">(Koehn et al., 2007)</ref> and segment each word into subword units using BPE <ref type="bibr" target="#b33">(Sennrich et al., 2016)</ref>. We use 40k tokens from both source and target for all the tasks. For WMT'14 En-De, we use newstest-2013 and newstest-2014 as development and test sets. For WMT'16 En-Ro, we use newsdev-2016 and newstest-2016 as development and test sets. For IWSLT'16 En-De, we use test2013 for validation.</p><p>We closely follow the setting by <ref type="bibr" target="#b11">Gu et al. (2017)</ref>. In the case of IWSLT'16 En-De, we use the small model (d model = 278, d hidden = 507, p dropout = 0.1, n layer = 5 and n head = 2). 2 For WMT'14 En-De and WMT'16 En-Ro, we use the base transformer by <ref type="bibr" target="#b36">Vaswani et al. (2017)</ref> (d model = 512, d hidden = 512, p dropout = 0.1, n layer = 6 and n head = 8). We use the warm-up learning rate scheduling <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> for the WMT tasks, while using linear annealing (from 3 × 10 −4 to 10 −5 ) for the IWSLT task. We do not use label smoothing nor average multiple check-pointed models. These decisions were made based on the preliminary experiments. We train each model either on a single  <ref type="table">Table 1</ref>: Generation quality (BLEU↑) and decoding efficiency (tokens/sec↑ for translation, images/sec↑ for image captioning). Decoding efficiency is measured sentence-by-sentence. AR: autoregressive models. b: beam width. i dec : the number of refinement steps taken during decoding. Adaptive: the adaptive number of refinement steps. NAT: non-autoregressive transformer models <ref type="bibr" target="#b11">(Gu et al., 2017)</ref>. FT: fertility. NPD reranking using 100 samples. P40 (WMT'14 En-De and WMT'16 En-Ro) or on a single P100 (IWSLT'16 En-De) with each minibatch consisting of approximately 2k tokens. We use four P100's to train non-autoregressive models on WMT'14 En-De.</p><p>Image Caption Generation: MS COCO We use MS COCO <ref type="bibr" target="#b26">(Lin et al., 2014)</ref>. We use the publicly available splits <ref type="bibr" target="#b20">(Karpathy and Li, 2015)</ref>, consisting of 113,287 training images, 5k validation images and 5k test images. We extract 49 512-dimensional feature vectors for each image, using a ResNet-18 <ref type="bibr" target="#b12">(He et al., 2016)</ref> pretrained on ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. The average of these vectors is copied as many times to match the length of the target sentence (reference during training and predicted during evaluation) to form the initial input to Decoder 1. We use the base transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> except that n layer is set to 4. We train each model on a single 1080ti with each minibatch consisting of approximately 1,024 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Length Prediction</head><p>We formulate the target length prediction as classification, predicting the difference between the target and source lengths for translation and the target length for image captioning. All the hidden vectors from the n layer layers of the encoder are summed and fed to a softmax classifier after affine transformation. We however do not tune the encoder's parameters for target length prediction. We use this length predictor only during test time. We find it important to accurately predict the target length for good overall performance. See Appendix A for an analysis on our length prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>We use Adam (Kingma and Ba, 2014) and use L = 3 in Eq. (1) during training (i train = 4 from hereon.) We use p DAE = 0.5. We use the deterministic strategy for IWSLT'16 En-De, WMT'16 En-Ro and MS COCO, while the stochastic strategy is used for WMT'14 En-De. These decisions were made based on the validation set performance. After both the non-autogressive sequence model and target length predictor are trained, we decode by first predicting the target length and then running iterative refinement steps until the outputs of consecutive iterations are the same (or Jaccard distance between consecutive decoded sequences is 1). To assess the effectiveness of this adaptive scheme, we also test a fixed number of steps (i dec ). In machine translation, we remove any repetition by collapsing multiple consecutive occurrences of a token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Analysis</head><p>We make some important observations in <ref type="table">Table 1</ref>. First, the generation quality improves across all the tasks as we run more refinement steps i dec even beyond that used in training (i train = 4), which supports our interpretation as a conditional denoising autoencoder in Sec. 3.2. To further verify this, we run decoding on WMT'14 (both directions) up to 100 iterations. As shown in <ref type="figure">Fig. 1 (a)</ref>, the quality improves well beyond the number of refinement steps used during training.</p><p>Second, the generation efficiency decreases as more refinements are made. We plot the average seconds per sentence in <ref type="figure">Fig. 1 (b)</ref>, measured on GPU while sequentially decoding one sentence at a time. As expected, decoding from the autoregressive model linearly slows down as the sen-  tence length grows, while decoding from the nonautoregressive model with a fixed number of iterations has the constant complexity. However, the generation efficiency of non-autoregressive model decreases as more refinements are made. To make a smooth trade-off between the quality and speed, the adaptive decoding scheme allows us to achieve near-best generation quality with a significantly lower computational overhead. Moreover, the adaptive decoding scheme automatically increases the number of refinement steps as the sentence length increases, suggesting that this scheme captures the amount of information in the input well. The increase in latency is however less severe than that of the autoregressive model. We also observe that the speedup in decoding is much clearer on GPU than on CPU. This is a consequence of highly parallel computation of the proposed non-autoregressive model, which is better suited to GPUs, showcasing the potential of using the non-autoregressive model with a specialized hardware for parallel computation, such as Google's TPUs <ref type="bibr" target="#b16">(Jouppi et al., 2017)</ref>. The results of our model decoded with adaptive decoding scheme are comparable to the results from <ref type="bibr" target="#b11">(Gu et al., 2017)</ref>, without relying on any external tool. On WMT'14 En-De, the proposed model outperforms the best model from <ref type="bibr" target="#b11">(Gu et al., 2017)</ref> by two points.</p><p>Lastly, it is encouraging to observe that the proposed non-autoregressive model works well on image caption generation. This result confirms the generality of our approach beyond machine translation, unlike that by <ref type="bibr" target="#b11">Gu et al. (2017)</ref> which was for machine translation or by <ref type="bibr" target="#b30">Oord et al. (2017)</ref> which was for speech synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We use IWSLT'16 En-De to investigate the impact of different number of refinement steps during training (denoted as i train ) as well as probability of using denoising autoencoder objective during training (denoted as p DAE ). The   <ref type="table" target="#tab_2">Table 2</ref>. First, we observe that it is beneficial to use multiple iterations of refinement during training. By using four iterations (one step of decoder 1, followed by three steps of decoder 2), the BLEU score improved by approximately 1.5 points in both directions. We also notice that it is necessary to use the proposed hybrid learning strategy to maximize the improvement from more iterations during training (i train = 4 vs. i train = 4, p DAE = 1.0 vs. i train = 4, p DAE = 0.5.) Knowledge distillation was crucial to close the gap between the proposed deterministic non-autoregressive sequence model and its autoregressive counterpart, echoing the observations by <ref type="bibr" target="#b11">Gu et al. (2017)</ref> and <ref type="bibr" target="#b30">Oord et al. (2017)</ref>. Finally, we see that removing repeating consecutive symbols improves the quality of the best trained models (i train = 4, p DAE = 0.5) by approximately +1 BLEU. This suggests that the proposed iterative refinement is not enough to remove repetitions on its own. Further investigation is necessary to properly tackle this issue, which we leave as a future work.</p><p>We then compare the deterministic and stochastic approximation strategies on IWSLT'16 En→De and WMT'14 En→De. According to the results in <ref type="table" target="#tab_4">Table 3</ref>, the stochastic strategy is crucial with a large corpus (WMT'14), while the deterministic strategy works as well or better with a small corpus (IWSLT'16). Both of the strategies benefit from knowledge distillation, but the gap between the two strategies when the dataset is large is much more apparent without knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Qualitative Analysis</head><p>Machine Translation In <ref type="table" target="#tab_5">Table 4</ref>, we present three sample translations and their iterative refinement steps from the development set of <ref type="bibr">IWSLT'16 (De→En)</ref>. As expected, the sequence generated from the first iteration is a rough version of translation and is iteratively refined over multiple steps. By inspecting the underlined sub-sequences, we see that each iteration does not monotonically improve the translation, but overall modifies the Src seitdem habe ich sieben Häuser in der Nachbarschaft mit den Lichtern versorgt und sie funktionierenen wirklich gut . Iter 1 and I 've been seven homes since in neighborhood with the lights and they 're really functional . Iter 2 and I 've been seven homes in the neighborhood with the lights , and they 're a really functional . Iter 4 and I 've been seven homes in neighborhood with the lights , and they 're a really functional . Iter 8 and I 've been providing seven homes in the neighborhood with the lights and they 're a really functional . Iter 20 and I 've been providing seven homes in the neighborhood with the lights , and they 're a very good functional . Ref since now , I 've set up seven homes around my community , and they 're really working .</p><p>Src er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die Nachrichten meistens deprimierten . Iter 1 he looked very happy , which was pretty unusual the , because the news was were usually depressing . Iter 2 he looked very happy , which was pretty unusual at the , because the news was s depressing . Iter 4 he looked very happy , which was pretty unusual at the , because news was mostly depressing . Iter 8 he looked very happy , which was pretty unusual at the time because the news was mostly depressing . Iter 20 he looked very happy , which was pretty unusual at the time , because the news was mostly depressing .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref</head><p>there was a big smile on his face which was unusual then , because the news mostly depressed him .</p><p>Src furchtlos zu sein heißt für mich , heute ehrlich zu sein . Iter 1 to be , for me , to be honest today . Iter 2 to be fearless , me , is to be honest today . Iter 4 to be fearless for me , is to be honest today . Iter 8 to be fearless for me , me to be honest today . Iter 20 to be fearless for me , is to be honest today . Ref so today , for me , being fearless means being honest . translation towards the reference sentence. Missing words are added, while unnecessary words are dropped. For instance, see the second example. The second iteration removes the unnecessary "were", and the fourth iteration inserts a new word "mostly". The phrase "at the time" is gradually added one word at a time.</p><p>Image Caption Generation <ref type="table">Table 5</ref> shows two examples of image caption generation. We observe that each iteration captures more and more details of the input image. In the first example (left), the bus was described only as a "yellow bus" in the first iteration, but the subsequent iterations refine it into "yellow and black bus". Similarly, "road" is refined into "lot". We notice this behavior in the second example (right) as well. The first iteration does not specify the place in which "a woman" is "standing on", which is fixed immediately in the second iteration: "standing on a tennis court". In the final and fourth iteration, the proposed model captures the fact that the "woman" is "holding" a racquet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Following on the exciting, recent success of nonautoregressive neural sequence modeling by <ref type="bibr" target="#b11">Gu et al. (2017)</ref> and Oord et al. <ref type="formula" target="#formula_4">(2017)</ref>, we proposed a deterministic non-autoregressive neural sequence model based on the idea of iterative refinement. We designed a learning algorithm specialized to the proposed approach by interpreting the entire model as a latent variable model and each refinement step as denoising.</p><p>We implemented our approach using the Transformer and evaluated it on two tasks: machine translation and image caption generation. On both tasks, we were able to show that the proposed nonautoregressive model performs closely to the autoregressive counterpart with significant speedup in decoding. Qualitative analysis revealed that the iterative refinement indeed refines a target sequence gradually over multiple steps.</p><p>Despite these promising results, we observed that proposed non-autoregressive neural sequence model is outperformed by its autoregressive counterpart in terms of the generation quality. The following directions should be pursued in the future to narrow this gap. First, we should investigate better approximation to the marginal logprobability. Second, the impact of the corruption process on the generation quality must be studied. Lastly, further work on sequence-to-sequence model architectures could yield better results in non-autoregressive sequence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Caption</head><p>Iter 1 a yellow bus parked on parked in of parking road . Iter 2 a yellow and black on parked in a parking lot . Iter 3 a yellow and black bus parked in a parking lot . Iter 4 a yellow and black bus parked in a parking lot .</p><p>Reference Captions a tour bus is parked on the curb waiting city bus parked on side of hotel in the rain . bus parked under an awning next to brick sidewalk a bus is parked on the curb in front of a building . a double decked bus sits parked under an awning Generated Caption</p><p>Iter 1 a woman standing on playing tennis on a tennis racquet . Iter 2 a woman standing on a tennis court a tennis racquet . Iter 3 a woman standing on a tennis court a a racquet . Iter 4 a woman standing on a tennis court holding a racquet .</p><p>Reference Captions a female tennis player in a black top playing tennis a woman standing on a tennis court holding a racquet . a female tennis player preparing to serve the ball . a woman is holding a tennis racket on a court a woman getting ready to reach for a tennis ball on the ground <ref type="table">Table 5</ref>: Two sample image captions from the proposed non-autoregressive sequence model. The images are from the development set of MS COCO. The first iteration is from decoder 1, while the subsequent ones are from decoder 2. Subsequences with changes across the refinement steps are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Impact of Length Prediction</head><p>The quality of length prediction has an impact on the overall translation/captioning performance. When using the reference target length (during inference), we consistently observed approximately 1 BLEU score improvement over reported results in the tables and figures across different datasets in the paper (see <ref type="table" target="#tab_7">Table 6</ref> for more detailed comparison).</p><p>We additionally compared our length prediction model with a simple baseline that uses length statistics of the corresponding training dataset (a non-parametric approach). To predict the target length for a source sentence with length L s , we take the average length of all the target sentences coupled with the sources sentences of length L s in the training set. Compared to this approach, our length prediction model predicts target length correctly twice as often (16% vs. 8%), and gives higher prediction accuracy within five tokens (83% vs. 69%) <ref type="bibr">IWSLT</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>(a) BLEU scores on WMT'14 En-De w.r.t. the number of refinement steps (up to 10 2 ). The x-axis is in the logarithmic scale. (b) the decoding latencies (sec/sentence) of different approaches on IWSLT'16 En→De. The y-axis is in the logarithmic scale. We compose three transformer blocks ("Encoder", "Decoder 1" and "Decoder 2") to implement the proposed non-autoregressive sequence model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>GPU CPU En→ Ro→ GPU CPU En→ De→ GPU CPU BLEU GPU CPU AR b = 1 28.64 34.11 70.3 32.2 31.93 31.55 55.6 15.7 23.77 28.15 54.0 15.8 23.47 4.3 2.1 b = 4 28.98 34.81 63.8 14.6 32.40 32.06 43.3 7.3 24.57 28.47 44.9 7.0 24.78 3.6 1.0</figDesc><table><row><cell></cell><cell></cell><cell cols="2">IWSLT'16 En-De</cell><cell cols="2">WMT'16 En-Ro</cell><cell cols="2">WMT'14 En-De</cell><cell></cell><cell cols="2">MS COCO</cell></row><row><cell cols="3">FT En→ De→ NAT 26.52 -FT+NPD 28.16 -</cell><cell>--</cell><cell>-27.29 29.06 -29.79 31.44</cell><cell>--</cell><cell>-17.69 21.47 -19.17 23.30</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>idec = 1</cell><cell cols="9">22.20 27.68 573.0 213.2 24.45 25.73 694.2 98.6 13.91 16.77 511.4 83.3 20.12 17.1 8.9</cell></row><row><cell>Our Model</cell><cell cols="10">idec = 2 idec = 5 idec = 10 27.11 32.31 98.8 24.1 29.32 30.19 93.1 14.8 21.61 25.48 90.4 12.3 21.24 2.0 1.2 24.82 30.23 423.8 110.9 27.10 28.15 332.7 62.8 16.95 20.39 393.6 49.6 20.88 12.0 5.7 26.58 31.85 189.7 52.8 28.86 29.72 194.4 29.0 20.26 23.86 139.7 23.1 21.12 6.2 2.8</cell></row><row><cell></cell><cell cols="10">Adaptive 27.01 32.43 125.9 29.3 29.66 30.30 118.3 16.5 21.54 25.43 107.2 20.3 21.12 10.8 4.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the dev set of IWSLT'16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Deterministic and stochastic approximation results are presented in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Three sample De→En translations from</cell></row><row><cell>the non-autoregressive sequence model. Source sen-</cell></row><row><cell>tences are from the dev set of IWSLT'16. The first</cell></row><row><cell>iteration corresponds to Decoder 1, and from thereon,</cell></row><row><cell>Decoder 2 is repeatedly applied. Sub-sequences with</cell></row><row><cell>changes across the refinement steps are underlined.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>En→ →En En→ →En pred 27.01 32.43 29.66 30.30 21.54 25.43 ref 28.15 33.11 30.42 31.26 22.10 26.40</figDesc><table><row><cell>'16</cell><cell>WMT'16</cell><cell>WMT'14</cell></row><row><cell>En→ →En</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>BLEU scores on each dataset when using reference length (ref) and predicted target length (pred).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We release the implementation, preprocessed datasets as well as trained models online at https://github.com/ nyu-dl/dl4mt-nonauto.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Due to the space constraint, we refer readers to<ref type="bibr" target="#b36">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b11">Gu et al., 2017)</ref> for more details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank support by AdeptMind, eBay, TenCent and NVIDIA. This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure). We also thank Jiatao Gu for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What regularized auto-encoders learn from the datagenerating distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to generate samples from noise through infusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katya</forename><surname>Gonina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01769</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Noisy parallel approximate decoding for conditional recurrent language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03835</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04805</idno>
		<title level="m">Quickedit: Editing text &amp; translations via simple delete actions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Nonautoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Decoding as continuous optimization in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02854</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Can active memory replace attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08228</idno>
		<title level="m">Neural GPUs learn algorithms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Iterative refinement for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06602</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
		<meeting>COLING 2012: Posters</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
