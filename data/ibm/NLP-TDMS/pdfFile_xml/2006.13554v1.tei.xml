<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Normalized Loss Functions for Deep Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
						</author>
						<title level="a" type="main">Normalized Loss Functions for Deep Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting.</p><p>To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels is of great practical importance. Different approaches have been proposed for robust learning with noisy labels. This includes 1) label correction methods that aim to identify and correct wrong labels <ref type="bibr" target="#b31">(Xiao et al., 2015;</ref><ref type="bibr" target="#b25">Vahdat, 2017;</ref><ref type="bibr" target="#b26">Veit et al., 2017;</ref><ref type="bibr" target="#b14">Li et al., 2017b)</ref>; 2) loss correction methods that correct the loss function based on an estimated noise transition matrix <ref type="bibr" target="#b22">(Sukhbaatar et al., 2014;</ref><ref type="bibr" target="#b21">Reed et al., 2014;</ref><ref type="bibr" target="#b19">Patrini et al., 2017;</ref><ref type="bibr" target="#b4">Han et al., 2018a)</ref>; 3) refined training strategies that modify the training procedure to be more adaptive to incorrect labels <ref type="bibr" target="#b7">(Jiang et al., 2018;</ref><ref type="bibr" target="#b29">Wang et al., 2018;</ref><ref type="bibr" target="#b24">Tanaka et al., 2018;</ref><ref type="bibr" target="#b16">Ma et al</ref> 2018; <ref type="bibr" target="#b5">Han et al., 2018b)</ref>; and 4) robust loss functions that are inherently tolerant to noisy labels <ref type="bibr" target="#b2">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b34">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b30">Wang et al., 2019c)</ref>. Compared to the first three approaches that may suffer from inaccurate noise estimation or involve sophisticated training procedure modifications, robust loss functions provide a simpler solution, which is also the main focus of this paper.</p><p>It has been theoretically shown that some loss functions such as Mean Absolute Error (MAE) are robust to label noise, while others are not, which unfortunately includes the commonly used Cross Entropy (CE) loss. This has motivated a body of work to design new loss functions that are inherently robust to noisy labels. For example, Generalized Cross Entropy (GCE) <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018)</ref> was proposed to improve the robustness of CE against noisy labels. GCE can be seen as a generalized mixture of CE and MAE, and is only robust when reduced to the MAE loss. Recently, a Symmetric Cross Entropy (SCE) <ref type="bibr" target="#b30">(Wang et al., 2019c)</ref> loss was suggested as a robustly boosted version of CE. SCE combines the CE loss with a Reverse Cross Entropy (RCE) loss, and only the RCE term is robust. Whilst these loss functions have demonstrated improved robustness, theoretically, they are only partially robust to noisy labels. Different from previous works, in this paper, we theoretically show that any loss can be made robust to noisy labels, and all is needed is a simple normalization. However, in practice, simply being robust is not enough for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they all suffer from an underfitting problem. Inspired by recent developments in this field, we propose to characterize existing loss functions into two types: 1) "Active" loss, which only explicitly maximizes the probability of being in the labeled class, and 2) "Passive" loss, which also explicitly minimizes the probabilities of being in other classes. Based on this characterization, we further propose a novel framework to build a new set of robust loss functions called Active Passive Losses (APLs). We show that under this framework, existing loss functions can be reworked to achieve the state-of-the-art for training DNNs with noisy labels. Our key contributions are:</p><p>• We provide new theoretical insights into robust loss functions demonstrating that a simple normalization can make any loss function robust to noisy labels. <ref type="bibr">arXiv:2006.13554v1 [cs.</ref>LG] 24 Jun 2020</p><p>• We identify that existing robust loss functions suffer from an underfitting problem. To address this, we propose a generic framework Active Passive Loss (APL) to build new loss functions with theoretically guaranteed robustness and sufficient learning properties.</p><p>• We empirically demonstrate that the family of new loss functions created following our APL framework can outperform the state-of-the-art methods by considerable margins, especially under large noise rates of 60% or 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review existing approaches for robust learning with noisy labels.</p><p>1) Label correction methods. The idea of label correction is to improve the quality of the raw labels, possibly correcting wrong labels into correct ones. One common approach is to apply corrections via a clean label inference step using complex noise models characterized by directed graphical models <ref type="bibr" target="#b31">(Xiao et al., 2015)</ref>, conditional random fields <ref type="bibr" target="#b25">(Vahdat, 2017)</ref>, neural networks <ref type="bibr" target="#b12">(Lee et al., 2017;</ref><ref type="bibr" target="#b26">Veit et al., 2017)</ref> or knowledge graphs <ref type="bibr" target="#b14">(Li et al., 2017b)</ref>. These methods require support from extra clean data or a potentially expensive detection process to estimate the noise model.</p><p>2) Loss correction methods. This approach improves robustness by modifying the loss function during training, based on label-dependent weights <ref type="bibr" target="#b18">(Natarajan et al., 2013)</ref> or an estimated noise transition matrix that defines the probability of mislabeling one class with another <ref type="bibr" target="#b4">(Han et al., 2018a)</ref>. Backward and Forward <ref type="bibr" target="#b19">(Patrini et al., 2017)</ref> are two noise transition matrix based loss correction methods. Work in <ref type="bibr" target="#b3">(Goldberger &amp; Ben-Reuven, 2017;</ref><ref type="bibr" target="#b22">Sukhbaatar et al., 2014)</ref> augments the correction architecture by adding a linear layer on top of the neural network. Bootstrap <ref type="bibr" target="#b21">(Reed et al., 2014)</ref> uses a combination of raw labels and their predicted labels. Label Smoothing Regularization (LSR) <ref type="bibr" target="#b23">(Szegedy et al., 2016;</ref><ref type="bibr" target="#b20">Pereyra et al., 2017)</ref> uses soft labels in place of one-hot labels to alleviate overfitting to noisy labels. Loss correction methods are sensitive to the noise transition matrix. Given that ground-truth is not always available, this matrix is typically difficult to estimate.</p><p>3) Refined training strategies. This direction designs adaptive training strategies that are more robust to noisy labels. MentorNet <ref type="bibr" target="#b7">(Jiang et al., 2018;</ref><ref type="bibr" target="#b33">Yu et al., 2019)</ref> supervises the training of a StudentNet by a learned sample weighting scheme in favor of probably correct labels. SeCoST extends MentorNet to a cascade of student-teacher pairs via a knowledge transfer method <ref type="bibr" target="#b10">(Kumar &amp; Ithapu, 2019)</ref>. Decoupling training strategy <ref type="bibr" target="#b17">(Malach &amp; Shalev-Shwartz, 2017)</ref> trains two networks simultaneously, and parameters are updated when their predictions disagree. Co-teaching <ref type="bibr" target="#b5">(Han et al., 2018b)</ref> allows one network learn from the other network's most confident samples. These studies all require an auxiliary network for sample weighting or learning supervision. D2L  uses subspace dimensionality adapted labels for learning, paired with a training process monitor. The joint optimization framework <ref type="bibr" target="#b24">(Tanaka et al., 2018)</ref> updates DNN parameters and labels alternately. <ref type="bibr" target="#b8">Kim et al. (2019)</ref> use complementary labels to mitigate overfitting to original labels. <ref type="bibr" target="#b32">Xu et al. (2019)</ref> introduce a Determinantbased Mutual Information (DMI) loss for robust fine-tuning of a CE pre-trained model. These methods either rely on complex interventions into the learning process which are hard to adapt and tune, or are sensitive to hyperparameters like training epochs and learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Robust loss functions. Compared to the above three types of methods, robust loss functions are a simpler and arguably more generic solution for robust learning. Previous work has theoretically proved that some loss functions such as Mean Absolute Error (MAE) are robust to noisy labels, while others like the commonly used Cross Entropy (CE) loss are not <ref type="bibr" target="#b2">(Ghosh et al., 2017)</ref>. However, training with MAE has been found very challenging due to slow convergence caused by gradient saturation <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018)</ref>. The Generalized Cross Entropy (GCE) loss <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018</ref>) applies a Box-Cox transformation to probabilities (power law function of probability with exponent ρ ∈ (0, 1]) which can behave like a generalized mixture of MAE and CE. Recently, <ref type="bibr" target="#b30">Wang et al. (2019c)</ref> proposed the Symmetric Cross Entropy (SCE) which combines a Reverse Cross Entropy (RCE) together with the CE loss. Both GCE and SCE are only partially robust to noisy labels. For example, GCE is only robust when it reduces to the MAE loss with ρ = 1. For SCE, only its RCE term is robust. Empirically (rather than theoretically) justified approaches that directly modify the magnitude of the loss gradients are also an active line of research <ref type="bibr" target="#b27">(Wang et al., 2019a;</ref>.</p><p>In this paper, we theoretically prove that, with simple normalization, any loss can be made robust to noisy labels. This new theoretical insight can serve as a basic principle for designing new robust loss functions. It also can reshape the design of new loss functions towards other properties rather than robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Any Loss can be Robust to Noisy Labels</head><p>We next introduce some background knowledge about robust classification with noisy labels, then propose a simple but theoretically sound normalization method that can be applied to any loss function to make it robust to noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Given a K-class dataset with noisy labels as D = {(x, y) (i) } n i=1 , with x ∈ X ⊂ R d denoting a sample and y ∈ Y = {1, · · · , K} its annotated label (possibly incorrect). We denote the distribution over different labels for sample x by q(k|x), and K k=1 q(k|x) = 1. In this paper, we focus on the common case where there is only one single label y for x: i.e. q(y|x) = 1 and q(k = y|x) = 0. In this case, q is simply the one-hot encoding of the label.</p><p>We denote the true label of x as y * . While noisy labels may arise in different ways, one common assumption is that, given the true labels, the noise is conditionally independent to the inputs, i.e., q(y = k|y * = j, x) = q(y = k|y * = j). Under this assumption, label noise can be either symmetric (or uniform), or asymmetric (or class-conditional). We denote the overall noise rate by η ∈ [0, 1] and the class-wise noise rate from class j to class k by η jk . Then, for symmetric noise, η jk = η K−1 for j = k and η jk = 1 − η for j = k. For asymmetric noise, η jk is conditioned on both the true class j and mislabeled class k.</p><p>Classification is to learn a function f : X → Y (as represented by a DNN) that maps the input space to the label space. For a sample x, we denote the probability output of a DNN classifier f (x) as: p(k|x) = e z k K j=1 e z j , where z k denotes the logits output of the network with respect to class k. Training classifier f is to find a set of optimal parameters θ that minimize the empirical risk defined by a loss function:</p><formula xml:id="formula_0">θ := arg min θ n i=1 L(f (x i ), y i ), where L(f (x), y)</formula><p>is the loss of f with respect to label y. Next, we briefly introduce four loss functions that are either popularly used or recently proposed for robust classification with noisy labels.</p><p>Existing loss functions. The commonly used Cross Entropy (CE) loss on sample x is defined as: CE = − K k=1 q(k|x) log p(k|x), which has been proved not robust to noisy labels <ref type="bibr" target="#b2">(Ghosh et al., 2017)</ref>.</p><p>Mean Absolute Error (MAE) is also a popular classification loss, and is defined as: M AE = K k=1 |p(k|x) − q(k|x)|. MAE is provably robust to label noise <ref type="bibr" target="#b2">(Ghosh et al., 2017)</ref>.</p><p>The recently proposed Reverse Cross Entropy (RCE) loss <ref type="bibr" target="#b30">(Wang et al., 2019c</ref>) is defined as: RCE = − K k=1 p(k|x) log q(k|x), with q(k = y|x) = 0 is truncated to a small value such that log(q(k = y|x)) = A (eg. A = −4). RCE has also been proved to be robust to label noise, and can be combined with CE to form the Symmetric Cross Entropy (SCE) for robust classification and boosted learning <ref type="bibr" target="#b30">(Wang et al., 2019c)</ref>.</p><p>Focal Loss (FL) <ref type="bibr" target="#b15">(Lin et al., 2017)</ref>, originally proposed for dense object detection, is also an effective loss function for classification. FL is also a generalization of the CE loss, and is defined as: F L = − K k=1 q(k|x)(1 − p(k|x)) γ log p(k|x), where γ ≥ 0 is a tunable parameter. FL reduces to the CE loss when γ = 0, and is not robust to noisy labels following <ref type="bibr" target="#b2">(Ghosh et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Normalized Loss Functions</head><p>Following <ref type="bibr" target="#b2">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b0">Charoenphakdee et al., 2019)</ref>, we know that if a loss function L satisfies K j L(f (x), j) = C, ∀x ∈ X , ∀f , where C is some constant, then L is noise tolerant under mild assumptions. Based on this, we propose to normalize a loss function by:</p><formula xml:id="formula_1">L norm = L(f (x), y) K j=1 L(f (x), j)</formula><p>.</p><p>(1)</p><p>A normalized loss has the property:</p><formula xml:id="formula_2">L norm ∈ [0, 1].</formula><p>Accordingly, we can normalize the above four loss functions defined in Section 3.1 as follows. The Normalized Cross Entropy (NCE) loss can be defined as:</p><formula xml:id="formula_3">N CE = − K k=1 q(k|x) log p(k|x) − K j=1 K k=1 q(y = j|x) log p(k|x) = log K k p(k|x) p(y|x),<label>(2)</label></formula><p>where, the last equality holds following the change of base rule in logarithm (eg. log a b = log b log a ). The Normalized Mean Absolute Error (NMAE) is:</p><formula xml:id="formula_4">N M AE = K k=1 |p(k|x) − q(k|x)| K j=1 K k=1 |p(k|x) − q(y = j|x)| = 1 K − 1 (1 − p(y|x)) = 1 2(K − 1) · M AE.<label>(3)</label></formula><p>The last two equalities hold due to K k=1 |p(k|x) − q(k|x)| = 2(1 − p(y|x)). As can be observed, NMAE is simply a scaled version of MAE by a factor of 1 2(K−1) . The Normalized Reverse Cross Entropy (NRCE) loss is:</p><formula xml:id="formula_5">N RCE = − K k=1 p(k|x) log q(k|x) − K j=1 K k=1 p(k|x) log q(y = j|x) = 1 K − 1 (1 − p(y|x)) = 1 A(K − 1) · RCE.<label>(4)</label></formula><p>The last two equalies hold as K k=1 p(k|x) log q(k|x) = A(1−p(y|x)). Similar to NMAE, NRCE is a scaled version of RCE by a factor of 1 A(K−1) . The Normalized Focal Loss (NFL) can be defined as:</p><formula xml:id="formula_6">N F L = − K k=1 q(k|x)(1 − p(k|x)) γ log p(k|x) − K j=1 K k=1 q(y = j|x)(1 − p(k|x)) γ log p(k|x) = log K k (1−p(k|x)) γ p(k|x) (1 − p(y|x)) γ p(y|x).<label>(5)</label></formula><p>Under this normalization scheme, the normalized forms of robust loss functions such as MAE and RCE are simply a scaled version of their original forms. This keeps their robustness property. For the rest of this paper, we will use the original forms for MAE and RCE if not otherwise explicitly stated. On the contrary, normalization on nonrobust loss functions such as CE and FL derives new loss functions. Note that the above four normalized losses are just a proof-of-concept, other loss functions can also be normalized following Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical Justification</head><p>Following previous works <ref type="bibr" target="#b2">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b30">Wang et al., 2019c)</ref>, we can show that normalized loss functions are noise tolerant to both symmetric and asymmetric label noise. Lemma 1. In a multi-class classification problem, any normalized loss function L norm is noise tolerant under symmetric (or uniform) label noise, if noise rate η &lt; K−1 K . Lemma 2. In a multi-class classification problem, given</p><formula xml:id="formula_7">R(f * ) = 0 and 0 ≤ L norm (f (x), k) ≤ 1 K−1 , ∀k, any normalized loss function L norm is noise tolerant under asymmetric (or class-conditional) label noise, if noise rate η jk &lt; 1 − η y .</formula><p>Detailed proofs for Lemma 1 and Lemma 2 can be found in Appendix A. We denote the risk of classifier f under clean labels as R(f ) = E x,y * L norm , and the risk under label noise rate η as R η (f ) = E x,y L norm . Let f * and f * η be the global minimizers of R(f ) and R η (f ), respectively. We need to prove f * is also a global minimizer of noisy risk R η (f ) for L to be robust. The noise rate conditions in Lemma 1 (η &lt; K−1 K ) and Lemma 2 (η jk &lt; 1 − η y ) generally requires that the correct labels are still the majority of the class. In Lemma 2, the restrictive condition R(f * ) = 0 may not be satisfied in practice (eg. the classes may not completely separable), however, good empirical robustness can still be achieved. While the condition 0 ≤ L norm (f (x), k) ≤ 1 K−1 , ∀k can be easily satisfied by a typical loss function. We refer the reader to <ref type="bibr" target="#b0">(Charoenphakdee et al., 2019)</ref> for more discussions of other theoretical properties such as classification calibration.</p><p>So far, we have presented a somewhat surprising but theoretically justified result that any loss function can be made robust to noisy labels. This advances current theoretical progresses in this field. While this finding is exciting, in the following, we will empirically show that robustness alone is not sufficient for obtaining good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robustness Alone is not Sufficient</head><p>In this section, we empirically show that the above four robust loss functions (eg. NCE, NFL, MAE and RCE) all suffer from an underfitting problem, and thus are not sufficient by themselves to train accurate DNNs. We then propose a new framework to build loss functions that are both theoretically robust and learning sufficient.</p><p>Robust losses can suffer from underfitting. To motivate this problem, we use an example on CIFAR-100 dataset with 0.6 symmetric noise. We train a ResNet-34 <ref type="bibr" target="#b6">(He et al., 2016)</ref> using both normalized and unnormalized loss functions (detailed setting can be found in Section 5.2). As can be observed in <ref type="figure" target="#fig_0">Figure 1</ref>, CE and FL losses become robust after normalization, however, this robustness does not lead to more accurate models. In fact, robust losses NCE and NFL demonstrate even worse performance than nonrobust CE and FL. Moreover, even without normalization, the originally robust loss functions MAE and RCE also suffer from underfitting: they even fail to converge in this scenario. We find that this underfitting issue occur across different training settings in terms of learning rate, learning rate scheduler, weight decay and the number of training epochs. We identify this problem as an underfitting problem of existing robust loss functions, at least for the four tested loss functions (eg. NCE, NFL, MAE and RCE). Next, we will propose a new loss framework to address this problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proposed Active Passive Loss (APL)</head><p>In <ref type="bibr" target="#b8">(Kim et al., 2019)</ref>, the use of complementary labels ("input does not belong to this complementary class") together with the original labels was shown to help learning and robustness. In <ref type="bibr" target="#b30">(Wang et al., 2019c)</ref>, a Reverse Cross Entropy term was found can provide a robust boost to the CE loss. To generalize these works taking a loss function perspective, we characterize existing robust functions into two types: "Active" and "Passive", based on their optimization (maximization/minimization) behaviors.</p><p>At a high level, a loss is defined "Active" if it only optimizes at q(k = y|x) = 1, otherwise, a loss is defined as "Passive". We denote the basic function of loss</p><formula xml:id="formula_8">L(f (x), y) by (f (x), k), that is L(f (x), y) = K k=1 (f (x), k)</formula><p>. Then, we can define the active and passive loss functions as: According to the above two definitions, active losses only explicitly maximize the network's output probability at the class position specified by the label y. For example in CE loss, only the probability at q(k = y|x) = 1 is explicitly maximized (the loss is zero at q(k = y|x) = 0). Different from active losses, passive losses also explicitly minimize the probability at at least one other class positions. For example in MAE, the probabilities at position k = y are also explicitly minimized along with the maximization of the probability at k = y. Note that this characterization applies to both robust and nonrobust loss functions. <ref type="table" target="#tab_1">Table 1</ref> summarizes examples of active and passive losses. Definition of APL. Inspired by the benefit of symmetric <ref type="bibr" target="#b30">(Wang et al., 2019c)</ref> or complementary learning <ref type="bibr" target="#b8">(Kim et al., 2019)</ref>, we propose to combine a robust active loss and a robust passive loss into an "Active Passive Loss" (APL) framework for both robust and sufficient learning. Formally,</p><formula xml:id="formula_9">L APL = α · L Active + β · L Passive ,<label>(6)</label></formula><p>where, α, β &gt; 0 are parameters to balance the two terms. An important requirement for the two loss terms is robustness, which means a nonrobust loss should be normalized following Eq. (1) for it to be used within our APL scheme. This guarantees the robustness property of APL loss functions (proof can be found in Appendix A):</p><p>Lemma 3. ∀α, ∀β, if L Active and L Passive are noise tolerant, then L APL = α · L Active + β · L Passive is noise tolerant.</p><p>In APL, the two loss terms optimize the same objective from two complementary directions (eg. maximizing p(k = y|x) and minimizing p(k = y|x)). For the four loss functions considered in this paper, there are four possible combinations satisfying our APL principle: 1) αN CE + βM AE, 2) αN CE + βRCE, 3) αN F L + βM AE and 4) αN F L + βRCE. For simplicity we omit the parameters α, β in the rest of this paper. According to our active/passive definitions, APL losses can be considered as passive losses. However, APL losses are different from passive losses that have only one term, since they contain at least two terms and one of them is an active loss term. Whilst different choices of the two loss terms may lead to different performance, we will show in Section 5 that APL losses generally achieve better or at least comparable performance to state-of-the-art noisy label learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">More Insights into APL Loss Functions</head><p>Here, we provide some insights into the underfitting issue of robust loss functions, and why the proposed APL losses can address underfitting.</p><p>Why robust loss functions underfit? Taking the NCE loss defined in Eq.</p><p>(2) as an example, the underfitting is caused by the extra terms introduced into the denominator by the normalization. In Eq.</p><p>(2), NCE is in the form of P P +Q , where P = − log(p y ) and Q = − k =y log(p k ). During training, the Q term may increase even when P is fixed (eg. p y is fixed), and it reaches the highest value when all p k =y equals to (1 − p y )/(K − 1) (eg. the highest entropy). This implies that the network may learn nothing for the prediction (as p y is fixed) even when the loss decreases (as Q increases). This tends to hinder the convergence and cause the underfitting problem. Other robust loss functions such as MAE and RCE all suffer from a similar issue.</p><p>Why APL can address underfitting? APL combines an active loss with a passive loss. By definition, the passive loss explicitly minimizes (at least one component of) the Q term discussed above so that it wont increase when p y is fixed. This directly addresses the underfitting issue of a robust active loss. Therefore, APL losses can leverage both the robustness and the convergence advantages. Note that, by definition, passive loss has a broader scope than active loss. A single passive loss like MAE can be decomposed into an active term and a passive term, with the two terms already form an APL loss. With proper balancing between the two terms, the reformulated MAE can also be a powerful new loss. For example, a recent work has shown that a reweighted MAE can outperform CE <ref type="bibr" target="#b27">(Wang et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Connection to Related Work</head><p>Our APL framework is a generalization of several state-ofthe-art methods. Following APL, better performance can be achieved with existing loss functions, rather than complex modifications on the training procedure. Although NLNL <ref type="bibr" target="#b8">(Kim et al., 2019)</ref> can improve robustness with complementary labels, it has slow convergence (10× slower than standard training), and requires a complex 3-stage training procedure: 1) training with complementary labels, 2) training with high confidence (above a threshold) complementary labels, and 3) training with high confidence original labels. From our APL perspective, NLNL switches back and forth between active learning (with original labels) and passive learning (with complementary labels). Such a learning scheme can instead be achieved alternatively using our APL. Indeed, when defined on complementary labels, the CE loss becomes -1/(C-1)log(1-RCE) with A=-1 in RCE, and our APL loss NCE+RCE can be seen as a simpler alternative for NLNL. Compared to the SCE (Wang et al., 2019c) loss (eg. CE+RCE), our APL loss NCE+RCE can be seen as its normalized version, which has theoretically guaranteed robustness. This modification to SCE can improve its performance considerably (see Section 5.2). Compared to the GCE loss <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018)</ref> which can be regraded as a mixture of CE and MAE, our APL loss NCE+MAE is an alternative solution that directly adds the two terms together with normalization. NCE+MAE is theoretically robust while GCE is not. Moreover, the GCE loss itself can be normalized and improved following our APL framework (see Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we empirically investigate our proposed APL loss functions on benchmark datasets <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, CIFAR-10/-100 <ref type="bibr" target="#b9">(Krizhevsky &amp; Hinton, 2009)</ref>, and a real-world noisy dataset WebVision <ref type="bibr" target="#b13">(Li et al., 2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Empirical Understandings</head><p>Normalized losses are robust. We first run a set of experiments on CIFAR-10 and CIFAR-100 to verify whether non-robust losses CE and FL become robust after normalization (NCE and NFL). We set the label noise to be symmetric, and the noise rate to 0.6 for both CIFAR-10 and CIFAR-100. We use an 8-layer convolutional neural network (CNN) for CIFAR-10 and a ResNet-34 <ref type="bibr" target="#b6">(He et al., 2016)</ref> for CIFAR-100. On each dataset, we train the same network using different loss functions, eg. normalized versus unnormalized. For FL/NFL loss we set γ = 0.5, while for RCE/NRCE loss, we set A = −4. Detailed settings are in Section 5.2. <ref type="figure" target="#fig_0">Figures 1 &amp; 2</ref>, both CE and FL losses exhibit significant overfitting after epoch 25. However, as we have theoretically proved, their normalized forms (eg. NCE and NFL) are robust: no overfitting was observed during the entire training process. Moreover, for the already robust loss functions MAE and RCE, normalization does not break their robustness property. We observe the same results across different datasets (eg. MNIST, CIFAR-10 and CIFAR-100) under different noise rates (η ∈ [0.2, 0.8]). In general, the higher the noise rate, the more overfitting of nonrobust loss functions, and their normalized forms are always robust. This empirically verifies our theoretical finding that any loss can be made robust following normalization in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Can scaling help sufficient learning? As one may have noticed in <ref type="figure" target="#fig_0">Figures 1 &amp; 2</ref>, NMAE and NRCE exhibit more severe underfitting than MAE and RCE, even though they are just scaled versions of MAE and RCE. This raises the question: can the underfitting problem be addressed by scaling the normalized losses up by a factor? In <ref type="figure">Figure  3</ref>, we show different scales applied to NCE, NFL, MAE and RCE for training on CIFAR-100 with 0.6 symmetric noise. We find that scaled NCE and NFL only slightly improve learning after epoch 150, when the learning rate is decayed to be smaller. This is because scaling the loss is equivalent to scaling the gradients, a similar effect to increasing the learning rate. Moreover, scaled MAE and RCE still fail to converge in this scenario. This highlights that scaling may not be an effective solution for sufficient learning, especially for challenging datasets like CIFAR-100. On the simple dataset CIFAR-10, proper scaling does help learning. But this can alternatively can be achieved by adjusting the learning rate.</p><p>APL losses are both robust and learning sufficient. We show the effectiveness of "Active+Passive" learning, compared to other forms of combinations. We run experiments on CIFAR-10 and CIFAR-100 under the same settings as above. The parameters α, β for our APL are simply set to 1.0 without any tuning. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the 4 APL loss functions demonstrate a clear advantage over either AAL ("Active+Active Loss") or PPL ("Passive+Passive Loss"), especially for sufficient learning (high accuracy). The AAL and PPL loss functions are robust but still suffer from the underfitting problem. This highlights that the overfitting and underfitting problems can be addressed simultaneously by the joint of active and passive losses by our APL.</p><p>Parameter Analysis of APL. We tune the parameters α and β for NCE+RCE loss, then directly use these parameters for all other APL losses. This is also done on CIFAR-10 and CIFAR-100 datasets under 0.6 symmetric noise. We test the combinations between α ∈ {0.1, 1.0, 10.0} and β ∈  {0.1, 1.0, 10.0, 100.0}, then select the optimal combination according to the validation accuracy on a randomly sampled validation set (20% training data). As shown in <ref type="figure">Figure 5</ref>, the optimal parameters for CIFAR-10 are α = 1, β = 1, and CIFAR-100 are α = 10, β = 0.1. In general, on more complex dataset (eg. CIFAR-100 &gt; CIFAR-10), it requires more active learning (eg. a large α) and less passive learning (eg. a small β) to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Benchmark Datasets</head><p>Baselines. We consider 3 state-of-the-art methods: 1) Generalized Cross Entropy (GCE) <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018)</ref>; 2) Negative Learning for Noisy Labels (NLNL) <ref type="bibr" target="#b8">(Kim et al., 2019)</ref>; and 3) Symmetric Cross Entropy (SCE) <ref type="bibr" target="#b30">(Wang et al., 2019c)</ref>. For APL, we consider 4 loss functions: 1) NCE+MAE, 2) NCE+RCE, 3) NFL+MAE and 4) NFL+RCE. We also train networks using CE and FL losses.</p><p>Noise generation. The noisy labels are generated following standard approaches in previous works <ref type="bibr" target="#b19">(Patrini et al., 2017;</ref><ref type="bibr" target="#b16">Ma et al., 2018)</ref>. Symmetric noise is generated by flipping labels in each class randomly to incorrect labels of other classes. For asymmetric noise, we flip the labels within a specific set of classes. For CIFAR-10, flipping TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, CAT ↔ DOG. For CIFAR-100, the 100 classes are grouped into 20 super-classes with each has 5 sub-classes, we then flip each class within the same super-class into the next in a circular fashion. We vary the noise rate η ∈ [0.2, 0.8] for symmetric noise, and η ∈ [0.1, 0.4] for asymmetric noise.</p><p>Networks and training. We use a 4-layer CNN for MNIST, an 8-layer CNN for CIFAR-10 and a ResNet-34 for CIFAR-100. We train the networks for 50, 120 and 200 epochs for MNIST, CIFAR-10, and CIFAR-100, respectively. For all the training, we use SGD optimizer with momentum 0.9 and cosine learning rate annealing. Weight decay is set to 1 × 10 −3 , 1 × 10 −4 and 1 × 10 −5 for MNIST, CIFAR-10 and CIFAR-100, respectively. The initial learning rate is set to 0.01 for MNIST/CIFAR-10 and 0.1 for CIFAR-100. Typical data augmentations including random width/height shift and horizontal flip are applied.</p><p>Parameter setting. We tune the parameters for all baseline methods and find that the optimal settings match their original papers. Specifically, for GCE, we set ρ = 0.7 (see detailed definition in Section 5.3). For SCE, we set A = −4, and α = 0.01, β = 1.0 for MNIST, α = 0.1, β = 1.0 for CIFAR-10, α = 6.0, β = 0.1 for CIFAR-100. For FL, we set γ = 0.5. For our APL losses, we empirically set α = 1, β = 100 for MNIST, α, β = 1 for CIFAR-10, and α = 10, β = 0.1 for CIFAR-100.</p><p>Results. The classification accuracies under symmetric label noise are reported in <ref type="table" target="#tab_3">Table 2</ref>. As can be seen, our APL loss functions achieved the top 2 best results in all test scenarios across all datasets. The superior performance of APL losses is more pronounced when the noise rates are extremely high and the dataset is more complex. For example, on CIFAR-10 with 0.6 symmetric noise, our APL losses NFL+RCE and NCE+RCE outperform the state-ofthe-art robustness (72.85% of NLNL) by more than 6%. On CIFAR-100 with 0.8 symmetric noise where CE and FL both fail to converge, our NCE+MAE and NCE+RCE outperform the state-of-the-art methods GCE and NLNL by at least 9%. In several cases, all our 4 APL losses are better than baseline methods.</p><p>Results for asymmetric noise are reported in <ref type="table">Table 3</ref>. Again, all top 2 best results are achieved by our APL loss functions across different datasets and noise rates. On CIFAR-100 with 0.4 asymmetric noise, the highest accuracy that can be achieved by current methods is 42.19% (by SCE), which is still 5% lower than our NCE+MAE and 4% lower than our NCE+RCE. Comparing results in both <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table">Table  3</ref>, we find that the best combination of our APL loss varies across different datasets, but within the same dataset, is quite consistent across different noise types and noise rates.</p><p>Overall, NCE+RCE demonstrates a consistently strong performance across different datasets. The strong performance of our APL losses verifies the importance of theoretically guaranteed robustness and "Active+Passive" learning. Our proposed APL framework can be used as a general principle for developing new robust loss functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Improving New Loss Functions using APL</head><p>Next, we take GCE <ref type="bibr" target="#b34">(Zhang &amp; Sabuncu, 2018)</ref> as an example and show how to improve a new loss function using our APL framework. Given a sample x, GCE loss is defined as:</p><formula xml:id="formula_10">GCE = K k=1 q(k|x) 1−p(k|x) ρ ρ ,</formula><p>where ρ ∈ (0, 1]. GCE reduces to the MAE/unhinged loss and CE loss when ρ = 1 and ρ → 0, respectively. Following Eq. (1), the Normalized Generalized Cross Entropy (NGCE) loss can be defined as:</p><formula xml:id="formula_11">N GCE = (1 − p(y|x) ρ )/(K − K k=1 p(k|x) ρ ).</formula><p>Both GCE and NGCE are active loss functions <ref type="bibr">(eg. (f (x)</ref>, k) = 0, ∀k = y). Thus, following our APL in Eq. (6), we can define two APL losses for NGCE: 1) NGCE+MAE and 2) NGCE+RCE. Here, we simply set α, β = 1.0 for both APL losses. We compare their performance to GCE (with ρ = 0.7) on CIFAR-10 under both symmetric and asymmetric noise. As shown in <ref type="table">Table 4</ref>, both NGCE+MAE and NGCE+RCE can improve the performance of GCE under different noise settings, except for NGCE+RCE under 0.4 asymmetric noise. Particularly, under 0.8 symmetric noise, NGCE+MAE is able to improve GCE by &gt; 20%. A new loss function may have multiple terms, in this case, we can normalize its non-robust terms, and then add an active or passive loss into the loss function if there are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effectiveness on Real-world Noisy Labels</head><p>Here, we test the effectiveness of our APL loss functions on large-scale real-world noisy dataset WebVision <ref type="bibr" target="#b13">(Li et al., 2017a)</ref>. WebVision contains 2.4 million images of realworld noisy labels, crawled from the web (eg. Flickr and Google) based on the 1,000 class labels of ImageNet ILSVRC12 <ref type="bibr" target="#b1">(Deng et al., 2009)</ref>. Here, we follow the "Mini" setting in <ref type="bibr" target="#b7">(Jiang et al., 2018)</ref> that only takes the first 50 classes of the Google resized image subset. We evaluate the trained networks on the same 50 classes of the ILSVRC12 validation set, which can be considered as a clean validation. We compare our APL losses NCE+MAE and NCE+RCE with GCE and SCE. For each loss, we train a ResNet-50 <ref type="bibr" target="#b6">(He et al., 2016)</ref> using SGD for 250 epochs with initial learning rate 0.4, nesterov momentum 0.9 and weight decay 3 × 10 −5 and batch size 512. The learning rate is multiplied by 0.97 after every epoch of training. We resize the images to 224 × 224. Typical data augmentations including random width/height shift, color jittering and random horizontal flip are applied. For GCE, we use the suggested α = 0.7, while for SCE, we use the setting with A = −4, α = 10.0, β = 1.0. For our two APL losses, we set α = 50.0, β = 0.1 for NCE+RCE and α = 50.0, β = 1.0 for NCE+MAE. The top-1 validation ac-  curacies of different loss functions on the clean ILSVRC12 validation set (eg. only the first 50 classes) are reported in <ref type="table" target="#tab_5">Table 5</ref>. As can be observed, both our APL losses outperform existing loss functions GCE and SCE by a clear margin. This verifies the effectiveness of our APL against real-world label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we revisited the robustness and sufficient learning properties of existing loss functions for deep learning with noisy labels. We revealed a new theoretical insight into robust loss functions that: a simple normalization can make any loss function robust to noisy labels. Then, we highlighted that robustness alone is not enough for a loss function to train accurate DNNs, and existing robust loss functions all suffer from an underfitting problem. To address this problem, we characterize existing robust loss functions into "Active" or "Passive" losses, and then proposed a mutually boosted framework Active Passive Loss (APL). APL allows us to create a family of new loss functions that not only have theoretically guaranteed robustness but also are effective for sufficient learning. We empirically verified the excellent performance of our APL loss functions compared to state-of-the-art methods on benchmark datasets. Our APL framework can serve as a basic principle for developing new robust loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs for Lemma 1, Lemma 2 and Lemma 3</head><p>Our proofs are inspired by <ref type="bibr" target="#b2">(Ghosh et al., 2017)</ref>. Lemma 1. In a multi-class classification problem, any normalized loss function L norm is noise tolerant under symmetric (or uniform) label noise, if noise rate η &lt; K−1 K .</p><p>Proof. For symmetric label noise, the noise risk can be defined as:</p><formula xml:id="formula_12">R η (f ) = E x,ŷ L norm (f (x),ŷ) = E x E y|x Eŷ |x,y L norm (f (x),ŷ) = E x E y|x (1 − η)L norm (f (x), y) + η K − 1 k =y L norm (f (x), k) = (1 − η)R(f ) + η K − 1 E x,y K k=1 L norm (f (x), k) − R(f ) = R(f ) 1 − ηK K − 1 + η K − 1 ,</formula><p>where the last equality holds due to K k=1 L norm (f (x), k) = 1, following Eq. (1). Thus,</p><formula xml:id="formula_13">R η (f * ) − R η (f ) = (1 − ηK K − 1 )(R(f * ) − R(f )) ≤ 0,</formula><p>because η &lt; K−1 K and f * is a global minimizer of R(f ). This proves f * is also the global minimizer of risk R η (f ), that is, L norm is noise tolerant to symmetric label noise.</p><p>Lemma 2. In a multi-class classification problem, given R(f * ) = 0 and 0 ≤ L norm (f * (x), k) ≤ 1 K−1 , any normalized loss function L norm is noise tolerant under asymmetric (or class-conditional) label noise, if noise rate η jk &lt; 1 − η y . Proof. For asymmetric or class-conditional noise, 1 − η y is the probability of a label being correct (i.e., k = y), and the noise condition η yk &lt; 1 − η y generally states that a sample x still has the highest probability of being in the correct class y, though it has probability of η yk being in an arbitrary noisy (incorrect) class k = y. Considering the noise transition matrix between classes [η ij ], ∀i, j ∈ {1, 2, · · · , K}, this condition only requires that the matrix is diagonal dominated by η ii (i.e., the correct class probability 1 − η y ). Following the symmetric case, here we have, (1 − ηy − η yk )Lnorm(f (x), k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>As f * η is the minimizer of R η (f ), R η (f * η ) − R η (f * ) ≤ 0. So, from 7 above, we have, Ex,y </p><p>Next, we prove, f * η = f * holds following Eq. (8). First, (1 − ηy − η yk ) &gt; 0 as per the assumption that η yk &lt; 1 − ηy. Thus, L * norm − L η * norm ≤ 0 for Eq. (8) to hold. Since we are given R(f * ) = 0, we have L(f * (x), y) = 0. Thus, following the definition of Lnorm in Eq. (1) and assumption Lnorm(f * (x), k) ≤ 1 K−1 , we have Lnorm(f * (x), k) = L(f * (x)=0,k) K j L(f * (x),j) = 1 K−1 , for all k = y. Also,</p><p>we have Lnorm(f * η (x), k) = L(f * η (x),k) K j L(f * η (x),j) ≤ 1 K−1 , ∀k = y. Thus, for Eq. (8) to hold (e.g. Lnorm(f * η (x), k) ≥ Lnorm(f * (x), k)), it must be the case that p k = 0, ∀k = y, that is, Lnorm(f * η (x), k) = Lnorm(f * (x), k) for all k ∈ {1, 2, · · · , K}, thus f * η = f * which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized Loss Functions for Deep Learning with Noisy Labels</head><p>Lemma 3. ∀α, ∀β, if L Active and L Passive are noise tolerant, then L APL = α · L Active + β · L Passive is noise tolerant.</p><p>Proof. Let α, β ∈ R, then K j L APL (f (x), j) = α · K j L Active (f (x), j) + β · K j L Passive (f (x), j) = α · C Active + β · C Passive = C. Following our proof for Lemma 1, for symmetric noise, we have,</p><formula xml:id="formula_15">R η (f ) = R(f ) 1 − ηK K − 1 + (α · C Active + β · C Passive )η K − 1 .</formula><p>Thus, R η (f * ) − R η (f ) = (1 − ηK K−1 )(R(f * ) − R(f )) ≤ 0. Given η &lt; K−1 K and f * is a global minimizer of R(f ), R(f * ) − R(f ), that is, f * is also the global minimizer of risk R η (f ). Thus, L APL is noise tolerant to symmetric label noise.</p><p>Following our proof for Lemma 2, for asymmetric noise, we have, R η (f ) = (α · CActive + β · CPassive)Ex,y(1 − ηy) − Ex,y k =y</p><p>(1 − ηy − η yk )Lnorm(f (x), k) .</p><p>(9)</p><p>As f * η is the minimizer of R η (f ), R η (f * η ) − R η (f * ) ≤ 0. So, from 9 above, we can derive the same equation as Eq. <ref type="formula" target="#formula_14">(8)</ref>,</p><formula xml:id="formula_16">Ex,y k =y (1 − ηy − η yk ) LAPL(f * (x), k) L * APL − LAPL(f * η (x), k) L η * APL ≤ 0.<label>(10)</label></formula><p>Thus, we can follow the same proof from Eq. (8), to f * η = f * , that is, LAPL is also noise tolerant to asymmetric noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Test accuracies of unnormalized versus normalized loss functions on CIFAR-100 under 0.6 symmetric noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Definition 1. (Active loss function) L Active is an active loss function if ∀(x, y) ∈ D ∀k = y (f (x), k) = 0. Definition 2. (Passive loss function) L Passive is a passive loss function if ∀(x, y) ∈ D ∃k = y (f (x), k) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Test accuracies of unnormalized versus normalized loss functions on CIFAR-10 under 0.6 symmetric noise. Test accuracies of scaled loss functions on CIFAR-100 with 0.6 symmetric noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Test accuracies of APL loss functions (NCE+MAE, NCE+RCE, NFL+MAE and NFL+RCE) versus "AAL" loss (NFL+NCE) or "PPL" loss (MAE+RCE) on CIFAR-10/CIFAR-100 with 0.6 symmetric noise. CIFAR-100 (η = 0.6)Figure 5. Validation accuracy of NCE+RCE loss with different parameters on CIFAR-10 and CIFAR-100 under symmetric noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>R</head><label></label><figDesc>η (f ) = E x,ŷ Lnorm(f (x),ŷ) = ExE y|x Eŷ |x,y Lnorm(f (x),ŷ) = ExE y|x (1 − ηy)Lnorm(f (x), y) + k =y η yk Lnorm(f (x), k) = Ex,y (1 − ηy) K k=1 Lnorm(f (x), k) − k =y Lnorm(f (x), k) + Ex,y k =y η yk Lnorm(f (x), k) = Ex,y (1 − ηy) 1 − k =y Lnorm(f (x), k) + Ex,y k =y η yk Lnorm(f (x), k) = Ex,y(1 − ηy) − Ex,y k =y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ηy − η yk ) Lnorm(f * (x), k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 The University of Melbourne, Australia 2 Shanghai Jiao Tong University, China. Correspondence to: Yisen Wang &lt;eewangyisen@gmail.com&gt;. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).</figDesc><table><row><cell>.,</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Examples of active and passive loss functions.</figDesc><table><row><cell>Loss Type</cell><cell>Active</cell><cell>Passive</cell></row><row><cell>Examples</cell><cell cols="2">CE, NCE, FL, NFL MAE, NMAE, RCE, NRCE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Test accuracies (%) of different methods on benchmark datasets with clean or symmetric label noise (η ∈ [0.2, 0.8]). The results (mean±std) are reported over 3 random runs and the top 2 best results are boldfaced. NCE+MAE 68.75 ± 0.54 65.25 ± 0.62 59.22 ± 0.36 48.06 ± 0.34 25.50 ± 0.76 NCE+RCE 69.02 ± 0.11 65.31 ± 0.07 59.48 ± 0.56 47.12 ± 0.62 25.80 ± 1.12</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell>Clean (η=0.0)</cell><cell>0.2</cell><cell cols="2">Symmetric Noise Rate (η) 0.4 0.6</cell><cell>0.8</cell></row><row><cell></cell><cell>CE</cell><cell>99.25 ± 0.08</cell><cell>97.42 ± 0.06</cell><cell>94.21 ± 0.54</cell><cell>86.00 ± 1.48</cell><cell>47.08 ± 1.15</cell></row><row><cell></cell><cell>FL</cell><cell>99.30 ± 0.02</cell><cell>97.45 ± 0.19</cell><cell>94.71 ± 0.25</cell><cell>85.76 ± 1.85</cell><cell>49.77 ± 2.26</cell></row><row><cell></cell><cell>GCE</cell><cell>99.27 ± 0.01</cell><cell>99.18 ± 0.06</cell><cell>98.72 ± 0.05</cell><cell>97.43 ± 0.23</cell><cell>12.77 ± 2.00</cell></row><row><cell></cell><cell>NLNL</cell><cell>99.27 ± 0.02</cell><cell>97.49 ± 0.30</cell><cell>96.64 ± 0.52</cell><cell>97.22 ± 0.06</cell><cell>10.32 ± 0.73</cell></row><row><cell></cell><cell>SCE</cell><cell>99.24 ± 0.08</cell><cell>99.15 ± 0.04</cell><cell>98.78 ± 0.09</cell><cell>97.45 ± 0.29</cell><cell>73.70 ± 0.84</cell></row><row><cell>MNIST</cell><cell cols="2">NFL+MAE 99.39 ± 0.04</cell><cell>99.12 ± 0.06</cell><cell>98.74 ± 0.14</cell><cell>96.91 ± 0.09</cell><cell>74.98 ± 1.99</cell></row><row><cell></cell><cell>NFL+RCE</cell><cell cols="4">99.38 ± 0.02 99.19 ± 0.06 98.79 ± 0.10 97.46 ± 0.03</cell><cell>74.59 ± 2.23</cell></row><row><cell></cell><cell cols="2">NCE+MAE 99.37 ± 0.02</cell><cell>99.14 ± 0.05</cell><cell>98.78 ± 0.00</cell><cell>96.76 ± 0.34</cell><cell>74.66 ± 1.11</cell></row><row><cell></cell><cell>NCE+RCE</cell><cell cols="5">99.37 ± 0.02 99.20 ± 0.04 98.79 ± 0.12 97.48 ± 0.13 75.18 ± 1.19</cell></row><row><cell></cell><cell>CE</cell><cell>90.36 ± 0.03</cell><cell>75.90 ± 0.28</cell><cell>60.28 ± 0.27</cell><cell>40.90 ± 0.35</cell><cell>19.65 ± 0.46</cell></row><row><cell></cell><cell>FL</cell><cell>89.63 ± 0.25</cell><cell>74.59 ± 0.49</cell><cell>57.55 ± 0.39</cell><cell>38.91 ± 0.62</cell><cell>19.43 ± 0.27</cell></row><row><cell></cell><cell>GCE</cell><cell>89.38 ± 0.23</cell><cell>87.27 ± 0.21</cell><cell>83.33 ± 0.39</cell><cell>72.00 ± 0.37</cell><cell>29.08 ± 0.80</cell></row><row><cell></cell><cell>NLNL</cell><cell>91.93 ± 0.20</cell><cell>83.98 ± 0.18</cell><cell>76.58 ± 0.44</cell><cell>72.85 ± 0.39</cell><cell>51.41 ± 0.85</cell></row><row><cell></cell><cell>SCE</cell><cell>91.30 ± 0.22</cell><cell>88.05 ± 0.26</cell><cell>82.06 ± 0.24</cell><cell>66.08 ± 0.25</cell><cell>30.69 ± 0.63</cell></row><row><cell>CIFAR-10</cell><cell cols="2">NFL+MAE 89.25 ± 0.19</cell><cell>87.33 ± 0.14</cell><cell>83.81 ± 0.06</cell><cell>76.36 ± 0.31</cell><cell>45.23 ± 0.52</cell></row><row><cell></cell><cell>NFL+RCE</cell><cell cols="5">90.91 ± 0.02 89.14 ± 0.13 86.05 ± 0.12 79.78 ± 0.13 55.06 ± 1.08</cell></row><row><cell></cell><cell cols="2">NCE+MAE 88.83 ± 0.34</cell><cell>87.12 ± 0.21</cell><cell>84.19 ± 0.43</cell><cell>77.61 ± 0.05</cell><cell>49.62 ± 0.72</cell></row><row><cell></cell><cell>NCE+RCE</cell><cell cols="5">90.76 ± 0.22 89.22 ± 0.27 86.02 ± 0.09 79.78 ± 0.50 52.71 ± 1.90</cell></row><row><cell></cell><cell>CE</cell><cell>70.89 ± 0.22</cell><cell>56.99 ± 0.41</cell><cell>41.40 ± 0.36</cell><cell>22.15 ± 0.40</cell><cell>7.58 ± 0.44</cell></row><row><cell></cell><cell>FL</cell><cell>70.61 ± 0.44</cell><cell>56.10 ± 0.48</cell><cell>40.77 ± 0.62</cell><cell>22.14 ± 1.00</cell><cell>7.21 ± 0.25</cell></row><row><cell></cell><cell>GCE</cell><cell>69.00 ± 0.56</cell><cell>65.24 ± 0.56</cell><cell>58.94 ± 0.50</cell><cell>45.18 ± 0.93</cell><cell>16.18 ± 0.46</cell></row><row><cell></cell><cell>NLNL</cell><cell>68.72 ± 0.60</cell><cell>46.99 ± 0.91</cell><cell>30.29 ± 1.64</cell><cell>16.60 ± 0.90</cell><cell>11.01 ± 2.48</cell></row><row><cell></cell><cell>SCE</cell><cell>70.38 ± 0.45</cell><cell>55.39 ± 0.18</cell><cell>39.99 ± 0.59</cell><cell>22.35 ± 0.65</cell><cell>7.57 ± 0.28</cell></row><row><cell>CIFAR-100</cell><cell cols="2">NFL+MAE 67.98 ± 0.52</cell><cell>63.58 ± 0.09</cell><cell>58.18 ± 0.08</cell><cell>46.10 ± 0.50</cell><cell>24.78 ± 0.82</cell></row><row><cell></cell><cell>NFL+RCE</cell><cell>68.23 ± 0.62</cell><cell>64.52 ± 0.35</cell><cell>58.20 ± 0.31</cell><cell>46.30 ± 0.45</cell><cell>25.16 ± 0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Test accuracy (%) of different methods on benchmark datasets with clean or asymmetric label noise (η ∈ [0.1, 0.4]). The results (mean±std) are reported over 3 random runs and the top 2 best results are boldfaced. NCE+RCE 89.95 ± 0.20 88.56 ± 0.17 85.58 ± 0.44 79.59 ± 0.40 Test accuracy (%) of APL losses NGCE+MAE and NGCE+RCE on CIFAR-10 under both symmetric and asymmetric noise. The top-2 best results are in bold.</figDesc><table><row><cell></cell><cell>Datasets</cell><cell>Methods</cell><cell>0.1</cell><cell cols="2">Asymmetric Noise Rate (η) 0.2 0.3</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell>CE</cell><cell>98.53 ± 0.11</cell><cell>96.75 ± 0.31</cell><cell>92.98 ± 1.41</cell><cell>85.74 ± 2.70</cell></row><row><cell></cell><cell></cell><cell>FL</cell><cell>98.97 ± 0.10</cell><cell>98.35 ± 0.17</cell><cell>96.57 ± 0.36</cell><cell>91.18 ± 2.02</cell></row><row><cell></cell><cell></cell><cell>GCE</cell><cell>99.25 ± 0.03</cell><cell>99.11 ± 0.04</cell><cell>96.99 ± 0.53</cell><cell>88.56 ± 2.40</cell></row><row><cell></cell><cell></cell><cell>NLNL</cell><cell>98.38 ± 0.17</cell><cell>95.98 ± 0.58</cell><cell>91.52 ± 1.14</cell><cell>86.36 ± 0.40</cell></row><row><cell></cell><cell></cell><cell>SCE</cell><cell>99.15 ± 0.07</cell><cell>99.05 ± 0.05</cell><cell>97.96 ± 0.40</cell><cell>91.89 ± 3.32</cell></row><row><cell></cell><cell>MNIST</cell><cell>NFL+MAE</cell><cell>99.31 ± 0.05</cell><cell>99.09 ± 0.12</cell><cell>97.88 ± 0.16</cell><cell>93.52 ± 0.19</cell></row><row><cell></cell><cell></cell><cell cols="2">NFL+RCE 99.33 ± 0.06</cell><cell>99.13 ± 0.01</cell><cell cols="2">97.99 ± 0.05 93.59 ± 0.82</cell></row><row><cell></cell><cell></cell><cell>NCE+MAE</cell><cell>99.26 ± 0.02</cell><cell cols="2">99.21 ± 0.04 98.99 ± 0.03</cell><cell>93.40 ± 1.28</cell></row><row><cell></cell><cell></cell><cell cols="3">NCE+RCE 99.34 ± 0.06 99.17 ± 0.02</cell><cell>97.94 ± 0.21</cell><cell>93.12 ± 1.17</cell></row><row><cell></cell><cell></cell><cell>CE</cell><cell>87.38 ± 0.16</cell><cell>83.62 ± 0.15</cell><cell>79.38 ± 0.28</cell><cell>75.00 ± 0.50</cell></row><row><cell></cell><cell></cell><cell>FL</cell><cell>86.35 ± 0.30</cell><cell>82.97 ± 0.14</cell><cell>79.48 ± 0.21</cell><cell>74.60 ± 0.15</cell></row><row><cell></cell><cell></cell><cell>GCE</cell><cell>88.42 ± 0.07</cell><cell>86.07 ± 0.31</cell><cell>80.78 ± 0.21</cell><cell>74.98 ± 0.32</cell></row><row><cell></cell><cell></cell><cell>NLNL</cell><cell>88.54 ± 0.25</cell><cell>84.74 ± 0.08</cell><cell>81.26 ± 0.43</cell><cell>76.97 ± 0.52</cell></row><row><cell></cell><cell></cell><cell>SCE</cell><cell>88.13 ± 0.21</cell><cell>83.92 ± 0.07</cell><cell>79.70 ± 0.27</cell><cell>78.20 ± 0.03</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>NFL+MAE</cell><cell>88.46 ± 0.20</cell><cell>86.81 ± 0.32</cell><cell>83.91 ± 0.34</cell><cell>77.16 ± 0.10</cell></row><row><cell></cell><cell></cell><cell cols="5">NFL+RCE 90.20 ± 0.15 88.73 ± 0.29 85.74 ± 0.22 79.27 ± 0.43</cell></row><row><cell></cell><cell></cell><cell>NCE+MAE</cell><cell>88.25 ± 0.09</cell><cell>86.44 ± 0.23</cell><cell>83.98 ± 0.52</cell><cell>78.23 ± 0.42</cell></row><row><cell></cell><cell></cell><cell>CE</cell><cell>65.42 ± 0.22</cell><cell>58.45 ± 0.45</cell><cell>51.09 ± 0.29</cell><cell>41.68 ± 0.45</cell></row><row><cell></cell><cell></cell><cell>FL</cell><cell>64.79 ± 0.18</cell><cell>58.59 ± 0.81</cell><cell>51.26 ± 0.18</cell><cell>42.15 ± 0.44</cell></row><row><cell></cell><cell></cell><cell>GCE</cell><cell>61.98 ± 0.81</cell><cell>59.99 ± 0.83</cell><cell>53.99 ± 0.29</cell><cell>41.49 ± 0.79</cell></row><row><cell></cell><cell></cell><cell>NLNL</cell><cell>59.55 ± 1.22</cell><cell>50.19 ± 0.56</cell><cell>42.81 ± 1.13</cell><cell>35.10 ± 0.20</cell></row><row><cell></cell><cell></cell><cell>SCE</cell><cell>64.15 ± 0.61</cell><cell>58.22 ± 0.47</cell><cell>49.85 ± 0.91</cell><cell>42.19 ± 0.19</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell cols="3">NFL+MAE 66.06 ± 0.23 63.10 ± 0.22</cell><cell>56.19 ± 0.61</cell><cell>43.51 ± 0.42</cell></row><row><cell></cell><cell></cell><cell cols="3">NFL+RCE 66.13 ± 0.31 63.12 ± 0.41</cell><cell>54.72 ± 0.38</cell><cell>42.97 ± 1.03</cell></row><row><cell></cell><cell></cell><cell>NCE+MAE</cell><cell>65.71 ± 0.34</cell><cell>62.38 ± 0.60</cell><cell cols="2">58.02 ± 0.48 47.22 ± 0.30</cell></row><row><cell></cell><cell></cell><cell>NCE+RCE</cell><cell>65.68 ± 0.25</cell><cell>62.68 ± 0.79</cell><cell cols="2">57.82 ± 0.41 46.79 ± 0.96</cell></row><row><cell>Methods</cell><cell cols="2">Symmetric noise 0.4 0.8</cell><cell>Asymmetric noise 0.4</cell><cell></cell><cell></cell></row><row><cell>GCE</cell><cell>83.33 ± 0.39</cell><cell>29.08 ± 0.80</cell><cell>74.98 ± 0.32</cell><cell></cell><cell></cell></row><row><cell cols="3">NGCE+MAE 84.14 ± 0.15 50.55 ± 1.08</cell><cell>76.55 ± 0.48</cell><cell></cell><cell></cell></row><row><cell>NGCE+RCE</cell><cell cols="2">85.76 ± 0.26 44.69 ± 4.93</cell><cell>71.65 ± 0.68</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Top-1 validation accuracies (%) on clean ILSVRC12 validation set of ResNet-50 models trained on WebVision using different loss functions, under the Mini setting in<ref type="bibr" target="#b7">(Jiang et al., 2018)</ref>. The top-2 best results are in bold.</figDesc><table><row><cell>Loss</cell><cell>CE</cell><cell>GCE</cell><cell>SCE</cell><cell cols="2">NCE+MAE NCE+RCE</cell></row><row><cell>Acc</cell><cell cols="3">58.88 53.68 61.76</cell><cell>62.36</cell><cell>62.64</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was undertaken using the LIEF HPC-GPU Facility hosted at the University of Melbourne with the assistance of LIEF Grant LE170100200.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On symmetric losses for learning from corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Co-teaching: robust training deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sequential cosupervision for weakly labeled audio event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Secost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11789</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cleannet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07131</idno>
		<title level="m">Transfer learning for scalable image classifier training with label noise</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dimensionalitydriven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Decoupling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Derivative manipulation for adjusting emphasis density function: A general example weighting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11233</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An information-theoretic noise-robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
