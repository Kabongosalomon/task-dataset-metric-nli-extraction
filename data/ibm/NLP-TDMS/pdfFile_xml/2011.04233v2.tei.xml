<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Lane Shape Prediction with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information Engineering</orgName>
								<orgName type="institution">Capital Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen Forward Innovation Digital Technology Co. Ltd</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Lane Shape Prediction with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection, the process of identifying lane markings as approximated curves, is widely used for lane departure warning and adaptive cruise control in autonomous vehicles. The popular pipeline that solves it in two stepsfeature extraction plus post-processing, while useful, is too inefficient and flawed in learning the global context and lanes' long and thin structures. To tackle these issues, we propose an end-to-end method that directly outputs parameters of a lane shape model, using a network built with a transformer to learn richer structures and context. The lane shape model is formulated based on road structures and camera pose, providing physical interpretation for parameters of network output. The transformer models nonlocal interactions with a self-attention mechanism to capture slender structures and global context. The proposed method is validated on the TuSimple benchmark and shows state-of-the-art accuracy with the most lightweight model size and fastest speed. Additionally, our method shows excellent adaptability to a challenging self-collected lane detection dataset, showing its powerful deployment potential in real applications. Codes are available at https: //github.com/liuruijin17/LSTR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-based lane marking detection is a fundamental module in autonomous driving, which has achieved remarkable performance on applications such as lane departure warning, adaptive cruise control, and traffic understanding <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b12">14]</ref>. In real applications, detecting lanes could be very challenging. The lane marking is a long and thin structure with strong shape prior but few appearance clues <ref type="bibr" target="#b12">[14]</ref>. Besides, lane markings vary in different types, light changes, and occlusions of vehicles and pedestrians, which requires the global context information to infer the vacancy or occluded part. Moreover, the high running efficiency and transferring adaptability of algorithms are indispensable for deployment on mobile devices <ref type="bibr" target="#b13">[15]</ref>.</p><p>Existing methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b5">7]</ref> take advantage of the powerful representation capabilities of convolution neural networks (CNNs) to improve the performance of lane detection task by a large margin over traditional methods <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref> which are based on hand-crafted features and Hough Transform. However, current CNNs-based methods still are flawed in addressing the aforementioned challenges. The earlier methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b13">15]</ref> typically first generate segmentation results and then employ post-processing, such as segment clustering and curve fitting. These methods are inefficient and ignore global context when learning to segment lanes <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b12">14]</ref>. To tackle the context learning issue, some methods <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b2">4]</ref> use message passing or extra scene annotations to capture the global context for enhancement of final performance, but these methods inevitably consume more time and data cost <ref type="bibr" target="#b4">[6]</ref>. Unlike these methods, a soft attention based method <ref type="bibr" target="#b4">[6]</ref> generates a spatial weighting map that distills a richer context without external consumes. However, the weighting map only measures the feature's importance, limiting its usage to consider the dependencies between features that support to infer slender structures. On the other hand, to improve the algorithm's efficiency, <ref type="bibr" target="#b6">[8]</ref> transfers the pipeline in object detection to detect lanes without the above segmentation procedure and post-processing, but it relies on complex anchor designing choices and additional non-maximal suppression, making it even slower than most lane detectors. Recently, a method <ref type="bibr" target="#b16">[18]</ref> reframes the task as lane markings fitting by polynomial regression, which achieves significant efficiency but still has a large accuracy gap with other methods due to the neglect of learning the global context.</p><p>To tackle these issues, we propose to reframe the lane detection output as parameters of a lane shape model and develop a network built with non-local building blocks to reinforce the learning of global context and lanes' slender structures. The output for each lane is a group of parameters which approximates the lane marking with an explicit mathematical formula derived from road structures and the camera pose. Given specific priors such as camera intrinsic, those parameters can be used to calculate the road curvature and camera pitch angle without any 3D sensors. Next, inspired by natural language processing models which widely employ transformer block <ref type="bibr" target="#b17">[19]</ref> to explicitly model long-range dependencies in language sequence, we develop a transformer-based network that summarizes information from any pairwise visual features, enabling it to capture lanes' long and thin structures and global context. The whole architecture predicts the proposed outputs at once and is trained end-to-end with a Hungarian loss. The loss applies bipartite matching between predictions and ground truths to ensure one-to-one disorderly assignment, enabling the model to eliminate an explicit non-maximal suppression process.</p><p>The effectiveness of the proposed method is validated in the conventional TuSimple lane detection benchmark <ref type="bibr">[1]</ref>. Without bells and whistles, our method achieves state-ofthe-art accuracy and the lowest false positive rate with the most lightweight model size and fastest speed. In addition, to evaluate the adaptability to new scenes, we collect a large scale challenging dataset called Forward View Lane (FVL) in multiple cities across various scenes (urban and highway, day and night, various traffic and weather conditions). Our method shows strong adaptability to new scenes even that the TuSimple dataset does not contain, e.g., night scenes.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We propose a lane shape model whose parameters serve as directly regressed output and reflect road structures and the camera pose.</p><p>• We develop a transformer-based network that considers non-local interactions to capture long and thin structures for lanes and the global context.</p><p>• Our method achieves state-of-the-art accuracy with the least resource consumption and shows excellent adaptability to a new challenging self-collected lane detection dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The authors of <ref type="bibr" target="#b9">[11]</ref> provide a good overview of the techniques used in traditional lane detection methods. Featurebased methods usually extract low-level features (lane segments) by Hough transform variations, then use clustering algorithms such as DBSCAN (Density Based Spatial Clustering of Applications with Noise) to generate final lane detections <ref type="bibr" target="#b11">[13]</ref>. Model-based methods use top-down priors such as geometry and road surface <ref type="bibr" target="#b18">[20]</ref>, which describe lanes in more detail and shows excellent simplicity.</p><p>In recent years, methods based on deep neural networks have been shown to outperform traditional approaches. Earlier methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b13">15]</ref> typically extract dense segmentation results and then employ post-processing such as segment clustering and curve fitting. Their performances are limited by the initial segmentation of lanes due to the difficulties of learning such long and thin structures. To address this issue, SCNN <ref type="bibr" target="#b12">[14]</ref> uses message passing to capture a global context, exploiting richer spatial information to infer occluded parts. <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b2">4]</ref> adopt extra scene annotations to guide the model's training, which enhances the final performance. Unlike them that need additional data and time cost, ENet-SAD [6] applies a soft attention mechanism, which generates weighting maps to filter out unimportant features and distill richer global information. In contrast to inferring based on dense segmentation results, PINet <ref type="bibr" target="#b5">[7]</ref> extracts a sparse point cloud to save the computations, but it also requires inefficient post-processing like outlier removal.</p><p>In contrast to these methods, our method directly outputs parameters of a lane shape model. The whole method works in an end-to-end fashion without any intermediate representation or post-processing.</p><p>In literature, some end-to-end lane detectors have been proposed recently. Line-CNN <ref type="bibr" target="#b6">[8]</ref> transfers the success of Faster-RCNN <ref type="bibr" target="#b14">[16]</ref> into lane detection by predicting offsets based on pre-designed straight rays (like anchor boxes), which achieves state-of-the-art accuracy. However, it inevitably suffers the drawbacks of complex ad-hoc heuristics choices to design rays and additional non-maximal suppress, making it even slower than most lane detectors. Poly-LaneNet <ref type="bibr" target="#b16">[18]</ref> reframes the lane detection task as a polynomial regression problem, which achieved the highest efficiency. However, its accuracy still has a large gap with other methods, especially has difficulty in predicting lane lines with accentuated curvatures due to neglect of learning global information and ignorant of road structures modeling.</p><p>In this work, our approach also expects a parametric output but differs in that these parameters are derived from a lane shape model which models the road structures and the camera pose. These output parameters have explicit physical meanings rather than simple polynomial coefficients. In addition, our network is built with transformer block <ref type="bibr" target="#b17">[19]</ref> that performs attention in modeling non-local interactions, enabling it to reinforce the capture of long and thin structures for lanes and learning of global context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our end-to-end method reframes the output as parameters of a lane shape model. Parameters are predicted by us-ing a transformer-based network trained with a Hungarian fitting loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lane Shape Model</head><p>The prior model of the lane shape is defined as a polynomial on the road. Typically, a cubic curve is used to approximate a single lane line on flat ground:</p><formula xml:id="formula_0">X = kZ 3 + mZ 2 + nZ + b,<label>(1)</label></formula><p>where k, m, n and b are real number parameters, k = 0.</p><p>The (X, Z) indicates the point on the ground plane. When the optical axis is parallel to the ground plane, the curve projected from the road onto the image plane is:</p><formula xml:id="formula_1">u = k v 2 + m v + n + b × v,<label>(2)</label></formula><p>where k , m , n , b are composites of parameters and camera intrinsic and extrinsic parameters, and (u, v) is a pixel at the image plane.</p><p>In the case of a tilted camera whose optical axis is at an angle of φ to the ground plane, the curve transformed from the untitled image plane to the tilted image plane is:</p><formula xml:id="formula_2">u = k × cos 2 φ (v − f sin φ) 2 + m cos φ (v − f sin φ) + n + b × v cos φ − b × f tan φ,<label>(3)</label></formula><p>here f is the focal length in pixels, and (u , v ) is the corresponding pitch-transformed position. When φ = 0, the curve function Eq. 3 will be simplified to Eq. 2. Details of the derivation can be reviewed in Sec. 7.</p><p>Curve re-parameterization. By combining parameters with the pitch angle φ, the curve in a tilted camera plane has the form of:</p><formula xml:id="formula_3">u = k (v − f ) 2 + m (v − f ) + n + b × v − b ,<label>(4)</label></formula><p>here, the two constant terms n and b are not integrated because they contain different physical parameters. Apart from that, the vertical starting and ending offset α, β are also introduced to parameterize each lane line. These two parameters provide essential localization information to describe the upper and lower boundaries of lane lines.</p><p>In real road conditions, lanes typically have a global consistent shape. Thus, the approximated arcs have a equal curvature from the left to the right lanes, so k , f , m , n will be shared for all lanes. Therefore, the output for the t-th lane is re-parameterized to g t :</p><formula xml:id="formula_4">g t = (k , f , m , n , b t , b t , α t , β t )<label>(5)</label></formula><p>where t ∈ {1, ..., T }, T is the number of lanes in an image. Each lane only differs in bias terms and lower/upper boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hungarian Fitting Loss</head><p>The Hungarian fitting loss performs a bipartite matching between predicted parameters and ground truth lanes to find positives and negatives. The matching problem is efficiently solved by the Hungarian algorithm. Then the matching result is used to optimize lane-specific regression losses. Bipartite matching. Our method predicts a fixed N curves, where N is set to be larger than the maximum number of lanes in the image of a typical dataset. Let us denote the predicted curves by</p><formula xml:id="formula_5">H = {h i |h i = (c i , g i )} N i=1 , where c i ∈ {0, 1} (0: non-lane, 1: lane). The ground truth lane marking is represented by a sequenceŝ = (û r ,v r ) R r=1 ,</formula><p>where r sequentially indexes the sample point within range R andv r+1 &gt;v r . Since the number of predicted curves N is larger than the number of ground truth lanes, we consider the ground truth lanes also as a set of size N padded with</p><formula xml:id="formula_6">non-lanes L = l i |l i = (ĉ i ,ŝ i ) N i=1</formula><p>. We formulate the bipartite matching between the set of curves and the set of ground truth lane markings as a cost minimization problem by searching an optimal injective function z : L → H, i.e., z (i) is the index of curve assigned to fitting ground-truth lane i:ẑ</p><formula xml:id="formula_7">= arg min z N i=1 d l i , h z(i) ,<label>(6)</label></formula><p>where d measures the matching cost given a specific permutation z between the i-th ground truth lane and a predicted curve with index z (i). Following prior work (e.g., <ref type="bibr" target="#b15">[17]</ref>), this problem can be solved efficiently by the Hungarian algorithm.</p><p>For the prediction with index z (i), the probability of classĉ i is defined as p z(i) (ĉ i ), and the fitting lane sequence is defined as s z(i) = (u ri ,v ri ) Ri r=1 , where R i is the length of i-th lane and u ri is calculated using Eq. 4 based on the predicted group of parameters g i . Then the matching cost d has the form of:</p><formula xml:id="formula_8">d = − ω 1 p z(i) (ĉ i ) + 1 (ĉ i = 1) ω 2 L 1 ŝ i , s z(i) + 1 (ĉ i = 1) ω 3 L 1 α i , α z(i) ,β i , β z(i) ,<label>(7)</label></formula><p>where 1 (·) is an indicator function, ω 1 , ω 2 and ω 3 adjusts the effect of the matching terms, and L 1 is the commonlyused mean absolute error. We use the probabilities instead of log-probabilities following <ref type="bibr" target="#b1">[3]</ref> because this makes the classification term commensurable to the curve fitting term. Regression loss. The regression loss calculates the error for all pairs matched in the previous step with the form of:  whereẑ is the optimal permutation calculated in Eq. 7. The ω 1 , ω 2 , and ω 3 also adjust the effect of the loss terms and are set to be the same values of coefficients in Eq. 7.</p><formula xml:id="formula_9">L = N i=1 − ω 1 log pẑ (i) (ĉ i ) + 1 (ĉ i = 1) ω 2 L 1 ŝ i , sẑ (i) + 1 (ĉ i = 1) ω 3 L 1 α i , αẑ (i) ,β i , βẑ (i) ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>The architecture shown in <ref type="figure">Fig. 1</ref> consists of a backbone, a reduced transformer network, several feed-forward networks (FFNs) for parameter predictions, and the Hungarian Loss. Given an input image I, the backbone extracts a low-resolution feature then flattens it into a sequence S by collapsing the spatial dimensions. The S and positional embedding E p are fed into the transformer encoder to output a representation sequence S e . Next, the decoder generates an output sequence S d by first attending to an initial query sequence S q and a learned positional embedding E LL that implicitly learns the positional differences, then computing interactions with S e and E p to attend to related features. Finally, several FFNs directly predict the parameters of proposed outputs.</p><p>Backbone. The backbone is built based on a reduced ResNet18. The original ResNet18 <ref type="bibr" target="#b3">[5]</ref> has four blocks and downsamples features by 16 times. The output channel of each block is "64, 128, 256, 512". Here, our reduced ResNet18 cuts the output channels into "16, 32, 64, 128" to avoid overfitting and sets the downsampling factor as 8 to reduce losses of lane structural details. Using an input image I as input, the backbone extracts a low-resolution feature that encodes high-level spatial representations for lanes with a size of H × W × C. Next, to construct a sequence as the input of encoder, we flatten that feature in spatial dimensions, resulting in a sequence S with the size of HW × C, where HW denotes the length of the sequence and C is the number of channels. Encoder. The encoder has two standard layers that are linked sequentially. Each of them consists of a self-attention module and a feed-forward layer shown in <ref type="figure">Fig.2</ref>. Given the sequence S that abstracts spatial representations, the sinusoidal embeddings E p based on the absolute positions <ref type="bibr" target="#b17">[19]</ref> is used to encode positional information to avoid the permutation equivariant. The E p has the same size as S. The encoder performs scaled dot-product attention by Eq. 9:</p><formula xml:id="formula_10">A = softmax QK T √ C , O = AV,<label>(9)</label></formula><p>where the Q, K, V denote sequences of query, key and value through a linear transformation on each input row, and A represents the attention map which measures nonlocal interactions to capture slender structures plus global context, and O indicates the output of self-attention. The output sequence of encoder S e with the shape of HW × C is obtained by following FFNs, residual connections with layer normalizations <ref type="bibr" target="#b0">[2]</ref>, and another same encoder layer.</p><p>Decoder. The decoder also has two standard layers. Unlike the encoder, each layer inserts the other attention module, which expects the output of the encoder, enabling it to perform attention over the features containing spatial information to associate with the most related feature elements. Facing the translation task, the original transformer <ref type="bibr" target="#b17">[19]</ref> shifts the ground truth sequence one position as the input of the decoder, making it output each element of the sequence in parallel at a time. In our task, we just set the input S q as an empty N × C matrix and directly decodes all curve parameters at a time. Additionally, we introduce a learned lane embedding E LL with the size of N × C, which serves as a positional embedding to implicitly learn global lane information. The attention mechanism works with the same formula Eq. 9 and the decoded sequence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. The widely-used TuSimple [1] lane detection dataset is used to evaluate our method. The TuSimple dataset consists of 6408 annotated images which are the last frames of video clips recorded by a high-resolution (720×1280) forward view camera across various traffic and weather conditions on America's highways in the daytime. It is split initially into a training set (3268), a validation set (358), and a testing set (2782). To evaluate the adaptive capability to new scenes, we introduce a much more complex self-collected dataset named Forward View Lane (FVL). The FVL contains 52970 images with a raw resolution of 720 × 1280. These images were collected by a monocular forward-facing camera typically located near the rear-view mirror in multiple cities across different scenes (urban and highway, day and night, various traffic and weather conditions). The FVL contains more challenging road conditions and will go public to help research for the community. Evaluation Metrics. To compare the performance against previous methods, we follow the literature and calculate the accuracy using TuSimple metrics. The prediction accuracy is computed as Accuracy = vc T P rvc vc Gtvc , where T P r vc is the number of true prediction points in the last frame of the video clip, and Gt vc is the number of ground truth points. A point is considered as a true positive if its distance from the corresponding label point is within 20 pixels as the TuSimple benchmark suggested <ref type="bibr" target="#b16">[18]</ref>. Besides, false positives (FP) and false negatives (FN) rates are also reported [1]. Implementation Details. The hyperparameter settings are the same for all experiments except for the ablation study. The input resolution is set to 360 × 640. The raw data are augmented by random scaling, cropping, rotating, color jittering, and horizontal flipping. The learning rate is set to be 0.0001 and decayed 10 times every 450k iterations. Batch size is set as 16, and loss coefficients ω 1 , ω 2 and ω 3 are set as 3, 5 and 2. The fixed number of predicted curves N is set as 7, and the number of training iterations is set as 500k.</p><p>All those hyper-parameters are determined by maximizing the performance on the TuSimple validation set. In the following section, we treat PolyLaneNet <ref type="bibr" target="#b16">[18]</ref> as the baseline method since they also predict parametric output for lanes and provide amazingly reproducible codes and baseline models. Besides, to best show our performance, we also compare with other state-of-the-art methods PINet <ref type="bibr" target="#b5">[7]</ref>, Line-CNN <ref type="bibr" target="#b6">[8]</ref>, ENet-SAD <ref type="bibr" target="#b4">[6]</ref>, SCNN <ref type="bibr" target="#b12">[14]</ref>, Fast-Draw <ref type="bibr" target="#b13">[15]</ref>. The proposed method was trained using both TuSimple training and validation set as previous works did. The time unit compares the FPS performance, and we also report MACs and the total number of parameters. All results are tested on a single GTX 1080Ti platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons with State-of-the-Art Methods</head><p>Tab. 1 shows the performance on TuSimple benchmark. Without bells and whistles, our methods outperforms the PolyLaneNet by 2.82% accuracy with 5 × fewer parameters and runs 3.6 × faster. Compared to the state-of-the-art method Line-CNN, ours is only 0.69% lower in accuracy but runs 14 × faster than it. Compared with other two stages approaches, our method achieves competitive accuracy and the lowest false positive rate with the fewest parameters and much faster speed. The high false positive rate would lead to more severe risks like false alarming and rapid changes than missing detections in real applications <ref type="bibr" target="#b5">[7]</ref>. In a word, our method has tremendous mobile deployment capabilities PolyLaneNet <ref type="bibr" target="#b16">[18]</ref> Ours <ref type="figure">Figure 3</ref>. Qualitative comparative results on TuSimple test set. The first row visualizes the predicted curves by the best model of officially public PolyLaneNet resources (red curves means these predictions are mismatched). The second row visualizes our predictions. than any other algorithms. The visualization of the lane detection results is given in <ref type="figure">Fig. 3</ref>. By comparing the areas closer to the horizon, our method is capable of catching structures with fewer details, showing an excellent performance on lane markings with large amplitude curvature than the baseline method. We attribute this to (1) the global lane shape consistency implicitly requires the model to learn consistent shapes across all supervision information from all lane markings; (2) the attention mechanism does capture non-local information, supplementing the contextual information for the missing details, which helps capture slender structures. Subsequent ablation experiments further corroborate these conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Investigation of Shape Model. To investigate the effect of the lane shape model, we tested different shape models. The comparison results are listed in Tab. 2. The header 'Curve Shape' denotes our hypothetical approximation of the lanes on the roadway. Without shape consistency constraint, the i-th predicted curve has its own predicted shape parameters, k i , f i , m i , n i , regressed by a 3-layer perceptron without averaging. i ∈ {1, ..., N }, N is the number of predictions.</p><p>From Tab. 2, we find that the best model is using cubic curve approximation with the shape consistency. The consensus in the lane detection field is that high order lane models always fit lane markings better than simple models <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b20">22]</ref>. Besides, the shape consistency further im-  proves accuracy. We attribute this to the effect of sample points equilibrium. In the TuSimple dataset, a single lane line close to the horizon is marked using far fewer points than the one close to the camera, and this imbalance makes the model more biased towards a better performance at the nearby areas <ref type="bibr" target="#b16">[18]</ref>. To tackle this issue, the shape consistency requires the model to fit the same curvature at areas close to the horizon, enabling it to use all remote points to infer the same curvature which is appropriate for all lanes. Number of encoder layers. To investigate the effects of performing attention on spatial features, we tested different numbers of encoder layers. From <ref type="figure">Fig. 5(a)</ref>, without self-attention mechanism, the accuracy drops by 1.01%. Meanwhile, we found that the attention mechanism could be overused. When the same accuracy of the training set ( ≈ 96.2%) was observed, a larger number led to a degradation of the model generalization performance. It appears that our model is approaching the capacity limit of the data's expressive ability. To better understand the effect of the encoder, we visualize the attention maps A of the last encoder layer in <ref type="figure" target="#fig_3">Fig. 4</ref>. We tested a few points on the lane markings with different conditions. The orange-red lane marking point is totally occluded, so the encoder seems to focus on its right unobstructed lane line features. The peach-puff point is located with clear lane markings, and its attention map shows a clear long and thin structure for the lane. Despite the lack of appearance clues in the local neighborhood of the light-blue point, the encoder still identifies a distinct slender structure by learning the global context (distant markings and nearby vehicles). Similarly, the green point misses many details, but the encoder still identifies a relevant long and thin structure by learning the global context. Number of decoder layers. To investigate the performance of auxiliary losses, we changed the number of decoder layers. From Tab. 3, the output of the last layer is the highest in each configuration, while as the layers become more numerous, the overall performance gradually degrades due to overfitting.</p><p>Similarly to visualizing encoder attention, we analyze the attention map for each output of the decoder in <ref type="figure">Fig. 6</ref>. We observe that decoder attention mainly focuses on its own slender structures which help the model to separate specific lane instances directly rather than using additional non-maximal suppression. Number of predicted curves. The predicted curves play a similar role as the anchor boxes <ref type="bibr" target="#b14">[16]</ref>, which generate positive and negative samples based on some matching rules. The difference is that we only find one-to-one matching without duplicates, so the number of predicted curves determines the number of negative samples. To investigate the impact of positive and negative sample proportions, we tested from 5 (the TuSimple has up to 5 ground truth lane markings in one image) to 10 in increments of 1.</p><p>From <ref type="figure">Fig. 5(b)</ref>, the best size is 7. As the number of predicted curves gets smaller, the network gradually loses its generalization capability because the lack of negatives makes the training inefficient, resulting in degenerate models <ref type="bibr" target="#b7">[9]</ref>. Moreover, as the number increases, the performance also degrades since lane curve fitting needs a sufficient number of positives. More negatives wrongly guide the model to optimizing loss by paying more attention to the classifica- tion for negatives, which weakens the performance of curve fitting for positives. <ref type="figure" target="#fig_5">Fig. 7</ref> demonstrates the qualitative transfer results on FVL datasets. Without any supervised training on FVL, we observed that our model exhibit excellent transfer performance. We attribute this to: (1) our method does not need any prior processing which heavily rely on the data distribution, making them hard to transfer; (2) the transformerbased network aggregates a richer context to focus on information that is more generalized to detect lane objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Results on FVL Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present an end-to-end lane detector that directly outputs parameters of a lane shape model. The lane shape model reflects the road structures and camera state, enabling it to enhance the interpretability of the output parameters. The network built with transformer blocks efficiently learns global context to help infer occluded part and capture the long and thin structures especially nearby the horizon. The whole method achieves state-of-the-art lane detection performance while requiring the least parameters and running time consumption. Meanwhile, our method adapts robustly to changes in datasets, making it easier to deploy on mobile devices and more reliable. It would be interesting to address complex and fine-grained lane detection tasks and introduce the tracking function in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Overall Architecture. The S, Se and Ep indicate flattened feature sequence, encoded sequence and the sinusoidal positional embeddings which are all tensors with shape HW × C. The Sq, E LL and S d represent query sequence, learned lane embedding and the decoded sequence which are all in shape N × C. Different color indicate different output slots. White hollow circles represent "non-lanes". Transformer Encoder and Decoder. The ⊕ and represent matrix addition and dot-product operations respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S d with the shape of N × C is obtained sequentially like the way in the encoder. When training, intermediate supervision is applied after each decoding layer. FFNs for Predicting Curve Parameters. The prediction module generates the set of predicted curves H using three parts. A single linear operation directly projects the S d into N × 2 then a softmax layer operates it in the last dimension to get the predicted label (background or lane) c i , i ∈ {1, . . . , N }. Meanwhile, one 3-layer perceptron with ReLU activation and hidden dimension C projects the S d into N × 4, where dimension 4 represents four groups of lane-specific parameters. The other 3-layer perceptron firstly projects a feature into N × 4 then averages in the first dimension, resulting in the four shared parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Encoder attention maps for different sampling points. The encoder seems to aggregate a lot contextual information and capture slender structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Quantitative evaluation of (a) encoders size and (b) number of predictions on TuSimple validation set (%). The decoder size is fixed to 2. Decoder attention maps for output slots. The decoder mainly focus on local structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative transfer results on FVL dataset. Our method even estimates exquisite lane lines without ever seeing the night scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of accuracy (%) on TuSimple testing Set. The number of multiply-accumulate (MAC) operations is given in G. The number of parameters (Para) is given in M (million). The PP means the requirement of post-processing.</figDesc><table><row><cell>Method</cell><cell cols="4">FPS MACs Para PP Acc FP FN</cell></row><row><cell>FastDraw [15]</cell><cell>90</cell><cell>-</cell><cell>-</cell><cell>95.20 .0760 .0450</cell></row><row><cell>SCNN [14]</cell><cell>7</cell><cell cols="2">-20.72</cell><cell>96.53 .0617 .0180</cell></row><row><cell>ENet-SAD [6]</cell><cell>75</cell><cell>-</cell><cell>0.98</cell><cell>96.64 .0602 .0205</cell></row><row><cell>PINet [7]</cell><cell>30</cell><cell>-</cell><cell>4.39</cell><cell>96.70 .0294 .0263</cell></row><row><cell>Line-CNN [8]</cell><cell>30</cell><cell>-</cell><cell>-</cell><cell>-96.87 .0442 .0197</cell></row><row><cell cols="5">PolyLaneNet [18] 115 1.784 4.05 -93.36 .0942 .0933</cell></row><row><cell>Ours</cell><cell cols="4">420 0.574 0.77 -96.18 .0291 .0338</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation of different shape models on TuSimple validation set (%).</figDesc><table><row><cell cols="3">Curve Shape Consistency Acc</cell><cell>FP</cell><cell>FN</cell></row><row><cell>Quadratic</cell><cell>-</cell><cell cols="2">91.94 0.1169 0.0975</cell></row><row><cell>Quadratic</cell><cell></cell><cell cols="2">93.18 0.1046 0.0752</cell></row><row><cell>Cubic</cell><cell>-</cell><cell cols="2">92.64 0.1068 0.0868</cell></row><row><cell>Cubic</cell><cell></cell><cell cols="2">93.69 0.0979 0.0724</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation of decoder size and different decoder layer on TuSimple validation set (%). The encoder size is set to be 2.</figDesc><table><row><cell>Size</cell><cell>Layer 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>2</cell><cell cols="2">93.55 93.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>4</cell><cell cols="4">92.52 93.08 93.15 93.15</cell><cell>-</cell><cell>-</cell></row><row><cell>6</cell><cell cols="6">92.70 93.07 93.05 93.13 93.14 93.16</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head><p>For reference, derivation for Eq. 3 is as follows. Given Eq. 1, according to the perspective projection, a pixel (u, v) in the image plane projects on to the point (X, Z) on the ground plane by:</p><p>where f u is the width of a pixel on the focal plane divided by the focal length, f v is the height of a pixel on the focal plane divided by the focal length, and H is the camera height. Submitting Eq. 10 into Eq. 1 and performing some polynomial simplification:</p><p>then combining parameters together we can get Eq. 2. Given the pitch angle φ, the transformation between a tilted and an untitled camera is:</p><p>where (u, v, f ) represents the location of a point in the untitled image plane, f is the focal length which is in pixels, and (u , v , f ) represents the pitch-transformed position of that point. According to Eq. 12, we get:</p><p>Submitting u = u and Eq. 13 into Eq. 2, the curve function in the tilted image plane can be obtained with the form of Eq. 3</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">EL-GAN: embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nóra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11129</biblScope>
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwuy</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moongu</forename><surname>Jeon</surname></persName>
		</author>
		<idno>abs/2002.06604</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Line-cnn: Endto-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video-based lane estimation and tracking for driver assistance: survey, system, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">C</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A review of recent advances in lane detection and departure warning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sandipann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradnya</forename><forename type="middle">N</forename><surname>Narote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbhilasha</forename><forename type="middle">S</forename><surname>Bhujbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj Manohar</forename><surname>Narote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="216" to="234" />
		</imprint>
	</monogr>
	<note type="report_type">Pattern Recognit</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust lane detection using two-stage feature extraction with curve fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoke</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="225" to="233" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7276" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fastdraw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11582" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Polylanenet: Lane estimation via deep polynomial regression. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Ferreira</forename><surname>Lucas Tabelini Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">M</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudine</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">F De</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lane detection using spline model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eam Khwang</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="502" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel multilane detection and tracking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Meuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Nunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Müller-Schneiders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Pauli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1084" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
