<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion-Appearance Co-Memory Networks for Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
							<email>jiyangga@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
							<email>kanchen@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<email>nevatia@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Motion-Appearance Co-Memory Networks for Video Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Question: What does the woman do after look uncertain? Answer: smile App. Att. Motion Att. Motion Att. App. Att. Attention</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time 2nd Pass 1st Pass <ref type="figure">Figure 1</ref>. Answering questions in videos involves both motion and appearance analysis, and usually requires multiple cycles of reasoning, especially for transitive questions, e.g. " What does the woman do after look uncertain?", we need to first localize when the woman looks uncertain, which requires motion evidence for looking uncertain and appearance evidence for the woman; and then focus on what the woman does (smile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance comemory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-ofthe-art significantly on all four tasks of TGIF-QA. * indicates equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding video temporal structure is an important topic in computer vision. To achieve this goal, various tasks have been proposed, such as temporal action localization <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b9">10]</ref>, action anticipation <ref type="bibr" target="#b10">[11]</ref> and video prediction <ref type="bibr" target="#b32">[32]</ref>. Besides these tasks, video Question Answering (QA) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">30]</ref> is another challenging task, which not only requires the understanding of video temporal structure, but also joint reasoning of videos and texts. In this paper, we tackle the problem of video QA.</p><p>Image and text question answering have achieved much progress recently. The success comes in part from the application of attention mechanisms <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b21">21]</ref> and memory mechanisms <ref type="bibr" target="#b20">[20]</ref> in deep neural networks. Attention mechanisms tell the neural network "where to look", while the memory mechanism refines answers in multiple reasoning cycles. Video QA is different from image QA <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b21">21]</ref> in two aspects: <ref type="bibr" target="#b0">(1)</ref> the questions are more about temporal reasoning of the videos, e.g. motion transition and action counting, than spatial attributes, such as colors, spatial locations, which require effective temporal representation modeling;</p><p>(2) the input source is a sequence of images, rather than a single image, which contains richer information not only in quantity but also in variety (appearance, motion, transition) to "remember", and it makes the reasoning process more complicated.</p><p>Dynamic Memory Networks (DMN) <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref> were orig-inally proposed for text and image question answering. It contained a memory module to encode the input sources multiple cycles and an attention mechanism allowing the reading process to focus on different contents in each cycle. Although DMN contains an input module and a memory module which are able to read and remember a long sequence information, which is applicable for videos, directly applying such a method to video QA task would not give satisfying results. Because it lacks motion analysis, especially joint analysis between motion and appearance in videos, and temporal modeling. To strengthen the memory mechanism, Na et al. <ref type="bibr" target="#b25">[25]</ref> proposed a read-write memory network that jointly encode the movie appearance and caption content, however it lacks motion analysis and dynamic memory update. Xu et al. <ref type="bibr" target="#b35">[35]</ref> exploited the appearance and motion via gradually refined attention, where the motion and appearance features are fused together.</p><p>We observe two unique attributes of answering questions in videos. The first is that the motion and appearance information are usually correlated with each other in the reasoning process. For example, in answering the question "what does the woman do after look uncertain?" as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, we need to first localize "the woman look uncertain" action, which requires motion evidence for looking uncertain and appearance evidence for the woman; after that, we need to ignore the man's interval, and then focus on what the woman does (smile). Appearance and motion information are both involved in the reasoning process and provide attention cues to each other. The second attribute is that different types of questions may require representations from different amounts of frames, for example, "what is the color of the bulldog?" needs only a single frame to produce the answer, while "How many times does the cat lick" needs the understanding of the whole video.</p><p>Based on these observations, we propose a motionappearance co-memory network for video QA. Our model is built on concepts of DMN/DMN+ <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref>, so we share the same terms with DMN <ref type="bibr" target="#b20">[20]</ref>, such as facts, memory and attention. Specifically, a video is converted to a sequence of motion and appearance features by the two-stream models <ref type="bibr" target="#b34">[34]</ref>. The motion and appearance features are then fed into a temporal convolutional and deconvolutional neural network to build multi-level contextual facts, which have the same temporal resolution but represent different contextual information. These contextual facts are used as input facts to the memory networks. The co-memory networks hold two separate memory states, one for motion and one for appearance. To jointly model and interact with the motion and appearance information, we design a co-memory attention mechanism that takes motion cues for appearance attention generation, and appearance cues for motion attention generation. Based on these attentions, we design dynamic fact ensemble method to produce temporal facts dynamically at each cycle of fact encoding. We evaluate our model on TGIF-QA dataset <ref type="bibr" target="#b15">[16]</ref>, and outperform state-of-the-art performance significantly on all four tasks in TGIF-QA.</p><p>The novelty of our method is three-fold compared with DMN/DMN+ <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref>:</p><p>(1) We design a co-memory attention mechanism to jointly model motion and appearance information.</p><p>(2) We use temporal conv-deconv networks to build multi-level contextual facts for video QA.</p><p>(3) We introduce a method called dynamic fact ensemble to dynamically produce temporal facts in each cycle of fact encoding.</p><p>In the following, we first introduce related work, and then outline the DMN/DMN+ framework. In Section 4, we present our motion-appearance co-memory network in detail, and in Section 5, we show the evaluation of our method on TGIF-QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image question answering. Image question answering aims to measure the capability of reasoning about linguistic and image inputs jointly. Many methods have been proposed <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref>. Among all these models, attention mechanism <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b42">42]</ref> provides guidance to deep models on "where to look" and memory mechanism <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref> allows the model to have multiple reasoning iterations and refine the answer gradually. Question-guided attention mechanism <ref type="bibr" target="#b4">[5]</ref> uses semantic representation of a question as query to search for the regions in an image that are related to the answer. Yang et al. <ref type="bibr" target="#b37">[37]</ref> presented a Stacked Attention Network (SAN) that queries an image multiple times to infer the answer progressively. Lu et al. <ref type="bibr" target="#b21">[21]</ref> argued that modeling "what words to listen to" is equally important to model "where to look", and proposed a co-attention model that jointly reasons about image-guided and question-guided attention. Instead of directly inferring answers from the abstract visual features, Yu et al. <ref type="bibr" target="#b38">[38]</ref> developed a semantic attention mechanism to select high-level question-related concepts. Dynamic memory network (DMN), which was first introduced by Kumar et al. <ref type="bibr" target="#b20">[20]</ref> to solve text based question answering, adopted episodic memories and attention mechanisms which allow multiple cycles of reasoning. Xiong et al. <ref type="bibr" target="#b33">[33]</ref> improved the memory and input module of DMN so that it can be applied to image QA.</p><p>Video question answering. Video QA is a relatively new task compared with image QA. Yu et al. <ref type="bibr" target="#b39">[39]</ref> adopted a semantic attention mechanism, which combines the detected concepts in videos with text encoding/decoding to generate answers. Comparing with images, temporal domain is unique to videos. A temporal attention mechanism is leveraged to selectively attend to one or more periods of a video in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b35">35]</ref>. Besides temporal attention mecha- <ref type="figure">Figure 2</ref>. General Dynamic Memory Network (DMN) <ref type="bibr" target="#b20">[20]</ref> architecture. The memory update process for the t-th cycle is : (1) the facts F are encoded by an attention-based GRU in episodic memory module, where the attention is generated by last memory m t−1 ; (2) the final hidden state of the GRU is called contextual vector c t , which is used to update the memory m t together with question embedding q. The question answer is generated from the final memory state m T . nism, Jang et al. <ref type="bibr" target="#b15">[16]</ref> and Xu et al. <ref type="bibr" target="#b35">[35]</ref> also utilized motion information along with appearance information in videos. Recently Na et al. <ref type="bibr" target="#b25">[25]</ref> and Kim et al. <ref type="bibr" target="#b18">[18]</ref> both introduced the memory mechanism to their models for video QA. However, their models <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b18">18]</ref> both lack motion analysis and dynamic memory update mechanism. Video temporal analysis. To answer the video-based questions correctly, temporal analysis of videos is necessary. Shou et al. <ref type="bibr" target="#b29">[29]</ref> presented a multi-stage Segment-CNN model to generate action proposals and localize actions in videos. Temporal Unit Regression Network (TURN) <ref type="bibr" target="#b8">[9]</ref> and Cascaded Boundary Regression (CBR) <ref type="bibr" target="#b9">[10]</ref> exploit the temporal boundary regression mechanism for proposal generation and action detection. Recently Gao et al. <ref type="bibr" target="#b7">[8]</ref> and Hendricks et al. <ref type="bibr" target="#b1">[2]</ref> proposed to localize activities by language queries, their methods involve of joint modeling of the videos and language queries, which also related to video QA.</p><formula xml:id="formula_0">Question GRU Answer Decoder # Memory Update … A-GRU A-GRU A-GRU 2nd Pass … A-GRU A-GRU A-GRU 1st Pass Memory Update $ $ &amp; $ … $ &amp; ) * + Facts</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">General Dynamic Memory Networks</head><p>As our work is closely related to DMN <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref>, we begin with introducing the general framework of DMN. It contains four distinct modules: an input module, a question module, an episodic memory module and an answer module, as shown in <ref type="figure">Figure 2</ref>.</p><p>Fact module. The fact module converts the input data (e.g. text, image, video) into a set of vectors called facts, which is denoted as F = [f 1 , f 2 , ..., f L ], where L is the total number of facts. For text-based QA, <ref type="bibr" target="#b20">[20]</ref> used a Gated Recurrent Unit (GRU) to encode all text information; for image-based QA, <ref type="bibr" target="#b33">[33]</ref> adopted a bi-directional GRU to encode the local region visual features to globally-aware facts.</p><p>Question module. The question module converts the question into an embedding q. Specifically, <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b33">33]</ref> used a GRU to encode the question sentence and use the final hidden state of the GRU as the question embedding.</p><p>Episodic memory module. Episodic memory is designed to retrieve the relevant information from the facts. To extract information related to the questions from the facts more effectively, especially when transitive reasoning is required in questions, the episodic memory module iterates over the input facts for multiple cycles, and updates the memory after each cycle. There are two important mechanisms in the episodic memory module: an attention mechanism and a memory update mechanism.</p><p>Suppose that the updated memory after t-th cycle is m t , the facts set F = [f 1 , f 2 , ..., f L ], the question embedding is q, then the attention gate g t i is given by</p><formula xml:id="formula_1">g t i = F a (f i , m t−1 , q)<label>(1)</label></formula><p>where F a is an attention function which takes the fact vector f i at step i, memory m t−1 at cycle t − 1 and the question q as inputs, and outputs a scalar value g t i , which represents the attention value for the fact f i in cycle t.</p><p>To effectively use the ordering and positional information in videos, an attention based GRU is designed. Instead of using the original update gate in the GRU, the attention gate g t i is used, the update equation for the modified GRU is</p><formula xml:id="formula_2">h i = g t i •h i + (1 − g t i ) • h i−1<label>(2)</label></formula><p>The final hidden state of the attention based GRU is used as the contextual feature c t for updating the episodic memory m t . Together with the question embedding q and the memory for cycle t − 1, the t-th cycle memory is updated by</p><formula xml:id="formula_3">m t = F m (m t−1 , c t , q)<label>(3)</label></formula><p>where F m is a memory update function. The final memory m T is passed to the answer module to generate the final answers, where T is the number of memory update cycle. Answer module. The answer module takes both q and m T to generate the models predicted answer. Different answer decoders may be applied for different tasks, e.g. a softmax output layer for single word answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Motion-Appearance Co-Memory Networks</head><p>In this section, we present our motion-appearance comemory networks, including multi-level contextual facts, co-memory module and answer module. The question module remains the same as the one in traditional DMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-level Contextual Facts</head><p>The videos are cut into small units <ref type="bibr" target="#b8">[9]</ref> (a sequence of frames). For each video unit, we use two-stream CNN models <ref type="bibr" target="#b34">[34]</ref> to extract unit-level motion and appearance features. More feature pre-processing details are given in Sec- To build multiple levels of temporal representations where each level represent different contextual information, we use the temporal convolutional layers to model the temporal contextual information and de-convolutional layers to recover temporal resolution, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Specifically, the lowest level feature sequence is built directly from the unit features,</p><formula xml:id="formula_4">A 1 L = {a i }, B 1 L = {b i }.</formula><p>The convolutional layers compute a feature hierarchy consisting of temporal feature sequences at several scales with a scaling step of 2, F 1</p><formula xml:id="formula_5">L , F 2 L/2 , F 3 L/4 , .</formula><p>.., as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Note that F could be A (for appearance features) or B (for motion features). The de-convolutional pathway hypothesizes higher resolution features F 2 L , F 3 L by upsampling temporally coarser, but semantically stronger, feature sequences. Thus, F 1 L , F 2 L and F 3 L have the same resolution but different temporal contextual coverage. Note that we only show 3 levels in <ref type="figure" target="#fig_0">Figure 3</ref>, more levels could be modeled by adding more convolutional and de-convolutional layers.</p><formula xml:id="formula_6">F L = {F 1 L , F 2 L , ..., F N L } is termed as contextual facts.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Motion-appearance Co-Memory Module</head><p>In this part, we introduce the co-memory attention mechanism and the dynamic fact ensemble method.</p><p>Co-memory attention. The questions in video QA usually involve both appearance and motion. Appearance usually provides useful cues for motion attention, i.e. guides the focus on motion content, and vice versa. To allow interaction between appearance and motion, we design a comemory attention mechanism. Specifically, two separate memory modules are used to hold motion memory m t b and appearance memory m t a , where t is the number of cycle for memory update. As indicated before, when the networks read motion facts to update motion memory, appearance memory provides useful cues to generate attentions; motion memory is also helpful for updating appearance attention. Therefore, m  cycle. As we build multiple levels of facts, we generate an attention score for each fact vector at each level. The motion attention gate for fact b i j is gb t i,j and the appearance attention for fact a i j is ga t i,j , where t means the number of cycle, i is the level of fact representation and j is the step of the facts.</p><formula xml:id="formula_7">za t i,j = tanh W 2 a a j i + W 1 a [m t−1 a , q] ga t i,j = W 4 a za t i,j + W 3 a [m t−1 b , q]<label>(4)</label></formula><formula xml:id="formula_8">zb t i,j = tanh W 2 b b j i + W 1 b [m t−1 b , q] gb t i,j = W 4 b zb t i,j + W 3 b [m t−1 a , q]<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">W 1 a , W 2 a , W 3 a , W 4 a , W 1 b , W 2 b , W 3 b and W 4 b</formula><p>are weight parameters. ga t i,j and gb t i,j are attentions used in dynamic fact ensemble and memory update.</p><p>Dynamic fact ensemble.</p><p>As shown in Section 4.1, we build a multi-layer contextual facts set F L = {F 1 L , F 2 L , ..., F N L } for motion and appearance separately, which have the same temporal resolution, but represent different contextual information. There are two reasons that the facts should be selected dynamically: (1) Different types of questions may require different level of representations, e.g. the "bulldog color" and the "cat lick" questions given in Section 1; (2) During the multiple cycles of the fact reading,  <ref type="figure">Figure 5</ref>. Multi-layer contextual facts are dynamically constructed via a soft attention fusion process, which computes a weighted average facts according to the attention. each cycle may focus on different level of information. We designed an attention-based fact ensemble methods shown in <ref type="figure">Figure 5</ref>. For simplicity, we use g t i,j to represent the attention gate, which is actually ga t i,j for appearance and gb t i,j for motion. We calculate Softmax over g t i,j along level axis (i.e. i) to get attention scores s t i,j . The ensemble facts can be represented as</p><formula xml:id="formula_10">F s t : {f t j = N i=0 s t i,j f i j } L j=1<label>(6)</label></formula><p>where f i,j is the fact vector of level i and step j in the contextual facts F L . The attention scores used in the later fact encoding process are given by</p><formula xml:id="formula_11">s t j = softmax( 1 N N i=0 g t i,j ), j = 1, 2, ..., L<label>(7)</label></formula><p>where the Softmax is computed along j axis. Memory update. The fact encoding processes are conducted separately for motion and appearance, which adopts an attention based GRU <ref type="bibr" target="#b33">[33]</ref> to generate contextual vectors c t a and c t b for appearance and motion in the t-th cycle. Motion memory m t b and appearance memory m t a are updated separately as follows.</p><formula xml:id="formula_12">m t a = FC([m t−1 a , q, c t a ])<label>(8)</label></formula><formula xml:id="formula_13">m t b = FC([m t−1 b , q, c t b ])<label>(9)</label></formula><p>where FC means fully-connected layer, ReLU is used as the non-linear activation. The final output memory m h is the concatenation of m T a and m T b , where T is the number of cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Answer Module</head><p>Following <ref type="bibr" target="#b15">[16]</ref>, we model the four tasks in TGIF-QA <ref type="bibr" target="#b15">[16]</ref> into three different types: multiple-choice, open-ended numbers and open-ended words.</p><p>For multiple-choice, we use a linear regression function that takes the memory state m h and outputs a real-valued score for each answer candidate.</p><formula xml:id="formula_14">s = W T m m h<label>(10)</label></formula><p>where W m are weight parameters. The model is optimized by hinge loss between the scores for correct answers s p and the scores for incorrect answers s n , max(0, 1 + s n − s p ). This decoder is used to solve repeating action and state transition tasks. For open-ended numbers, we also use a linear regression function which takes the memory state m h and outputs an integer-valued answer.</p><formula xml:id="formula_15">s = [W T n m h + b]<label>(11)</label></formula><p>where [.] means rounding. We adopt 2 loss between the groundtruth value and the predicted value to train the model, which is used to solve the repetition count task. For open-ended words, we treat this as a classification problem. A linear function that takes the final memory state m h followed by a softmax layer is adopted to generate an-</p><formula xml:id="formula_16">swers. o = softmax W w m h + b<label>(12)</label></formula><p>where W w are weight parameters and b is bias. Crossentropy loss is used to train the model and this type of decoder is used in Frame QA task.</p><p>For each task, we train a separate model by the answer decoder and loss mentioned above. The model of each task is trained and evaluated individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In this section, we describe the dataset and evaluation settings, and discuss the experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We evaluate the proposed model on TGIF-QA dataset <ref type="bibr" target="#b15">[16]</ref>, which is a large-scale dataset introduced by Jang et al. for Video QA. The dataset consists of 165k QA pairs collocted from 71k animated Tumblr GIFs. There are four types of tasks: repetition count, repetition action, state transition and frame QA. First three tasks are unique to videos and require temporal reasoning to answer them.</p><p>Tasks. Repetition count is an open-ended task to count the number of repetition of an action (e.g. "How many times does the cat lick?"). There are 11 possible answers (i.e. from 0 to 10+) in total. Repetition action is a 5-option multiple choice task, which is asking about the name of the action that happened specific times (e.g. "what does the duck do 3 times?"). State transition is also a 5-option multiple choice task which can be answered by understanding the transition of two states in a video (e.g. "What does the woman do after drink water?"). Besides, TGIF-QA also provides a traditional frame QA task (i.e. image QA). The image QA questions of previous datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22]</ref> can be answered by getting effective information from a single given image; but for frame QA in TGIF-QA dataset, the model needs to find the most relevant frame among all frames in the video  <ref type="table" target="#tab_0">Trans  Count Frame  Training  20,475 52,704 26,843 39,392  Testing  2,274  6,232  3,554 13,691  Total  22,749 58,936 30,397 53,083</ref> to answer the question correctly. Frame QA is defined as an open-ended task. The number of QA pairs of TGIF-QA for the four tasks are shown in table 1. Metric. For the task of repetition count, the Mean Square Error (MSE) between the predicted count value and the groundtruth count value is used for evaluation. For repetition action, state transition and frame QA, classification accuracy (ACC) is used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># QA pairs Action</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Appearance and motion features. Since the frames per second (FPS) of the GIFs in TGIF-QA <ref type="bibr" target="#b15">[16]</ref> vary, we extract frames from all GIFs with the FPS that is specified by the corresponding GIF file. The long videos are cut into small units, each unit contains 6 frames.</p><p>To extract unit-level video features, we use ResNet-152 <ref type="bibr" target="#b11">[12]</ref> to process the central frame of a unit, and the outputs of "pool5" layer (∈ R 2,048 ) of ResNet-152 is used as our appearance features. To utilize motion information, we extract optical flow inside a video unit, and use the flow CNN from two-stream model <ref type="bibr" target="#b34">[34]</ref> to get unit-level flow features. Specifically, the two-direction dense optical flows <ref type="bibr" target="#b5">[6]</ref> which are calculated between two adjacent frames in a six-consecutive-frame unit are fed into the pre-trained flow CNN model, which is a BN-Inception network <ref type="bibr" target="#b14">[15]</ref>. Then we take the feature map of the "global pool" layer (∈ R 1,024 ) as the raw optical flow features. Finally, we down-sample the feature dimension by average pooling and get a 2048-dimension vector as our two-direction optical flow feature. In this process, we pad the first or last frame if we didn't have enough frames centered at each step. We set the temporal resolution of video features to be 34, long feature sequences are cut and short one are padded.</p><p>Contextual facts. The output channel number of each layer in the conv-deconv networks is 1024, temporal conv filter size is 3 with stride 1, deconv layer with stride 2, max pool filter size is 2 with stride 2. We build N = 3 layers of contextual facts.</p><p>Co-memory module. The size of memory state m a and m b is set to be 1024. The hidden state size of the GRU for fact encoding is 512. za t i,j and zb t i,j in equation <ref type="formula" target="#formula_7">(4)</ref> and (5) are 512-dimensional.</p><p>Question and answer embedding. For each word in the question, we use a pre-trained word embedding model <ref type="bibr" target="#b26">[26]</ref> to convert it to a 300-dimension vector. All words in the question are processed by a two-layer GRU, whose hidden state size is 512. The final hidden state is used as question embedding. For action transition and repeating action, the candidate answers are a sequence of words, thus we use the same method as the one for encoding questions to encode the answer.</p><p>Training details. We set the batch size to 64. Adam optimizer <ref type="bibr" target="#b19">[19]</ref> is used to optimize the model, the learning rate is set to 0.001. For each task, we train the model for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">System Baselines</head><p>Besides co-memory networks, there are two direct methods to make use of motion and appearance information: fact concatenation and memory concatenation, which are used as system baselines.</p><p>Fact concatenation. This baseline method simply concatenate the input motion facts and appearance facts, {b i } and {a i } along the feature dimension. The concatenated vector {h i } which is d b + d a dimensional is used as input facts for multi-level contextual fact module. Only one memory module is used.</p><p>Memory concatenation. In this baseline method, instead of concatenating the input facts, we use two separate memory modules: one for appearance, the other for motion, and concatenate the final motion memory states m T b and the final appearance memory states m T a to m t f together, which are used to decode answers. Co-memory attention mechanism is not used in this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiments on TGIF-QA</head><p>We first evaluate the co-memory attention module by comparing it with the two baseline method "fact concatenation" and "memory concatenation". Second, we evaluate the multi-level contextual facts and the dynamic fact ensemble. Finally, we compare our method with the previous state-of-the-art methods.</p><p>Co-memory attention. In this experiment, we set the layer of contextual facts to be 1, and dynamic fact ensemble is not used. The number of memory updates T = 2. We compare co-memory attention mechanism with "fact concatenation" (fact-concat) and "memory concatenation" (mem-concat) to see the effectiveness of co-memory attention , the results are shown in <ref type="table">Table 2</ref>. We can see that co- <ref type="table">Table 2</ref>. Evaluation of co-memory attention mechanism on TGIF-QA. "Action" is repetition action (ACC %), "Trans" is state transition (ACC %), "Count" is repetition count (MSE) and "Frame" is frame QA (ACC %).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Action memory attention outperforms fact-concat and mem-concat in all four tasks, which shows the effectiveness of the comemory attention mechanism. We believe the reason is that co-memory attention exploits the knowledge that motion and appearance provide useful cues to each other in attention generation.</p><p>Contextual facts and dynamic fact ensemble. Dynamic fact ensemble collaborates with multi-level contextual facts to construct proper temporal fact representation, so we test them together. We build 3 layers of contextual facts and do experiments to test dynamic fact ensemble module. We use "fact concatenation" as the top memory network. The results are shown in <ref type="table">Table 3</ref>: "w/o ensemble" means that we don't build the multi-level contextual facts, but just use a single temporal conv layer (filter size is 1) to convert appearance and motion features into 1024dimension vectors, which are used as input facts. <ref type="table">Table 3</ref>. Evaluation of dynamic fact ensemble on TGIF-QA. "Action" is repetition action (ACC %), "Trans" is state transition (ACC %), "Count" is repetition count (MSE) and "Frame" is frame QA (ACC %).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Action It can be seen that the ensemble provides better results. We believe the reason is that the attention-based fact fusion optimizes the ensemble process by using weighted average of the contextual facts, and avoids just using only one of them, which may make the facts sub-optimal.</p><p>How many cycles of memory update are sufficient? We test the co-memory attention model with different memory update times T = 1, 2, 3 to see how many cycles of memory update are sufficient for video QA task. The dynamic fact ensemble is not used in this experiment. The results are shown in <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>. Comparison on cycles of memory update on TGIF-QA. "Action" is repetition action (ACC %), "Trans" is state transition (ACC %), "Count" is repetition count (MSE) and "Frame" is frame QA (ACC %). We can see that two cycles (T = 2) of memory update gives the best performance on the task of "Action", "Trans" and "Count". For "Frame", T = 2 and T = 3 have similar results. Comparing the results of T = 2 and T = 1 in "Trans", we can see that T = 2 improves the performance by 3.3%, we believe the reason is that multiple cycles of fact <ref type="table">Table 5</ref>. Comparison with the state-of-the-art method on TGIF-QA dataset. "Action" is repetition action (ACC %), "Trans" is state transition (ACC %), "Count" is repetition count (MSE) and "Frame" is frame QA (ACC %).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Action Trans Count Frame</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Action Trans Frame Count 1 VIS+LSTM(aggr) <ref type="bibr" target="#b27">[27]</ref> 46.8 56.9 34. <ref type="bibr" target="#b5">6</ref> 5.09 VIS+LSTM(avg) <ref type="bibr" target="#b27">[27]</ref> 48. <ref type="bibr" target="#b7">8</ref>  Comparison with state-of-the-art method. There are two version of TGIF-QA, we report the performance of the second version, which is released by the authors of <ref type="bibr" target="#b15">[16]</ref> on Arxiv. The first version is originally reported in the CVPR version of <ref type="bibr" target="#b15">[16]</ref>. State-of-the-art method <ref type="bibr" target="#b15">[16]</ref> on TGIF-QA adopted a dual-LSTM based approach with both spatial and temporal attention. Originally, their model is trained on C3D <ref type="bibr" target="#b31">[31]</ref> temporal feature and ResNet-152 <ref type="bibr" target="#b11">[12]</ref> frame feature. However, our method adopts Flow CNN model (Inception) for motion and ResNet-152 for appearance. Thus, for fair comparison, we train their model (https://goo.gl/SVKTP9) with our features on all four tasks in TGIF-QA. The results are shown in <ref type="table">Table 5</ref>. In <ref type="table">Table  5</ref>, "SP" means spatial attention, "TP" means temporal attention, "(R+C)" means ResNet-152 features and C3D features, "(R+F)" means ResNet-152 features and Flow CNN features (our feature). We also list methods "VIS-LSTM" <ref type="bibr" target="#b27">[27]</ref> and "VQA-MCB" <ref type="bibr" target="#b6">[7]</ref>, which are provided in <ref type="bibr" target="#b15">[16]</ref>.</p><p>There are two co-memory variants shown in <ref type="table">Table 5</ref>: "co-memory (w/o DFE)" uses co-memory attention with T = 2 memory update, but not dynamic fact ensemble; "co-memory (full)" uses co-memory attention with T = 2 memory update and dynamic fact ensemble (soft fusion) on 3-layer contextual facts. We can see that our method outperforms the state-of-the-art method significantly on all four tasks. Some visualization examples are shown in <ref type="figure">Figure 6</ref>.  <ref type="figure">Figure 6</ref>. Examples on state transition, repetition action, repetition count and frame QA are shown in 1st, 2nd, 3rd and 4th row. ST-TP is the temporal attention model from <ref type="bibr" target="#b15">[16]</ref>. Green is for correct prediction and red is for wrong prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Comparing with image QA, video QA deals with long sequences of images, which contains richer information in both quantity and variety. In addition, motion and appearance information are both important for video analysis, and usually correlated with each other and able to provide useful attention cues to the other. Motivated by these observations, we propose a motion-appearance co-memory network for video QA. Specifically, we design a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention, a temporal conv-deconv network to generate multi-level contextual facts, and a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and outperforms state-of-the-art performance significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The input temporal representations are processed by temporal conv-deconv layers to build multi-layer contextual facts, which have the same temporal resolution but different contextual information. tion 5. The sequence of unit-level appearance features and motion features is represented as {a i } and {b i } respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>− 1 b</head><label>1</label><figDesc>and m t−1 a are both used to generate attentions for motion and appearance fact encoding in the t-th</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Co-memory attention module extracts useful cues from both appearance and motion memories to generate attention ga t /gb t for motion and appearance separately. Dynamic fact ensemble takes the multi-layer contextual facts AL/BL and the attention scores ga t /gb t to construct proper facts As/h L /B s/h L , which are encoded by an attention-based GRU. The final hidden state c t b /c t a of the GRU is used to update the memory m t b /m t a . The final output memory m h is the concatenation of the motion and appearance memory, and it is used to generate answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Q: 4 Q:</head><label>4</label><figDesc>What does the man do before look surprise? Co-memory: blink eye ST-TP: pet Q: What does the man do after open a door? Co-memory: grab an object ST-TP: say something while pop cap off of a pen Q: What does the person do 2 times? Co-memory: chew meat ST-TP: rub finger across face Q: What does the woman do 2 times? Co-memory: fold both hands ST-TP: bob head Q: What is the color of the hat? Co-memory: white ST-TP: blue Q: What is the man performing a trick falls and crashes? Co-memory: motorcycle ST-TP: bicycle Q: How many times does the man dip his body? Co-memory: 2 ST-TP: How many times does the woman turn eyes? Co-memory: 3 ST-TP: 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Number of samples of different tasks in TGIF-QA dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We found an evaluation mistake in<ref type="bibr" target="#b15">[16]</ref> (https://goo.gl/SVKTP9) on count task. The new performances updated by the authors are listed here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was supported, in part, by the Office of Naval Research under grant N00014-18-1-2050.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge aided consistency for weakly supervised phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abc-cnn: An attention based convolutional neural network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05960</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image analysis</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="363" to="370" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoderdecoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepstory: video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Marioqa: Answering questions by watching gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A read-write memory network for movie story understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual semantic planning using deep successor representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
