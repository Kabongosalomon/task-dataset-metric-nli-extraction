<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EagerMOT: 3D Multi-Object Tracking via Sensor Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
						</author>
						<title level="a" type="main">EagerMOT: 3D Multi-Object Tracking via Sensor Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-object tracking (MOT) enables mobile robots to perform well-informed motion planning and navigation by localizing surrounding objects in 3D space and time. Existing methods rely on depth sensors (e.g., LiDAR) to detect and track targets in 3D space, but only up to a limited sensing range due to the sparsity of the signal. On the other hand, cameras provide a dense and rich visual signal that helps to localize even distant objects, but only in the image domain. In this paper, we propose EagerMOT, a simple tracking formulation that eagerly integrates all available object observations from both sensor modalities to obtain a well-informed interpretation of the scene dynamics. Using images, we can identify distant incoming objects, while depth estimates allow for precise trajectory localization as soon as objects are within the depth-sensing range. With EagerMOT, we achieve state-of-the-art results across several MOT tasks on the KITTI and NuScenes datasets. Our code is available at https://github.com/aleksandrkim61/EagerMOT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>For safe robot navigation and motion planning, mobile agents need to be aware of surrounding objects and foresee their future states. To this end, they need to detect, segment, and -especially critical in close proximity of the vehicleprecisely localize objects in 3D space across time.</p><p>As shown by Weng and Kitani <ref type="bibr" target="#b34">[35]</ref>, even a simple method that relies on linear motion models and 3D overlapdriven two-frame data association yields a competitive tracking performance when using a strong LiDAR-based 3D object detector <ref type="bibr" target="#b29">[30]</ref>. However, compared to their imagebased counterparts, methods that rely on depth sensors are more sensitive to reflective and low-albedo surfaces, and can operate only within a limited sensing range due to the sparsity of the input signal. On the other hand, image-based methods leverage a rich visual signal to gain robustness to partial occlusions and localize objects with pixel-precision in the image domain, even when objects are too far away to be localized reliably in 3D space <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b27">[28]</ref>. However, 3D localization of the surrounding objects is vital in mobile robot scenarios.</p><p>In this paper, we present EagerMOT, a simple tracking framework that fuses all available object observations originating from 3D and 2D object detectors, to obtain a well-informed interpretation of the scene dynamics. Using cameras, our method identifies and maintains tracks in the image domain, while 3D detections allow for precise 3D trajectory localization as soon as objects enter the LiDAR sensing area. We achieve this via the two-stage association procedure. First, we associate object detections originating All authors are with the Technical University of Munich. Email:</p><p>aleksandr.kim@tum.de, aljosa.osep@tum.de, leal.taixe@tum.de from different sensor modalities. Then, we employ a tracking formulation that allows us to update track states even when only partial (either image-based or LiDAR-based) object evidence is available. This way, our EagerMOT is robust to false negatives originating from different sensor modalities and can initialize object tracks before objects enter the depthsensing range.</p><p>Our method is versatile enough to be applied to several different sensory configurations, such as LiDAR combined with a front-facing camera (as used in KITTI <ref type="bibr" target="#b12">[13]</ref>), or combined with multiple cameras with non-overlapping view frustums (as employed in NuScenes <ref type="bibr" target="#b4">[5]</ref>). With EagerMOT, we establish a new state-of-the-art on the large-scale NuScenes 3D MOT benchmark <ref type="bibr" target="#b4">[5]</ref> and KITTI tracking benchmark <ref type="bibr" target="#b12">[13]</ref> for 2D multi-object tracking and segmentation.</p><p>Our method merely assumes a mobile platform with a calibrated sensory setup equipped with a LiDAR and (possibly multiple) camera sensors. Given a pre-trained object detector for both sensor modalities, our method can be easily deployed on any mobile platform without additional training and imposes a minimal additional computational cost at the inference time.</p><p>In summary, our contributions are the following: (i) we propose a simple yet effective multi-stage data association approach that can leverage a variety of different object detectors, originating from potentially different modalities; (ii) we show that our approach can be applied to a variety of MOT tasks (2D/3D MOT and MOTS) and on different sensor configurations; and finally (iii), we perform a thorough analysis of our method, demonstrating through ablation studies the effectiveness of the proposed approach to data association and state-of-the-art results on three different benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>2D MOT. The majority of the existing vision-based tracking methods rely on recent advances in the field of object detection <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b14">[15]</ref> to detect and track objects in the image domain. TrackR-CNN <ref type="bibr" target="#b33">[34]</ref> extends Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> with 3D convolutional networks to improve temporal consistency of the detector and uses object re-identification as a cue for the association. Tracktor <ref type="bibr" target="#b1">[2]</ref> re-purposes the regression head of Faster R-CNN <ref type="bibr" target="#b26">[27]</ref> to follow the targets. Similarly, CenterTrack <ref type="bibr" target="#b41">[42]</ref> augments the object detector <ref type="bibr" target="#b42">[43]</ref> with an offset-regression head used for cross-frame association. Recent trends are going in the direction of end-to-end learning <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b11">[12]</ref> and learning to associate using graph neural networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>3D MOT. Early methods for LiDAR-based multi-object tracking first perform bottom-up segmentation of LiDAR scans, followed by segment association and track classification <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Due to recent advances in point cloud representation learning <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and 3D object detection <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b29">[30]</ref>, LiDAR and stereo-based tracking-by-detection has recently been gaining popularity <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The recent method by Weng et al. <ref type="bibr" target="#b34">[35]</ref> proposes a simple yet wellperforming 3D MOT method; however, due to its strong reliance on 3D-based detections, it is susceptible to false positives and struggles with bridging longer occlusion gaps. A follow-up method <ref type="bibr" target="#b9">[10]</ref> replaces the intersection-over-union with a Mahalanobis distance-based association measure. The recently proposed CenterPoint <ref type="bibr" target="#b38">[39]</ref> method detects 3D centers of objects and associates them across frames using the predicted velocity vectors. In contrast, we propose a method that combines complementary 3D LiDAR object detectors that precisely localize objects in 3D space, and 2D object detectors, that are less susceptible to partial occlusions and remain reliable even when objects are far away from the sensor.</p><p>Fusion-based methods. Fusing object evidence from 2D and 3D during tracking is an under-explored area. Ošep et al. <ref type="bibr" target="#b22">[23]</ref> propose a stereo vision-based approach. At its core, their method uses a tracking state filter that maintains each track's position jointly, in the 3D and the image domain, and can update them using only partial object evidence. In contrast, our method treats different sensor modalities independently. We track targets in both domains simultaneously, but we do not explicitly couple their 2D-3D states. Alternatively, BeyondPixels <ref type="bibr" target="#b27">[28]</ref> leverages monocular SLAM to localize tracked objects in 3D space. MOTSFusion <ref type="bibr" target="#b18">[19]</ref> fuses optical flow, scene flow, stereo-depth, and 2D object detections to track objects in 3D space. Different from that, our method relies only on bounding box object detections obtained from two complementary sensor modalities and scales well across different sensory environments (e.g., single LiDAR and multiple cameras <ref type="bibr" target="#b4">[5]</ref>). The recently proposed GNN3DMOT <ref type="bibr" target="#b35">[36]</ref> learns to fuse appearance and motion models, independently trained for both images and LiDAR sequences. We compare their method to ours in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our EagerMOT framework combines complementary 2D, and 3D (e.g., LiDAR) object evidence obtained from pretrained object detectors. We provide a general overview of our method in <ref type="figure">Fig. 2</ref>. As input at each frame, our method takes a set of 3D bounding box detections 3d D t and a set of 2D detections 2d D t . Then, the observation fusion module (i) associates 2D and 3D detections originating from the same objects, (ii) the two-stage data association module associates detections across time, and, based on the available detection information (full 2D+3D, or partial) we update the track states and (iv) we employ a simple track management mechanism to initialize or terminate the tracks.</p><p>This formulation allows all detected objects to be associated to tracks, even if they are not detected either in the image domain or by a 3D sensor. This way, our method can recover from short occlusions and maintain approximate 3D location when one of the detectors fails, and, importantly, we can track far-away objects in the image domain before objects enter the 3D sensing range. Once objects enter the sensing range, we can smoothly initialize a 3D motion model for each track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fusion</head><p>We obtain two sets of object detections at the input, extracted from the input video (2D) and LiDAR (3D) streams. LiDAR-based object detections 3d D t are parametrized as 3D object-oriented bounding boxes, while image-based object detections 2d D t are defined by a rectangular 2D bounding box in the image domain. First, we establish a matching between the two sets.</p><p>The fusion module performs this task by greedily associating detections in 3d D t to detections in 2d D t based on their 2D overlap in the image domain and produces a set of fused object instances I t ={I 0 t , ..., I i t }. We define 2D overlap for a pair 3d D i t and 2d D i t as the intersection over union (IoU) between the 2D projection of 3d D i t in the camera image plane and 2d D i t . We note that while different associations criteria could be additionally taken into account (as in, e.g., <ref type="bibr" target="#b22">[23]</ref>), this simple approach was empirically proven to be robust.</p><p>During the greedy association, we sort all possible detection pairings by their overlap in descending order. Pairs are considered one-by-one and are combined to form a single fused instance both I i t when (i) their overlap is above a threshold θ f usion and (ii) neither 2D, or 3D detection has been matched yet. Fused instances both I t ⊆ I t contain information from both modalities: a precise 3D location of the object and its 2D bounding box. Instances may also store additional available information, e.g., a 2D segmentation mask <ref type="bibr" target="#b33">[34]</ref>. We refer to the remaining (unmatched) detections, which form instances 3d I i t ⊆ I t and 2d I i t ⊆ I t , as partial observations, containing information about only one of the two modalities. Note that both I t ⊆ 3d I t and both I t ⊆ 2d I t .</p><p>Multi-camera setup. For scenarios where multiple cameras are available (e.g., in the NuScenes dataset <ref type="bibr" target="#b4">[5]</ref>), we adapt our fusion algorithm as follows. In each camera, we perform</p><formula xml:id="formula_0">2d I t 3d I t 3d D t 2d D t 3D Detector 2D Detector Fusion State Update Track Initialization Track Termination 1u T t T t+1</formula><p>1st Data Association Tracks</p><formula xml:id="formula_1">2m IT t + 2u I t 2nd Data Association 1m IT t + 1u I t 2u T t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>A high-level overview of our tracking framework: at the input, we obtain object detections from different sensor modalities, e.g., an image-based detector/segmentation model and a LiDAR/stereo-based 3D object detector. We then fuse these detections into fused object instances, parameterized jointly in 3D and/or 2D space. We then pass them through a two-stage association procedure that allows us to update object tracks, even if detections originating only from one sensor modality are available. In the first stage, instances with 3D information (with/without 2D information) are matched to existing tracks. In the second association stage, unmatched tracks from the previous step 1u T t are matched with instances, localized only in 2D. fusion as explained above; in case a 3D detection is not visible in a particular camera, we consider its overlap with 2D detections in that image plane to be empty. After we perform fusion in each 2D plane individually, 3D detections visible through more than one camera might have multiple potential matches. We always associate only one 2D detection with a track and heuristically pick a detection from the view in which the projected 3D bounding box covers the largest area. Other potential pairings from other views are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Matching</head><p>During each frame t, fused instances I t enter a two-stage matching process to update existing tracks T t with new 3D and/or 2D information.</p><p>Track parameterization. As in <ref type="bibr" target="#b22">[23]</ref>, we maintain 2D and 3D state of tracks T t in parallel. However, we treat them independently. We represent the 3D state of a track by a 3D object-oriented bounding box and a positional velocity vector (excluding angular velocity, as in <ref type="bibr" target="#b34">[35]</ref>), while a 2D bounding box represents its 2D state. Since we track objects primarily in 3D, a track's confidence score is equal to its 3D state's confidence. Note that these states do not have to be fully observed for each frame, tracks might be updated using only</p><formula xml:id="formula_2">3D information 3d T t ⊆ T t , only 2D information 2d T t ⊆ T t , or both both T t ⊆ T t , both T t ⊆ 3d T t , both T t ⊆ 2d T t .</formula><p>For tracks 3d T t we additionally maintain a constantvelocity motion model, modeled by a linear Kalman filter. For each new frame t + 1, existing tracks 3d T t predict their location (an oriented 3D bounding box) in the current frame based on previous observations and velocity estimates.</p><p>First stage data association. In the first association stage, we match instances detected in 3D with existing tracks using track 3D state information. In particular, we greedily pair detected instances 3d I t with tracks 3d T t based on the scaled distance between instances' oriented bounding boxes and tracks' predicted oriented boxes. We define the scaled distance for a pair of oriented 3D bounding boxes as the Euclidean distance between them, multiplied by the normalized cosine distance between their orientation vectors:</p><formula xml:id="formula_3">d(B i , B j ) = B i ρ − B j ρ * α(B i , B j ),<label>(1)</label></formula><formula xml:id="formula_4">α(B i , B j ) = 2 − cos B i γ , B j γ , ∈ [1, 2],<label>(2)</label></formula><p>where B i ρ = [x, y, z, h, w, l] is a vector containing the 3D location and dimensions of the bounding box and B i γ represents the orientation of the box around the vertical axis.</p><p>Compared to planar Euclidean distance, this approach takes into account orientation similarity, which can be informative for non-omnidirectional objects such as vehicles or pedestrians. Experimentally, we found this association criterion to be more robust compared to 3D IoU <ref type="bibr" target="#b34">[35]</ref> and Mahalanobis distance that takes the predictive and observational uncertainty into account <ref type="bibr" target="#b9">[10]</ref>), especially in low frame rate scenarios (e.g., in NuScenes dataset <ref type="bibr" target="#b4">[5]</ref>).</p><p>Similar to subsection III-A, best-matching instance-track pairs (below a maximum threshold θ 3d ) form successful matching tuples 1m IT t = {(I i t , T j t ), ...}. We label the rest as unmatched, i.e., instances 1u I t and tracks 1u T t . After this first matching stage, all object instances detected in 3D should be successfully associated with existing tracks or be labeled as unmatched and will not participate in further matching.</p><p>Second stage data association. In the second stage, we match detected instances to tracks in the 2D image domain. We greedily associate instances 2d I t \ both I t to remaining tracks 1u T t ∪ 2d T t based on the 2D IoU criterion. For each instance-track pair, we evaluate the overlap between the instance's 2D bounding box in the current frame and the 2D projection of the track's predicted 3D bounding box or the last observed 2D bounding box in case a 3D prediction is not available (for 2d T t ). Note that instances that were detected in 3D do not participate in this matching stage even if they were also detected in 2D, i.e., both I t .</p><p>This association stage is identical to the first one, except we use here 2D box IoU as the association metric, together with its threshold θ 2d . Similarly, the output of this stage are a set of matches 2m IT t = {(I i t , T j t ), ...}, a set of unmatched instances 2u I t , and unmatched tracks 2u T t . In the case of multiple cameras being available, we modify the algorithm as described earlier in Part III-A.</p><p>We use a 3D motion model to obtain 2D bounding box predictions in the image domain using a camera projection operation. There is not enough 3D evidence to initialize the motion model reliably for certain tracks -this usually happens for objects observed outside of the LiDAR sensing range. In such scenarios, the apparent bounding box motion is usually negligible, and association can be made purely based on observed 2D boxes. Adding a prediction model for the 2D state (as in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>) or a (learned) appearance model <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b33">[34]</ref> could be used to improve the second association stage further and remains our future work.</p><p>State update. We use matched detected instances to update corresponding tracks with new 3D and/or 2D state information. We simply update the 2D state (top-left and bottomright bounding box corners) by over-writing the previous state with the newly-detected 2D bounding box. We model the 3D state of a track (i.e., object-oriented bounding box parameters) as a multi-variate Gaussian and filter its parameters using a constant-velocity linear Kalman filter (exactly as in <ref type="bibr" target="#b34">[35]</ref>). When 3D object detection information is not available (e.g., we have a partial observation providing only a 2D bounding box or a segmentation mask in the image domain, we only perform the Kalman filter prediction step to extrapolate the state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Track lifecycle</head><p>Following AB3DMOT <ref type="bibr" target="#b34">[35]</ref>, we employ a simple set of rules to manage object trajectories and their lifecycle. A track is discarded if it has not been updated with any instance (either 3D or 2D) in the last Age max frames. As 3D object detectors are usually not as reliable as image-based detectors in terms of precision, a track is considered confirmed if it was associated with an instance in the current frame and has been updated with 2D information in the last Age 2d frames. Finally, all detected instances 2u I t that were never matched start new tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We evaluate our method using two datasets, KITTI <ref type="bibr" target="#b12">[13]</ref> and NuScenes <ref type="bibr" target="#b4">[5]</ref> using four different multi-object tracking benchmarks: (i) NuScenes 3D MOT, (ii) KITTI 3D MOT, (iii) KITTI 2D MOT, and (iv) KITTI MOTS <ref type="bibr" target="#b33">[34]</ref>. For NuScenes 3D MOT, KITTI 2D MOT, and KITTI MOTS, we use the official benchmarks and compare our method to published and peer-reviewed state-of-the-art methods.    <ref type="bibr" target="#b35">[36]</ref>. Note that several methods are only reported results for the car class.</p><p>Evaluation measures. We discuss the results using standard CLEAR-MOT evaluation measures <ref type="bibr" target="#b2">[3]</ref> and focus the discussion on the multi-object tracking accuracy (MOTA) metric. For KITTI 3D MOT, we follow the evaluation setting of <ref type="bibr" target="#b34">[35]</ref> and report averaged variants of CLEAR-MOT evaluation measures (AMOTA and AMOTP stand for averaged MOTA and MOTP). For MOTS, we follow the evaluation protocol of <ref type="bibr" target="#b33">[34]</ref> and report multi-object tracking and segmentation accuracy (MOTSA) and precision (MOTSP). On KITTI 2D MOT and MOTS benchmarks we additionally report the recently introduced higher-order tracking accuracy (HOTA) metric <ref type="bibr" target="#b19">[20]</ref> 1 . HOTA dis-entangles detection and tracking aspects of the task by separately measuring detection accuracy (DetA) that evaluates detection performance, and association accuracy (AssA) that evaluates detection association.</p><p>3D detections. For our final model on NuScenes, we use detections provided by CenterPoint <ref type="bibr" target="#b38">[39]</ref>. On KITTI 3D MOT, we report and compare results obtained using state-of-the-art Point-GNN <ref type="bibr" target="#b30">[31]</ref> and Point R-CNN <ref type="bibr" target="#b29">[30]</ref> (as used by <ref type="bibr" target="#b34">[35]</ref>) 3D object detectors. For our model, submitted to the KITTI benchmark, we used Point-GNN <ref type="bibr" target="#b30">[31]</ref>. We do not pre-filter 3D object detections and take all of them as input to our tracking pipeline.    <ref type="bibr" target="#b25">[26]</ref> detector for the car class, same as MOTSFusion <ref type="bibr" target="#b18">[19]</ref> and BeyondPixels <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation studies</head><p>Data association. In <ref type="table" target="#tab_1">Table IV</ref>, we compare different variants of our method, evaluated on the NuScenes validation set. The significant difference between "Full" (0.712 AMOTA) and "No 2D info" (0.651 AMOTA) highlights the impact of leveraging 2D object detections on the overall performance. We note that as we improve the recall (+0.054) with our full model, we observe a decrease in AMOTP (−0.018), which measures localization precision, averaged over all trajectories. This is because our method can leverage 2D object detections and update track states even when 3D detections are not available. In this case, we cannot update the track state using 3D evidence. However, we can still localize objects by performing Kalman filter predictions at the loss of overall 3D localization precision. Next, we ablate the impact of our data association function. The configuration "No 2D info; 2D distance" highlights the performance of a variant that does not use 2D detection information and performs association by simply computing Euclidean distance (on the estimated 2D ground-plane) between the track prediction and detections as an association criterion for the (only) matching stage. The variant "No 2D info; 3D IoU" is the variant that uses 3D IoU (as in <ref type="bibr" target="#b34">[35]</ref>) as the association metric. As can be seen, our association function is more robust compared to 2D distance (+0.004 AMOTA) and 3D IoU (+0.036 AMOTA). We conclude that 3D IoU is not suitable for NuScenes due to a significantly lower scan-rate compared to KITTI.</p><p>Detection sources. In <ref type="table" target="#tab_7">Table V</ref>, we show the impact of detection quality on overall performance. One of the advantages of our method is its flexibility. Unlike other trackers, our framework does not need expensive training and can be easily applied to off-the-shelf detectors. As expected, better detectors lead to better tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark results</head><p>NuScenes. We report the results obtained using the official NuScenes large-scale tracking benchmark in <ref type="table" target="#tab_1">Table I</ref>. In addition to published methods, we include in our analysis the highest-ranking unpublished method <ref type="bibr" target="#b38">[39]</ref>   is a challenging benchmark due to a wide variety of object classes and a low frame rate of 2FPS. For a fair comparison, we use the same 3D detections as CenterPoint <ref type="bibr" target="#b38">[39]</ref>. However, we only use 3D bounding box information (and not the predicted velocity vectors). The difference in recall supports our assumption that fusing 2D detections helps to bridge occlusions and recover tracks that would otherwise be lost. <ref type="table" target="#tab_1">Table II</ref>, we compare our 3D MOT performance to several baselines, as reported in <ref type="bibr" target="#b35">[36]</ref>. Methods marked with † use the Point R-CNN <ref type="bibr" target="#b29">[30]</ref> 3D object detector. As our method uses the same 3D detections as AB3DMOT, we can conclude that the improvements (+5.15 and +7.79 sAMOTA for car and pedestrian classes, respectively) show the merit of our two-stage association procedure that leverages 2D information to improve 3D MOT performance. <ref type="table" target="#tab_1">Table III</ref>, we report 2D MOT results we obtain on the KITTI test set. Even though we track objects in 3D space, we can report 2D tracking results by projecting 3D bounding boxes to the image plane using camera intrinsics and report minimal axis-aligned 2D bounding boxes that fully enclose those projections as tracks' 2D positions. Even though we track objects only in 3D, use 2D detections only as a secondary cue and report approximate 2D locations, we achieve state-of-the-art results in terms of the HOTA metric. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we highlight examples where 3D detector fails due to signal sparsity or occlusions; however, we obtain 2D object detections, which we use to update the track states. This example demonstrates that 3D LiDAR and image-based detections are complementary cues for tracking. We note that our method performs especially well in terms of the association accuracy (74. <ref type="bibr" target="#b15">16</ref> AssA on the car class), confirming that the secondary association stage does improve not only the detection aspect of the task but also the temporal association. For the pedestrian class, we obtain lower performance compared to current top entries, as we are using weaker object detectors (TrackR-CNN <ref type="bibr" target="#b33">[34]</ref>, trained on the non-amodal MOTS dataset) and Point-GNN <ref type="bibr" target="#b30">[31]</ref>. KITTI MOTS. Multi-object tracking and segmentation (MOTS) extends MOT with pixel-precise localization of object tracks. We can easily adapt our MOTS approach by additionally passing segmentation masks from instances to tracks after the data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI 3D MOT. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI 2D MOT. In</head><p>In <ref type="table" target="#tab_1">Table VI</ref>, we report our MOTS performance on the KITTI test set and compare it to other published methods. As can be seen, we obtain better results compared to MOTSFusion on both classes (+1.03 for car and +3.61 for pedestrian class) despite using the same set of 2D segmentation masks. We note, however, that EagerMOT additionally used 3D object detections obtained from the LiDAR stream, while MOTFusion relies on stereo cameras. Our method is applicable to a wide variety of LiDAR-centric sensory configurations, often employed in modern automotive datasets, e.g., NuScenes <ref type="bibr" target="#b4">[5]</ref>, Waymo Open Dataset <ref type="bibr" target="#b31">[32]</ref> and Argoverse <ref type="bibr" target="#b6">[7]</ref>. Moreover, our method runs at 90 FPS on KITTI (LiDAR + single camera) compared to MOTSFusion at 2 FPS (stereo cameras). <ref type="bibr" target="#b1">2</ref> Finally, our method establishes new state-of-theart results for both, car (74.66) and pedestrian (57.65) classes in terms of HOTA. Furthermore, our method performs especially well in terms of association accuracy (AssA), again confirming that using additional sensor observations helps to maintain track consistency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Runtime discussion</head><p>Excluding the time spent on object detection and egomotion estimation, our Python implementation runs at 4 FPS on NuScenes. It is slower (but more accurate) compared to StanfordIPRL-TRI <ref type="bibr" target="#b9">[10]</ref> and AB3DMOT <ref type="bibr" target="#b34">[35]</ref> that only use LiDAR data and run at 10 FPS.</p><p>On KITTI, our method runs at 90 FPS because we only have a single camera and do not need to perform multicamera association. We report higher frame rates compared to several 3D MOT methods reported on the KITTI benchmark, including GNN3DMOT (5 FPS), mmMOT (4 FPS), and FANTrack <ref type="bibr">(25 FPS)</ref>, that also leverage both 2D and 3D input.</p><p>Implementation details. On KITTI, we use θ f usion = 0.01, θ 3d = 0.01, θ 2d = 0.3, Age max = 3, and Age 2d = 3 for both classes. For 2D MOT evaluation, we report 2D projections of estimated 3D bounding boxes only for confirmed tracks.</p><p>On NuScenes, we use θ f usion = 0.3, θ 2d = 0.5, and Age max = 3 for all seven classes. Other parameters are class-specific: θ 3d = (7.5, 1.8, 4.4, 8.15, 7.5, 4.9, 7.5), Age 2d = (2, 3, 1, 3, 3, 2, 2) for car, pedestrian, bicycle, bus, motorcycle, trailer, truck. Additionally, θ f usion = 0.01 for trailer and θ f usion = 0.3 for all other classes.</p><p>For 3D MOT evaluation on KITTI and NuScenes, we report estimated 3D boxes for confirmed tracks with their original confidence scores. Estimates for unconfirmed tracks are also reported. However, we halve their scores for each frame for which we do not perform 2D updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a tracking framework that can leverage different sources of object detections originating from varying sensor modalities through a two-stage association procedure. Our experimental evaluation reveals that our method performs consistently well across different datasets and tracking tasks and can be used in combination with a variety of different object detectors -without requiring any additional detector-specific fine-tuning. We hope that our framework will serve as a baseline for future research in sensor-fusionbased multi-object tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Our method eagerly associates different sources of object detection/segmentation information (2D/3D detections, instance segmentation) when available to obtain an, as complete as possible, interpretation of the scene dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2D</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of objects overlooked by the 3D detector but recognized by the image-based detector. From the top: out of range, partially occluded, detector failure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results on the NuScenes 3D MOT benchmark. Methods marked in gray are not yet peer-reviewed.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Input</cell><cell>sAMOTA</cell><cell>MOTA</cell><cell>MOTP</cell><cell>IDs</cell></row><row><cell></cell><cell>Ours</cell><cell>2D+3D</cell><cell>94.94</cell><cell>96.61</cell><cell>80.00</cell><cell>2</cell></row><row><cell></cell><cell>Ours  †</cell><cell>2D+3D</cell><cell>96.93</cell><cell>95.29</cell><cell>76.97</cell><cell>1</cell></row><row><cell>car</cell><cell>GNN3DMOT [36] mmMOT [40]</cell><cell>2D+3D 2D+3D</cell><cell>93.68 70.61</cell><cell>84.70 74.07</cell><cell>79.03 78.16</cell><cell>10 125</cell></row><row><cell></cell><cell>FANTrack [1]</cell><cell>2D+3D</cell><cell>82.97</cell><cell>74.30</cell><cell>75.24</cell><cell>202</cell></row><row><cell></cell><cell>AB3DMOT  † [35]</cell><cell>3D</cell><cell>91.78</cell><cell>83.35</cell><cell>78.43</cell><cell>0</cell></row><row><cell>ped.</cell><cell>Ours Ours  † AB3DMOT  † [35]</cell><cell>2D+3D 2D+3D 3D</cell><cell>92.92 80.97 73.18</cell><cell>93.14 81.85 66.98</cell><cell>73.22 66.16 67.77</cell><cell>36 0 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>3D MOT evaluation on the KITTI val set (fol-</figDesc><table /><note>lowing evaluation protocol by [35]). Methods marked with † use the Point R-CNN [30] 3D object detector. Baseline results taken from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Results on the 2D MOT KITTI benchmark. Note: reported methods use different object detectors, e.g. our method uses the RRC</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, marked with gray. The test set includes 150 scenes, 80 seconds each. This</figDesc><table><row><cell>Method</cell><cell>AMOTA</cell><cell>AMOTP</cell><cell>Recall</cell><cell>IDs</cell></row><row><cell>Full</cell><cell>0.712</cell><cell>0.569</cell><cell>0.752</cell><cell>899</cell></row><row><cell>No 2D info</cell><cell>0.651</cell><cell>0.587</cell><cell>0.698</cell><cell>864</cell></row><row><cell>No 2D; 2D distance</cell><cell>0.647</cell><cell>0.595</cell><cell>0.689</cell><cell>783</cell></row><row><cell>No 2D; 3D IoU</cell><cell>0.615</cell><cell>0.658</cell><cell>0.692</cell><cell>2749</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Data association ablation study, performed on the NuScenes 3D MOT val set.</figDesc><table><row><cell>3D source</cell><cell>2D source</cell><cell>MOTA car</cell><cell>MOTA ped</cell></row><row><cell>Point-GNN [30]</cell><cell>RRC [26] + Track-RCNN [34]</cell><cell>92.5</cell><cell>72.4</cell></row><row><cell>Point R-CNN [35]</cell><cell>RRC [26] + Track-RCNN [34]</cell><cell>92.7</cell><cell>65.6</cell></row><row><cell>Point-GNN [30]</cell><cell>Cascade R-CNN [6]</cell><cell>89.0</cell><cell>69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Ablation on the effect of using different object detection sources (KITTI 2D MOT val set).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Results on the 2D KITTI MOTS benchmark (for car and pedestrian classes). Note: our method uses the same set of object detections and segmentation masks as MOTSFusion.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The official KITTI 2D MOT benchmark switched to HOTA-based evaluation shortly before releasing this paper, therefore we only report benchmark results using this metric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Both exclude the time spent on object detection and ego-motion estimation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This project was funded by the Humboldt Foundation through the Sofja Kovalevskaja Award. We thank Paul Voigtlaender for his feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fantrack: 3d multi-object tracking with feature association network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intel. Vehicles Symp</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIVP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brasó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Probabilistic 3d multi-object tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prioletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Behavioral pedestrian tracking using a camera and lidar sensors on a moving vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end learning of multi-sensor 3d tracking by detection. ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smat: Smart multiple affinity metrics for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ospina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Calvez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple object tracking with attention to appearance, structure, motion and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="104423" to="104434" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<title level="m">Learning by tracking: Siamese cnn for robust target association. CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Track to reconstruct and reconstruct to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1803" to="1810" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hota: A higher order metric for evaluating multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint self-localization and tracking of generic objects in 3d range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d pedestrian tracking using local structure constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="347" to="358" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combined imageand world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multiobject tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jrmot: A real-time 3d multi-object tracker and a new large-scale dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards 3D object recognition via classification of arbitrary object tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MOTS: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<title level="m">3D Multi-Object Tracking: A Baseline and New Evaluation Metrics. IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GNN3DMOT: Graph neural network for 3d multi-object tracking with 2d-3d multi-feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How to train your deep multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11275</idno>
		<title level="m">Center-based 3d object detection and tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust multi-modality multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust multi-modality multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
