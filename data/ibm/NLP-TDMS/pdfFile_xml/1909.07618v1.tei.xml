<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cycle-consistent Conditional Adversarial Transfer Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 21-25, 2019. 2019. October 21-25, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Science and Technology of China</orgName>
								<orgName type="institution">University of Electronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Indiana University-Purdue University Indianapolis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
							<affiliation key="aff5">
								<orgName type="department">Science and Technology of China</orgName>
								<orgName type="institution">University of Electronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erpeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cycle-consistent Conditional Adversarial Transfer Networks</title>
					</analytic>
					<monogr>
						<title level="m">Pro-ceedings of the 27th ACM International Conference on Multimedia (MM &apos;19)</title>
						<meeting> <address><addrLine>Nice, France; Nice</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 21-25, 2019. 2019. October 21-25, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3350902</idno>
					<note>, France. ACM, New York, NY, USA, 9 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision</term>
					<term>Transfer learn- ing</term>
					<term>Neural networks KEYWORDS domain adaptation, adversarial training, transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation investigates the problem of cross-domain knowledge transfer where the labeled source domain and unlabeled target domain have distinctive data distributions. Recently, adversarial training have been successfully applied to domain adaptation and achieved state-of-the-art performance. However, there is still a fatal weakness existing in current adversarial models which is raised from the equilibrium challenge of adversarial training. Specifically, although most of existing methods are able to confuse the domain discriminator, they cannot guarantee that the source domain and target domain are sufficiently similar. In this paper, we propose a novel approach named cycle-consistent conditional adversarial transfer networks (3CATN) to handle this issue. Our approach takes care of the domain alignment by leveraging adversarial training. Specifically, we condition the adversarial networks with the crosscovariance of learned features and classifier predictions to capture the multimodal structures of data distributions. However, since the classifier predictions are not certainty information, a strong condition with the predictions is risky when the predictions are not accurate. We, therefore, further propose that the truly domaininvariant features should be able to be translated from one domain to the other. To this end, we introduce two feature translation losses and one cycle-consistent loss into the conditional adversarial domain adaptation networks. Extensive experiments on both classical and large-scale datasets verify that our model is able to outperform previous state-of-the-arts with significant improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The amount of data we produce every day is truly mind-boggling, reported by Forbes, there are 2.5 quintillion bytes of data created each day at our current pace. These data consists of not only what we are familiar with but also novel and unseen instances. Conventional machine learning paradigms commonly assume that the test data are seen during the training stage <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>. Thus, it is challenging to handle the novel and unseen data in reality with conventional methods. As a practical alternative, transfer leaning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> has been verified to be crucial for the success in novel environment. One of the most essential topics of transfer learning is domain adaptation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> which investigates the problem of cross-domain knowledge transfer where the labeled source domain and unlabeled target domain have distinctive data distributions.</p><p>The very objective of domain adaptation is to reduce the domain distribution shifts between the source domain data and target domain data, so that the models trained on the well labeled source domain can be adapted to the target domain. Existing domain adaptation methods can be categorized into either shallow branch <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref> or deep branch <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. Typically, methods in the shallow branch are built as two-step formulations. The first step is used for feature extraction and the second step focuses on domain alignment. Deep methods, however, incorporate the feature extraction and domain alignment into a united architecture. Since the feature extraction component can receive feedbacks from the domain alignment component and reinforce itself, deep approaches achieved state-of-the-art results in recent years <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Among various deep domain adaptation methods, adversarial domain adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref> attracted a lot of attention lately. Adversarial domain adaptation, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, introduces adversarial training into the deep architecture with the similar idea of generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>. Specifically, adversarial domain adaptation trains a domain discriminator which aims to distinguish whether a feature is from the source domain  or the target domain. At the same time, a deep feature representation model tries to learn domain-invariant features to fool the domain discriminator. Once the domain discriminator fails to tell the feature sources, adversarial domain adaptation assumes that the learned features are well aligned. Then, a classifier trained on the source domain features can be directly deployed to the target domain features <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The basic idea of adversarial domain adaptation is sound and it has also been empirically verified to be effective in many applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>. However, there is still a fatal weakness existing in current adversarial models which is raised from the equilibrium challenge of adversarial training <ref type="bibr" target="#b1">[2]</ref>. Specifically, although most of existing methods are able to confuse the domain discriminator, they cannot guarantee that the source domain and target domain are sufficiently similar <ref type="bibr" target="#b1">[2]</ref>. As recently claimed by Long et al. <ref type="bibr" target="#b25">[26]</ref>, adversarial adaptation methods may fail to capture the complex multimodal structures which is reflected only by the cross-covariance between the features and corresponding classifier predictions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In this paper, we propose a novel approach named cycle-consistent conditional adversarial transfer networks (3CATN) to align the two domains. At first, inspired by very recent work <ref type="bibr" target="#b25">[26]</ref>, we deploy a conditional domain discriminator by leveraging the cross-covariance of learned features and corresponding classifier predictions, so that the complex multimodal structures embedded in the data can be captured. However, since the classifier predictions are not certainty information, deploying a strong condition from the classifier is very risky when the predictions are not sufficiently accurate. If we put an inaccurate condition on the adversarial domain adaptation networks, the learned features may be able to confuse the domain discriminator, but there is no guarantee that the features are truly domain-invariant. Therefore, we further argue that truly domaininvariant features should be able to be translated from one domain to the other. This assumption does make sense by considering that domain-invariant actually indicates the feature space is shared by the two domains. In other words, the domain-invariant features are represented by the same bases and thus can be represented by each other. To this end, we train two feature translators, one translates features from the source domain to the target domain and the other translates features from the target domain to the source domain. In addition, we calculate a cycle-consistent loss by leveraging the two feature translators. With such a formulation, our method is able to not only capture the complex multimodal data structures but also avoid the negative effects caused by inaccurate conditions. In summary, we list the main contributions of this paper as follows: 1) We propose a novel deep method named cycle-consistent conditional adversarial transfer networks (3CATN) for domain adaptation by taking advantage of adversarial training. Compared with existing adversarial domain adaptation methods, our approach is able to not only capture the complex multimodal structures but also avoid the negative effects caused by uncertain conditions. 2) We argue that the condition with classifier predictions is risky when the the predictions are not sufficiently accurate. To address this, we further propose that truly domain-invariant features should be able to be translated from one domain to the other. 3) Experiments on both classical and large-scale datasets verify that our method is able to outperform previous state-of-the-arts with significant advantages. We show that our method is especially remarkable on relatively hard-to-transfer tasks (predictions on target data are less accurate), which verifies the claims in 2).</p><p>The rest of this paper is organized as follows. In section 2, we give a brief review of previous work reported in the community. In section 3, we present the proposed method. Section 4 reports the experiments. At last, we draw the conclusion in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Transfer Learning and Domain Adaptation</head><p>Domain adaptation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> is a popular branch of transfer learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> which was proposed to handle the training data scarcity issue in supervised learning. Since most successful machine learning models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> are trained in a supervised fashion, it is challenging to handle new environment where there are no sufficient labeled data <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref>. At the same time, it is also unwise to build every model from scratch with plenty of related models are available <ref type="bibr" target="#b31">[32]</ref>. Transfer learning leverages available resources to challenge new situations. In fact, the very widespread success of fine-tuning on pre-trained deep models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> has verified the effectiveness of transfer leaning.</p><p>A common assumption behind conventional machine learning algorithms is that the training set and test set have identical data distributions. However, this assumption does not always hold in real-world applications <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Domain adaptation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref> was proposed to reduce the domain shifts between the labeled source domain and unlabeled target domain. Early domain adaptation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> focus on knowledge transfer techniques which are invariant with samples features. For instance, most of early domain adaptation approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> learn a common subspace which is shared by the two domains, some of them <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> reweight samples according to the distribution distance with target domain.</p><p>With the sweeping success of deep learning, neural networks are also deployed in domain adaptation. At first, deep domain adaptation approaches align the two domains by minimizing a pre-defined metric which measures the feature distributions, e.g., minimizing MMD distance <ref type="bibr" target="#b27">[28]</ref>, minimizing covariance distance <ref type="bibr" target="#b39">[40]</ref>. Recently, adversarial training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref> was introduced into domain adaptation. The main idea of adversarial domain adaptation is to train a domain discriminator which can distinguish whether a feature is from the source domain or the target domain. The learned features would be considered as domain-invariant once the domain discriminator is confused by the learned features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Domain Adaptation</head><p>Adversarial domain adaptation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> is similar with generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> but not exactly the same. GANs typically have a generator and a discriminator where the generator synthesizes fake samples from noises to fool the discriminator. Adversarial domain adaptation, however, usually does not have a "generator" which can synthesize something new. The role of "generator" in adversarial domain adaptation is played by a feature extractor. Specifically, the feature extractor learns new feature representations which is able to confuse the domain discriminator. Instead of generating from random noises in GANs, the feature extractor learns meaningful feature representations from both source domain data and target domain data. The adversarial training can be performed at either feature level or pixel level. Feature adaptation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref> minimize the domain discrepancy on the feature space. For instance, Ganin et al. <ref type="bibr" target="#b7">[8]</ref> optimize the standard minimax objective on the feature space. Tzeng et al. <ref type="bibr" target="#b41">[42]</ref> optimize an inverted label objective in a unified framework. Pixel adaptation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> propose that the learned representations can be recovered back to raw images. As a result, pixel adaptation methods are much easier to be understood by eyes and more friendly to the end visual tasks. For instance, CoGANs <ref type="bibr" target="#b23">[24]</ref> learns a joint distribution of multi-domain images. However, pixel adaptation methods generally cost much much more than feature-based ones. They are also sensitive to noises and missing pixels. For most classification or segmentation tasks, especially for large-scale applications, it is also unnecessary to directly handle raw pixels. Meaningful features have more discrimination power than pixels. It is worth noting that one of major differences between our method and Hoffman et al. <ref type="bibr" target="#b12">[13]</ref> is that the latter emphasizes on pixel adaptation while our method leverages feature translation. Thus, our method is more computational efficient than <ref type="bibr" target="#b12">[13]</ref>. In addition, as claimed by Long et al. <ref type="bibr" target="#b25">[26]</ref>, the pixel-level adaptation methods are carefully tailored to the digits and synthetic to real adaptation tasks.</p><p>In adversarial domain adaptation networks, the idea of conditional GANs <ref type="bibr" target="#b28">[29]</ref> is widely used to promote the discriminative ability of both feature extractor and domain discriminator. For instance, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a global and class-specific domain adversarial learning framework for road scene segmenters. Long et al. <ref type="bibr" target="#b25">[26]</ref> propose two novel strategies to condition the domain discriminator with classifier predictions. Our work is related with CDAN <ref type="bibr" target="#b25">[26]</ref>. We also condition our features with the classifier predictions to preserve the multimodal data structures. However, our method is also significantly different from CDAN. The major difference is that CDAN did not consider the situation where the classifier predictions may be not sufficiently accurate (Although entropy condition is proposed in CDAN, entropy itself is an inaccurate metric to measure the classification result). As a result, putting a wrong condition on the discriminator will lead the feature extractor into risky situations. This situation is common in fine-grained visual classification tasks. For instance, a pickup truck can be mispredicted as a car. With the condition of car on the discriminator, pickups would be classified as cars for good. What is worse, the errors caused by the predictions can be propagated and magnified. In this paper, we argue that truly domain-invariant features should be able to be translated from one domain to the other. In our model, therefore, we introduce two feature translators to map features from one domain to the other. In addition, a cycle-consistent loss is calculated to guarantee that the source samples translated to target style can be translated back to the source domain. Experiments verify that such a formulation is effective for domain adaptation, especially for relatively difficult tasks where the gap between the source domain and the target domain is significantly large. In other words, compared with CDAN, our formulation is able to not only capture the complex multimodal data structures but also prevent and penalize the errors caused by classifier predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD 3.1 Notations and Definitions</head><p>In this paper, we use subscripts s and t to denote source and target, respectively. Thus, we have n s labeled source samples {X s , Y s } and n t unlabeled target samples X t for unsupervised domain adaptation. The main challenge of domain adaptation is that the data distributions, both marginal distributions P s (X s ), P t (X t ) and conditional distributions P s (Y s |X s ), P t (Y t |X t ) are distinctive from the source domain to the target domain. Therefore, our goal is to train a deep network F : x → y which is able to learn discriminative domain-invariant features where the domain shifts can be sufficiently reduced. We define the related concepts as follows:</p><p>Definition 1: (Domain) A domain D is defined as a sample set X along with its probability distribution P(X ), i.e, D = {X , P(X )}. The source domain and target domain are denoted as D s and D t .</p><p>Definition 2: (Adversarial Discriminator) An adversarial discriminator D is defined as a binary classifier which is able to distinguish real from fake. In this paper, we introduce a domain discriminator D d which distinguishes a source domain feature from the target domain. We also train a target sample discriminator D t to distinguish the real target samples and fake target samples translated from the source instances, and a source sample discriminator D s to distinguish the real source samples and fake source samples translated from the target domains.</p><p>Definition 3: (Feature Learner) A feature learner F is defined as a deep neural network which is able to learn domain-invariant feature representations for both the the source and target domains. We denote the feature representation of sample x as f = F (x).</p><p>Definition 4: (Feature Predictor) A feature predictor P is defined as a classifier layer which is trained to predict possible categories of a feature representation, i.e, p = P(f).</p><p>Definition 5: (Feature Translator) A feature translator T is defined as a mapping which is able to translate the features from one domain to the style of the other domain. In this paper, we train two feature translators T s2t and T t 2s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Idea</head><p>Our cycle-consistent conditional adversarial network consists of a feature learner F , a domain discriminator D d , a feature predictor P, two feature translators T s2t , T t 2s along with their sample discriminators D s and D t .</p><p>The main goal of our model is to train an effective F which is able to learn domain-invariant feature representations, so that the classifier trained on the source domain can be applied to the target domain. To this end, we introduce the domain discriminator D d to evaluate the feature quality of F by an adversarial manner. The feature predictor P is further added to condition the domain discriminator D d , so that we can promote the discriminative ability of the learned features and preserve the multimodal structures embedded in the data. However, since the results of P are not always sufficiently accurate, we further argue that truly domain-invariant features should be able to be translated from one domain to the other. Therefore, we train two feature translators T s2t , T t 2s and learn a cycle-loss. The sample discriminators D t and D s are used to evaluate T s2t and T t 2s , respectively, in an adversarial manner.</p><p>For clarity, we show the core ideas of our method in <ref type="figure" target="#fig_2">Fig. 2</ref>. The base network of our 3CATN is the deep feature extractor F . Additionally, three adversarial networks are introduced to guide F . The first one is the domain discriminator D d which conditioned by the outputs P. The other two are {T s2t , D t } and {T t 2s , D s }. Assuming that T t 2s (T s2t (x s )) should be similar (or same in an ideal situation) with x s , we calculate a cycle-loss to balance the loss of D d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Conditional Domain Adversarial Nets</head><p>Since most of the real tasks are multi-class classification rather than binary classification, the features learned from a multi-way classification deep network naturally have multimodal structures <ref type="bibr" target="#b25">[26]</ref>. For instance, a liger shares many characteristics of both a lion and a tiger. Thus, the visual features of a liger would have not only its specific structures but also the structures of a lion and a tiger. Such a phenomenon can be reflected by the classifier predictions, e.g., the classifier indicates the probabilities of categorizing a liger image into liger, lion and tiger are 0.8, 0.12 and 0.08, respectively. In other words, the classifier prediction p carries the possible discriminative information of the multimodal data structures. Inspired by CDAN <ref type="bibr" target="#b25">[26]</ref>, we condition both the feature learner F and the domain discriminator D d with the classifier prediction p. As a result, we have the following minimax game:</p><formula xml:id="formula_0">min F, P max D d L con = −E[ C c=1 1 [y s =c] logσ (F (x s ))] +λ(E[loдD d (δ (h s ))] + E[loд(1 − D d (δ (h t )))]),<label>(1)</label></formula><p>where λ &gt; 0 is a balancing parameter. In this paper, we follow CDAN and fix λ = 1 for fair comparisons. The first term in Eq. (1) is a supervised cross-entropy loss on the source domain, in which 1 <ref type="bibr">[·]</ref> is an indicator, σ is the softmax and C is the possible categories. The second term is a conditional loss which is very similar with conditional GAN <ref type="bibr" target="#b28">[29]</ref>. It is worth noting that h = (f, p) is the joint variable of the domain specific features and corresponding classification predictions. δ is the conditioning strategy defined as follows:</p><formula xml:id="formula_1">δ (h) = δ ⊗ (f, p) if dim f × dim p ≤ 4096 δ ⊙ (f, p) otherwise,<label>(2)</label></formula><p>where dim indicates the dimensionality of a vector. δ ⊗ is a multilinear map and δ ⊙ is an explicit randomized multilinear map. More details of δ ⊗ and δ ⊙ can be found in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Bidirectional Translation</head><p>We can learn a conditional domain adversarial network by optimizing Eq. (1) which is able to capture the multimodal data structures. However, the success of Eq. (1) builds on the assumption that the classification prediction p is sufficiently accurate. The bad news is that such an assumption does not always hold in real-world applications. If the prediction contains significant errors, the errors would be propagated to the feature learner. What is worse, the errors can be magnified with the increase of iterations.</p><p>The ultimate goal of adversarial domain adaptation is to learn domain-invariant features. However, F with flawed conditions is able to fool the domain discriminator but cannot learn truly domaininvariant features. In this paper, we argue that the truly domaininvariant features should be able to be translated from one domain to the other. To this end, we first train a feature translator T s2t to translate the features from the source domain to the target domain. Specifically, we train T s2t in an adversarial fashion. A discriminator D t is simultaneously trained to distinguish real target domain features from the translated source features. As a result, we have the follow loss function:</p><formula xml:id="formula_2">min T s 2t max D t L s2t = E[loдD t (f t )] + E[loд(1 − D t (T s2t (f s )))]. (3)</formula><p>For convenience, letf t = T s2t (f s ). Sincef t is translated from f s , it is expected thatf t should have the same class information with f s . Thus, we further introduce a supervised classification loss onf t to preserve the semantic consistency and rewritten L s2t as:</p><formula xml:id="formula_3">min P,T s 2t max D t L s2t = E[loдD t (f t )] + E[loд(1 − D t (T s2t (f s )))] −βE[ C c=1 1 [ŷ t =c] logσ (f t )].<label>(4)</label></formula><p>Similarly, we can also train a mapping from the the other direction. Specifically, we further train T t 2s in an adversarial fashion. A discriminator D s is simultaneously trained to distinguish real source domain features from the translated target features. As a result, we have the follow loss function:</p><formula xml:id="formula_4">min T t 2s max D s L t 2s = E[loдD s (f s )] + E[loд(1 − D s (T t 2s (f t )))]. (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Cycle-consistent Loss</head><p>Inspired by the recent work on image-to-image translation <ref type="bibr" target="#b44">[45]</ref>, we encourage our model to preserve the original data information. For instance, preserving the information of f s when translating it tof t . Let us explain this consideration by an example. The handwritten digit 1 can be similar to 7 if the upper part of 1 is distorted during the translation. However, if we preserve the original information of 1, such a error would not happen. In our model, therefore, we deploy a cycle-consistent loss to keep the consistency of original features before and after the translation. Formally, we expect that T t 2s (T s2t (f s )) ≈ f s and T s2t (T t 2s (f t )) ≈ f t . As a result, we define the cycle-loss as:</p><formula xml:id="formula_5">min T t 2s ,T t 2s L cyc = E[∥T t 2s (T s2t (f s )) − f s ∥ 2 2 ] +E[∥T s2t (T t 2s (f t )) − f t ∥ 2 2 ],<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Overall Objective Function</head><p>By considering above all discussions on conditional adversarial training, bidirectional translation and cycle-consistency, we have the following loss for our 3CATN model:</p><formula xml:id="formula_6">L 3CAT N = L con + η 1 (L s2t + L t 2s ) + η 2 L cyc ,<label>(7)</label></formula><p>where η 1 , η 2 &gt; 0 are two balancing parameters to control the contribution of each part. Since we claim that truly domain-invariant features should be able to be translated from one domain to the other, we deploy the same weight for L z2t and L t 2s in this paper to show the equality of the bidirectional translations. With the overall loss, we have our overall objective function:</p><formula xml:id="formula_7">min F, P,T t 2s ,T t 2s max D d , D s , D t L 3CAT N .<label>(8)</label></formula><p>Once the model is well-trained, we can deploy the feature learner F to learn domain invariant features for both the source and target domains. Then, the classifier trained on F (X s ) can be directly used to handle F (X t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets Description</head><p>MNIST, USPS and Street View House Numbers (SVHN) <ref type="bibr" target="#b29">[30]</ref>, are three widely used handwritten digits dataset. MNIST consists of 60,000 training samples and 10,000 test samples. USPS is comprised of 7,291 training samples and 2,007 test samples. SVHN contains over 600,000 labeled digits cropped from Street View images.</p><p>Office-31 <ref type="bibr" target="#b34">[35]</ref> is the most popular benchmark in domain adaptation. It consists of three subsets, i.e., Amazon (A), Webcam (W) and DSLR (D). Specifically, the images in amazon is downloaded from amazon.com. Webcam and DSLR contain images captured by a web camera and a digital SLR camera, respectively. In total, there are 4,652 samples from 31 categories in office-31 dataset.</p><p>VisDA <ref type="bibr" target="#b32">[33]</ref> classification dataset is the currently largest dataset in the community. It consists of a training domain, a validation domain and a test domain. In total, the VisDA dataset contains over 280K images from 12 classes. In this paper, we follow the same settings in previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> and use the training set as the source domain and the validation domain as the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Protocols</head><p>Our model mainly consists of two parts. The first part is inspired by CDAN <ref type="bibr" target="#b25">[26]</ref> to take advantage of conditioning with classifier predictions. However, it is easy to figure out that an inaccurate condition will lead to negative results. Thus, the second part of our model is a cycle-consistent translation to prevent the negative results. We argue that truly domain-invariant features should be able to be translated from one domain to the other. Apparently, CDAN can be regarded as a baseline of our method. Furthermore, CDAN is published very recently. The results of CDAN represent state-of-art performance in the community. As a result, we mainly compare our method with CDAN. For fair comparisons, we keep exactly the same experiment protocols with CDAN <ref type="bibr" target="#b25">[26]</ref>. Our codes are available at github 1 .</p><p>In this paper, we deploy the standard unsupervised domain adaptation settings which were widely used in previous work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. Labeled source domain samples {X s , Y s } and unlabeled target domain samples X t are used for training. The reported results are the classification accuracy on target samples: accuracy = |x : x ∈ X t ∧ŷ t = y t |/|x : x ∈ X t |, whereŷ t is the predicted label of the target domain generated by our model, and y t is the ground truth label vector.</p><p>For digits recognition on MNIST, USPS and SVHN, we use the same setting in CDAN. Specifically, 60000, 7291 and 73257 images from MNIST, USPS and SVHN, respectively, are used for training. The basic network structure for digits recognition is similar with LeNet. We set the batch size to 224 and the learning rate as 10 −3 .</p><p>For object recognition on Office-31, we also deploy the same setting in CDAN. The basic network for feature extraction is ResNet-50 <ref type="bibr" target="#b11">[12]</ref> which is pre-trained on ImageNet. The batch size is 32 and the learning rate is 10 −3 . We optimize our model by mini-batch stochastic gradient descent with a weight decay of 5 × 10 −4 and momentum of 0.9. For the experiments on VisDA 2017, we also deploy ResNet-50 as the base architecture. The feature translators are implemented by one fc layer and three conv layers. The discriminator D s and D t are implemented by two conv layers and one fc layer. The domain discriminator D d is implemented by three fc layers. All the hyper-parameters of this paper are tuned by importance weighted cross validation <ref type="bibr" target="#b38">[39]</ref>. The sensitivity of parameters are discussed later in this section.</p><p>Traditional methods with deep features: TCA <ref type="bibr" target="#b30">[31]</ref> and GFK <ref type="bibr" target="#b9">[10]</ref>. Deep CNN methods: DANN <ref type="bibr" target="#b6">[7]</ref>, DDC <ref type="bibr" target="#b42">[43]</ref>, DAN <ref type="bibr" target="#b24">[25]</ref>, DRCN <ref type="bibr" target="#b8">[9]</ref>, JAN <ref type="bibr" target="#b27">[28]</ref> and SimNet <ref type="bibr" target="#b33">[34]</ref>. GAN methods: ADDA <ref type="bibr" target="#b41">[42]</ref>, JAN-A <ref type="bibr" target="#b27">[28]</ref>, RevGrad <ref type="bibr" target="#b7">[8]</ref>, CoGAN <ref type="bibr" target="#b23">[24]</ref>, MCD <ref type="bibr" target="#b35">[36]</ref> and CDAN <ref type="bibr" target="#b25">[26]</ref> are used for comparison. The results of some compared methods are cited from JAN and CDAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Digits Recognition</head><p>The results of hand-written digits recognition are reported in Table 1. We also report the results on source only as a baseline. The source only means the model is trained on only the source data and then directly applied on the target data. The other compared methods are mainly based on GANs. From the results we can see that our method is able to outperform previous state-of-the-arts. Specifically, we achieved 0.5%, 0.3% and 3.3% accuracy improvement on MNIST→USPS, USPS→MNIST and SVHN →MNIST, respectively. It is worth noting that the improvement is very hard to achieve since previous state-of-the-arts are very close to the results of target full supervised setting. Nevertheless, we achieve a significant improvement on the hardest task SVHN →MNIST.</p><p>CDAN performs very well when the classifier predictions are sufficiently accurate. The results on the hand-written digits are already sufficiently accurate for most recent works. Thus, our advantages are not quite clear on these evaluations. We perform comparably with the target supervised model. As we claimed before, our model would significantly improve CDAN when the classifier perditions are inaccurate. This is somewhat proved by the results on SVHN →MNIST. It will be further verified in following evaluations. In conclusion, our method is able to take full advantage of classifier predictions condition when they are accurate. At the same time, we can also avoid the negative effects when the classifier conditions are inaccurate. Our approach achieves a dynamic balance by maximizing the advantages and minimizing the shortages.</p><p>It is also worth noting that the entropy conditioning in CDAN is designed to handle the issue of inaccurate classifier predictions. However, sample entropy does not necessarily indicate the classification accuracy. A strong evidence is that CDAN+E (CDAN with entropy conditioning) does not show significant advantage against vanilla CDAN. For instance, the average accuracies of CDAN+E and CDAN on Office-31 are 87.7% and 86.6%, respectively. The results of CDAN reported in this paper are all from CDAN+E since it is generally better than CDAN. We did not specific CDAN+E from CDAN for the sake of simplicity and clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Objects Recognition on Office-31</head><p>The results of different methods on Office-31 are reported in <ref type="table" target="#tab_1">Table 2</ref>. It is worth noting that we report two average results, e.g., Average 1 and Average 2. Average 1 is the average over all six evaluations. Average 2 is the average over four evaluations which excludes W→D and D→W since the two have very high accuracy for almost every method. As a result, Average 2 can reflect the performance on relatively harder tasks. From the results, we can have the following observations:</p><p>1) Transfer learning methods perform much better than the baseline ResNet-50 which is trained on only the source domain data. It proves that domain adaptation is able to minimize the domain shift and it is practical for real-world applications.</p><p>2) End-to-end deep methods outperform two-step methods which split feature extraction and knowledge adaptation. By simultaneously optimizing the two parts, the feature learner is able to receive the feedback from latter layers. The feedback can benefit the feature learner to learn more domain-invariant features.</p><p>3) Compared with previous end-to-end deep models, adversarial domain adaptation achieves state-of-the-art performance recently. Comparing JAN with CDAN, although both of them use the same base architecture, it is obvious that a conditional domain discriminator can significantly improve the domain adaptation performance. 4) Comparing average 1 and average 2, we can see that most methods have a nearly overall average accuracy since the results on W→D and D→W are very high, the two soft the overall average on tough tasks. However, the baseline ResNet also performs very well on the two evaluations. Thus, average 2 is better to reflect the performance on hard tasks of Office-31. Regarding average 2, we can see that CDAN really improves state-of-the art, which verifies that leveraging the condition with classifier prediction is quite effective. Our method is also benefited from the classifier predictions. At the same time, we can also observe that CDAN performs remarkably on A→D and A→W where the classifier predictions are relatively high (over 80% for JAN). However, for the cases D→A and W→A where the classifier predictions are relatively poor (below 70% for JAN), CDAN has no advantage against previous state-of-the-art approaches. It is even worse than GTA on both D→A and W→A. This comparison exactly verifies that the condition with classifier predictions is not reliable when the predictions are inaccurate. 5) Although we deployed the same condition with CDAN, our model is able to pervert the negative effects caused by inaccurate classifier conditions. We argue that truly domain-invariant features should be able to translate from one domain the other. As a result, we introduce two feature translation losses and one cycle-consistent loss. The results verify that our model can perform well with good classifier conditions. At the same time, our model is also robust with bad classifier conditions. This verifies our claim that our approach is able to not only capture the complex multimodal structures but also avoid the negative effects caused by uncertain conditions. 6) In terms of numbers, we achieved 1.2% accuracy improvement w.r.t. average 1 and 1.7% accuracy improvement w.r.t. average 2. It is worth noting that we outperform state-of-the-art CDAN 2.1% and 2.2% on the two hardest tasks D→A and W→A, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Large-scale Test</head><p>We further evaluate our model on large-scale domain adaptation datasets VisDA 2017. It is worth noting that VisDA is more challenging than Office-31. Compared with Office-31, VisDA has much more images. It is also worth noting that VisDA challenge has classification and segmentation tasks. Limited by space, in this paper we only focus on the classification task. The classification results on VisDA are reported in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>The results on VisDA also verify that our method can outperform previous state-of-the-art with significant advantages. We achieve 3.2% accuracy improvement over the 12 classes in VisDA. From the results on VisDA, we can see that our method is able to perform well on large-scale datasets. It is able to maximize the advantages of accurate predictions and, at the same time, prevent the negative effects caused by inaccurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Evaluation</head><p>For a better understanding, we conduct a qualitative evaluation on W→A. We show some images which are misclassified by CDAN but well-handled by our method in <ref type="figure">Fig. 4</ref>. On the evaluation of W→A, there are 163 out of 2849 samples are misclassified by CDAN but well-handled by our 3CATN. At the same time, almost all the correctly classified samples by CDAN are also correctly classified by our 3CATN. There are only 15 exceptions. For the sake of fairness, we also show some of them in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Model Analysis</head><p>Parameters. The parameters in our model are tuned by importance weighted cross validation <ref type="bibr" target="#b38">[39]</ref>. In most evaluations, we fix λ = 1 as in CDAN. Since β plays the similar role with λ, we also set β = 1. The weight for feature translators η 1 is set as 0.01 and the weight for cycle-loss η 2 is set as 0.1. To fully evaluate our model, we report the parameter sensitivities of β, η 1 and η 2 in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, <ref type="figure" target="#fig_3">Fig. 3(b)</ref> and <ref type="figure" target="#fig_3">Fig. 3(c)</ref>, respectively. It can be seen that β is not sensitive. Our model performs stably with different values of β. η 1 and η 2 are suggested to be set smaller than 1. Stability. Adversarial training is famous for hard of training. Therefore, we report the training stability of our model in <ref type="figure" target="#fig_3">Fig. 3(d)</ref>. It can be seen that 3CATN performs stably with the increase of iterations and it usually can achieve a stable result within 10, 000 iterations. Ablation. To show the effect of each component in our model, we report the results of ablation study in <ref type="table" target="#tab_3">Table 4</ref>. The basic network of our model is the ResNet which learns new feature representations. The result of ResNet is reported as S0. Then, we leverage the conditional adversarial training to capture the multimodal data embedded in the data. The result is reported as S1. Furthermore, two feature translators are introduced to learn truly domain-invariant features and balance the inaccurate conditioning issue, which is reported as S2. At last, a cycle-consistent loss is calculated to preserve the consistency before and after feature translation. The result is reported as S3. The results in <ref type="table" target="#tab_3">Table 4</ref> verify that each part in our model plays an important role for the final results.</p><p>It is worth noting that S1 is based on CDAN, S2 and S3 are our contributions. Combining the results in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure">Fig. 4</ref>, we can find that CDAN is risky when the classifier predictions are not sufficiently accurate, e.g., the classifier may be confused with a phone and a calculator and putting a condition of calculator on the category phone leads to the misclassification of phone images. Our method, however, is able to alleviate the negative effects caused by inaccurate conditions. In the introduction, we argued that truly domain-invariant features should be able to be translated from one domain to the other. To this end, we introduce two feature translators and one cycle-consistent loss. The results of ablation study and qualitative evaluation verify that both translators and   <ref type="figure">Figure 4</ref>: Qualitative results on W→A. Blue label is the ground-truth and gray label is the prediction. This figure mainly shows some randomly selected samples which are wrongly predicted by CDAN but correctly classified by our 3CATN. Although almost all of the samples which are correctly predicted by CDAN are also correctly handled by our method, we show several exceptions within the dash lines for the sake of fairness. cycle-loss definitely benefit the model. For instance, we can see that feature translators and cycle-loss boost the overall performance on D→A with 0.9% and 1.2%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a novel adversarial domain adaptation method 3CATN. 3CATN takes advantage of conditional adversarial training. In order to address the inaccurate conditioning issue, we argue that truly domain-invariant features should be able to be translated from one domain to the other. As a result, two feature translators and one corresponding cycle-consistent loss are introduced into conditional adversarial domain adaptation networks. Extensive experiments verify that our method is able to outperform previous state-of-the-art methods with significant advantages. In our model, we address the inaccurate conditioning issue with external forces, e.g., balancing the weight between classifier condition and feature translation. In our further work, we will study handling the problem from internal side, i.e., explicitly evaluating the accuracy of classifier predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Adversarial domain adaptation networks. 1) The source domain D s and the target domain D t have different data distributions. 2) A domain discriminator is trained to distinguish source domain features from target domain features. 3) The feature representation network tries to fool the domain discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Idea illustration of our 3CATN. 1) A deep CNN, e.g., ResNet, is trained as a feature learner to learn domain-invariant feature representations. 2) A domain discriminator D d is trained to distinguish source domain features from target domain features. 3) Two feature translatorT s2t andT t 2s along with their corresponding discriminators D t and D s are trained to translate features from one domain to the other. 4) By leveraging the two feature translators, we calculate a cycle-loss to preserve the translation consistency. 5) The classifier predictions p s and p t are deployed to condition the adversarial domain adaptation networks. 6) All losses are backpropagated to the feature representation network to learn truly domain-invariant features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Parameter sensitivity (a-c) and training stability (d). The parameters in our model are tuned by importance weighted cross validation<ref type="bibr" target="#b38">[39]</ref>. W→A and A→W on Office-31 dataset are evaluated as examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results (%) of digits recognition. The best results are highlighted by bold numbers. The compared methods have the same experimental settings except for UNIT on SVHN →MNIST which uses a larger training set. The results of the baselines are cited from corresponding papers.</figDesc><table><row><cell>Method</cell><cell cols="3">MNIST→USPS USPS→MNIST SVHN →MNIST</cell></row><row><cell>Source only</cell><cell>82.2 ± 0.8</cell><cell>69.6 ± 3.8</cell><cell>67.1 ± 0.6</cell></row><row><cell>DANN [7]</cell><cell>-</cell><cell>-</cell><cell>73.6</cell></row><row><cell>ADDA [42]</cell><cell>89.4 ± 0.2</cell><cell>90.1 ± 0.8</cell><cell>76.0 ± 1.8</cell></row><row><cell>DRCN [9]</cell><cell>91.8 ± 0.1</cell><cell>73.7 ± 0.1</cell><cell>82.0 ± 0.1</cell></row><row><cell>RevGrad [8]</cell><cell>89.1 ± 0.2</cell><cell>89.9 ± 0.3</cell><cell>−</cell></row><row><cell>CoGAN [24]</cell><cell>91.2</cell><cell>89.1</cell><cell>−</cell></row><row><cell>UNIT [23]</cell><cell>95.9</cell><cell>93.6</cell><cell>90.5</cell></row><row><cell>CDAN [26]</cell><cell>95.6</cell><cell>98.0</cell><cell>89.2</cell></row><row><cell>3CATN [Ours]</cell><cell>96.1 ± 0.2</cell><cell>98.3 ± 0.2</cell><cell>92.5 ± 0.3</cell></row><row><cell>Target Supervised</cell><cell>96.3</cell><cell>99.2</cell><cell>99.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Domain adaptation results (accuracy %) on Office-31. Where average 1 is the overall average and average 2 is the average over 4 challenging evaluations except for W→D and D→W. The best results are highlighted by bold numbers.</figDesc><table><row><cell>Method</cell><cell>A→D</cell><cell>A→W</cell><cell>D→A</cell><cell>D→W</cell><cell>W→A</cell><cell>W→D</cell><cell>Average 1</cell><cell>Average 2</cell></row><row><cell>ResNet [12]</cell><cell>68.9 ± 0.2</cell><cell>68.4 ± 0.2</cell><cell>62.5 ± 0.3</cell><cell>96.7 ± 0.1</cell><cell>60.7 ± 0.3</cell><cell>99.3 ± 0.1</cell><cell>76.1</cell><cell>65.1</cell></row><row><cell>TCA [31]</cell><cell>74.1 ± 0.0</cell><cell>72.7 ± 0.0</cell><cell>61.7 ± 0.0</cell><cell>96.7 ± 0.0</cell><cell>60.9 ± 0.0</cell><cell>99.6 ± 0.0</cell><cell>77.6</cell><cell>67.4</cell></row><row><cell>GFK [10]</cell><cell>74.5 ± 0.0</cell><cell>72.8 ± 0.0</cell><cell>63.4 ± 0.0</cell><cell>95.0 ± 0.0</cell><cell>61.0 ± 0.0</cell><cell>98.2 ± 0.0</cell><cell>77.5</cell><cell>67.9</cell></row><row><cell>DDC [43]</cell><cell>76.5 ± 0.3</cell><cell>75.6 ± 0.2</cell><cell>62.2 ± 0.4</cell><cell>96.0 ± 0.2</cell><cell>61.5 ± 0.5</cell><cell>98.2 ± 0.1</cell><cell>78.3</cell><cell>69.0</cell></row><row><cell>DAN [25]</cell><cell>78.6 ± 0.2</cell><cell>80.5 ± 0.4</cell><cell>63.6 ± 0.3</cell><cell>97.1 ± 0.2</cell><cell>62.8 ± 0.2</cell><cell>99.6 ± 0.1</cell><cell>80.4</cell><cell>71.4</cell></row><row><cell>RevGrad [8]</cell><cell>79.7 ± 0.4</cell><cell>82.0 ± 0.4</cell><cell>68.2 ± 0.4</cell><cell>96.9 ± 0.2</cell><cell>67.4 ± 0.5</cell><cell>99.1 ± 0.1</cell><cell>82.2</cell><cell>74.3</cell></row><row><cell>MCD [36]</cell><cell>74.5 ± 0.6</cell><cell>68.3 ± 0.2</cell><cell>49.9 ± 0.5</cell><cell>90.7 ± 0.8</cell><cell>43.5 ± 0.5</cell><cell>98.3 ± 0.5</cell><cell>70.9</cell><cell>59.1</cell></row><row><cell>JAN [28]</cell><cell>84.7 ± 0.3</cell><cell>85.4 ± 0.3</cell><cell>68.6 ± 0.3</cell><cell>97.4 ± 0.2</cell><cell>70.0 ± 0.4</cell><cell>99.8 ± 0.2</cell><cell>84.3</cell><cell>77.2</cell></row><row><cell>JAN-A [28]</cell><cell>85.1 ± 0.4</cell><cell>86.0 ± 0.4</cell><cell>69.2 ± 0.4</cell><cell>96.7 ± 0.3</cell><cell>70.7 ± 0.5</cell><cell>99.7 ± 0.1</cell><cell>84.6</cell><cell>77.8</cell></row><row><cell>GTA [37]</cell><cell>87.7 ± 0.5</cell><cell>89.5 ± 0.5</cell><cell>72.8 ± 0.3</cell><cell>97.9 ± 0.3</cell><cell>71.4 ± 0.4</cell><cell>99.8 ± 0.4</cell><cell>86.5</cell><cell>80.3</cell></row><row><cell>CDAN [26]</cell><cell>92.9 ± 0.2</cell><cell>94.1 ± 0.1</cell><cell>71.0 ± 0.3</cell><cell>98.6 ± 0.1</cell><cell>69.3 ± 0.3</cell><cell>100 ± 0.0</cell><cell>87.7</cell><cell>81.8</cell></row><row><cell>3CATN [Ours]</cell><cell>94.1 ± 0.3</cell><cell>95.3 ± 0.2</cell><cell>73.1 ± 0.2</cell><cell>99.3 ± 0.5</cell><cell>71.5 ± 0.6</cell><cell>100 ± 0.0</cell><cell>88.9</cell><cell>83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Domain adaptation results (accuracy %) on VisDA-2017 dataset. All of the base networks are ResNet-50. Method ResNet[12] DAN [25] RTN [27] RevGrad [8] JAN [28] SimNet [34] CDAN [26] 3CATN [Ours]</figDesc><table><row><cell>Result</cell><cell>49.5</cell><cell>53.0</cell><cell>53.6</cell><cell>55.0</cell><cell>61.6</cell><cell>69.6</cell><cell>70.0</cell><cell>73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The results of ablation study on D→A and W→A.</figDesc><table><row><cell>Settings</cell><cell cols="2">D→A W→A</cell></row><row><cell>S0: ResNet</cell><cell>62.5</cell><cell>60.7</cell></row><row><cell>S1: S0+ conditional transfer loss</cell><cell>71.0</cell><cell>69.3</cell></row><row><cell>S2: S1 + feature translation loss</cell><cell>71.9</cell><cell>70.4</cell></row><row><cell>S3: S2 + cycle-consistent loss</cell><cell>73.1</cell><cell>71.5</cell></row><row><cell>S4: S3 -conditional transfer loss</cell><cell>67.5</cell><cell>66.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/lijin118/3CATN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Landmarks-based kernelized subspace alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. JMLR. org</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised Deep Domain Adaptation via Coupled Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="5214" to="5224" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Low-Rank Transfer Subspace Learning for Missing Modality Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1192" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<title level="m">Unsupervised domain adaptation by backpropagation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-Consistent Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning cross-domain landmarks for heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ren</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5081" to="5090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging the Invariant Side of Generative Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7402" to="7411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locality Preserving Joint Transfer for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From Zero-Shot Learning to Cold-Start Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two birds one stone: on both cold-start and long-tail recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM MM. ACM</title>
		<imprint>
			<biblScope unit="page" from="898" to="906" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer Independently Together: A Generalized Framework for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint Feature Selection and Structure Preservation for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jidong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1697" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning with Joint Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1710.06924</idno>
		<title level="m">VisDA: The Visual Domain Adaptation Challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Element</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings of conditional distributions with applications to dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Mãžller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04394</idno>
		<title level="m">Zero-Shot Learning-The Good, the Bad and the Ugly</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
