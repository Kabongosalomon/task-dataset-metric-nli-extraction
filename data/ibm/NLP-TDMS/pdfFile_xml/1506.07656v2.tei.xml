<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepMatching: Hierarchical Deformable Dense Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepMatching: Hierarchical Deformable Dense Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Noname manuscript No. (will be inserted by the editor) the date of receipt and acceptance should be inserted later</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Non-rigid dense matching, optical flow</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images.</p><p>We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk <ref type="figure">(Mikolajczyk et al 2005)</ref>, the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets. DeepMatching outperforms the stateof-the-art algorithms and shows excellent results in particular for repetitive textures.</p><p>We also propose a method for estimating optical flow, called DeepFlow, by integrating DeepMatching in the large displacement optical flow (LDOF) approach of <ref type="bibr" target="#b6">Brox and Malik (2011)</ref>. Compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. DeepFlow obtains competitive performance on public benchmarks for optical flow estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b14">(Forsyth and Ponce 2011;</ref><ref type="bibr" target="#b42">Szeliski 2010)</ref><p>. The goal of a matching algorithm is to discover shared visual content between two images, and to establish as many as possible precise point-wise correspondences, called matches. An essential aspect of matching approaches is the amount of rigidity they assume when computing the correspondences. In fact, matching approaches range between two extreme cases: stereo matching, where matching hinges upon strong geometric constraints, and matching "in the wild", where the set of possible transformations from the source image to the target one is large and the problem is basically almost unconstrained. Effective approaches have been designed for matching rigid objects across images in the presence of large viewpoint changes <ref type="bibr" target="#b29">(Lowe 2004;</ref><ref type="bibr" target="#b2">Barnes et al 2010;</ref><ref type="bibr" target="#b17">HaCohen et al 2011)</ref>. However, the performance of current state-of-the-art matching algorithms for images "in the wild", such as consecutive images in real-world videos featuring fast non-rigid motion, still calls for improvement <ref type="bibr" target="#b53">(Xu et al 2012;</ref><ref type="bibr" target="#b10">Chen et al 2013)</ref>. In this paper, we aim at tackling matching in such a general setting.</p><p>Matching algorithms for images "in the wild" need to accommodate several requirements, that turn out to be often in contradiction. On one hand, matching objects necessarily requires rigidity assumptions to some extent. It is also mandatory that these objects have sufficiently discriminative textures to make the problem well-defined. On the other hand, many objects or regions are not rigid objects, like humans or animals. Furthermore, large portions of an image are usually occupied by weakly-to-no textured regions, often with repetitive textures, like sky or bucolic background.</p><p>Descriptor matching approaches, such as SIFT <ref type="bibr" target="#b29">(Lowe 2004)</ref> or HOG <ref type="bibr" target="#b11">(Dalal and Triggs 2005;</ref><ref type="bibr" target="#b6">Brox and Malik 2011)</ref> matching, compute discriminative feature representations from rectangular patches. However, while these approaches succeed in case of rigid motion, they fail to match regions with weak or repetitive textures, as local patches are poorly discriminative. Furthermore, matches are usually poor and imprecise in case of nonrigid deformations, as these approaches rely on rigid patches. Discriminative power can be traded against increased robustness to non-rigid deformations. Indeed, propagation-based approaches, such as Generalized Patch-Match <ref type="bibr" target="#b2">(Barnes et al 2010)</ref> or Non-rigid Dense Correspondences <ref type="bibr" target="#b17">(HaCohen et al 2011)</ref>, compute simple feature representations from small patches and propagate matches to neighboring patches. They yield good performance in case of non-rigid deformations. However, matching repetitive textures remains beyond the reach of these approaches.</p><p>In this paper we propose a novel approach, called DeepMatching, that gracefully combines the strengths of these two families of approaches. DeepMatching is computed using a multi-layer architecture, which breaks down patches into a hierarchy of sub-patches. This architecture allows to work at several scales and handle repetitive textures. Furthermore, within each layer, local matches are computed assuming a restricted set of feasible rigid deformations. Local matches are then propagated up the hierarchy, which progressively discard spurious incorrect matches. We called our approach DeepMatching, as it is inspired by deep convolutional approaches. In summary, we make three contributions:</p><p>• Dense matching: we propose a matching algorithm, DeepMatching, that allows to robustly determine dense correspondences between two images. It explicitly handles non-rigid deformations, with bounds on the deformation tolerance, and incorporates a multiscale scoring of the matches, making it robust to repetitive or weak textures. Furthermore, our approach is based on gradient histograms, and thus robust to appearance changes caused by illumination and color variations.</p><p>• Fast, scale/rotation-invariant matching: we propose a computationally efficient version of Deep-Matching, which performs almost as well as exact Deep-Matching, but at a much lower memory cost. Furthermore, this fast version of DeepMatching can be extended to a scale and rotation-invariant version, making it an excellent competitor to state-of-the-art descriptor matching approaches.</p><p>• Large-displacement optical flow: we propose an optical flow approach which uses DeepMatching in the matching term of the large displacement variational energy minimization of <ref type="bibr" target="#b6">Brox and Malik (2011)</ref>. We show that DeepMatching is a better choice compared to the HOG descriptor used by <ref type="bibr" target="#b6">Brox and Malik (2011)</ref> and other state-of-the-art matching algorithms. The approach, named DeepFlow, obtains competitive results on public optical flow benchmarks. This paper is organized as follows. After a review of previous works (Section 2), we start by presenting the proposed matching algorithm, DeepMatching, in Section 3. Then, Section 4 describes several extensions of DeepMatching. In particular, we propose an optical flow approach, DeepFlow, in Section 4.3. Finally, we present experimental results in Section 5.</p><p>A preliminary version of this article has appeared in <ref type="bibr" target="#b50">Weinzaepfel et al (2013)</ref>. This version adds (1) an indepth presentation of DeepMatching; (2) an enhanced version of DeepMatching, which improves the match scoring and the selection of entry points for backtracking; (3) proofs on time and memory complexity of Deep-Matching as well as its deformation tolerance; (4) a discussion on the connection between Deep Convolutional Neural Networks and DeepMatching; (5) a fast approximate version of DeepMatching; (6) a scale and rotation invariant version of DeepMatching; and <ref type="formula" target="#formula_7">(7)</ref> an extensive experimental evaluation of DeepMatching on several state-of-the-art benchmarks. The code for Deep-Matching as well as DeepFlow are available at http:// lear.inrialpes.fr/src/deepmatching/ and http:// lear.inrialpes.fr/src/deepflow/. Note that we provide a GPU implementation in addition to the CPU one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section we review related work on "general" image matching, that is matching without prior knowledge and constraints, and on matching in the context of optical flow estimation, that is matching consecutive images in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General image matching</head><p>Image matching based on local features has been extensively studied in the past decade. It has been applied successfully to various domains, such as wide baseline stereo matching <ref type="bibr" target="#b15">(Furukawa et al 2010)</ref> and image retrieval <ref type="bibr" target="#b35">(Philbin et al 2010)</ref>. It consists of two steps, i.e., extracting local descriptors and matching them. Image descriptors are extracted in rigid (generally square) local frames at sparse invariant image locations <ref type="bibr" target="#b32">(Mikolajczyk et al 2005;</ref><ref type="bibr" target="#b42">Szeliski 2010</ref>). Matching then equals nearest neighbor search between descriptors, followed by an optional geometric verification. Note that a confidence value can be obtained by computing the uniqueness of a match, i.e., by looking at the distance of its nearest neighbors <ref type="bibr" target="#b29">(Lowe 2004;</ref><ref type="bibr" target="#b6">Brox and Malik 2011)</ref>. While this class of techniques is well suited for welltextured rigid objects, it fails to match non-rigid objects and weakly textured regions.</p><p>In contrast, the proposed matching algorithm, called DeepMatching, is inspired by non-rigid 2D warping and deep convolutional networks <ref type="bibr" target="#b25">(LeCun et al 1998a;</ref><ref type="bibr" target="#b46">Uchida and Sakoe 1998;</ref><ref type="bibr" target="#b22">Keysers et al 2007)</ref>. This family of approaches explicitly models non-rigid deformations. We employ a novel family of feasible warpings that does not enforce monotonicity nor continuity constraints, in contrast to traditional 2D warping <ref type="bibr" target="#b46">(Uchida and Sakoe 1998;</ref><ref type="bibr" target="#b22">Keysers et al 2007)</ref>. This makes the problem computationally much less expensive.</p><p>It is also worthwhile to mention the similarity with non-rigid matching approaches developed for a broad range of applications. <ref type="bibr" target="#b13">Ecker and Ullman (2009)</ref> proposed a similar pipeline to ours (albeit more complex) to measure the similarity of small images. However, their method lacks a way of merging correspondences belonging to objects with contradictory motions, e.g., on different focal planes. For the purpose of establishing dense correspondences between images, <ref type="bibr" target="#b52">Wills et al (2006)</ref> estimated a non-rigid matching by robustly fitting smooth parametric models (homography and splines) to local descriptor matches. In contrast, our approach is non-parametric and model-free.</p><p>Recently, fast algorithms for dense patch matching have taken advantage of the redundancy between overlapping patches <ref type="bibr" target="#b2">(Barnes et al 2010;</ref><ref type="bibr" target="#b24">Korman and Avidan 2011;</ref><ref type="bibr" target="#b41">Sun 2012;</ref><ref type="bibr" target="#b54">Yang et al 2014)</ref>. The insight is to propagate good matches to their neighborhood in a loose fashion, yielding dense non-rigid matches. In practice, however, the lack of a smoothness constraint leads to highly discontinuous matches. Several works have proposed ways to fix this. <ref type="bibr" target="#b17">HaCohen et al (2011)</ref> reinforce neighboring matches using an iterative multiscale expansion and contraction strategy, performed in a coarse-to-fine manner. <ref type="bibr" target="#b54">Yang et al (2014)</ref> include a guided filtering stage on top of PatchMatch, which obtains smooth correspondence fields by locally approximating a MRF. Finally, <ref type="bibr" target="#b23">Kim et al (2013)</ref> propose a hierarchical matching to obtain dense correspondences, using a coarse-to-fine (top-down) strategy. Loopy belief propagation is used to perform inference.</p><p>In contrast to these approaches, DeepMatching proceeds bottom-up and, then, top-down. Due to its hierarchical nature, DeepMatching is able to consider patches at several scales, thus overcoming the lack of distinctiveness that affects small patches. Yet, the multi-layer construction allows to efficiently perform matching allowing semi-rigid local deformations. In addition, Deep-Matching can be computed efficiently, and can be fur-ther accelerated to satisfy low-memory requirements with negligible loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Matching for flow estimation</head><p>Variational energy minimization is currently the most popular framework for optical flow estimation. Since the pioneering work of <ref type="bibr" target="#b19">Horn and Schunck (1981)</ref>, research has focused on alleviating the drawbacks of this approach. A series of improvements were proposed over the years <ref type="bibr" target="#b4">(Black and Anandan 1996;</ref><ref type="bibr" target="#b51">Werlberger et al 2009;</ref><ref type="bibr" target="#b8">Bruhn et al 2005;</ref><ref type="bibr" target="#b34">Papenberg et al 2006;</ref><ref type="bibr" target="#b1">Baker et al 2011;</ref><ref type="bibr" target="#b40">Sun et al 2014b;</ref><ref type="bibr" target="#b47">Vogel et al 2013a)</ref>. The variational approach of <ref type="bibr" target="#b7">Brox et al (2004)</ref> combines most of these improvements in a unified framework. The energy decomposes into several terms, resp. the data-fitting and the smoothness terms. Energy minimization is performed by solving the Euler-Lagrange equations, reducing the problem to solving a sequence of large and structured linear systems.</p><p>More recently, the addition of a descriptor matching term in the energy to be minimized was proposed by <ref type="bibr" target="#b6">Brox and Malik (2011)</ref>. Following this idea, several papers <ref type="bibr" target="#b44">(Tola et al 2008;</ref><ref type="bibr" target="#b6">Brox and Malik 2011;</ref><ref type="bibr" target="#b28">Liu et al 2011;</ref><ref type="bibr" target="#b18">Hassner et al 2012)</ref> show that dense descriptor matching improves performance. Strategies such as reciprocal nearest-neighbor verification <ref type="bibr" target="#b6">(Brox and Malik 2011)</ref> allow to prune most of the false matches. However, a variational energy minimization approach that includes such a descriptor matching term may fail at locations where matches are missing or wrong.</p><p>Related approaches tackle the problem of dense scene correspondence. SIFT-flow <ref type="bibr" target="#b28">(Liu et al 2011)</ref>, one of the most famous method in this context, also formulates the matching problem in a variational framework. <ref type="bibr" target="#b18">Hassner et al (2012)</ref> improve over SIFT-flow by using multiscale patches. However, this decreases performance in cases where scale invariance is not required. <ref type="bibr" target="#b53">Xu et al (2012)</ref> integrate matching of SIFT <ref type="bibr" target="#b29">(Lowe 2004)</ref> and PatchMatch <ref type="bibr" target="#b2">(Barnes et al 2010)</ref> to refine the flow initialization at each level. Excellent results are obtained for optical flow estimation, yet at the cost of expensive fusion steps. <ref type="bibr" target="#b27">Leordeanu et al (2013)</ref> extends sparse matches with locally affine constraints to dense matches and, then, uses a total variation algorithm to refine the flow estimation. We present here a computationally efficient and competitive approach for large displacement optical flow by integrating the proposed DeepMatching algorithm into the approach of <ref type="bibr" target="#b6">Brox and Malik (2011)</ref>. SIFT descriptor in the first image. Middle: second image with optimal standard SIFT matching (rigid). Right: second image with optimal moving quadrant SIFT matching. In this example, the patch covers various objects moving in different directions: for instance the car moves to the right while the cloud to the left. Rigid matching fails to capture this, whereas the moving quadrant approach is able to follow each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeepMatching</head><p>This section introduces our matching algorithm Deep-Matching. DeepMatching is a matching algorithm based on correlations at the patch-level, that proceeds in a multi-layer fashion. The multi-layer architecture relies on a quadtree-like patch subdivision scheme, with an extra degree of freedom to locally re-optimize the positions of each quadrant. In order to enhance the contrast of the spatial correlation maps output by the local correlations, a nonlinear transformation is applied after each layer.</p><p>We first give an overview of DeepMatching in Section 3.1 and show that it can be decomposed in a bottomup pass followed by a top-down pass. We, then, present the bottom-up pass in Section 3.2 and the top-down one in Section 3.3. Finally, we analyze DeepMatching in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the approach</head><p>A state-of-the-art approach for matching regions between two images is based on the SIFT descriptor <ref type="bibr" target="#b29">(Lowe 2004)</ref>. SIFT is a histogram of gradients with 4 × 4 spatial and 8 orientation bins, yielding a robust descriptor R ∈ R 4×4×8 that effectively encodes a square image region. Note that its 4×4 cell grid can also be viewed as 4 so-called "quadrants" of 2×2 cells, see <ref type="figure" target="#fig_0">Figure 1</ref>. We can,</p><formula xml:id="formula_0">then, rewrite R = [R 0 , R 1 , R 2 , R 3 ] with R i ∈ R 2×2×8 .</formula><p>Let R and R be the SIFT descriptors of the corresponding regions in the source and target image. In order to remove the effect of non-rigid motion, we propose to optimize the positions p i ∈ R 2 of the 4 quadrants of the target descriptor R (rather than keeping them fixed), in order to maximize where R i (p i ) is the descriptor of a single quadrant extracted at position p i and sim() a similarity function. Now, sim(R, R ) is able to handle situations such as the one presented in <ref type="figure" target="#fig_0">Figure 1</ref>, where a region contains multiple objects moving in different directions. Furthermore, if the four quadrants can move independently (of course, within some extent), it can be calculated more efficiently as:</p><formula xml:id="formula_1">sim(R, R ) = max {pi} 1 4 3 i=0 sim (R i , R i (p i )) ,<label>(1)</label></formula><formula xml:id="formula_2">sim(R, R ) = 1 4 3 i=0 max pi sim (R i , R i (p i )) ,<label>(2)</label></formula><p>When applied recursively to each quadrant by subdivided it into 4 sub-quadrants until a minimum patch size is reached (atomic patches), this strategy allows for accurate non-rigid matching. Such a recursive decomposition can be represented as a quad-tree, see Nevertheless, in order to first determine the set of matching regions between the two images, we need to compute beforehand the matching scores (i.e. similarity) of all large-enough patches in the two images (as in <ref type="figure" target="#fig_0">Figure 1</ref>), and keep the pairs with maximum similarity. As indicated by Eq. (2), the score is formed by averaging the max-pooled scores of the quadrants. Hence, the process of computing the matching scores is bottom-up. In the following, we call correlation map the matching scores of a single patch from the first image at every position in the second image. Selecting matching patches then corresponds to finding local maxima in the correlation maps.</p><p>To sum-up, the algorithm can be decomposed in two steps: (i) first, correlation maps are computed using a bottom-up algorithm, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Correlation maps of small patches are first computed and then aggregated to form correlation maps of larger patches; (ii) next, a top-down method estimates the motion of atomic patches starting from matches of large patches.</p><p>In the remainder of this section, we detail the two steps described above (Section 3.2 and Section 3.3), before analyzing the properties of DeepMatching in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bottom-up correlation pyramid computation</head><p>Let I and I be two images of resolution W × H and W × H .</p><p>Bottom level. We use patches of size 4 × 4 pixels as atomic patches. We split I into non-overlapping atomic patches, and compute the correlation map with image I for each of them, see <ref type="figure">Figure 3</ref>. The score between two atomic patches R and R is defined as the average pixel-wise similarity:</p><formula xml:id="formula_3">sim(R, R ) = 1 16 3 i=0 3 j=0 R i,j R i,j ,<label>(3)</label></formula><p>where each pixel R i,j is represented as a histogram of oriented gradients pooled over a local neighborhood. We detail below how the pixel descriptor is computed.</p><p>Pixel descriptor R i,j : We rely on a robust pixel representation that is similar in spirit to SIFT and DAISY <ref type="bibr" target="#b29">(Lowe 2004;</ref><ref type="bibr" target="#b45">Tola et al 2010)</ref>. Given an input image I, we first apply a Gaussian smoothing of radius ν 1 in order to denoise I from potential artifacts caused for example by JPEG compression. We then extract the gradient (δx, δy) at each pixel and compute its nonnegative projection onto 8 orientations (cos i π 4 , sin i π 4 ) i=1...8 . At this point, we obtain 8 oriented gradient maps. We smooth each map with a Gaussian filter of radius ν 2 . Next we cap strong gradients using a sigmoid x → 2/(1 + exp(−ςx)) − 1, to help canceling out effects of varying illumination. We smooth gradients one more time for each orientation with a Gaussian filter of radius ν 3 . Finally, the descriptor for each pixel is obtained by the 2 -normalized concatenation of 8 oriented gradients and a ninth small constant value µ. Appending µ amounts to adding a regularizer that will reduce the importance of small gradients (i.e. noise) and ensures that two pixels lying in areas without gradient information will still correlate positively. Pixel descriptors R i,j are compared using dot-product and the similarity function takes value in the interval [0, 1]. In Section 5.2.1, we evaluate the impact of the parameters of this pixel descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-overlapping atomic patches</head><p>Correlations Level 1 correlation maps <ref type="figure">Fig. 3</ref> Computing the bottom level correlation maps {C 4,p } p∈G4 . Given two images I and I , the first one is split into non-overlapping atomic patches of size 4 × 4 pixels.</p><p>For each patch, we compute the correlation at every location of I to obtain the corresponding correlation map.</p><p>Bottom-level correlation map: We can express the correlation map computation obtained from Eq. (3) more conveniently in a convolutional framework. Let I N,p be a patch of size N × N from the first image centered at p (N ≥ 4 is a power of 2). Let G 4 = {2, 6, 10, ..., W − 2} × {2, 6, 10, ..., H − 2} be a grid with step 4 pixels. G 4 is the set of the centers of the atomic patches. For each p ∈ G 4 , we convolve the flipped patch I F 4,p over I</p><formula xml:id="formula_4">C 4,p = I F 4,p I ,<label>(4)</label></formula><p>to get the correlation map C 4,p , where . F denotes an horizontal and vertical flip 1 . For any pixel p of I , C 4,p (p ) is a measure of similarity between I 4,p and I 4,p . Examples of such correlation maps are shown in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>. Without surprise we can observe that atomic patches are not discriminative. Recursive aggregation of patches in subsequent stages will be the key to create discriminative responses.</p><p>Iteration. We then compute the correlation maps of larger patches by aggregating those of smaller patches. As shown in <ref type="figure">Figure 5</ref>, a N × N patch I N,p is the concatenation of 4 patches of size N/2 × N/2:</p><formula xml:id="formula_5">I N,p = I N 2 ,p+ N 4 oi i=0..3 with            o 0 = [−1, −1] , o 1 = [−1, +1] , o 2 = [+1, −1] , o 3 = [+1, +1] .<label>(5)</label></formula><p>They correspond respectively to the bottom-left, topleft, bottom-right and top-right quadrants. The correlation map of I N,p can thus be computed using its children's correlation maps. For the sake of clarity, we define the short-hand notation s N,i = N 4 o i describing the positional shift of a children patch i ∈ [0, 3] relatively to its parent patch (see <ref type="figure">Figure 5</ref>).</p><p>Using the above notations, we rewrite Eq. (2) by replacing sim(R, R ) def = C N,p (p ) (i.e. assuming here that patch R = I N,p and that R is centered at p ∈ I ).  Similarly, we replace the similarity between children patches sim (R i , R i (p i )) by C N 2 ,p+s N,i (p i ). For each child, we retain the maximum similarity over a small neighborhood Θ i of width and height N 8 centered at p +s N,i . We then obtain:</p><formula xml:id="formula_6">C N,p (p ) = 1 4 3 i=0 max m ∈Θi C N 2 ,p+s N,i (m )<label>(6)</label></formula><p>We now explain how we can break down Eq. (6) into a succession of simple operations. First, let us assume that N = 4 × 2 , where ≥ 1 is the current iteration. During iteration , we want to compute the correlation maps C N,p of every patch I N,p from the first image for which correlation maps of its children have been computed in the previous iteration. Formally, the position G N of such patches is defined according to the position of children patches G N 2 according to Eq. (5):</p><formula xml:id="formula_7">G N = {p | p + s N,i ∈ [0, W − 1] × [0, H − 1] ∧ p + s N,i ∈ G N 2 , i = 0, . . . , 3 .<label>(7)</label></formula><p>We observe that the larger a patch is (i.e. after several iterations), the smaller the spatial variation of its correlation map (see <ref type="figure" target="#fig_4">Figure 4</ref>). This is due to the statistics of natural images, in which low frequencies significantly dominate over high frequencies.</p><p>As a consequence, we choose to subsample each map C N,p by a factor 2. We express this with an operator S:</p><formula xml:id="formula_8">S : C(p ) → C(2p ).<label>(8)</label></formula><p>The subsampling reduces by 4 the area of the correlation maps and, as a direct consequence, the computational requirements. Instead of computing the subsampling on top of Eq. <ref type="formula" target="#formula_6">(6)</ref>, it is actually more efficient to propagate it towards the children maps and perform it jointly with max-pooling. It also makes the maxpooling domain Θ i become independent from N in the subsampled maps, as it exactly cancels out the effect of doubling N = 4 × 2 at each iteration. We call P the max-pooling operator with the iteration-independent domain Θ = {−1, 0, 1} × {−1, 0, 1}:</p><formula xml:id="formula_9">P : C(p ) → max m∈{−1,0,1} 2 C(p + m).<label>(9)</label></formula><p>For the same reason, the shift s N,i = N 4 o i = 2 o i applied to the correlation maps in Θ i 's definition becomes simply o i after subsampling. Let T t be the shift (or translation) operator on the correlation map:</p><formula xml:id="formula_10">T t : C(p ) → C(p − t).<label>(10)</label></formula><p>Finally, we incorporate an additional non-linear mapping at each iteration on top of Eq. (6) by applying a power transform R λ <ref type="bibr" target="#b30">(Malik and Perona 1990;</ref><ref type="bibr" target="#b25">LeCun et al 1998a)</ref>:</p><formula xml:id="formula_11">R λ : C(.) → C(.) λ<label>(11)</label></formula><p>This step, commonly referred to as rectification, is added in order to better propagate high correlations after each level, or, in other words, to counterbalance the fact that max-pooling tends to retain only high scores. Indeed, its effect is to decrease the correlation values (which are in [0, 1]) as we use λ &gt; 1. Such post-processing is commonly used in deep convolutional networks <ref type="bibr" target="#b26">(LeCun et al 1998b;</ref><ref type="bibr" target="#b3">Bengio 2009</ref>). In practice, good performance is obtained with λ 1.4, see Section 5. The final expression of Eq. (6) is:  <ref type="figure" target="#fig_5">Figure 6</ref> illustrates the computation of correlation maps for different patch sizes and Algorithm 1 summarizes our approach. The resulting set of correlation maps across iterations is referred to as multi-level correlation pyramid.</p><formula xml:id="formula_12">C N,p = R λ 1 4 3 i=0 (T oi • S • P) C N 2 ,p+s N,i<label>(12)</label></formula><p>Boundary effects: In practice, a patch I N,p can overlap with the image boundary, as long as its center p remains inside the image (from Eq. <ref type="formula" target="#formula_7">(7)</ref>). For instance, a patch I N,p0 with center at p 0 = (0, 0) ∈ G N has only a single valid child (the one for which i = 3 as p 0 + s N,3 ∈ I).</p><p>In such degenerate cases, the average sum in Eq. <ref type="formula" target="#formula_1">(12)</ref> is carried out on valid children only. For I N,p0 , it thus only comprises one term weighted by 1 instead of 1 4 . Note that Eq. (12) implicitly defines the set of possible displacements of the approach, see Figures 2 and 9. Given the position of a parent patch, each child patch can move only within a small extent, equal to the quarter of its own size. <ref type="figure" target="#fig_4">Figure 4</ref> shows the correlation maps for patches of size 4, 8 and 16. Clearly, correlation maps for larger patch are more and more discriminative, while still allowing non-rigid matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Top-down correspondence extraction</head><p>A score S = C N,p (p ) in the multi-level correlation pyramid represents the deformation-tolerant similarity of two patches I N,p and I N,p . Since this score is built from the similarity of 4 matching sub-patches at the lower pyramid level, we can thus recursively backtrack a set of correspondences to the bottom level (corresponding to matches of atomic patches). In this section, we first describe this backtracking. We, then, present the procedure for merging atomic correspondences backtracked from different entry points in the multi-level Algorithm 1 Computing the multi-level correlation pyramid.</p><formula xml:id="formula_13">Input: Images I, I For p ∈ G 4 do C 4,p = I F 4,p I (convolution, Eq. (4)) C 4,p ← R λ (C 4,p ) (rectification, Eq. (11)) N ← 4 While N &lt; max(W, H) do For p ∈ G N do C N,p ← (S • P)(C N,p ) (max-pooling and subsampling) N ← 2N For p ∈ G N do C N,p = 1 4 3 i=0 To i C N 2 ,p+s N,i (shift and average) C N,p ← R λ (C N,p ) (rectification, Eq. (11))</formula><p>Return the multi-level correlation pyramid C N,p N,p pyramid, which constitute the final output of Deep-Matching.</p><p>Compared to our initial version of DeepMatching <ref type="bibr" target="#b50">(Weinzaepfel et al 2013)</ref>, we have updated match scoring and entry point selection to optimize the execution time and the matching accuracy. A quantitative comparison is provided in Section 5.2.2.</p><p>Backtracking atomic correspondences. Given an entry point C N,p (p ) in the pyramid (i.e. a match between two patches I N,p and I N,p 2 ), we retrieve atomic correspondences by successively undoing the steps used to aggregate correlation maps during the pyramid construction, see <ref type="figure" target="#fig_8">Figure 7</ref>. The entry patch I N,p is itself composed of four moving quadrants I i N,p , i = 0 . . . 3. Due to the subsampling, the quadrant I i N,p = I N matches with I N 2 ,2(p +oi)+mi where m i = arg max</p><formula xml:id="formula_14">m∈{−1,0,1} 2 C N 2 ,p+s N,i (2(p + o i ) + m) .<label>(13)</label></formula><p>For the sake of clarity, we define the short-hand notations p i = p + s N,i and p i = 2(p + o i ) + m i . Let B be the function that assigns to a tuple (N, p, p , s), representing a correspondence between pixel p and p for patch of size N with a score s ∈ R, the set of the correspondences of children patches:</p><formula xml:id="formula_15">B(N, p, p , s) =    {(p, p , s)} if N = 4, N 2 , p i , p i , s + C N 2 ,p i (p i ) 3 i=0</formula><p>else. Thus, the algorithm for backtracking correspondences is the following. Consider an entry match M = {(N, p, p , C N,p (p ))}. We repeatedly apply B on M. After N = log 2 (N/4) calls, we get one correspondence for each of the 4 N atomic patches. Furthermore, their score is equal to the sum of all patch similarities along their backtracking path.</p><p>Merging correspondences. We have shown how to retrieve atomic correspondences from a match between two deformable (potentially large) patches. Despite this flexibility, a single match is unlikely to explain the complex set of motions that can occur, for example, between two adjacent frames in a video, i.e., two objects moving independently with significantly different motions exceeds the deformation range of DeepMatching. We quantitatively specify this range in the next subsection.</p><p>We thus merge atomic correspondences gathered from different entry points (matches) in the pyramid. In the initial version of DeepMatching <ref type="bibr" target="#b50">(Weinzaepfel et al 2013)</ref>, entry points were local maxima over all correlation maps. This is now replaced by a faster procedure, that starts with all possible matches in the top pyramid level (i.e. M = {(N, p, p , C N,p (p ))|N = N max }). Using this level only results in significantly less entry points than starting from all maxima in the entire pyramid. We did not observe any impact on the matching performance, see Section 5.2.2. Because M contains a lot of overlapping patches, most of the computation during repeated calls to M ← B(M) can be factorized. In other words, as soon as two tuples in M are equal in terms of N , p and p , the one with the lowest score is simply eliminated. We thus obtain a set of atomic correspondences M : that we filter with reciprocal match verification. The final set of correspondences M is obtained as:</p><formula xml:id="formula_16">M = (B • . . . • B)(M)<label>(15)</label></formula><formula xml:id="formula_17">M = (p, p , s)|BestAt(p) = BestAt (p ) (p,p ,s)∈M<label>(16)</label></formula><p>where BestAt(p) (resp. BestAt (p )) returns the best match in a small vicinity of 4 × 4 pixels around p in I (resp. around p in I ) from M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion and Analysis of DeepMatching</head><p>Multi-size patches and repetitive textures. During the bottom-up pass of the algorithm, we iteratively aggregate correlation maps of smaller patches to form the correlation maps of larger patches. Doing so, we effectively consider patches of different sizes (4 × 2 , ≥ 0), in contrast to most existing matching methods. This is a key feature of our approach when dealing with repetitive textures. As one moves up to upper levels, the matching problem gets less ambiguous. Hence, our method can correctly match repetitive patterns, see for instance <ref type="figure" target="#fig_7">Figure 8</ref>.</p><p>Quasi-dense correspondences. Our method retrieves dense correspondences for every single match between large regions (i.e. entry point for the backtracking in the toplevel correlation maps), even in weakly textured areas; this is in contrast to correspondences obtained when matching descriptors (e.g. SIFT). A quantitative assessment, which compares the coverage of matches obtained with several matching schemes, is given in Section 5.</p><p>Non-rigid deformations. Our matching algorithm is able to cope with various sources of image deformations: object-induced or camera-induced. The set of feasible deformations, explicitly defined by Eq. (6), theoretically  allows to deal with a scaling factor in the range [ 1 2 , 3 2 ] and rotations approximately in the range [−26 o , 26 o ]. Note also that DeepMatching is translation-invariant by construction, thanks to the convolutional nature of the processing.</p><p>Proof Given a patch of size N = 4 × 2 located at level 1, Eq. (6) allows each of its children patches to move by at most N/8 pixels from their ideal location in Θ i . By recursively summing the displacements at each level, the maximal displacements for an atomic patch is d N = i=1 2 i−1 = 2 − 1. An example is given in <ref type="figure" target="#fig_9">Figure 9</ref> with N = 32 and = 3. Relatively to N , we thus have lim N →∞ (N + 2d N )/N = 3 2 and lim N →∞ (N − 2d N )/N = 1 2 . For a rotation, the rationale is similar, see <ref type="figure" target="#fig_9">Figure 9</ref>.</p><p>Note that the displacement tolerance in Θ i from Eq. (6) could be extended to x × N/8 pixels with x ∈ {2, 3, . . .} (instead of x = 1). Then the above formula for computing the lower bound on the scale factor of Deep-Matching generalizes to LB(x) = lim N →∞ (N −2xd N )/N .  Hence, for x ≥ 2 we obtain LB(x) &lt; 0 instead of LB(1) = 1 2 . This implies that the deformation range is extended to a point where any patch can be matched to a single pixel, i.e., this results in unrealistic deformations. For this reason, we choose to not expand the deformation range of DeepMatching.</p><p>Built-in smoothing. Furthermore, correspondences generated through backtracking of a single entry point in the correlation maps are naturally smooth. Indeed, feasible deformations cannot be too "far" from the identity deformation. To verify this assumption, we conduct the following experiment. We artificially generate two types of correspondences between two images of size 128×128. The first one is completely random, i.e. for each atomic patch in the first image we assign randomly a match in the second image. The second one respects the backtracking constraints. Starting from a single entry point in the top level we simulate the backtracking procedure from Section 3.3 by replacing in Eq. (13) the max operation by a random sampling over {−1, 0, 1} 2 . By gener-ating 10,000 sets of possible atomic correspondences, we simulate a set which respects the deformations allowed by DeepMatching. <ref type="figure" target="#fig_0">Figure 10</ref> compares the smoothness of these two types of artificial correspondences. Smoothness is measured by interpreting the correspondences as flow and measuring the gradient flow norm, see <ref type="bibr">Eq. (19)</ref>. Clearly, the two types of warpings are different by orders of magnitude. Furthermore, the one which respects the built-in constraints of DeepMatching is close to the identity warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation to Deep Convolutional Neural Networks (CNNs).</head><p>DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches (Le-Cun et al 1998a). In the following we describe the major similarities and differences.</p><p>Deep networks learn from data the weights of the convolutions. In contrast, DeepMatching does not learn any feature representations and instead directly computes correlations at the patch level. It uses patches from the first image as convolution filters for the second one. However, the bottom-up pipeline of DeepMatching is similar to CNNs. It alternates aggregating channels from the previous layer with channel-wise max-pooling and subsampling. As in CNNs, max-pooling in Deep-Matching allows for invariance w.r.t. small deformations. Likewise, the algorithm propagates pairwise patch similarity scores through the hierarchy using non-linear rectifying stages in-between layers. Finally, DeepMatching includes a top-down pass which is not present in CNNs. Proof Computing the initial correlations is a O(LL ) operation. Then, at each level of the pyramid, the process is repeated while the complexity is divided by a factor 4 due to the subsampling step in the target image (since the cardinality of |{G N }| remains approximately constant). Thus, the total complexity of the correlation maps computation is, at worst, O( ∞ n=0 LL /4 n ) = O(LL ). During the top-down pass, most backtracking paths can be pruned as soon as they cross a concurrent path with a higher score (see Section 3.3). Thus, all correlations will be examined at most once, and there are ∞ n=0 LL /4 n values in total. However, this analysis is worst-case. In practice, only correlations lying on maximal paths are actually examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extensions of DeepMatching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approximate DeepMatching</head><p>As a consequence of its O(LL ) space complexity, Deep-Matching requires an amount of RAM that is orders of magnitude above other state-of-the-art matching methods. This could correspond to several gigabytes for images of moderate size (800×600 pixels); see Section 5.2.3. This section introduces an approximation of DeepMatching that allows to trade matching quality for reduced time and memory usage. As shown in Section 5.2.3, nearoptimal results can be obtained at a fraction of the original cost.</p><p>Our approximation proposes to compress the representation of atomic patches {I 4,p }. Atomic patches carry little information, and thus are highly redundant. For instance, in uniform regions, all patches are nearly identical (i.e., gradient-wise). To exploit this property, we index atomic patches with a small set of patch prototypes. We substitute each patch with its closest neighbor in a fixed dictionary of D prototypes. Hence, we need to perform and store only D convolutions at the first level, instead of O(L) (with D O(L)). This significantly reduces both memory and time complexity. Note that higher pyramid levels also benefit from this optimization. Indeed, two parent patches at the second level have the exact same correlation map in case their children are assigned the same prototypes. The same reasoning also holds for all subsequent levels, but the gains rapidly diminish due to statistical unlikeliness of the required condition. This is not really an issue, since the memory and computational cost mostly rests on the initial levels; see Section 3.4.</p><p>In practice, we build the prototype dictionary using k-means, as it is designed to minimize the approximation error between input descriptors and resulting centroids (i.e. prototypes). Given a pair of images to match, we perform on-line clustering of all descriptors of atomic patches {I 4,p } = {R} in the first image. Since the original descriptors lie on an hypersphere (each pixel descriptor R i,j has norm 1), we modify the k-means approach so as to project the estimated centroids on the hypersphere at each iteration. We find experimentally that this is important to obtain good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scale and rotation invariant DeepMatching</head><p>For a variety of tasks, objects to be matched can appear under image rotations or at different scales <ref type="bibr" target="#b29">(Lowe 2004;</ref><ref type="bibr" target="#b32">Mikolajczyk et al 2005;</ref><ref type="bibr" target="#b42">Szeliski 2010</ref>; HaCohen et al 2011). As discussed above, DeepMatching (DM) is only robust to moderate scale changes and rotations. We now present a scale and rotation invariant version.</p><p>The approach is straightforward: we apply DM to several rotated and scaled versions of the second image. According to the invariance range of DM, we use steps of π/4 for image rotation and power of √ 2 for scale changes. While iterating over all combinations of scale changes and rotations, we maintain a list M of all atomic correspondences obtained so far, i.e. corresponding positions and scores. As before, the final output correspondences consists of the reciprocal matches in M . Storing all matches and finally choosing the best ones based on reciprocal verification permits to capture distinct motions possibly occurring together in the same scene (e.g. one object could have undergone a rotation, while the rest of the scene did not move). The steps of the approach are described in Algorithm 2.</p><p>Since we iterate sequentially over a fixed list of rotations and scale changes, the space and time complexity of the algorithm remains unchanged (i.e. O(LL )). In practice, the run-time compared to DM is multiplied by a constant approximately equal to 25, see Section 5.2.4. Note that the algorithm permits a straightforward parallelization.</p><p>Algorithm 2 Scale and rotation invariant version of DeepMatching (DM). I σ denotes the image I downsized by a factor σ, and R θ denotes rotation by an angle θ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DeepFlow</head><p>We now present our approach for optical flow estimation, DeepFlow. We adopt the method introduced by <ref type="bibr" target="#b6">Brox and Malik (2011)</ref>, where a matching term penalizes the differences between optical flow and input matches, and replace their matching approach by Deep-Matching. In addition, we make a few minor modifications introduced recently in the state of the art: (i) we add a normalization in the data term to downweight the impact of locations with high spatial image derivatives <ref type="bibr" target="#b56">(Zimmer et al 2011)</ref>; (ii) we use a different weight at each level to downweight the matching term at finer scales <ref type="bibr" target="#b38">(Stoll et al 2012)</ref>; and (iii) the smoothness term is locally weighted <ref type="bibr" target="#b53">(Xu et al 2012)</ref>. Let I 1 , I 2 : Ω → R c be two consecutive images defined on Ω with c channels. The goal is to estimate the flow w = (u, v) : Ω → R 2 . We assume that the images are already smoothed using a Gaussian filter of standard deviation σ. The energy we optimize is a weighted sum of a data term E D , a smoothness term E S and a matching term E M :</p><formula xml:id="formula_18">E(w) =ˆΩ E D + αE S + βE M dx<label>(17)</label></formula><p>For the three terms, we use a robust penalizer Ψ (s 2 ) = √ s 2 + 2 with = 0.001 which has shown excellent results <ref type="bibr" target="#b40">(Sun et al 2014b)</ref>.</p><p>Data term. The data term is a separate penalization of the color and gradient constancy assumptions with a normalization factor as proposed by <ref type="bibr" target="#b56">Zimmer et al (2011)</ref>. We start from the optical flow constraint assuming brightness constancy: (∇ 3 I)w = 0 with ∇ 3 = (∂x, ∂y, ∂t) the spatio-temporal gradient. A basic way to build a data term is to penalize it, i.e. E D = Ψ (w J 0 w) with J 0 the tensor defined by J 0 = (∇ 3 I)(∇ 3 I). As highlighted by <ref type="bibr" target="#b56">Zimmer et al (2011)</ref>, such a data term adds a higher weight in locations corresponding to high spatial image derivatives. We normalize it by the norm of the spatial derivatives plus a small factor to avoid division by zero, and to reduce a bit the influence in tiny gradient locations <ref type="bibr" target="#b56">(Zimmer et al 2011)</ref>. LetJ 0 be the normalized tensorJ 0 = θ 0 J 0 with θ 0 = ( ∇ 2 I 2 + ζ 2 ) −1 . We set ζ = 0.1 in the following. To deal with color images, we consider the tensor defined for a channel i denoted by upper indicesJ i 0 and we penalize the sum over channels: Ψ ( c i=1 w J i 0 w). We consider images in the RGB color space.</p><p>We separately penalize the gradient constancy assumption <ref type="bibr" target="#b8">(Bruhn et al 2005)</ref>. Let I x and I y be the derivatives of the images with respect to the x and y axis respectively. LetJ i xy be the tensor for the channel i including the normalization</p><formula xml:id="formula_19">J i xy = (∇ 3 I i x )(∇ 3 I i x )/( ∇ 2 I i x 2 + ζ 2 ) + (∇ 3 I i y )(∇ 3 I i y )/( ∇ 2 I i y 2 + ζ 2 ).</formula><p>The data term is the sum of two terms, balanced by two weights δ and γ:</p><formula xml:id="formula_20">E D = δΨ c i=1 w J i 0 w + γΨ c i=1 w J i xy w<label>(18)</label></formula><p>Smoothness term. The smoothness term is a robust penalization of the gradient flow norm:</p><formula xml:id="formula_21">E S = Ψ ∇u 2 + ∇v 2 .<label>(19)</label></formula><p>The smoothness weight α is locally set according to image derivatives <ref type="bibr" target="#b53">Xu et al 2012)</ref> with α(x) = exp(−κ∇ 2 I(x)) where κ is experimentally set to κ = 5.</p><p>Matching term. The matching term encourages the flow estimation to be similar to a precomputed vector field w . To this end, we penalize the difference between w and w using the robust penalizer Ψ . Since the matching is not totally dense, we add a binary term c(x) which is equal to 1 if and only if a match is available at x. We also multiply each matching penalization by a weight φ(x), which is low in uniform regions where matching is ambiguous and when matched patches are dissimilar. To that aim, we rely onλ(x), the minimum eigenvalue of the autocorrelation matrix multiplied by 10. We also compute the visual similarity between matches as ∆(</p><formula xml:id="formula_22">x) = c i=1 |I i 1 (x)−I i 2 (x−w (x))|+ |∇I i 1 (x)−∇I i 2 (x−w (x))|.</formula><p>We then compute the score φ as a Gaussian kernel on ∆ weighted byλ with a parameter σ M , experimentally set to σ M = 50. More precisely, we define φ(x) at each point x with a match w (x) as:</p><formula xml:id="formula_23">φ(x) = λ (x)/(σ M √ 2π) exp(−∆(x)/2σ M ).</formula><p>The matching term is then E M = cφΨ ( w − w 2 ).</p><p>Minimization. This energy objective is non-convex and non-linear. To solve it, we use a numerical optimization algorithm similar as <ref type="bibr" target="#b7">Brox et al (2004)</ref>. An incremental coarse-to-fine warping strategy is used with a downsampling factor η = 0.95. The remaining equations are still non-linear due to the robust penalizers. We apply 5 inner fixed point iterations where the nonlinear weights and the flow increments are iteratively updated while fixing the other. To approximate the solution of the linear system, we use 25 iterations of the Successive Over Relaxation (SOR) method <ref type="bibr" target="#b55">(Young and Rheinboldt 1971)</ref>.</p><p>To downweight the matching term on fine scales, we use a different weight β k at each level as proposed by <ref type="bibr" target="#b38">Stoll et al (2012)</ref>. We set β k = β(k/k max ) b where k is the current level of computation, k max the coarsest level and b a parameter which is optimized together with the other parameters, see Section 5.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents an experimental evaluation of Deep-Matching and DeepFlow. The datasets and metrics used to evaluate DeepMatching and DeepFlow are introduced in Section 5.1. Experimental results are given in Sections 5.2 and 5.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and metrics</head><p>In this section we briefly introduce the matching and flow datasets used in our experiments. Since consecutive frames of a video are well-suited to evaluate a matching approach, we use several optical flow datasets for evaluating both the quality of matching and flow, but we rely on different metrics.</p><p>The Mikolajczyk dataset was originally proposed by <ref type="bibr" target="#b32">Mikolajczyk et al (2005)</ref> to evaluate and compare the performance of keypoint detectors and descriptors. It is one of the standard benchmarks for evaluating matching approaches. The dataset consists of 8 sequences of 6 images each viewing a scene under different conditions, such as illumination changes or viewpoint changes. The images of a sequence are related by homographies. During the evaluation, we comply to the standard procedure in which the first image of each scene is matched to the 5 remaining ones. Since our goal is to study robustness of DeepMatching to geometric distortions, we follow HaCohen et al (2011) and restrict our evaluation to the 4 most difficult sequences with viewpoint changes: bark, boat, graf and wall.</p><p>The MPI-Sintel dataset <ref type="bibr" target="#b9">(Butler et al 2012)</ref> is a challenging evaluation benchmark for optical flow estimation, constructed from realistic computer-animated films. The dataset contains sequences with large motions and specular reflections. In the training set, more than 17.5% of the pixels have a motion over 20 pixels, approximately 10% over 40 pixels. We use the "final" version, featuring rendering effects such as motion blur, defocus blur and atmospheric effects. Note that ground-truth optical flows for the test set are not publicly available.</p><p>The Middlebury dataset <ref type="bibr" target="#b1">(Baker et al 2011)</ref> has been extensively used for evaluating optical flow methods. The dataset contains complex motions, but most of the motions are small. Less than 3% of the pixels have a motion over 20 pixels, and no motion exceeds 25 pixels (training set). Ground-truth optical flows for the test set are not publicly available.</p><p>The Kitti dataset <ref type="bibr" target="#b16">Geiger et al (2013)</ref> contains realworld sequences taken from a driving platform. The dataset includes non-Lambertian surfaces, different lighting conditions, a large variety of materials and large displacements. More than 16% of the pixels have motion over 20 pixels. Again, ground-truth optical flows for the test set are not publicly available.</p><p>Performance metric for matching. Choosing a performance measure for matching approaches is delicate. Matching approaches typically do not return dense correspondences, but output varying numbers of matches. Furthermore, correspondences might be concentrated in different areas of the image.</p><p>Most matching approaches, including DeepMatching, are based on establishing correspondences between patches. Given a pair of matching patches, it is possible to obtain a list of pixel correspondences for all pixels within the patches. We introduce a measure based on the number of correctly matched pixels compared to the overall number of pixels. We define "accuracy@T " as the proportion of "correct" pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its pixel match in the second image is closer than T pixels to ground-truth. In practice, we use a threshold of T = 10 pixels, as this represents a sufficiently precise estimation (about 1% of image diagonal for all datasets), while allowing some tolerance in blurred areas that are difficult to match exactly. If a pixel belongs to several matches, we choose the one with the highest score to predict its correspondence. Pixels which do not belong to any patch have an infinite error.</p><p>Performance metric for optical flow. To evaluate optical flow, we follow the standard protocol and measure the average endpoint error over all pixels, denoted as "EPE". The "s10-40" variant measures the EPE only for pixels with a ground-truth displacement between 10 and 40 pixels, and likewise for "s0-10" and "s40+". In all cases, scores are averaged over all image pairs to yield the final result for a given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Matching Experiments</head><p>In this section, we evaluate DeepMatching (DM). We present results for all datasets presented above but Middlebury, which does not feature long-range motions, the main difficulty in image matching. When evaluating on the Mikolajczyk dataset, we employ the scale and rotation invariant version of DM presented in Section 4.2. For all the matching experiments reported in this section, we use the Mikolajczyk dataset and the training sets of MPI-Sintel and Kitti.  <ref type="figure" target="#fig_0">Fig. 11</ref> Impact of the parameters to compute pixel descriptors on the different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Impact of the parameters</head><p>We optimize the different parameters of DM jointly on all datasets. To prevent overfitting, we use the same parameters across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel descriptor parameters:</head><p>We first optimize the parameters of the pixel representation (Section 3.2): ν 1 , ν 2 , ν 3 (different smoothing stages), ς (sigmoid slope) and µ (regularization constant). After performing a grid search, we find that good results are obtained at ν 1 = ν 2 = ν 3 = 1, ς = 0.2 and µ = 0.3 across all datasets. <ref type="figure" target="#fig_0">Figure 11</ref> shows the accuracy@10 in the neighborhood of these values for all parameters. Image pre-smoothing seems to be crucial for JPEG images <ref type="bibr">(Mikolajczyk dataset)</ref>, as it smooths out compression artifacts, whereas it slightly degrades performance for uncompressed PNG images (MPI-Sintel and Kitti). As expected, similar findings are observed for the regularization constant µ since it acts as a regularizer that reduces the impact of small gradients (i.e. noise). In the following, we thus use low values of ν 1 and µ when dealing with PNG images (we set ν 1 = 0 and µ = 0.1, other parameters are unchanged).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-linear rectification:</head><p>We also evaluate the impact of the parameter λ of the non-linear rectification obtained by applying power normalization, see Eq. (11). <ref type="figure" target="#fig_0">Figure 12</ref> displays the accuracy@10 for various values of λ. We can observe that the optimal performance is achieved at λ = 1.4 for all datasets. We use this value in the remainder of our experiments.  <ref type="table">Table 1</ref> Detailed comparison between the preliminary and current versions of DeepMatching in terms of performance, run-time and memory usage. R denotes the input image resolution and BT backtracking. Run-times are computed on 1 core @ 3.6 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation of the backtracking and scoring schemes</head><p>We now evaluate two improvements of DM with respect to the previous version published in <ref type="bibr" target="#b50">Weinzaepfel et al (2013)</ref>, referred to as DM*:</p><p>-Backtracking (BT) entry points: in DM* we select as entry points local maxima in the correlation maps from all pyramid levels. The new alternative is to start from all possible points in the top pyramid level. -Scoring scheme: In DM* we scored atomic correspondences based on the correlation values of start and end point of the backtracking path. The new scoring scheme is the sum of correlation values along the full backtracking path.</p><p>We report results for the different variants in <ref type="table">Table 1</ref> on each dataset. The first two rows for each dataset correspond to the exact settings used for DM* (i.e. with an image resolution of 1/4 and 1/2). We observe a steady increase in performance on all datasets when we add the new scoring and backtracking approach. We can observe that starting from all possible entry points in the top pyramid level (i.e. considering all possible translations) yields slightly better results than starting from local maxima. This demonstrates that some groundtruth matches are not covered by any local maximum.</p><p>By enumerating all possible patch translations from the top-level, we instead ensure to fully explore the space of all possible matches.</p><p>Furthermore, it is interesting to note that memory usage and run-time significantly decreases when using the new options. This is because (1) searching and storing local maxima (which are exponentially more numerous in lower pyramid levels) is not necessary anymore, and (2) the new scoring scheme allows for further optimization, i.e. early pruning of backtracking paths (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Approximate DeepMatching</head><p>We now evaluate the performance of approximate Deep-Matching (Section 4.1) and report its run-time and memory usage. We evaluate and compare two different ways of reducing the computational load. The first one simply consists in downsizing the input images, and upscaling the resulting matches accordingly. The second option is the compression scheme proposed in Section 4.1.</p><p>We evaluate both schemes jointly by varying the input image size (expressed as a fraction R of the original resolution) and the size D of the prototype dictionary (i.e. parameter of k-means in Section 4.1). R = 1 corresponds to the original dataset image size (no downsizing). We display the results in terms of matching accuracy (accuracy@10) against memory consumption in <ref type="figure" target="#fig_0">Figure 13</ref> and as a function of D in <ref type="figure" target="#fig_0">Figure 14</ref>. <ref type="figure" target="#fig_0">Figure 13</ref> shows that DeepMatching can be computed in an approximate manner for any given memory budget. Unsurprisingly, too low settings (e.g. R ≤ 1/8, D ≤ 64) result in a strong loss of performance. It should be noted that that we were unable to compute DeepMatching at full resolution (R = 1) for D &gt; 64, as the memory consumption explodes. As a consequence, all subsequent experiments in the paper are done at R = 1/2. In <ref type="figure" target="#fig_0">Figure 14</ref>, we observe that good trades-off are achieved for dictionary sizes comprised in D ∈ <ref type="bibr">[64,</ref><ref type="bibr">1024]</ref>. For instance, on MPI-Sintel, at D = 1024, 94% of the performance of the uncompressed case (D = ∞) is reached for half the computation time and one third of the memory usage. Detailed timings of the different stages of Deep-Matching are given in <ref type="table" target="#tab_3">Table 2</ref>. As expected, only the bottom-up pass is affected by the approximation, with a run-time of the different operations involved (patch correlations, max-pooling, subsampling, aggregation and non-linear rectification) roughly proportional to D (or to |G 4 |, the actual number of atomic patches, if D = ∞). The overhead of clustering the dictionary prototypes with k-means appears negligible, with the exception of the largest dictionary size (D = 4096) for which it in-   </p><formula xml:id="formula_24">D =4 D =64 D =4 D =64 D =1024 D =∞ D =4 D =64 D =1024 D =∞ D =∞<label>(</label></formula><formula xml:id="formula_25">D =4 D =64 D =4 D =64 D =1024 D =∞ D =4 D =64 D =1024 D =∞ D =∞ (b) MPI-Sintel</formula><formula xml:id="formula_26">D =4 D =64 D =4 D =64 D =1024 D =∞ D =4 D =64 D =1024 D =∞ D =∞ (c) Kitti dataset R = 1/1 R = 1/2 R = 1/4 R = 1/8 Fig. 13</formula><p>Trade-off between memory consumption and matching performance for the different datasets. Memory usage is controlled by changing image resolution R (different curves) and dictionary size D (curve points).</p><p>duces a slightly longer run-time than in the uncompressed case. Overall, the proposed method for approximating DeepMatching is highly effective.</p><p>GPU Implementation. We have implemented DM on GPU in the Caffe framework <ref type="bibr" target="#b20">(Jia et al 2014)</ref>. Using existing Caffe layers like ConvolutionLayer and PoolingLayer, the implementation is straightforward for most layers. We had to specifically code a few layers which are not available in Caffe (e.g. the backtracking pass 3 ). For the aggregation layer which consists in selecting and averaging 4 children channels out of many channels, we relied on the sparse matrix multiplication in the cuSPARSE toolbox. Detailed timings are given in <ref type="table" target="#tab_3">Table 2</ref> on a GeForce Titan X. Our code runs in about 0.2s for a pair of MPI-Sintel image. As expected, the computation bottleneck essentially lies in the computation of bottom-level patch correlations and the backtracking pass. Note that computing patch descriptors takes significantly more time, in proportion, than on CPU: it takes about 0.024s = 11% of total time (not shown in <ref type="table">table)</ref>. This is because it involves a succession of many small layers (image smoothing, gradient extraction and projection, etc.), which causes overhead and is rather inefficient.    <ref type="figure" target="#fig_4">. (4)</ref>), joint max-pooling and subsampling, correlation map aggregation, non linear rectification (resp. S • P, To i , and R λ in Eq. (12)), and correspondence backtracking (Section 3.3). Other operations (e.g. reciprocal verification of Eq. (16)) have negligible run-time. For operations applied at several levels like the non-linear rectification, a cumulative timing is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Comparison to the state of the art</head><p>We compare DM with several baselines and state-ofthe-art matching algorithms, namely:</p><p>-SIFT keypoints extracted with DoG detector <ref type="bibr" target="#b29">(Lowe 2004)</ref> and matched with FLANN <ref type="bibr" target="#b33">(Muja and Lowe 2009</ref>), referred to as SIFT-NN, 4 dense HOG matching, followed by nearest-neighbor matching with reciprocal verification as done in LDOF <ref type="bibr" target="#b6">(Brox and Malik 2011)</ref>, referred to as HOG-NN 4 , -Generalized PatchMatch (GPM) <ref type="bibr" target="#b2">(Barnes et al 2010)</ref>, with default parameters, 32x32 patches and 20 iterations (best settings in our experiments) 5 , -Kd-tree PatchMatch (KPM) <ref type="bibr" target="#b41">(Sun 2012)</ref>, an improved version of PatchMatch based on better patch descriptors and kd-trees optimized for correspondence propagation 4 , -Non-Rigid Dense Correspondences (NRDC) <ref type="bibr" target="#b17">(HaCohen et al 2011)</ref>, an improved version of GPM based on a multiscale iterative expansion/contraction strategy 6 , -SIFT-flow <ref type="bibr" target="#b28">(Liu et al 2011)</ref>, a dense matching algorithm based on an energy minimization where pixels are represented as SIFT features and a smoothness term is incorporated to explicitly preserve spatial discontinuities 5 , -Scale-less SIFT (SLS) <ref type="bibr" target="#b18">(Hassner et al 2012)</ref>, an improvement of SIFT-flow to handle scale changes (multiple sized SIFTs are extracted and combined to form a scale-invariant pixel representation) 5 , -DaisyFilterFlow (DaisyFF) <ref type="bibr" target="#b54">(Yang et al 2014)</ref>, a dense matching approach that combines filter-based efficient flow inference and the Patch-Match fast search algorithm to match pixels described using the DAISY representation <ref type="bibr" target="#b45">(Tola et al 2010)</ref>   <ref type="table" target="#tab_6">Table 3</ref>. Coverage is computed as the proportion of points on a regular grid with 10 pixel spacing for which there exists a correspondence (in the raw output of the considered method) within a 10 pixel neighborhood. Thus, it measures how well matches "cover" the image. <ref type="table" target="#tab_6">Table 3</ref> shows that DeepMatching outputs 2 to 7 times more matches than SIFT-NN and a comparable number to HOG-NN. Yet, the coverage for DM matches is much higher than for HOG-NN and SIFT-NN. This shows that DM matches are well distributed over the entire image, which is not the case for HOG-NN and SIFT-NN, as they have difficulties estimating matches in regions with weak or repetitive textures.</p><p>Quantitative results are listed in <ref type="table" target="#tab_7">Table 4</ref>, and qualitative results in <ref type="figure" target="#fig_0">Figures 15, 16</ref> and 17. Overall, DM significantly outperforms all other methods, even when reduced settings are used (e.g. for image resolution R = 1/2 and D = 1024 prototypes). As expected, SIFT- <ref type="bibr">6</ref> We report results from the original paper.    <ref type="figure" target="#fig_0">Figure 15</ref>). In conclusion, DM outperforms all other methods on the 3 datasets, including DSP which also relies on a hierarchical matching.</p><p>In terms of computing resources, DeepMatching with full settings (R = 1/2, D = ∞) is one of the most costly method (only SLS and DaisyFF require the same order of memory and longer run-time). The scale and rotation invariant version of DM, used for the Mikolajczyk dataset, is slow compared to most other approaches, due to its sequential processing (i.e. treating each combination of rotation and scaling sequentially), yet yields near perfect results. However, running DM with reduced settings is very competitive to the other approaches. On MPI-Sintel and Kitti, for instance, DM with a quarter resolution has a run-time comparable to the fastest method, SIFT-NN, with a reasonable memory usage, while still outperforming nearly all methods in terms of the accuracy@10 measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optical Flow Experiments</head><p>We now present experimental results for the optical flow estimation. Optical flow is predicted using the variational framework presented in Section 4.3 that takes as input a set of matches. In the following, we evaluate the impact of DeepMatching against other matching methods, and compare to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Optimization of the parameters</head><p>We optimize the parameters of DeepFlow on a subset of the MPI-Sintel training set (20%), called "small" set, and report results on the remaining image pairs (80%, called "validation set") and on the training sets of Kitti and Middlebury. Ground-truth optical flows for the three test sets are not publicly available, in order to prevent parameter tuning on the test set.</p><p>We first optimize the different flow parameters (β, γ, δ, σ and b) by employing a gradient descent strategy with multiple initializations followed by a local grid search. For the data term, we find an optimum at δ = 0, which is equivalent to removing the color constancy assumption. This can be explained by the fact that the "final" version contains atmospheric effects, reflections, blurs, etc. The remaining parameters are optimal at β = 300, γ = 0.8, σ = 0.5, b = 0.6. These parameters are used in the remaining of the experiments for Deep-Flow, i.e. using matches obtained with DeepMatching, except when reporting results on Kitti and Middlebury test sets in Section 5.3.3. In this case the parameters are optimized on their respective training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Impact of the matches on the flow</head><p>We examine the impact of different matching methods on the flow, i.e., different matches are used in Deep-Flow, see Section 4.3. For all matching approaches evaluated in the previous section, we use their output as matching term in Eq. (17). Because these approaches may output matches with statistics different from DM, we separately optimize the flow parameters for each matching approach on the small training set of MPI-Sintel 7 . <ref type="table">Table 5</ref> shows the endpoint error, averaged over all pixels. Clearly, a sufficiently dense and accurate matching like DM allows to considerably improve the flow estimation on datasets with large displacements (MPI-Sintel, Kitti). In contrast, none of the methods presented have a tangible effect on the Middlebury dataset, where the displacements are small.</p><p>The relatively small gains achieved by SIFT-NN and HOG-NN on MPI-Sintel and Kitti are due to the fact that a lot of regions with large displacements are not covered by any matches, such as the sky or the blurred character in the first and second column of <ref type="figure" target="#fig_0">Figure 18</ref>. Hence, SIFT-NN and HOG-NN have only a limited impact on the variational approach. On the other hand, the gains are also small (or even negative) for the dense methods despite the fact that they output significantly more correspondences. We observe for these methods that the weight β of the matching term tends to be small after optimizing the parameters, thus indicating that the matches are found unreliable and noisy during training. The cause is clearly visible in <ref type="figure" target="#fig_0">Figure 17</ref>, where large portions containing repetitive textures (e.g. road, trees) are incorrectly matched. The poor quality of these matches even leads to a significant drop in performance on the Kitti dataset.</p><p>In contrast, DeepMatching generates accurate matches well covering the image that enable to boost the optical flow accuracy in case of large displacements. Namely, we observe a relative improvement of 30% on MPI-Sintel and of 50% on Kitti. It is interesting to observe that DM is able to effectively prune false matches arising in occluded areas (black areas in <ref type="figure" target="#fig_0">Figures 16 and 17)</ref>. This is due to the reciprocal verification filtering incorporated in DM (Eq. (16)). When using the approximation with 1024 prototypes, however, a significant drop is observed on the Kitti dataset, while the performance remains good on MPI-Sintel. This indicates that approximating DeepMatching can result in a significant loss of robustness when matching repetitive textures, that are more frequent in Kitti than in MPI-Sintel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Comparison to the state of the art</head><p>In this section, we compare DeepFlow to the state of the art on the test sets of MPI-Sintel, Kitti and Middlebury datasets. For theses datasets, the results are submitted to a dedicated server which performs the evaluation. Prior to submitting our results for Kitti and Middlebury test sets, we have optimized the parameters on the respective training set.</p><p>Results on MPI-Sintel. <ref type="table">Table 6</ref> compares our method to state-of-the-art algorithms on the MPI-Sintel test set. A comparison with the preliminary version of Deep-Flow <ref type="bibr" target="#b50">(Weinzaepfel et al 2013)</ref>, referred to as Deep-Flow*, is also provided. In this early version, we used a constant smoothness weight instead of a local one here (see Section 4.3) and used DM* as input matches. We can see that DeepFlow is among the best performing methods on MPI-Sintel, particularly for large displacements. This is due to the use of a reliable matching term in the variational approach, and this property is shared by all top performing approaches, e.g. (Revaud et al 2015; <ref type="bibr" target="#b27">Leordeanu et al 2013)</ref>. Furthermore, it is interesting to note that among the top performers on MPI-Sintel, 3 methods out of 6 actually employ DeepMatching. In particular, the top-3 method  <ref type="table">Table 6</ref> Results on MPI-Sintel test set (final version). EPEocc is the EPE on occluded areas. s0-10 is the EPE for pixels with motions between 0 and 10 px and similarly for s10-40 and s40+. DeepFlow* denotes the preliminary version of DeepFlow published in <ref type="bibr" target="#b50">Weinzaepfel et al (2013)</ref>.</p><p>EpicFlow <ref type="bibr" target="#b37">(Revaud et al 2015)</ref> relies on the output of DeepMatching to produce a piece-wise affine flow, and SparseFlowFused (Timofte and Van Gool 2015) combines matches obtained with DeepMatching and another algorithm.</p><p>We refer to the webpage of the MPI-Sintel dataset for complete results including the "clean" version.</p><p>Timings. As mentioned before, DeepMatching at half the resolution takes 15 seconds to compute on CPU and 0.2 second on GPU. The variational part requires 10 additional seconds on CPU. Note that by implementing it on GPU, we could obtain a significant speed-up as well. DeepFlow consequently takes 25 seconds in total on a single CPU core @ 3.6 GHz or 10.2s with GPU+CPU. This is in the same order of magnitude as the fastest among the best competitors, EpicFlow <ref type="bibr" target="#b37">(Revaud et al 2015)</ref>.</p><p>Results on Kitti. <ref type="table">Table 7</ref> summarizes the main results on the Kitti benchmark (see official website for complete results), when optimizing the parameters on the Kitti training set. EPE-Noc is the EPE computed only in non-occluded areas. "Out 3" corresponds to the proportion of incorrect pixel correspondences for an error threshold of 3 pixels, i.e. it corresponds to 1 − accuracy@3, and likewise for "Out-Noc 3" for non-occluded areas. In terms of EPE-noc, DeepFlow is on par with the best approaches, but performs somewhat worse in the occluded areas. This is due to a specificity of the For non-dense methods, pixel displacements have been inferred from matching patches. Areas without correspondences are in black.</p><p>To improve visualization, the sparse Kitti ground-truth is made dense using bilateral filtering.</p><p>Kitti dataset, in which motion is mostly homographic (especially on the image borders, where most surfaces like roads and walls are planar). In such cases, flow is better predicted using an affine motion prior, which locally well approximates homographies (a constant motion prior is used in DeepFlow). As a matter of facts, all top performing methods in terms of total EPE output piece-wise affine optical flow, either due to affine regularizers (BTF-ILLUM <ref type="formula" target="#formula_24">(</ref> ). Note that the learned parameters on Kitti and MPI-Sintel are close. In particular, running the experiments with the same parameters as MPI-Sintel decreases EPE-Noc by only 0.1 pixels on the training set. This shows that our method does not suffer from overfitting.</p><p>Results on Middlebury. We optimize the parameters on the Middlebury training set by minimizing the average angular error with the same strategy as for MPI-Sintel. We find weights quasi-zero for the matching term due to the absence of large displacements. DeepFlow obtained  <ref type="table">Table 7</ref> Results on Kitti test set. EPE-noc is the EPE over non-occluded areas. Out-Noc 3 (resp. Out 3) refers to the percentage of pixels where flow estimation has an error above 3 pixels in non-occluded areas (resp. all pixels). DeepFlow* denotes the preliminary version of DeepFlow published in <ref type="bibr" target="#b50">Weinzaepfel et al (2013)</ref>. • denotes the usage of a GPU.</p><p>an average endpoint error of 0.4 on the test which is competitive with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced a dense matching algorithm, termed DeepMatching. The proposed algorithm gracefully handles complex non-rigid object deformations and repetitive textured regions. DeepMatching yields state-of-theart performance for image matching, on the Mikola- jczyk <ref type="bibr" target="#b32">(Mikolajczyk et al 2005)</ref>, the MPI-Sintel <ref type="bibr" target="#b9">(Butler et al 2012)</ref> and the Kitti <ref type="bibr" target="#b16">(Geiger et al 2013)</ref> datasets. Integrated in a variational energy minimization approach <ref type="bibr" target="#b6">(Brox and Malik 2011)</ref>, the resulting approach for optical flow estimation, termed DeepFlow, shows competitive performance on optical flow benchmarks.</p><p>Future work includes incorporating a weighting of the patches in Eq. (2) instead of weighting all patches equally to take into account that different parts of a large patch may belong to different objects. This could improve the performance of DeepMatching for thin objects, such as human limbs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Illustration of moving quadrant similarity: a quadrant is a quarter of a SIFT patch, i.e. a group of 2 × 2 cells. Left:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Left: Quadtree-like patch hierarchy in the first image. Right: one possible displacement of corresponding patches in the second image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2. Given an initial pair of two matching regions, retrieving atomic patch correspondences is then done in a top-down fashion (i.e. by recursively applying Eq. (2) to the quadrant's positions {p i }).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Correlation maps for patches of different size. Middle-left: correlation map of a 4x4 patch. Bottom-right: correlation map of a 16x16 patch obtained by aggregating correlation responses of children 8x8 patches (bottom-left), themselves obtained from 4x4 patches. The map of the 16x16 patch is clearly more discriminative than previous ones despite the change in appearance of the region. image1 Fig. 5 A patch I N,p from the first image (blue box) and one of its 4 quadrants I N 2 ,p+ N 4 o3 (red box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Computing the multi-level correlation pyramid. Starting with the bottom-level correlation maps, seeFigure 3, they are iteratively aggregated to obtain the upper levels. Aggregation consists of max-pooling, subsampling, computing a shifted average and non-linear rectification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Given a set M of such tuples, let B(M) be the union of the sets B(c) for all c ∈ M. Note that if all candidate correspondences c ∈ M corresponds to atomic patches, then B(M) = M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>Matching result between two images with repetitive textures. Nearly all output correspondences are correct. Wrong matches are due to occluded areas (bottom-right of the first image) or situations where the deformation tolerance of Deep-Matching is exceeded (bottom-left of the first image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7</head><label>7</label><figDesc>Backtracking atomic correspondences from an entry point (red dot) in the top pyramid level (left). At each level, the backtracking consists in undoing the aggregation performed previously in order to recover the position of the four children patches in the lower level. When the bottom level is reached, we obtain a set of correspondences for atomic patches (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>Extent of the tolerance of DeepMatching to deformations. From left to right: up-scale of 1.5x, down-scale of 0.5x, rotation of 26 o . The plain gray (resp. dashed red) square represents the patch in the reference (resp. target) image. For clarity, only the corner pixels are maximally deformed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>) warping Sampled from the set of feasable warpings W Random warpings over the same region</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10</head><label>10</label><figDesc>Histogram over smoothness for identity warping, warping respecting the built-in constraints in DeepMatching and random warping. The x-axis indicates the smoothness value. The smoothness value is low when there are few discontinuities, i.e., the warpings are smooth. The histogram is obtained with 10,000 different artificial warpings. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Time and space complexity. DeepMatching has a complexity O(LL ) in memory and time, where L = W H and L = W H are the number of pixels per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>#</head><label></label><figDesc>Input: I, I are the images to be matched Initialize an empty set M = {} of correspondences For σ ∈ {−2, −1.5 . . . , 1.5, 2} do σ 1 ← max 1, 2 +σ # either downsize image 1 σ 2 ← max (1, 2 −σ ) # or downsize image 2 For θ ∈ 0, π 4 . . . , 7π 4 do # get raw atomic correspondences (Eq. (15)) M σ,θ ← DeepMatching Iσ 1 , R −θ * I σ2 # Geometric rectification to the input image space: M R σ,θ ← (σ1p, σ 2 R θ p , s) | ∀(p, p , s) ∈ M σ,θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 14</head><label>14</label><figDesc>Performance, memory usage and run-time for different levels of compression corresponding to the size D of the prototype dictionary (we set the image resolution to R = 1/2). A dictionary size D = ∞ stands for no compression. Run-times are for a single image pair on 1 core @ 3.6 GHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17</head><label>17</label><figDesc>Comparison of different matching methods on three challenging pairs from Kitti. Each pair of columns shows motion maps (left column) and the corresponding error maps (right column). The top row presents the ground-truth (GT) as well as one image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Demetz et al 2014), NLTGB-SC (Ranftl et al 2014), TGV2ADCSIFT (Braux-Zin et al 2013)) or due to local affine estimators (EpicFlow (Revaud et al 2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 18</head><label>18</label><figDesc>Each column shows from top to bottom: two consecutive images, the ground-truth optical flow, the DeepMatching, our flow prediction (DeepFlow ), and two state-of-the-art methods, LDOF<ref type="bibr" target="#b6">(Brox and Malik 2011)</ref> and MDP-Flow2<ref type="bibr" target="#b53">(Xu et al 2012)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Detailed timings of the different stages of Deep-</figDesc><table><row><cell>Matching, measured for a single image pair from MPI-Sintel on</cell></row><row><cell>CPU (1 core @ 3.6GHz) and GPU (GeForce Titan X) in sec-</cell></row><row><cell>onds. Stages are: patch clustering (only for approximate DM, see</cell></row><row><cell>Section 4.1), patch correlations (Eq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Statistics of the different matching methods.</figDesc><table><row><cell>The "#"</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Matching performance, run-time and memory usage for state-of-the-art methods and DeepMatching (DM). For the proposed method, R and D denote the input image resolution and the dictionary size (∞ stands for no compression).</figDesc><table><row><cell>Run-times</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Comparison of matching results of different methods on the Mikolajczyk dataset. Each row shows pixels with correct correspondences for different methods (from top to bottom: ground-truth, SIFT-NN, GPM, NRDC and DM). For each scene, we select two images to match and fade out regions which are unmatched, i.e. those for which the matching error is above 15px or can not be matched. DeepMatching outperforms the other methods, especially on difficult cases like graf and wall.</figDesc><table><row><cell>bark</cell><cell>boat</cell><cell>graf</cell><cell>wall</cell></row><row><cell>Ground-truth</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT-NN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NRDC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DaisyFF</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepMatching</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 15</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Comparison of different matching methods on three challenging pairs with non-rigid deformations from MPI-Sintel. Each pair of columns shows motion maps (left column) and the corresponding error maps (right column). The top row presents the ground-truth (GT) as well as one image. For non-dense methods, pixel displacements have been inferred from matching patches. Areas without correspondences are in black.</figDesc><table><row><cell>Correspondence field</cell><cell cols="3">Image/Error map</cell><cell></cell><cell cols="3">Correspondence field</cell><cell>Image/Error map</cell><cell>Correspondence field</cell><cell>Image/Error map</cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT-NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOG-NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KPM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT-flow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SLS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DaisyFF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DSP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 16 Method</cell><cell></cell><cell>EPE</cell><cell>EPE-occ</cell><cell>s0-10</cell><cell>s10-40</cell><cell>s40+</cell><cell>Time</cell></row><row><cell>FlowFields (Bailer et al 2015)</cell><cell></cell><cell>5.810</cell><cell>31.799</cell><cell>1.157</cell><cell>3.739</cell><cell>33.890</cell><cell>23s</cell></row><row><cell>DiscreteFlow (Menze et al 2015)</cell><cell></cell><cell>5.810</cell><cell>31.799</cell><cell>1.157</cell><cell>3.739</cell><cell>33.890</cell><cell>180s</cell></row><row><cell>EpicFlow (Revaud et al 2015)</cell><cell></cell><cell>6.285</cell><cell>32.564</cell><cell>1.135</cell><cell>3.727</cell><cell>38.021</cell><cell>16.4s</cell></row><row><cell>TF+OFM (Kennedy and Taylor 2015)</cell><cell></cell><cell>6.727</cell><cell>33.929</cell><cell>1.512</cell><cell>3.765</cell><cell>39.761</cell><cell>∼400s</cell></row><row><cell>DeepFlow</cell><cell></cell><cell>6.928</cell><cell>38.166</cell><cell>1.182</cell><cell>3.859</cell><cell>42.854</cell><cell>25s</cell></row><row><cell cols="2">SparseFlowFused Timofte and Van Gool (2015)</cell><cell>7.189</cell><cell>3.286</cell><cell>1.275</cell><cell>3.963</cell><cell>44.319</cell><cell>20</cell></row><row><cell>DeepFlow* (Weinzaepfel et al 2013)</cell><cell></cell><cell>7.212</cell><cell>38.781</cell><cell>1.284</cell><cell>4.107</cell><cell>44.118</cell><cell>19s</cell></row><row><cell>S2D-Matching (Leordeanu et al 2013)</cell><cell></cell><cell>7.872</cell><cell>40.093</cell><cell>1.172</cell><cell>4.695</cell><cell>48.782</cell><cell>∼2000s</cell></row><row><cell>LocalLayering (Sun et al 2014a)</cell><cell></cell><cell>8.043</cell><cell>40.879</cell><cell>1.186</cell><cell>4.990</cell><cell>49.426</cell></row><row><cell>Classic+NL-P (Sun et al 2014b)</cell><cell></cell><cell>8.291</cell><cell>40.925</cell><cell>1.208</cell><cell>5.090</cell><cell>51.162</cell><cell>∼800s</cell></row><row><cell>MDP-Flow2 (Xu et al 2012)</cell><cell></cell><cell>8.445</cell><cell>43.430</cell><cell>1.420</cell><cell>5.449</cell><cell>50.507</cell><cell>709s</cell></row><row><cell>NLTGV-SC (Ranftl et al 2014)</cell><cell></cell><cell>8.746</cell><cell>42.242</cell><cell>1.587</cell><cell>4.780</cell><cell>53.860</cell></row><row><cell>LDOF (Brox and Malik 2011)</cell><cell></cell><cell>9.116</cell><cell>42.344</cell><cell>1.485</cell><cell>4.839</cell><cell>57.296</cell><cell>30s</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This amounts to the cross-correlation of the patch and I .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">,p+s N,i 2 Note that I N,p only roughly corresponds to a N × N square patch centered at 2 p in I , due to subsampling and possible deformations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Although the backtracking is conceptually close to the backpropagation training algorithm, it differs in term of how the scores are accumulated for each path.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We implemented this method ourselves.5  We used the online code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that this systematically improves the endpoint error compared to using the raw dense correspondence fields as flow.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the European integrated project AXES, the MSR/INRIA joint project, the LabEx PERSYVAL-Lab (ANR-11-LABX-0025), and the ERC advanced grant ALLEGRO.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<title level="m">Flow Fields: Dense Correspondence Fields for Highly</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Accurate Large Displacement Optical Flow Estimation 5.3.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJCV 2.2, 5.1</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The generalized PatchMatch correspondence algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In: ECCV 1, 2.1, 2.2, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI. Foundations and Trends in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A general dense image matching framework combining direct and feature-based costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Braux-Zin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<idno>ICCV 5.3.3</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans PAMI (document)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>2.1, 2.2, 4.3, 5.2.4, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>In: ECCV 2.2, 4.3</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational optical flow computation in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Image</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Processing 2.2, 4.3</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (document), 5.1</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>CVPR 1</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno>CVPR 1</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning brightness transfer functions for the joint recovery of illumination changes and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Demetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<idno>ECCV 5.3.3</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A hierarchical non-parametric method for capturing non-rigid deformations. Image and Vision Computing 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computer Vision: A Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards internet-scale multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The KITTI dataset. IJRR (document)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Non-rigid dense correspondence with applications for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>SIGGRAPH 1, 2.1, 4.2, 5.1, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On sifts and their scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mayzels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In: CVPR 2.2, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Determining Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bkp</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arXiv:14085093 5.2.3</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In: EMM-CVPR 5.3.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deformation models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gollan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In: CVPR 2.1, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Coherency sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Korman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>2.1, 3.2, 3.4</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Locally affine sparse-to-dense matching for motion and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In: ICCV 2.2, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SIFT flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IEEE Trans PAMI 2.2, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>IJCV 1, 2.1, 2.2, 3.1, 3.2, 4.2, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A: Optics, Image Science, and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discrete Optimization for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In: GCPR 5.3.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>IJCV (document), 2.1, 4.2, 5.1, 6</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application VISSAPP&apos;09)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>INSTICC Press 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Descriptor learning for efficient retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Non-local total generalized variation for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In: ECCV 5.3.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In: CVPR 5.3.3, 5.3.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adaptive integration of feature matches into variational optical flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In: ACCV 4.3, 4.3</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno>CVPR 5.3.3</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IJCV 2.2, 4.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Computing nearest-neighbor fields via propagationassisted kd-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In: CVPR 2.1, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Computer Vision: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>2.1, 4.2</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sparse flow: Sparse matching for small to large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
	<note>WACV) 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A fast local descriptor for dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">DAISY: An Efficient Dense Descriptor Applied to Wide Baseline Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IEEE Trans PAMI 3.2, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A monotonic and continuous twodimensional warping based on dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<idno>ICPR 2.1</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An evaluation of data costs for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>GCPR 2.2</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Piecewise rigid scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno>ICCV 5.3.3</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Structure-and motion-adaptive regularization for high accuracy optic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno>ICCV 4.3</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV 1, 3.3, 3.3, 5.2.2, 5.3.3, 6, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<title level="m">Anisotropic Huber-L1 optical flow</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A feature-based approach for dense segmentation and estimation of large disparity motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Trans PAMI 1, 2.2, 4.3, 4.3, 5.3.3</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">DAISY filter flow: A generalized discrete approach to dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In: CVPR 2.1, 5.2.4</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Iterative solution of large linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rheinboldt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Academic Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Optic flow in harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJCV 4.3, 4.3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
