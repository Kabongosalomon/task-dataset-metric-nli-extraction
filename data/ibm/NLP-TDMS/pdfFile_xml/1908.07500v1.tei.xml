<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Synthesis From Reconfigurable Layout and Style</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE and the Visual Narrative Initiative</orgName>
								<orgName type="institution">North Carolina State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
							<email>tianfuwu@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE and the Visual Narrative Initiative</orgName>
								<orgName type="institution">North Carolina State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Synthesis From Reconfigurable Layout and Style</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a longstanding problem to learn generative models that are capable of synthesizing realistic and sharp images from reconfigurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout-and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation and Objective</head><p>Remarkable recent progress has been made on both unconditional and conditional image synthesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. The former aims to generate highfidelity images from some random latent codes. The latter needs to do so with given conditions satisfied in terms of some consistency metrics. The conditions may take many forms such as categorical labels, desired attributes, descriptive sentences, scene graphs, and paired or unpaired images/semantic maps. From the perspective of generative learning, the solution space of the latter is much difficult to capture than that of the former. Conditional image synthesis, especially with coarse yet complicated and reconfigurable conditions, remains a long-standing problem. Once powerful systems are developed, they can facilitate to pave a way for computers to truly understand visual patterns via analysis-by-synthesis. They will also enable a wide range of practical applications, e.g., generating high-fidelity data for long-tail scenarios in different vision tasks such as autonomous driving.</p><p>In this paper, we are interested in conditional image synthesis from layout and style. The layout consists of labeled bounding boxes configured in an image lattice (e.g., 64×64 or 128 × 128). The style is represented by some latent code. Layout represents a sweet yet challenging spot for   conditional image synthesis: First, layout is usually used as the intermediate representation for other conditional image synthesis such as text-to-image <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref> and scene-graphto-image <ref type="bibr" target="#b15">[16]</ref>. Second, layout is more flexible, less constrained and easier to collect than semantic segmentation maps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>. Third, layout-to-image requires addressing challenging one-to-many mapping and consistent multiobject generation (e.g., occlusion handling for overlapped bounding boxes and uneven, especially long-tail distributions of objects).</p><formula xml:id="formula_0">: , o b j 1 ℓ 1 b b o x 1 : , o b j 2 ℓ 2 b b o x 2 : , o b j m ℓ m b b o x m ⋯ ... ...</formula><p>Layout-to-image is a relatively new task with many new technical challenges for state-of-the-art image synthesis frameworks and only a few work have been proposed in the very recent literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref>. Recently, we have seen remarkable progress on the high-fidelity class-conditional image synthesis in ImageNet by the BigGAN <ref type="bibr" target="#b0">[1]</ref>, and on the amazing style control for specific objects (e.g., faces and cars) by the unconditional StyleGAN <ref type="bibr" target="#b17">[18]</ref> (which may be considered as implicitly conditional image synthesis since only one category is usually handled in training). Despite the big successes in generative learning, the problem considered in this paper is still more challenging since the solution space is much more difficult to capture and has much more complicated distributions. For example, we can use the BigGAN to generate a cat image, and as long as the generated image looks realistic and sharp, we think it does a great job. Similarly, we can use the StyleGAN to generate a face image, and we are happy (even shocked sometimes) if a realistic and sharp face image is generated with a natural style (e.g., smile or sad). Layout-to-image needs to tackle many spatial and semantic (combinatorial) relationships among multiple objects besides the naturalness.</p><p>In this paper, we further focus on image synthesis from reconfigurable layout and style. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of layout and style latent code ( <ref type="figure" target="#fig_0">Figure 1</ref>). State-of-the-art methods on reconfigurable layout-to-image still mainly focus on low resolution (64 × 64) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref> (which are, in part, due to computationally expensive designs in the pipelines such as convolutional LSTM used in <ref type="bibr" target="#b37">[38]</ref>). Beside the resolution issue, another drawback of existing methods is that the diversity of generated images (i.e., style control) is not sufficiently high to preserve the intrinsic one-to-many mapping. We aim to improve both the resolution and the style diversity in reconfigurable layout-to-image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Method Overview</head><p>To address the challenges in layout-to-image and inspired by the recent StyleGANs <ref type="bibr" target="#b17">[18]</ref>, we present a LayOutand STyle-based architecture for GANs (termed LostGANs) in the paper ( <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>First, since layout-to-image entails highly expressive neural architectures handling multi-object generation and their diverse occurrence and configurations in layouts. We utilize ResNet <ref type="bibr" target="#b7">[8]</ref> for both the generator and discriminator in the proposed LostGAN, as done in the projection-based cGAN <ref type="bibr" target="#b23">[24]</ref> and BigGAN <ref type="bibr" target="#b0">[1]</ref>.</p><p>Second, to account for the gap between bounding boxes in a layout and underlying object shapes, we introduce an encoder for layout to predict masks for each bounding box. As we will show in experiments, our LostGAN can predict reasonably good masks in a weakly-supervised manner. The masks help place objects in the generated images with fine-grained geometric properties. So, we address layout-to-image by computing layout-to-mask-to-image ( <ref type="figure">Figure 3</ref>), which is motivated by impressive recent progress on conditional image synthesis from semantic label maps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Third, to achieve instance-sensitive and layout-aware style control, we extend the Adaptive Instance Normalization (AdaIN) used in the StyleGAN <ref type="bibr" target="#b17">[18]</ref> to object instancespecific and layout-aware feature normalization (ISLA-Norm) for the generator for fine-grained spatially distributed multi-object style control. ISLA-Norm computes the mean and variance as done in BatchNorm <ref type="bibr" target="#b13">[14]</ref>, but computes object instance-specific and layout-aware affine transformations (i.e., gamma and beta parameters) separately for each sample in a min-batch as done in AdaIN ( <ref type="figure">Figure 3</ref>). We utilize the projection-based approach proposed in <ref type="bibr" target="#b0">[1]</ref>. From the layout encoder, we compute object instance-specific style latent codes (gamma and beta parameters) via simple linear projection. Then, we place the projection-based latent codes in the corresponding predicted masks, and thus induce layout-aware affine transformations for recalibrating normalized feature responses.</p><p>Lastly, we utilize both image and object adversarial hinge losses <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref> as adopted in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> in the end-toend training. Object adversarial loss follows the projection based method in <ref type="bibr" target="#b23">[24]</ref> which is the state-of-the-art approach for embedding labels.</p><p>We deliberately try to keep our LostGAN as simple as possible by exploiting the best practices in the literature of conditional image synthesis. We hope it can stimulate more exploration on this relatively new task, image synthesis from reconfigurable layout and style.</p><p>In experiments, our LostGAN is tested in the COCO-Stuff dataset <ref type="bibr" target="#b1">[2]</ref> and the Visual Genome (VG) dataset <ref type="bibr" target="#b19">[20]</ref>. It obtains state-of-the-art performance on both datasets in terms of the inception score <ref type="bibr" target="#b29">[30]</ref>, Frèchet Inception Distance <ref type="bibr" target="#b8">[9]</ref>, diversity score <ref type="bibr" target="#b36">[37]</ref>, and classification accuracy <ref type="bibr" target="#b27">[28]</ref>, which supports the effectiveness of our ILSA-Norm and LostGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Conditional Image Synthesis. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b5">[6]</ref> have achieved great success in image synthesis conditioned on additional input information (i.e. class information <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>, source image <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13</ref>], text description <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>, etc). How to feed conditional information to model has been studied in various ways. In <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> vector encoded from conditional information concatenated with noise vector is passed as input to generator. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26]</ref>, conditional information is provided to generator by conditional gains and bias in BatchNorm <ref type="bibr" target="#b13">[14]</ref> layers. Concurrent work <ref type="bibr" target="#b25">[26]</ref> learns spatially adaptive normalization from well annotated semantic masks, while our proposed ISLA-Norm learns from coarse layout information. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> feed the conditional information into discriminator by naively concatenation with the input or intermediate feature vector. In <ref type="bibr" target="#b23">[24]</ref>, projection based way to incorporate conditional information to discriminator effectively improve the quality of class conditional image generation. In our proposed method, layout condition is adopted to generator with ISLA-Norm, and objects information is utilized in projection based discriminator as <ref type="bibr" target="#b23">[24]</ref>.</p><p>Image Synthesis from Layout. Spatial layout conditioned image generation has been studied in recent literature. In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>, layout and object information is utilized in text-to-image generation. <ref type="bibr" target="#b10">[11]</ref> controls location of multiple objects in text-to-image generation by adding an object pathway to both the generator and discriminator. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref> performs text-to-image synthesis in two steps: semantic layout (class label and bounding boxes) generation from text first, and image synthesis conditioned on predicted semantic layout and text description. However, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> requires pixel-level instance segmentation annotation, which is labor intensive to collect, for training of shape generator, while our method does not require pixel-level annotation and can learn segmentation mask in a weaklysupervised manner. <ref type="bibr" target="#b37">[38]</ref> studied similar task with us, where variational autoencoders based network is adopted for scene image generation from layout. Our Contributions. This paper makes the following main contributions to the field of conditional image synthesis.</p><p>• It presents a layout-and style-based architecture for GANs (termed LostGANs) which integrates the best practices in conditional and unconditional GANs for a relatively new task, image synthesis from reconfigurable layout and style.</p><p>• It presents an object instance-specific and layout-aware feature normalization scheme (termed ISLA-Norm) which is inspired by the projection-based conditional BatchNorm used in cGANs <ref type="bibr" target="#b0">[1]</ref> and the Adaptive Instance Normalization (AdaIN) used in StyleGAN <ref type="bibr" target="#b17">[18]</ref>. It explicitly accounts for the layout information in the affine transformations.</p><p>• It shows state-of-the-art performance in terms of the inception score <ref type="bibr" target="#b29">[30]</ref>, Frèchet Inception Distance <ref type="bibr" target="#b8">[9]</ref>, diversity score <ref type="bibr" target="#b36">[37]</ref> and classification accuracy <ref type="bibr" target="#b27">[28]</ref> on two widely used datasets, the COCO-Stuff <ref type="bibr" target="#b1">[2]</ref> and the Visual Genome <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>In this section, we first define the problem and then present details of our LostGAN and ISLA-Norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Denote by Λ an image lattice (e.g., 64 × 64). Let</p><formula xml:id="formula_1">L = {( i , bbox i ) m i=1</formula><p>} be a layout consisting of n labeled bounding boxes, where label i ∈ C (e.g., |C| = 171 in the COCO-Stuff dataset), and bounding box bbox i ⊆ Λ. Different bounding boxes may have occlusions. Let z img be the latent code controlling image style and z obji the latent code controlling object instance style for ( i , bbox i ) (e.g., the latent codes are sampled from the standard normal distribution, N (0, 1) under i.i.d. setting). Denote by Z obj = {z obji } m i=1 the set of object instance style latent codes.</p><p>Image synthesis from layout and style is the problem of learning a generation function which is capable of synthesizing an image defined on λ for a given input (L, z img , Z obj ),</p><formula xml:id="formula_2">I = G(L, z img , Z obj ; Θ G )<label>(1)</label></formula><p>where Θ G represents the parameters of the generation function. Ideally, G(·) is expected to capture the underlying conditional data distribution p(I|L, z img , Z obj ) in the highdimensional space.</p><p>Reconfigurability of G(·). We are interested in three aspects in this paper:</p><p>• Image style reconfiguration: If we fix the layout L, is G(·) capable of generating images with different styles for different (z img , Z obj )?</p><p>• Object style reconfiguration: If we fix the layout L, the image style z img and object styles Z obj \ z obji , is G(·) capable of generating consistent images with different styles for the object ( i , bbox i ) using different z obji ?</p><p>• Layout reconfiguration: Given a (L, z img , Z obj ), is G(·) capable of generating consistent images for different (L + , z img , Z + obj ) where we can add a new object to L + or just change the bounding box location of an existing object? When a new object is added, we also sample a new z obj to add in Z + obj . It is a big challenge to address the three aspects by learning a single generation function. It may be even difficult for well-trained artistic people to do so at scale (e.g., handling the 171 categories in the COCO-Stuff dataset). Due to the complexity that the generation function (Eqn. 1) needs to handle, it is parameterized (often over-parameterized) by powerful deep neural networks (DNNs). It is also wellknown that training the DNN-based generation function individually is a extremely difficult task. Generative adversarial networks (GANs) <ref type="bibr" target="#b5">[6]</ref> are entailed which are formulated under two-player minmax game settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The LostGAN</head><p>As <ref type="figure" target="#fig_1">Figure 2</ref> shows, our LostGAN follows the traditional GAN pipeline with the following modifications. <ref type="figure">Figure 3</ref> (a) shows the generator which utilizes the ResNet <ref type="bibr" target="#b7">[8]</ref> architecture as backbone. Consider generating 64×64 images, the generator consists of 4 residual building blocks (ResBlocks). The image style latent code z img is a d noise -dim vector (d noise = 128 in our experiments) whose elements are sampled from standard normal distribution under i.i.d. setting. Through a linear fully connected (FC) layer, z img is projected to a 4 × 4 × (16 × ch) dimensional vector which is then reshaped to (4, 4, 16×ch) (representing height, width and channels) where ch is a hyperparameter to control model complexity (e.g., ch = 64 for generating 64 × 64 images). Then, each of the four ResBlocks upsamples its input with ratio 2 and bilinear interpolation. In the meanwhile, the feature channel will be decreased by ratio 2. For generating 128 × 128 images, we use 5 ResBlocks with ch = 64 and the same d noise = 128 for z img . <ref type="figure">Figure 3</ref> (b) shows the detail of ResBlock and the proposed ISLA-Norm. The ResBlock uses the basic block design as adopted in the projection-based cGAN <ref type="bibr" target="#b23">[24]</ref> and Big-GAN <ref type="bibr" target="#b0">[1]</ref>. Our ISLA-Norm first computes the mean and variance as done in BatchNorm <ref type="bibr" target="#b13">[14]</ref>, and then learns object instance-specific layout-aware affine transformation for each sample in a batch similar in spirit to the AdaIN used by the StyleGAN <ref type="bibr" target="#b17">[18]</ref>. So, the feature normalization is computed in a batch manner, and the affine transformation is recalibrated in a sample-specific manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">The ISLA-Norm</head><p>Denote by x the input 4D feature map of ISLA-Norm, and x nhwc the feature response at position (n, h, w, c) (using the convention order of axes for batch, spatial height and width axis, and channel). We have n ∈ In training, ISLA-Norm first normalizes x nhwc by,</p><formula xml:id="formula_3">x nhwc = x nhwc − µ c σ c ,<label>(2)</label></formula><p>where the channel-wise batch mean µ c = 1 N ·H·W n,h,w x nhwc and standard deviation (std) σ c = 1 N ·H·W n,h,w (x nhwc − µ c ) 2 + ( is a small positive constant for numeric stability). In standard Batch-Norm <ref type="bibr" target="#b13">[14]</ref>, for the affine transformation, a channel-wise γ c and β c will be learned and shared with all spatial locations and all samples in a batch. our ISLA-Norm will learn object instance-specific and layout-aware affine transformation parameters, γ nhwc and β nhwc , and then recalibrate the normalized feature responses by,</p><formula xml:id="formula_4">x nhwc = γ nhwc ·x nhwc + β nhwc .<label>(3)</label></formula><p>Computing γ nhwc and β nhwc . Without loss of generality, we show how to compute the gamma and beta parameters for one sample, i.e., γ hwc and β hwc . As shown in <ref type="figure">Figure 3</ref> (b), we have the following four steps. i) Label Embedding. We use one-hot label vector for the m object instances and then we obtain the m × d one-hot label matrix (e.g., d = 171 in COCO-Stuff). For label embedding, we use a learnable d × d e embedding matrix to obtain the vectorized representation for labels, resulting in the m × d e label-to-vector matrix, where d e represents the embedding dimension (e.g., d e = 128 in our experiments). We also have the object style latent codes Z obj which is a m × d noise noise matrix (e.g., d noise = 128 the same as z img ). We then concatenate the label-to-vector matrix and the noise matrix as the final m × (d e + d noise ) embedding matrix. So, the object instance style will depends on both the label embedding (semantics) and i.i.d. latent code (accounting for style variations).</p><p>ii) Object instance-specific projection. With the final embedding matrix, we compute object instance-specific channel-wise γ and β via linear projection with a learnable (d e +d noise )×2C projection matrix where C is the number of channels.</p><p>iii) Mask prediction. The s × s mask for each object instance (e.g., s = 16 in our experiments) is predicted by a sub-network consisting of several up-sample convolution followed by sigmoid transformation. So, our predicted masks are not binary. Then, we resize the predicted masks to the sizes of corresponding bounding boxes.</p><p>iv) ISLA γ and β computation. We unsqueeze the object instance-specific channel-wise γ and β to their corresponding bounding boxes with the predicted mask weights multiplied. Then, we add them together with averaged sum used for overlapping regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The Discriminator</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our discriminator consists of three components: the shared ResNet backbone, the image head classifier and the object head classifier.</p><p>The ResNet backbone has several ResBlocks (4 for 64×64 and 5 for 128×128) as in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref>. The image head classifier consists of a ResBlock, a global average pooling layer and a fully-connected (FC) layer with one output unit, while object head classifier consists of ROI Align <ref type="bibr" target="#b6">[7]</ref>, a global average pooling layer and a FC layer with one output unit.</p><p>Following the projection-based cGANs <ref type="bibr" target="#b23">[24]</ref> and the practice in BigGANs <ref type="bibr" target="#b0">[1]</ref>, we learn a separate label embedding for computing object adversarial hinge loss.</p><p>Denote by D(·; Θ D ) the discriminator with parameters Θ D . Given an image I (real or synthesized) and a layout L, the discriminator computes the prediction score for image and the average score for cropped objects, and we have,</p><formula xml:id="formula_5">(s img , s obj ) = D(I, L; Θ D )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">The Loss Functions</head><p>To train (Θ G , Θ D ) in our LostGAN, we utilize the hinge version <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref> of the standard adversarial loss <ref type="bibr" target="#b5">[6]</ref>,</p><formula xml:id="formula_6">l t (I, L) = min(0, −1 + s t ); if I is real min(0, −1 − s t ); if I is fake (5)</formula><p>where t ∈ {img, obj}. Let l(I, L) = l img (I, L) + λ · l obj (I, L) with λ the trade-off parameter for controlling the quality between synthesized images and objects (λ = 1 in our experiments). We have the expected losses for the discriminator and the generator,</p><formula xml:id="formula_7">L(Θ D |Θ G ) = − E (I,L)∼p(I,L) [l(I, L)] L(Θ G |Θ D ) = − E (I,L)∼p f ake (I,L) [D(I, L; Θ D )]<label>(6)</label></formula><p>where p(I, L) represents all the real and fake (by the current generator) data and p f ake (I, L) represents the fake data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We test our LostGAN in the COCO-Stuff dataset <ref type="bibr" target="#b1">[2]</ref> and the Visual Genome (VG) dataset <ref type="bibr" target="#b19">[20]</ref>. We evaluate it for generating images at two resolutions 64×64 and 128×128. In comparison, the state-of-the-art methods include the very recent Layout2Im method <ref type="bibr" target="#b37">[38]</ref>, the scene graph to image (sg2im) method <ref type="bibr" target="#b15">[16]</ref> and the pix2pix method <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The COCO-Stuff 2017 <ref type="bibr" target="#b1">[2]</ref> augments the COCO dataset with pixel-level stuff annotations. The annotation contains 80 thing classes (person, car, etc.) and 91 stuff classes (sky, road, etc.) Following settings of <ref type="bibr" target="#b15">[16]</ref>, objects covering less than 2% of the image are ignored, and we use images with 3 to 8 objects. The Visual Genome dataset <ref type="bibr" target="#b19">[20]</ref>. Following settings of <ref type="bibr" target="#b15">[16]</ref> to removing small and infrequent objects, we have 62,565 training, 5,506 val and 5,088 testing images with 3 to 30 objects from 178 categories in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We evaluate quality and visual appearance of generated images by Inception Score (higher is better) <ref type="bibr" target="#b29">[30]</ref> and Frèchet Inception Distance (FID, lower is better) <ref type="bibr" target="#b9">[10]</ref>, which use pretrained Inception <ref type="bibr" target="#b30">[31]</ref> network to encourage recognizable objects within images and diversity across images. Diversity score computes perceptual similarity between two images (higher is better). We adopt LPIPS metric <ref type="bibr" target="#b36">[37]</ref> to compute perceptual similarity in feature space between two images generated from same layout as diversity score. We also evaluate our model by recently proposed Classification Accuracy Score (CAS) <ref type="bibr" target="#b27">[28]</ref>.   FID, diversity score and classification accuracy. Our Lost-GAN outperforms the most recent Layout2Im <ref type="bibr" target="#b37">[38]</ref> in terms of both Inception score and Diversity score. For 64×64 images, the improvement of Inception score, FID and classi-fication accuracy indicates higher visual quality of image generated by our model. Diversity score is improved significantly which shows that our LostGAN can generate images with various appearance for a given layout. We also  <ref type="table" target="#tab_1">Table 1</ref>. Quantitative comparisons using Inception Score (higher is better), FID (lower is better) and Diversity Score (higher is better) evaluation on COCO-Stuff and VG dataset. Images for pix2pix <ref type="bibr" target="#b14">[15]</ref>, sg2im <ref type="bibr" target="#b15">[16]</ref> and Layout2Im <ref type="bibr" target="#b37">[38]</ref> are at 64×64 resolution.    conduct experiments at the resolution of 128×128, and our LostGAN obtains consistently better results.  <ref type="figure">Figure 8</ref>. Synthesized images and learned masks for given layouts. Our proposed model learns masks from given layout in a weaklysupervised manner as ground truth mask for each object is not utilized during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative results</head><formula xml:id="formula_8">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k)</formula><formula xml:id="formula_9">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Classification  <ref type="table">Table 2</ref>. Classification Accuracy Comparisons. We train resnet-101 on cropped objects from generated images (generate five samples for each layout) and evaluate on objects from real images. <ref type="figure" target="#fig_5">Figure 4</ref> shows results of different models generating images from the same layout on both COCO-Stuff and VG. The input layouts are quite complex. Our LostGAN can generate visually more appealing images with more recognizable objects that are consistent with input layouts at resolution 64×64, and is further capable of synthesizing images at 128 × 128 resolution with better image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative results</head><p>We also conduct some ablation studies on the three aspects of reconfigurability and mask prediction.</p><p>Layout reconfiguration is demonstrated by adding object to or moving a bounding box in a layout ( <ref type="figure" target="#fig_7">Figure 5</ref>). Our LostGAN shows better layout reconfigurability than the Layout2Im <ref type="bibr" target="#b37">[38]</ref>. When adding extra objects or moving bounding box of one instance, our model can generate reasonable objects at desired position while keeping existing objects unchanged as we keep the input style of existing objects fixed. When moving bounding box of one object, style of generated object in new position can also be kept consistent, like (f) and (g), the person is moved while keep style feature like pose and color of clothes unaffected.</p><p>Image style reconfiguration To assess diversity of generation, multiple images are sampled from our LostGAN for each input layout ( <ref type="figure" target="#fig_8">Figure 6</ref>). Our model can synthesize images with different visual appearance for a given layout while preserving objects at desired location.</p><p>Object instance style reconfiguration Our LostGAN is also capable of controlling styles at object instance level. <ref type="figure" target="#fig_9">Figure 7</ref> shows results of gradually morphing styles of one instance in different images. Top row shows how the style of sky gradually turns from blue to dusk while keeping styles of other objects unaltered. Bottom row displays how the style of grass transforms from green to withered.</p><p>Weakly-supervised mask prediction <ref type="figure">Figure 8</ref> shows generated semantic label map when synthesizing images from given layouts. For pixels where bounding boxes of different objects overlap, their semantic labels are assigned by objects with the highest predicted mask weight. Unlike <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> where ground truth masks is adopted to guide learning of shape generator, our model can learn semantic masks in a weakly-supervised manner. Even for objects with overlapped bounding box, like person and surfboard in (f), synthesized images and learned masks are consistent and semantically reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a layout-and style-based architecture for generative adversarial networks (LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. The proposed LostGAN can learn fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and proposes the object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. State-of-the-art performance is obtained on COCO-Stuff and VG dataset. Qualitative results demonstrate the proposed model is capable of generating scene images with reconfigurable layout and instance-level style control.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the proposed method. Left: Our model preserves one-to-many mapping for image synthesis from layout and style. Three samples are generated for each input layout by sampling the style latent codes. Right: Our model is also adaptive w.r.t. reconfigurations of layouts (by adding new object bounding boxes or changing the location of a bounding box). The results are generated at resolution 128 × 128. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed layout-and style-based GANs (LostGANs) for image synthesis from reconfigurable layout and style. Both the generator and discriminator use ResNets as backbones. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 mFigure 3 .</head><label>13</label><figDesc>Illustration of the generator (a) and the ISLA-Norm (b) in our LostGAN. See text for details. Best viewed in magnification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[0, N − 1], h ∈ [0, H −1], w ∈ [0, W −1] and c ∈ [0, C −1] where H, W, C depend on the stage of a ResBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Generated samples from given layouts on COCO-Stuff (top) and Visual Genome (bottom). Images generated by pix2pix, sg2im, and layout2im are at 64×64 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Generation results by adding new objects or change spatial position of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Multiple samples generated from same layout. Synthesized images have various visual appearance while preserving objects at desired location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Linear interpolation of instance style. Top row indicates interpolation of style in sky, bottom row shows style morphing of grass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>2 summarizes comparisons between our model and state-of-the-art models with respect to inception score,</figDesc><table><row><cell>Layout</cell><cell>clouds sea boat skycraper building boat bridge</cell><cell>mountain horse dirt grass clothes person tree</cell><cell>clouds tree cow cow grass</cell><cell>mirror sink</cell><cell>wall toilet</cell><cell>playfield person</cell><cell cols="2">fence clouds person clouds building airplane mountain</cell><cell>tree giraffe wood hill</cell><cell>clouds giraffe grass</cell><cell>wall plastic</cell><cell>person food</cell><cell>bush rock zebra zebra zebra</cell><cell>grass person person</cell><cell>person dirt</cell><cell>dirt elephant elephant bush</cell></row><row><cell>pix2pix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sg2im</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layout2im</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell cols="2">(d)</cell><cell cols="2">(e)</cell><cell>(f)</cell><cell>(g)</cell><cell></cell><cell></cell><cell>(h)</cell><cell>(i)</cell><cell cols="2">(j)</cell><cell>(k)</cell></row><row><cell>Layout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pix2pix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sg2im</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layout2im</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell cols="2">(d)</cell><cell cols="2">(e)</cell><cell>(f)</cell><cell>(g)</cell><cell></cell><cell></cell><cell>(h)</cell><cell>(i)</cell><cell cols="2">(j)</cell><cell>(k)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported in part by ARO grant W911NF1810295, NSF IIS-1909644, Salesforce Inaugural Deep Learning Research Grant (2018) and ARO DURIP grant W911NF1810209. The views presented in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating multiple objects at spatially distinct locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00686</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Objectdriven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10740</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10887</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11389</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
