<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
							<email>luiten@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idil</forename><surname>Esen Zulfikar</surname></persName>
							<email>idil.esen.zuelfikar@rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address Unsupervised Video Object Segmentation (UVOS), the task of automatically generating accurate pixel masks for salient objects in a video sequence and of tracking these objects consistently through time, without any input about which objects should be tracked. Towards solving this task, we present UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking) as a simple and generic algorithm which is able to track and segment a large variety of objects. This algorithm builds up tracks in a number stages, first grouping segments into short tracklets that are spatio-temporally consistent, before merging these tracklets into long-term consistent object tracks based on their visual similarity. In order to achieve this we introduce a novel tracklet-based Forest Path Cutting data association algorithm which builds up a decision forest of track hypotheses before cutting this forest into paths that form long-term consistent object tracks. When evaluating our approach on the DAVIS 2017 Unsupervised dataset we obtain state-of-the-art performance with a mean J &amp;F score of 67.9% on the val, 58% on the test-dev and 56.4% on the test-challenge benchmarks, obtaining first place in the DAVIS 2019 Unsupervised Video Object Segmentation Challenge. UnOVOST even performs competitively with many semi-supervised video object segmentation algorithms even though it is not given any input as to which objects should be tracked and segmented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video Object Segmentation (VOS) aims at automatically generating accurate pixel masks for objects in each frame of a video, then associating those proposed object pixel masks in the successive frames to obtain temporally consistent tracks. VOS has mostly been tackled in a semi-supervised fashion <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>, where the object masks of the objects to be tracked in the first-frame are given, and only those objects need to be tracked and segmented throughout the rest <ref type="figure">Figure 1</ref>. Example results of UnOVOST on three sequences from the DAVIS Unsupervised Dataset. UnOVOST is able to accurately segment and track many diverse objects simultaneously. Frames 1, T 4 , T 2 , <ref type="bibr">3T 4</ref> and T are shown, where T is the number of frames in the sequence. of the video.</p><p>In this paper we tackle VOS in the more general unsupervised setting <ref type="bibr" target="#b4">[5]</ref>. In such a setting we need to detect all of the possible objects in the video and track and segment them throughout the whole video. Results of our method on this task can be seen in <ref type="figure">Figure 1</ref>. In this setting, methods are evaluating against a possibly incomplete set of ground-truth objects. As such methods are not penalized for segmenting more objects than present in the ground-truth. However, the number of predictions that can be made is limited in that predicted masks may not have overlapping pixels, and a maximum number of objects may be proposed across a whole video. As such UVOS methods must seek to segment and track the most salient objects in a video regardless of the category of those objects. Saliency here is defined as the objects that catch and maintain the gaze of a viewer across the whole of the video sequence. The definition of an object is also important and possibly ambiguous. Two important factors in determining objectness are that objects should consistently have common fate, moving together consistently throughout the scene, and that they should also be semanti-cally consistent.</p><p>An algorithm that tackles the UVOS task has many interesting real-world applications. One such example is in robotics and autonomous vehicles where it is of crucial importance to be able to understand the precise location and motion of a huge variety of objects, from far more categories than present in any labeled dataset.</p><p>To solve this UVOS task, we present the UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking) algorithm. This algorithm hierarchically builds up object segmentation tracks in multiple stages (see <ref type="figure">Figure 2</ref>). After obtaining a set of candidate object proposal masks per frame using Mask R-CNN <ref type="bibr" target="#b9">[10]</ref>. We then reduce the set of mask proposals to a set which does not contain overlapping pixels by sub-selecting and clipping the given proposals. In order to perform segment tracking we use two main similarity cues, the spatio-temporal consistency of the mask segments in contiguous frames under optical flow warping, and the appearance-based visual similarity of the mask segments encoded as an object re-identification vector.</p><p>We then develop a novel data-association algorithm using these two similarity cues which accurately merges these mask segments into tracks. Our algorithm works in two stages. The first stage uses only the spatio-temporal consistency cues to merge segments in contiguous frames into short-tracklets which contain segments that are very likely to belong to the same object. In a second stage, we then merge these short-tracklets into long-term consistent tracks using their visual similarity. This two stage process has the benefit that the easy tracking decisions are made early and then fixed, reducing the size of the required search-space for performing data-association, and enabling information to be pooled over segments within a tracklet to better model object properties used for tracking.</p><p>For the second-stage, we propose a novel Forest Path Cutting (FPC) algorithm. This algorithm builds a forest consisting of decision trees of possible track hypotheses. The final set of object tracks is then produced by iteratively cutting paths from this forest, until the forest is divided into a non-conflicting set of paths which are the final tracks. This algorithm is both simple and efficient while being powerful enough to model the combinatorial complexity of the longterm data-association problem.</p><p>When evaluating UnOVOST on the unsupervised DAVIS benchmark dataset <ref type="bibr" target="#b4">[5]</ref> we achieve state-of-the-art results compared to all previous methods, as well as results competitive with semi-supervised methods using the given firstframe mask as guidance for which objects to track and segment. Our method also achieves the first place in the DAVIS 2019 Unsupervised Video Object Segmentation Challenge. When extending our method to the task of Video Instance Segmentation (VIS) by adding classifying our object tracks, we also obtain state-of-the-art results on the YouTube-VIS benchmark and won the 2019 YouTube-VIS challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-Object Unsupervised Video Object Segmentation. The UVOS task (also known as zero-shot VOS) is quite recent, and there are few methods that tackle this task. UVOS <ref type="bibr" target="#b4">[5]</ref> was proposed as a challenge task for the 2019 DAVIS Challenge on Video Object Segmentation. <ref type="bibr" target="#b4">[5]</ref> evaluate the RVOS (Recurrent Video Object Segmentation) <ref type="bibr" target="#b31">[32]</ref> method for the UVOS task. This method uses a number of recurrent neural networks, one over the set of objects, and one over time to generate tracks. Our method, UnOVOST, outperforms RVOS by more than 25 percentage points on the J &amp;F metric on all benchmarks. In the 2019 DAVIS Challenge, our method obtained first place. The second <ref type="bibr" target="#b44">[45]</ref> and third <ref type="bibr" target="#b6">[7]</ref> place methods presented very different approaches to the UVOS task. <ref type="bibr" target="#b44">[45]</ref> propose to run a detector on each frame, as well as a series of single object trackers used to merge the detections into tracks. <ref type="bibr" target="#b6">[7]</ref> adapts <ref type="bibr" target="#b20">[21]</ref> from semi-supervised VOS to UVOS task, while adding a proposal pruning step after a number of initial frames, and then tracking these objects as though this was a semisupervised task. Single-Object Unsupervised Video Object Segmentation. There has been a number of papers tackling singleobject unsupervised video object segmentation (SOUVOS) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. This is inherently a different problem to the multi-object task that we tackle in this paper. SOUVOS is closer related to foreground/background segmentation as it requires only one foreground area to be segmented which often is a grouping of multiple objects into one foreground object. This task is often evaluated on the DAVIS 2016 single object benchmark <ref type="bibr" target="#b26">[27]</ref>. This task requires estimating the single most salient grouping of foreground objects in a video. Methods that tackle this task, such as <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b30">[31]</ref> often perform two class segmentation on an image concatenated with optical-flow. Motion Segmentation. Another related field is motion segmentation. This task differs from video object segmentation in that it only requires the segmentation of objects that are moving <ref type="bibr" target="#b1">[2]</ref>, whereas UVOS requires the segmentation of all objects whether they are moving or not. Motion segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8]</ref> are often based on low-level vision features such as the optical-flow. <ref type="bibr" target="#b7">[8]</ref> adapts Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> to operate on both image and optical-flow input. <ref type="bibr" target="#b39">[40]</ref> extract features from the combination of the image and the optical-flow and clusters these. <ref type="bibr" target="#b2">[3]</ref> develops a twostage model that estimates piece-wise rigid motions, which are then merged into objects. This is evaluated as either a multi-object task, or a foreground/background estimation task often using the FSMB <ref type="bibr" target="#b23">[24]</ref> dataset. Multi-Object Semi-Supervised Video Object Segmentation.</p><p>Semi-Supervised Video Object Segmentation  <ref type="figure">Figure 2</ref>. An overview of the UnOVOST algorithm. From an input video (row 1) a number of object mask proposals per frame are generated, sub-selected and clipped to have non-overlapping pixels (row 2). These are grouped into short-tracklets using the spatio-temporal consistency of these segments under optical-flow (row 3). These tracklets are then merged into long-term consistent object tracks using the tracklets' visual similarity and our novel Forest Path Cutting (FPC) data association algorithm.</p><p>(SSVOS) is where the objects that need to be tracked are given as segmentation masks in the first frame. Algorithms that tackle this task often finetune a segmentation network on the given first frame <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>, or propagate from the given first-frame mask directly to the rest of the video <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. These methods are not able to be easily adapted to UVOS as they rely heavily on the first-frame mask. <ref type="bibr" target="#b20">[21]</ref> is the closest related SSVOS to our method, as it also produces generic object segmentation proposals and links these in time with spatio-temporal and visual similarity cues. However, unlike our method, <ref type="bibr" target="#b20">[21]</ref> finetunes all of its components heavily on the first-frame, uses the given-first frame to guide which objects to track, and performs data association in a simple frame-by-frame fashion. SSVOS is often evaluated on the DAVIS 2017 semi-supervised dataset.</p><p>Video Instance Segmentation and Multi-Object Tracking and Segmentation. Recently, the related tasks of Video Instance Segmentation (VIS) <ref type="bibr" target="#b42">[43]</ref> and Multi-Object Tracking and Segmentation (MOTS) <ref type="bibr" target="#b33">[34]</ref> has been proposed. These tasks are similar to UVOS in that objects need to be tracked and segmented without being given guidance on which particular instances are to be tracked. However, these tasks differs from UVOS in that only objects belonging to a specified categories need to be tracked and segmented, as well as these being classified correctly. This significantly simplifies the task, and limits the applicability of methods that tackle these tasks. MOTS differs from VIS in that in MOTS sequences are much longer and many more instances are present with objects disappearing and reappearing much more often. MOTS is evaluated on the KITTI and MOTChallenge datasets <ref type="bibr" target="#b33">[34]</ref>. VIS on the YouTubeVIS benchmark <ref type="bibr" target="#b42">[43]</ref>. We extend UnOVOST from the UVOS task to the VIS task by classifying our resulting tracks, and also achieve state-of-the-art performance on this task. Category Agnostic Multi-Object Tracking. Previously, a number of methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> have attempted to extent multiobject tracking methods beyond tracking objects from a predefined set of object categories. These methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> have typically relied on the presence of stereo-camera input to obtain 3D information for evaluating the objectness of generic object proposals. These methods also make no attempt to create a set of object tracks without overlapping segment masks, instead they create a large set of track proposals that have large overlap with one-another, often tracking the same object multiple times on different scales. Our method by contrast works on monocular video and creates a set of segmentation tracks without overlapping segment masks.</p><p>Data-Association for Multi-Object Tracking. The task of multi-object tracking (MOT) has a long research history <ref type="bibr" target="#b15">[16]</ref>. The leading paradigm for MOT has become trackingby-detection, where a set of object detections are proposed, and tracking is reduced to a data-association problem. In this paper we propose a new data-association algorithm designed specifically for the UVOS task. Previous dataassociation methods are either too simple or unnecessarily complex. Many methods such as those used in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b20">[21]</ref> only take into account associations in the previous frame, or the previous and first frame <ref type="bibr" target="#b20">[21]</ref>, and do not use the context from the whole video. On the other hand, data-association algorithms such as <ref type="bibr" target="#b13">[14]</ref> are unnecessarily complex in that they produce an exponentially large number of potential track hypotheses and score each of these individually. Our data-association algorithm is able to take advantage of a number of simplifications to be able to use the whole video context to evaluate the likelihood of a tracking hypothesis, while being much simpler and efficient. We take advantage of the fact that mask segments can not overlap to significantly reduce the set of possible tracking combinations. Furthermore, we split tracking into two components, firstly grouping proposals based on spatio-temporal consistency, before only using visual similarity in a second stage. These simplifications in combination with our efficient Forest Path Cutting algorithm, results in an algorithm that is accurate, powerful and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section we detail the specifics of our novel Un-OVOST algorithm for tackling the UVOS task. Our method begins by generating a large set of generic object proposals and sub-selects and clips these to be non-overlapping. These proposals are then grouped over contiguous frames into short-tracklets based on the spatio-temporal consistency of the object proposals under an optical-flow warping. We then merge these tracklets into long-term consistent object tracks using our novel Forest Path Cutting algorithm and visual similarity cues between tracklets. Finally, a final set of object tracks is selected based on their video saliency.</p><p>Unlike previous approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8]</ref> our algorithm is able to segment and track objects regardless of their object class, whether they are static or undergo motion, and whether they are present in the foreground or background of the scene. Our algorithm instead relies on a more general concept of objectness to determine what should consist of an object to be tracked and segmented. An overview can be seen in <ref type="figure">Figure 2</ref>. Object Mask Proposal Generation. We generate a large number of proposals, segmentation masks which cover potential objects, for a diverse range of objects. Specifically we use a Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> implementation by <ref type="bibr" target="#b36">[37]</ref> with a ResNet101 <ref type="bibr" target="#b10">[11]</ref> backbone trained on COCO <ref type="bibr" target="#b19">[20]</ref>. Although this network has been trained to detect the 80 COCO categories, we find that when using a low-confidence threshold this network produces adequate mask proposals for objects beyond these 80 categories. This network produces masks, bounding boxes, object categories and confidence scores for object proposals as outputs. We discard the object categories and treat all detections as if they come from the same foreground object category. We extract all proposal masks with a confidence score greater than 0.1. An example of generated overlapping proposals can be found in <ref type="figure" target="#fig_1">Figure 3</ref>. Proposal Sub-Selection and Clipping. In order to simplify the tracking and segmentation problem, we initially ensure that our set of segment masks do not overlap in each frame individually before tracking these segments throughout the video. All proposal masks in a frame are compared against one another using their intersection over union (IoU) to detect overlaps. If the IoU between two proposal masks is higher than 0.2, then the proposal mask with higher confidence score is kept and the other proposal mask is removed. This is a form of mask-based non-maximum suppression. For all the remaining masks, we clip overlaps so that the mask with the highest confidence score is on top of a mask with a lower score. This set of proposals without overlaps has three advantages, it reduces the number of proposals that need to be tracked, simplifies the matching using temporal consistency cues as conflicting proposals are removed, and removes many spurious masks not belonging to real objects. See <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Forest Path Cutting Algorithm (FPC)</head><p>Data: Tracklets L i with average ReID vectors R i , beginning timestep b i and ending timestep e i , ordered by increasing b i . Result: Tracks F j which are groupings of tracklets.</p><formula xml:id="formula_0">Define:Visual Similarity V i,j V i,j := 1 − R i −R j max m∈{1...T },n∈{1...T } Rm−Rn</formula><p>Part 1: Build a forest of track hypotheses, by calculating optimal predecessors M i for each tracklet L i .</p><formula xml:id="formula_1">for i ∈ {1 . . . |L|} do if {j | e j &lt; b i } = ∅ then k := argmax j|e j &lt;b i {V i,j } while {j | e j &lt; b i , b j &gt; e k , M j = L k } = ∅ do l := argmax j|e j &lt;b i ,j =k {V i,j } if l ∈ {j | e j &lt; b i , b j &gt; e k , M j = L k } then k := l else break end end M i := L k else M i := ∅ end end Part 2:</formula><p>Define the set of track hypotheses H as the paths from root nodes to leaf nodes through the hypothesis forest, and calculate a score C i for each path. Select final tracks F by iteratively cutting the optimal paths from the forest.</p><formula xml:id="formula_2">H := {{L i , M i , M j|L j =M i , . . . , L k|M k =∅ }|L i = Mm∀m} F := ∅ while H = ∅ do for H i ∈ H do C V i := min m,n|Lm∈H i ,Ln∈H i {Vm,n} C T i := j|L j ∈H i e j − b j + 1 C i := 0.1C V i + 0.9C T i end k := argmax i|H i ∈H C i F := F ∪ {H k } for H i ∈ H \ {H k } do H i := H i \ H k end H := H \ {H k } end</formula><p>Tracklet Generation. A tracklet is a series of proposals in contiguous frames which have been merged to belong to the same object identity. In the first stage of our tracking algorithm we join proposals in contiguous frames into tracklets if they have a very high spatio-temporal consistency.</p><p>The spatio-temporal consistency score between two proposals in contiguous timesteps is calculated as the IoU between the proposal projection from the earlier frame and the proposal in the later frame. The proposal projection is the segmentation mask generated by warping a proposal by its corresponding optical flow vectors calculated using PWC-Net <ref type="bibr" target="#b28">[29]</ref>. Effectively the projection of this proposal into the next timestep.  This first stage proceeds frame by frame. For each pair of contiguous timesteps, we create a complete bipartite graph whose nodes are the proposals in successive frames and whose edge scores are the spatio-temporal consistency scores. Edges are dropped from the graph if their score is less than 0.05. We then solve the matching problem between the two sets of nodes using the Hungarian matching algorithm, which finds an optimal set of matches between the two frames. If any proposal is not matched this ends a tracklet. Tracklets may span only a single frame. Merging Tracklets into Tracks. A track is a set of proposals over an entire video which belong to the same object identity. A track often contains multiple tracklets with potentially frames in-between them without proposals.</p><p>The second stage of UnOVOST merges tracklets into long term tracks. To do this we introduce a novel Forest Path Cutting (FPC) algorithm. An overview of this algorithm can be seen in Algorithm 1 and <ref type="figure" target="#fig_3">Figure 4</ref>. This algorithm merges tracklets based on visual similarity cues. Note that spatio-temporal consistency now provides very little further value for data-association. If the tracklets could be easily determined to belong together by spatio-temporal consistency they would have been merged in the first stage. The remaining data association decisions are more difficult such as tracking objects through heavy or total occlusion.</p><p>For each proposal we calculate a ReID vector, a representation of the visual appearance of a proposal which can be used to compare the visual similarity of proposals or tracklets, and thus to re-identify a proposal or tracklet as belonging to a certain object identity. To calculate these vectors we use an appearance embedding network <ref type="bibr" target="#b25">[26]</ref> which extracts an embedding from an image crop. This network is inspired from the person re-identification community. This is a wide ResNet <ref type="bibr" target="#b37">[38]</ref> trained with a batch-hard soft-margin version of the triplet loss. This is pretrained to distinguish classes on COCO <ref type="bibr" target="#b19">[20]</ref>, before being trained to distinguish instances on YouTube-VOS <ref type="bibr" target="#b41">[42]</ref>. It is trained so that the embedding for instances in the same track are pulled closer together in embedding space that for difference tracks. We average ReID vectors over all proposals in a tracklet to achieve a more robust appearance representation.</p><p>To compare two tracklets we define a visual similarity score as the L2 distance of the two ReID vectors normalised to between 0 and 1, with 1 being identical, and 0 being the maximum distance between all tracklets in a video. We subtract this from one to convert it to a similarity score.</p><p>To perform long-term tracking we enumerate a set of potential track hypotheses as different combinations of tracklets. A final set of tracks can be selected as a valid subset of this set of track hypotheses.</p><p>For this task we introduce our Forest Path Cutting (FPC) algorithm as can be seen in Algorithm 1 and <ref type="figure" target="#fig_3">Figure 4</ref>. Initially (Part 1 and Box A) our algorithm builds up a forest of potential tracking hypotheses throughout the video by determining an optimal predecessor for each tracklet. Our FPC algorithm draws parallels to dynamic programming, as we wish to determine an optimal back-pointer for each tracklet to a previous predecessor tracklet. However, a naive implementation of a dynamic programming algorithm would not be able to take advantage of the desirable properties of a UVOS solution.</p><p>To calculate optimal back-pointers, our algorithm iterates over the tracklets in order from the earliest to the latest starting time. For the current tracklet L i , if there are <ref type="figure">Figure 5</ref>. An example of the final stage of UnOVOST, where the final set of tracks is reduced to a maximum of 20 objects over the whole video using our video object saliency metric. Note that objects that would capture an observer's attention are retained while the rest are discarded. any tracklets ending before the tracklet's start time, it determines the most similar predecessor tracklet L k based on the visual similarity score. This is an initial guess for the best predecessor tracklet, however this may belong to the same object, but not be the direct predecessor if there is another tracklet between the two that also belongs to the same object. We check if there are any compatible tracklets between L i and L k , which have tracklet L k as their predecessor. Choosing one of these tracklets as the predecessor would result in L k still being an earlier predecessor. However, we only wish to choose one of these tracklets if it is the most visually similar tracklet to tracklet L i (except for L k ). We repeat this procedure iteratively until there are no more tracklets between the L i its current predecessor L k , or another tracklet which does not have L k as its predecessor is the most visually similar tracklet not in the current set of predecessors.</p><p>We now have a forest of track hypotheses, each tracklet may only have a single predecessor, but a tracklet can be the predecessor for multiple successor tracklets. The resulting forest has at least one tree whose root node corresponds to one of the tracklets with the earliest starting time. Each path H i through the trees from a root node to a leaf node in this forest is a possible long-term object track. We now cut this forest into a set of paths which is the best possible set of object tracks by applying a greedy recursive track selection strategy. This selects an optimal path from the forest, which is then added to the set of final long-term object tracks F . This path is then cut from forest, with all nodes belonging to this path being removed from the forest, and the forest rearranging itself into a new set of trees with the remaining nodes. This can be seen in Algorithm 1 part 2 and <ref type="figure" target="#fig_3">Figure 4</ref> parts B-E. To score paths we use a combination of a Visual Consistency Score C V i and a Temporal Density Score C T i . The visual consistency score is the minimum visual similarity embedding distance between any two tracklets in a path. The temporal density score is the fraction of frames of a video where there is a segment present. This penalizes large temporal gaps between tracklets, making it more likely that objects undergoing short occlusion are correctly tracked, and ensures that the most salient objects are grouped consistently throughout the video, as objects to be Ours VSD <ref type="bibr" target="#b44">[45]</ref> KIS <ref type="bibr" target="#b6">[7]</ref> RVOS <ref type="bibr" target="#b31">[32]</ref> U17 tracked in UVOS are present in mostly all frames. The final path score is a weighted sum of 90% the temporal density and 10% the appearance consistency. We select paths with the highest score through the forest, add these to a final list of tracks F , and cut these from the current forest, reshaping the forest into a new set of trees. This algorithm select a set of object tracks which do not include any overlapping tracklets and have long-term temporal consistency. Final Tracks Selection. In UVOS algorithms are not penalized for making predictions that do not overlap with ground-truth. However, the number of total object tracks that can be predicted in still limited. In the DAVIS 2017 unsupervised benchmark this is limited to 20 predictions over the whole video.</p><p>UnOVOST predicts a potentially large number of tracks, therefore a final object video saliency estimation step is performed to estimate the 20 most salient objects over the whole video to report for evaluation.</p><p>Our video saliency score S sal,i is calculated for each track using each tracklet t j in the track i:</p><formula xml:id="formula_3">S sal,i = j temp(t j ) conf (t j )<label>(1)</label></formula><p>where temp(t j ) is the temporal length of tracklet j and conf (t j ) is the average of the confidence scores of proposals in tracklet t j . This video saliency metric prefers tracks Ours PReMVOS <ref type="bibr" target="#b20">[21]</ref> DyeNet <ref type="bibr" target="#b18">[19]</ref> FEELVOS <ref type="bibr" target="#b32">[33]</ref> OSVOS-S <ref type="bibr" target="#b22">[23]</ref> CINM <ref type="bibr" target="#b0">[1]</ref> RGMP <ref type="bibr" target="#b38">[39]</ref> OnAVOS <ref type="bibr" target="#b34">[35]</ref> VideoMatch <ref type="bibr" target="#b11">[12]</ref> OSVOS <ref type="bibr" target="#b3">[4]</ref> FAVOS <ref type="bibr" target="#b5">[6]</ref> SiamMask <ref type="bibr" target="#b35">[36]</ref> OSMN <ref type="bibr" target="#b43">[44]</ref>  <ref type="figure">Figure 6</ref>. Quality versus timing plot comparing UnOVOST to state-of-the-art semi-supervised methods on DAVIS17 val. All methods other than ours are "semi-supervised" and use the given first-frame ground-truth. Our methods obtains similar results while working in an "unsupervised" manner without using any given information about which objects should be tracked. that are present in many frames and that have high objectness confidence. An example of this is shown in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Unsupervised VOS Evaluation. We evaluate UnOVOST on the DAVIS 2017 Unsupervised dataset <ref type="bibr" target="#b4">[5]</ref>. This contains videos in four sets, with 60 train, 30 val, 30 test-dev and 30 test-challenge videos. The train and val sets contain the same videos as the DAVIS 2017 semisupervised dataset, however they have been re-annotated according to the definition of the UVOS task. The test-dev and test-challenge sets contain new videos. All of these datasets include multiple objects per video sequence. Methods are ranked on the J &amp;F metric which is the average of an area overlap (J ) and a boundary overlap (F) metric. More details can be found in <ref type="bibr" target="#b26">[27]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows our results on these three UVOS benchmarks, and compares our method to three other methods that have presented UVOS results. UnOVOST outperforms all other previous UVOS algorithms over all datasets, often by a large margin. The val set is significantly easier than the other two datasets, in this easier setting UnOVOST has the largest margin over the other methods. This shows that when scenes are not too crowded or complex our method does an exceptional job of successfully tracking and segmenting objects through videos. The test-dev set is significantly more difficult, and yet the UnOVOST algorithm can still perform extremely well, especially when compared to the performance of RVOS <ref type="bibr" target="#b31">[32]</ref>. We present additional qualitative results in <ref type="figure" target="#fig_6">Figure 8</ref>. Comparison to Semi-Supervised VOS Methods. As well as comparing to other UVOS methods, we also compare our results on the DAVIS 2017 val set to the current stateof-the-art semi-supervised VOS methods. <ref type="figure">Figure 6</ref> plots the J &amp;F metric of approaches against their runtime per frame. Although these methods use the given first-frame  mask, so they know exactly what objects need to be tracked in the video, our UnOVOST algorithm outperforms many of these methods, even though it operates completely unsupervised without having access to the first frame. Furthermore, many of these methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> extensively finetune their segmentation and tracking algorithms on the appearance of the given first-frame objects. Our method still outperforms many of these methods while being significantly faster. Runtime Analysis. In <ref type="table" target="#tab_2">Table 2</ref> we provide detailed runtime analysis of our UnOVOST algorithm across the three datasets that we test on. The whole algorithm is able to run at around 1 frame per second (fps). The bottleneck of our algorithm is the proposal generation using Mask R-CNN which is more than 70% of the total runtime. We use two other networks to extract features for matching these proposals over time. Our optical-flow and appearance embedding networks, while both reasonably fast at around 10 fps each, make up a combined 20% of the runtime. The remaining runtime for actually running our algorithm, including proposal sub-selection and clipping, tracklet generation, tracklet merging with the FPC algorithm, and track saliency estimation and selection runs in around 0.07 seconds per frame, or at around 15 fps. Ablation of the Method. We perform an extensive ablation of all design decisions for UnOVOST, for which the results can be found in <ref type="figure" target="#fig_5">Figure 7</ref>. For all design decisions we use the method which performs best on the training set, even though this is often not the best on the other data splits, except for limiting the output to 20 objects which is required by the evaluation. Interestingly across all experiments, results on train and val are very similar, whereas test-dev is much harder, and often shows different trends in the results than the other two sets. First we ablate different Mask R-CNN threshold values for our input proposals. With too small or too large a threshold performance degrades significantly. We then ablate the threshold for which to remove proposals if their masks overlap. Again it is important to select a reasonable threshold. Next we ablate the IoU threshold required for merging proposals into tracklets, which is not so important for the easier validation set, but has large effects on the test set results. We use two different tracklet merging strategies, either us- ing the Hungarian algorithm for associating proposals into tracklets or alternatively using greedy merging. Hungarian performs better on the train and validation splits but worse on the test split. We show resulting scores when evaluating just our tracklets from the first stage, compared to merging these tracklets in a second stage. Across all sets the second stage is incredibly important. We ablate the use of different similarity features for comparing tracklets, as well as the ReID vectors we compare to using last layer activations of pretrained ResNet 101 <ref type="bibr" target="#b10">[11]</ref> and VGG <ref type="bibr" target="#b27">[28]</ref> models trained on ImageNet. The ReID vectors outperform the other similarity features on the train and test set, but the ResNet 101 features perform slightly better on the validation set. We ablate different weightings of the Visual Consistency Score and the Temporal Density Score used in our algorithm. Interestingly, the 90:10 ratio which we use as it slightly outperforms other ratios on the training set, is far from the optimal weighting on the other sets. A 50:50 weighting of the two scores performs the best on both the validation and testing set, with more heavily relying on either of scores performing worse. Finally we test the result of our algorithm if we relax the constraint that we can only select 20 objects for evaluation (we could not do this for the hidden test test). Our algorithm performs slightly better in this setting indicating that this restriction removes some correct objects. We recommend that UnOVOST to be used without this restriction step when being applied in the wild, and consider this only an adaption to the dataset and evaluation.</p><p>Extension to Video Instance Segmentation. The task of Video Instance Segmentation (VIS) is very similar to UVOS, however in VIS the objects to be tracked must be classified into a set of predefined classes rather than just being salient throughout a video. To investigate the generalization of UnOVOST beyond the UVOS domain we run our algorithm on the YouTube-VIS dataset <ref type="bibr" target="#b42">[43]</ref> after training our detector and segmentor on the set of 40 classes in this dataset and adding another classification network to im- prove classification results. Apart from that we run Un-OVOST with exactly the same parameters as for the unsupervised DAVIS task. Details of how we trained our detector and segmentor for VIS, and of the classifier we used can be found in the supplemental material.</p><p>The VIS task is evaluated using the mAP metric. This is similar to the mAP metric used for instance segmentation <ref type="bibr" target="#b19">[20]</ref>, however it has been extended from single images to video. Details of mAP for VIS can be found in <ref type="bibr" target="#b42">[43]</ref>.</p><p>The previous state-of-the-art VIS method is MaskTrack R-CNN <ref type="bibr" target="#b42">[43]</ref>, which achieves a mAP scores of 30.3 and 32.2 on the YouTube-VIS validation and test set respectively. UnOVOST significantly outperforms this, achieving mAP scores of 44.8 and 46.7 on the two benchmarks respectively. With these scores UnOVOST also won the 2019 YouTube-VIS Challenge on Video Instance Segmentation, outperforming 18 other methods. In the supplementary material we present a table comparing results to all previous benchmarks and 2019 challenge entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present the novel UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking) algorithm for tackling the unsupervised video object segmentation task. Our algorithm is able to track and segment a huge variety of objects in complex scenes by combining both spatio-temporal consistency and visual similarity cues in a novel tracklet based Forest Path Cutting algorithm for performing data association. UnOVOST outperforms all previous UVOS methods, while even performing competitively with many semi-supervised video object segmentation algorithms without requiring any human input as to which objects should be tracked and segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material: UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Adapting UnOVOST to Video Instance Segmentation</head><p>We adapt UnOVOST to the Video Instance Segmentation (VIS) domain is the following way. Detection. For detection we adapt the Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> detector to the YT-VIS benchmark to detect the 40 object classes.</p><p>To adapt this network to VIS, we created a training set by combining the YT-VIS <ref type="bibr" target="#b42">[43]</ref>, COCO <ref type="bibr" target="#b19">[20]</ref> and OpenImages <ref type="bibr" target="#b14">[15]</ref> datasets. We trained this detector on 39 classes, the 40 classes of YT-VIS with "monkey" and "ape" combined. This is because OpenImages only has a class which is a mix, and because in the YT-VIS training set it is unclear exactly what the difference between these two classes should be (e.g. baboons are labeled as both ape and monkey, some gorillas mislabeled as monkeys). Thus we detect these classes together and rely on our classifier later to distinguish between the two.</p><p>For COCO we use the 19 classes which overlap with the YT-VIS classes. The "bird" class was set to ignore regions (as multiple birds such as owl, eagle and duck are in YouTube-VIS). We map the OpenImages classes to YouTube-VIS classes, with all of our 39 classes being mapped to by at least one OpenImages class. We only use images that contain at least one annotation from our 39 classes that is not a person (because of too many people in OpenImages). We set all of the background of OpenImages images to be ignore regions and we don't sample negatives from this dataset (as OpenImages is not densely annotated). We reweight how often we sample each image during training for class balancing. Classes are sampled such that in one epoch there are at least 5000 examples of each class. This results in sharks being sampled 18 times more often than horses. Also images form the YT-VIS dataset are sampled three times more often than those in COCO and OpenImages. Classification. The classification branch our Mask R-CNN detector works reasonably well, but still often misclassifies examples. To improve this, we use a ResNeXt-101 32x48d classifier <ref type="bibr" target="#b40">[41]</ref> pretrained on 940 million Instagram images <ref type="bibr" target="#b21">[22]</ref>, before being trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>. We then defined a mapping of ImageNet (INet) classes to YT-VIS classes.</p><p>This mapping results in 310 of the 1000 INet classes being mapped to our 40 YT-VIS classes, with 123 INet classes being mapped to dog and 20 to truck. Some classes are not represented (person, skateboard, giraffe, hand and surfboard). Some INet classes are mapped to multiple YT-VIS classes, e.g. "Amphibious vehicle" being mapped to both  boat and truck. There are 11 INet classes mapped to just monkey, 2 to just ape and 7 to both due to the ambiguity in YT-VIS as to what is a ape and what is a monkey. The final INet classification score for each YT-VIS class is then the sum of the classification scores for all of the contributing INet classes.</p><p>The final classification scores were then a weighted combination of the scores from our Mask R-CNN detector and our INet trained classifier. Segmentation. We finetune the segmentation head of Mask R-CNN on the YT-VIS dataset separately for the 40 classes.</p><p>Tracking. We use UnOVOST exactly as in the main paper for unsupervised VOS with exactly the same parameters. The only difference is that different input proposals are input to UnOVOST. Putting it all together. In VIS segmentations are allowed to overlap, thus when we are not sure which class a track belongs to we propose the existence of the same track multiple times with different classes and scores.</p><p>To obtain a track's score for each class, we average the class scores for the mask in each timestep. Frames with no masks are given 0 score thus short tracks are down weighted. We do this for both detection scores and INet scores. The final score is the weighted average of these two scores (with equal weighting). We output each track multiple times for every class with a score greater than 0.0001. Note that the detector doesn't discriminate between apes and monkey, so the one detection score is used for both. Also our INet classifier doesn't give scores for 5 of the 40 classes, so for these we only use detector scores.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Initial stage of the UnOVOST algorithm. Mask proposals are generated by Mask R-CNN with a low scoring threshold that generates a large number of overlapping proposals. These are then sub-selected and clipped based on their score and intersection to produce a set of non-overlapping mask proposals in each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual representation of our Forest Path Cutting (FPC) algorithm. In box A, tracklets are visualized as black lines showing their temporal extent on the horizontal axis. The optimal predecessors for each tracklet are shown as blue and orange dotted lines. Box B shows that the set of tracklets with predecessors from box A defines a forest of track hypotheses. Box C shows an optimal path (in red), selected from the forest and added to the final object tracks.This optimal path is cut from its tree, dividing it into a number of sub-trees (green and purple circles). Box D shows the resulting new forest produced with sub-trees that only contain a single path added to the list of final tracks. Box E shows the final result after this process is iterated until the forest is completely divided into a set of tracks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(seconds) Region and contour quality (J &amp;F)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Results of ablating a number of design decisions forUnOVOST on the three splits of the DAVIS 2017 Unsupervised dataset. A cross indicates the selected option chosen as the best performing option on the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Additional qualitative results of UnOVOST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>J &amp;F</cell><cell>Mean</cell><cell>56.4</cell><cell>56.2</cell><cell>51.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>53.4</cell><cell>53.5</cell><cell>48.7</cell><cell>-</cell></row><row><cell></cell><cell>J</cell><cell>Recall</cell><cell>60.9</cell><cell>61.3</cell><cell>55.1</cell><cell>-</cell></row><row><cell>T-C</cell><cell></cell><cell>Decay</cell><cell>1.5</cell><cell>-2.1</cell><cell>4.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>59.4</cell><cell>59.0</cell><cell>54.5</cell><cell>-</cell></row><row><cell></cell><cell>F</cell><cell>Recall</cell><cell>64.1</cell><cell>63.2</cell><cell>59.4</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Decay</cell><cell>5.8</cell><cell>0.1</cell><cell>7.7</cell><cell>-</cell></row><row><cell></cell><cell>J &amp;F</cell><cell>Mean</cell><cell>58.0</cell><cell>56.5</cell><cell>54.2</cell><cell>22.5</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>54.0</cell><cell>51.7</cell><cell>50.0</cell><cell>17.7</cell></row><row><cell>U17</cell><cell>J</cell><cell>Recall</cell><cell>62.9</cell><cell>59.9</cell><cell>58.9</cell><cell>16.2</cell></row><row><cell>T-D</cell><cell></cell><cell>Decay</cell><cell>3.5</cell><cell>21.7</cell><cell>8.4</cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>62.0</cell><cell>61.4</cell><cell>58.3</cell><cell>27.3</cell></row><row><cell></cell><cell>F</cell><cell>Recall</cell><cell>66.6</cell><cell>65.7</cell><cell>62.1</cell><cell>24.8</cell></row><row><cell></cell><cell></cell><cell>Decay</cell><cell>6.6</cell><cell>15.7</cell><cell>11.4</cell><cell>1.8</cell></row><row><cell></cell><cell>J &amp;F</cell><cell>Mean</cell><cell>67.9</cell><cell>56.6</cell><cell>59.9</cell><cell>41.2</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>66.4</cell><cell>51.7</cell><cell>-</cell><cell>36.8</cell></row><row><cell>U17</cell><cell>J</cell><cell>Recall</cell><cell>76.4</cell><cell>-</cell><cell>-</cell><cell>40.2</cell></row><row><cell>Val</cell><cell></cell><cell>Decay</cell><cell>-0.2</cell><cell>-</cell><cell>-</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>69.3</cell><cell>61.4</cell><cell>-</cell><cell>45.7</cell></row><row><cell></cell><cell>F</cell><cell>Recall</cell><cell>76.9</cell><cell>-</cell><cell>-</cell><cell>46.4</cell></row><row><cell></cell><cell></cell><cell>Decay</cell><cell>0.01</cell><cell>-</cell><cell>-</cell><cell>1.7</cell></row></table><note>Our results compared to all other UVOS methods on the DAVIS 2017 unsupervised benchmarks: test-challenge (U17 T-C), test-dev (U17 T-D), and val (U17 Val). VSD [45] and KIS [7] obtained second and third place (after our method) in the 2019 DAVIS Unsupervised VOS Challenge.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Runtime analysis of UnOVOST on the DAVIS 2017 Unsupervised val, test-dev and test-challenge datasets. Times are seconds per frame.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results in the 2019 YouTube-VIS Challenge, compared to top 8 other participants, and the previous state-of-the-art.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This project has been funded, in parts, by ERC Consolidator Grant DeeViSe (ERC-2017-COG-773161) and by a Google Faculty Research Award. We would like to thank Paul Voigtlaender for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in a cnn-based higher-order spatiotemporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A detailed rubric for motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10033</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining cnns and geometric constraints for hierarchichal motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Key instance selection for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2019 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards segmenting anything that moves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<title level="m">Multiple hypothesis tracking revisited. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tracking the trackers: an analysis of the state of the art in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02781</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation with motion-based bilateral networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Track, then decide: Category-agnostic vision-based multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale object mining for object discovery from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02371</idno>
		<title level="m">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FEELVOS: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MOTS: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B G</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Object discovery in videos as foreground motion clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04804</idno>
		<title level="m">Video instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video segmentation by detection for the 2019 unsupervised davis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2019 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
