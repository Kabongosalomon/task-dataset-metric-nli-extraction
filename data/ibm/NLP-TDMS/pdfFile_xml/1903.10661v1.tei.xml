<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Queens University</orgName>
								<address>
									<settlement>Belfast</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Visionfinity Inc</orgName>
								<orgName type="institution" key="instit2">ObjectEye Inc</orgName>
								<orgName type="institution" key="instit3">Universal AI Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Queens University</orgName>
								<address>
									<settlement>Belfast</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
							<email>jqwang@nlpr.ia.ac.cn4huguosheng100@gmail.com5n.robertson@qub.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Lab of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Visionfinity Inc</orgName>
								<orgName type="institution" key="instit2">ObjectEye Inc</orgName>
								<orgName type="institution" key="instit3">Universal AI Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning based facial landmark detection has achieved great success. Despite this, we notice that the semantic ambiguity greatly degrades the detection performance. Specifically, the semantic ambiguity means that some landmarks (e.g. those evenly distributed along the face contour) do not have clear and accurate definition, causing inconsistent annotations by annotators. Accordingly, these inconsistent annotations, which are usually provided by public databases, commonly work as the groundtruth to supervise network training, leading to the degraded accuracy. To our knowledge, little research has investigated this problem. In this paper, we propose a novel probabilistic model which introduces a latent variable, i.e. the 'real' ground-truth which is semantically consistent, to optimize. This framework couples two parts (1) training landmark detection CNN and (2) searching the 'real' groundtruth. These two parts are alternatively optimized: the searched 'real' ground-truth supervises the CNN training; and the trained CNN assists the searching of 'real' groundtruth. In addition, to recover the unconfidently predicted landmarks due to occlusion and low quality, we propose a global heatmap correction unit (GHCU) to correct outliers by considering the global face shape as a constraint. Extensive experiments on both image-based (300W and AFLW) and video-based (300-VW) databases demonstrate that our method effectively improves the landmark detection accuracy and achieves the state of the art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning methods <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">26]</ref> have achieved great success on landmark detection due to the strong modeling capacity. Despite this success, precise and credible landmark detection still has many challenges, one * equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non semantic moving</head><p>Semantic moving <ref type="figure">Figure 1</ref>. The landmark updates in training after the model is roughly converged. Due to 'semantic ambiguity', we can see that many optimization directions, which are random guided by random annotation noises along with the contour and 'non semantic'. The others move to the semantically accurate positions. Red and green dots denote the predicted and annotation landmarks, respectively.</p><p>of which is the degraded performance caused by 'semantic ambiguity'. This ambiguity results from the lack of clear definition on those weak semantic landmarks on the contours (e.g. those on face contour and nose bridge). In comparison, strong semantic landmarks on the corners (e.g. eye corner) suffer less from such ambiguity. The 'semantic ambiguity' can make human annotators confused about the positions of weak semantic points, and it is inevitable for annotators to introduce random noises during annotating. The inconsistent and imprecise annotations can mislead CNN training and cause degraded performance. Specifically, when the deep model roughly converges to the ground-truth provided by public databases, the network training is misguided by random annotation noises caused by 'semantic ambiguity', shown in <ref type="figure">Fig. 1</ref>. Clearly these noises can make the network training trapped into local minima, leading to degraded results.</p><p>In this paper, we propose a novel Semantic Alignment method which reduces the 'semantic ambiguity' intrinsi-cally. We assume that there exist 'real' ground-truths which are semantically consistent and more accurate than human annotations provided by databases. We model the 'real' ground-truth as a latent variable to optimize, and the optimized 'real' ground-truth then supervises the landmark detection network training. Accordingly, we propose a probabilistic model which can simultaneously search the 'real' ground-truth and train the landmark detection network in an end-to-end way. In this probabilistic model, the prior model is to constrain the latent variable to be close to the observations of the 'real' ground truth, one of which is the human annotation. The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of 'real' ground-truth. The heatmap generated by the hourglass architecture <ref type="bibr" target="#b17">[17]</ref> represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood. Apart from the proposed probabilistic framework, we further propose a global heatmap correction unit (GHCU) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images. We conduct experiments on 300W <ref type="bibr" target="#b21">[21]</ref>, AFLW <ref type="bibr" target="#b8">[9]</ref> and 300-VW <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b2">3]</ref> databases and achieve the state of the art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In recent years, convolutional neural networks (CNN) achieves very impressive results on many computer vision tasks including face alignment. Sun et al <ref type="bibr" target="#b23">[23]</ref> proposes to cascade several DCNN to predict the shape stage by stage. Zhang et al <ref type="bibr" target="#b32">[30]</ref> proposes a single CNN and jointly optimizes facial landmark detection together with facial attribute recognition, further enhancing the speed and performance. The methods above use shallow CNN models to directly regress facial landmarks, which are difficult to cope the complex task with dense landmarks and large pose variations.</p><p>To further improve the performance, many popular semantic segmentation and human pose estimation frameworks are used for face alignment <ref type="bibr" target="#b31">[29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">14]</ref>. For each landmark, they predict a heatmap which contains the probability of the corresponding landmark. Yang et al. <ref type="bibr" target="#b31">[29]</ref> uses a two parts network, i.e., a supervised transformation to normalize faces and a stacked hourglass network <ref type="bibr" target="#b17">[17]</ref> to get prediction heatmaps. Most recently, JMFA <ref type="bibr" target="#b4">[5]</ref> and FAN <ref type="bibr" target="#b1">[2]</ref> also achieve the state of the art accuracy by leveraging stacked hourglass network. However, these methods do not consider the 'semantic ambiguity' problem which potentially degrades the detection performance.</p><p>Two recent works, LAB <ref type="bibr" target="#b26">[26]</ref> and SBR <ref type="bibr" target="#b5">[6]</ref>, are related to this 'semantic ambiguity' problem. By introducing more information than pixel intensity only, they implicitly alle-viate the impact of the annotation noises and improve the performance. LAB <ref type="bibr" target="#b26">[26]</ref> trains a facial boundary heatmap estimator and incorporates it into the main landmark regression network. LAB uses the well-defined facial boundaries which provide the facial geometric structure to reduce the ambiguities, leading to improved performance. However, LAB is computational expensive. SBR <ref type="bibr" target="#b5">[6]</ref> proposes a registration loss which uses the coherency of optical flow from adjacent frames as its supervision. The additional information from local feature can mitigate the impact of random noises. However, the optical flow is not always credible in unconstrained environment and SBR trains their model on the testing video before the test, limiting its applications. To summarize, LAB and SBR do not intrinsically address the problem of 'semantic ambiguity' because the degraded accuracy is actually derived from the inaccurate labels (human annotations provided by databases). In this work, we solve the 'semantic ambiguity' problem in a more intrinsic way. Specifically, we propose a probabilistic model which can simultaneously search the 'real' ground-truth without semantic ambiguity and train a hourglass landmark detector without using additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic ambiguity</head><p>The semantic ambiguity indicates that some landmarks do not have clear and accurate definition. In this work, we find the semantic ambiguity can happen on any facial points, but mainly on those weak semantic facial points. For example, the landmarks are defined to evenly distribute along the face contour without any clear definition of the exact positions. This ambiguity can potentially affect: (1) the accuracy of the annotations and (2) the convergence of deep model training. For (1), when annotating a database, annotators can introduce random errors to generate inconsistent ground-truths on those weak semantic points due to the lack of clear definitions. For (2), the inconsistent ground-truths generate inconsistent gradients for back-propagation, leading to the difficulty of model convergence. In this section, we qualitatively analyze the influence of semantic ambiguity on landmark detection.</p><p>Before this analysis, we briefly introduce our heatmapbased landmark detection network. Specifically, we use a four stage Hourglass (HGs) <ref type="bibr" target="#b17">[17]</ref>. It can generate the heatmap which provides the probability of the corresponding landmark located at every pixel, and this probability can facilitate our analysis of semantic ambiguity.</p><p>Firstly, we find CNN provides a candidate region rather than a confirmed position for a weak semantic point. In <ref type="figure" target="#fig_0">Fig. 2</ref> (a), we can see that the heatmap of a strong semantic point is nearly Gaussian, while the 3D heatmap of a weak semantic point has a 'flat hat', meaning that the confidences in that area are very similar. Since the position with the highest confidence is chosen as the output. The landmark (a) The difference between the heatmap of the eye corner (strong semantic) points and the eye contour (weak semantic) points. Col 2 and 3 represent 2D and 3D heatmaps respectively. In the 3D Gaussian, the x, y axes are image coordinates and z axis is the prediction confidence. We can see the 3D heatmap of a weak semantic point has a 'flat hat'.</p><p>(b) The predictions from a series of checkpoints after convergence. When the model has roughly converged, we continue training and achieve the predictions from different iterations. Red and green dots denote the predicted and annotation landmarks, respectively. We can see the predicted landmarks from different checkpoints fluctuate in the neighborhood area of the annotated position (green dots). Secondly, we analyze the 'semantic ambiguity' by visualizing how the model is optimized after convergence. When the network has roughly converged, we continue training the network and save a series of checkpoints. In <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>, the eyebrow landmarks, from different checkpoints fluctuate along with the edge of eyebrow, which always generates considerable loss to optimize. However, this loss is ineffectual since the predicted points from different checkpoints also fluctuate in the neighborhood area of the annotated position (green dots in <ref type="figure" target="#fig_0">Fig. 2 (b)</ref>). It can be concluded that the loss caused by random annotation noises dominate the back-propagated gradients after roughly convergence, making the network training trapped into local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Semantically consistent alignment</head><p>In this section, we detail our methodology. In Section 4.1, we model the landmark detection problem using a probabilistic model. To deal with the semantic ambiguity caused by human annotation noise, we introduce a latent variablê y which represents the 'real' ground-truth. Then we model the prior model and likelihood in Section 4.2 and 4.3, respectively. Section 4.4 proposes an alternative optimization strategy to searchŷ and train the landmark detector. To recover the unconfidently predicted landmarks due to occlusion and low quality, we propose a global heatmap correction unit (GHCU) in Section 4.5, which refines the predictions by considering the global face shape as a constraint, leading to a more robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A probabilistic model of landmark prediction</head><p>In the probabilistic view, training a CNN-based landmark detector can be formulated as a likelihood maximization problem:</p><formula xml:id="formula_0">max W L(W) = P (o|x; W),<label>(1)</label></formula><p>where o ∈ R 2N is the coordinates of the observation of landmarks (e.g. the human annotations). N is the number of landmarks, x is the input image and W is the CNN parameters. Under the probabilistic view of Eq. (1), one pixel value on the heatmap works as the confidence of one particular landmark at that pixel. Therefore, the whole heatmap works as the probability distribution over the image. As analyzed in Section 3, the annotations provided by public databases are usually not fully credible due to the 'semantic ambiguity'. As a result, the annotations, in particular those of weak semantic landmarks, contain random noises and are inconsistent among faces. In this work, we assume that there exists a 'real' ground-truth without semantic ambiguity and can better supervise the network training. To achieve this, we introduce a latent variableŷ as the 'real' ground-truth which is optimized during learning. Thus, Eq. (1) can be reformulated as:</p><formula xml:id="formula_1">max y,W L(ŷ, W) = P (o,ŷ|x; W) = P (o|ŷ)P (ŷ|x; W),<label>(2)</label></formula><p>where o is the observation ofŷ, for example, the annotation can be seen as an observation ofŷ from human annotator. P (o|ŷ) is a prior ofŷ given the observation o and P (ŷ|x; W) is the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prior model of 'real' ground-truth</head><p>To optimize Eq. <ref type="formula" target="#formula_1">(2)</ref>, an accurate prior model is important to regularizeŷ and reduce searching space. We assume that the kth landmarkŷ k is close to the o k , which is the observation ofŷ. Thus, this prior is modeled as Gaussian similarity over all {o k ,ŷ k } pairs: where σ 1 can control the sensitivity to misalignment. To explain o k , we should know in advance that our whole framework is iteratively optimized detailed in Section 4.4. o k is initialized as the human annotation in the iteration, and will be updated by better observation with iterations.</p><formula xml:id="formula_2">P (o|ŷ) ∝ k exp − o k −ŷ k 2 2σ 2 1 = exp − k o k −ŷ k 2 2σ 2 1 ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network likelihood model</head><p>We now discuss the likelihood P (ŷ|x; W) of Eq. <ref type="formula" target="#formula_1">(2)</ref>. The point-wise joint probability can be represented by the confidence map, which can be modelled by the heatmap of the deep model. Note that our hourglass architecture learns to predict heatmap consisting of a 2D Gaussian centered on the ground-truthŷ k . Thus, for any position y, the more the heatmap region around y follows a standard Gaussian, the more the pixel at y is likely to beŷ k . Therefore, the likelihood can be modeled as the distribution distance between the predicted heatmap (predicted distribution) and the standard Gaussian region (expected distribution). In this work, we use Pearson Chi-square test to evaluate the distance of these two distributions:</p><formula xml:id="formula_3">χ 2 (y|x; W) = i (E i − Φ i (y|x; W)) 2 E i<label>(4)</label></formula><p>where E is a standard Gaussian heatmap (distribution), which is a template representing the ideal response; i is the pixel index; Φ is a cropped patch (of the same size as Gaussian template) from the predicted heatmap centered on y.</p><p>Finally, the joint probability can also be modeled as a product of Gaussian similarities maximized over all landmarks:</p><formula xml:id="formula_4">P (ŷ|x; W) = exp − k χ 2 k (ŷ|x; W) 2σ 2 2<label>(5)</label></formula><p>where k is the landmark index, σ 2 is the bandwidth of likelihood.</p><p>To keep the likelihood credible, we first train a network with the human annotations. Then in the likelihood, we can consider the trained network as a super annotator to guide the searching of the real ground-truth. It results from the fact that a well trained network is able to capture the statistical law of annotation noise from the whole training set, so that it can generate predictions with better semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Optimization</head><p>Combining Eq. (2), (3) and <ref type="bibr" target="#b4">(5)</ref> and taking log of the likelihood, we have:</p><formula xml:id="formula_5">log L(ŷ, W) = k − o k −ŷ k 2 2σ 2 1 − χ 2 (ŷ|x; W) 2σ 2 2<label>(6)</label></formula><p>Reduce Searching Space To optimize the latent semantically consistent 'real' landmarkŷ k , the prior Eq. <ref type="formula" target="#formula_2">(3)</ref> indicates that the latent 'real' landmark is close to the observed landmark o k . Therefore, we reduce the search space of y k to a small patch centered on o k . Then, the optimization problem of Eq. <ref type="formula" target="#formula_5">(6)</ref> can be re-written as:</p><formula xml:id="formula_6">min y,W − log L(ŷ, W) s.t.ŷ k ∈ N (o k )<label>(7)</label></formula><p>where N (o k ) represents a region centered on o k .</p><p>Alternative Optimization To optimize Eq. <ref type="formula" target="#formula_6">(7)</ref>, an alternative optimization strategy is applied. In each iteration, y is firstly searched with the network parameter W fixed. Thenŷ is fixed and W is updated (landmark prediction network training) under the supervision of newly searchedŷ.</p><p>Step 1: When W is fixed, to search the latent variableŷ, the optimization becomes a constrained discrete optimization problem for each landmark:</p><formula xml:id="formula_7">min y k o k −ŷ k 2 2σ 2 1 + χ 2 (ŷ k |x; W) 2σ 2 2<label>(8)</label></formula><p>where all the variables are known exceptŷ k . We searchŷ k by going through all the pixels in N (o k ) (a neighborhood area of o k as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>) and the one with minimal loss in Eq. (8) is the solution. Since the searching space N (o k ) is very small, i.e. 17 × 17 in this work for 256×256 heatmap, the optimization is very efficient. Note that in the prior part of Eq. (8), o k is the observation ofŷ k : In the 1st iteration, o k is set to the human annotations which are the observation of human annotators; From the 2nd iteration, o k is set toŷ k t−1 (where t is the iteration). Note thatŷ k t−1 is the estimated 'real' ground-truth  from the last iteration. With the iterations,ŷ k t is converging to the 'real' ground-truth because both the current observation o k (i.e.ŷ k t−1 ) and CNN prediction iteratively become more credible.</p><p>Step 2: Whenŷ is fixed, the optimization becomes:</p><formula xml:id="formula_8">min W k χ 2 (ŷ k |x; W) 2σ 2 2<label>(9)</label></formula><p>The optimization becomes a typical network training process under the supervision ofŷ. Hereŷ is set to the estimate of the latent 'real' ground-truth obtained in Step 1. <ref type="figure" target="#fig_2">Figure 4</ref> shows an example of the gradual convergence from the observation o (ŷ of the last iteration) to the estimate of real ground-truthŷ. The optimization ofŷ in our semantic alignment can easily converge to a stable position, which does not have hard convergence problem like the traditional landmark training as shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Global heatmap correction unit</head><p>Traditional heatmap based methods predict each landmark as an individual task without considering global face shape. The prediction might fail when the model fits images of low-quality and occlusion as shown in <ref type="figure" target="#fig_3">Fig. 5b</ref>. The outliers such as occlusions destroy the face shape and significantly reduce overall performance. Existing methods like local feature based CLM <ref type="bibr" target="#b3">[4]</ref> and deep learning based LGCN <ref type="bibr" target="#b14">[14]</ref> apply a 2D shape PCA as their post-processing step to remove the outliers. However, PCA based method is weak to model out-of-plane rotation and very slow (about 0.8 fps in LGCN <ref type="bibr" target="#b14">[14]</ref>). In this work, we propose a Global Heatmap Correction Unit (GHCU) to recover the outliers efficiently. We view the predicted heatmaps as input and directly regress the searched/optimizedŷ through a light weight CNN as shown in Tab. 1. The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Our experiments demonstrate the GHCU completes fitting with the speed 8 times faster than PCA on the same hardware platform and achieves higher accuracy than PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datesets. We conduct evaluation on three challenging datasets including image based 300W <ref type="bibr" target="#b21">[21]</ref>, AFLW <ref type="bibr" target="#b8">[9]</ref>, and video based 300-VW <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>300W <ref type="bibr" target="#b21">[21]</ref> is a collection of multiple face datasets, including the LFPW <ref type="bibr" target="#b0">[1]</ref>, HELEN <ref type="bibr" target="#b11">[11]</ref>, AFW <ref type="bibr" target="#b19">[19]</ref> and XM2VTS <ref type="bibr" target="#b15">[15]</ref> which have 68 landmarks. The training set contains 3148 training samples, 689 testing samples which are further divided into the common and challenging subsets.</p><p>AFLW <ref type="bibr" target="#b8">[9]</ref> is a very challenging dataset which has a wide range of pose variations in yaw (−90 • to 90 • ). In this work, we follow the AFLW-Full protocol <ref type="bibr" target="#b35">[33]</ref> which ignores two landmarks of ears and use the remaining 19 landmarks.</p><p>300-VW <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b2">3]</ref> is a large dataset for video-based face alignment, which consists of 114 videos in various conditions. Following <ref type="bibr" target="#b22">[22]</ref>, we utilized all images from 300W and 50 sequences for training and the remaining 64 sequences for testing. The test set consists of three categories: well-lit, mild unconstrained and challenging.</p><p>Evaluation metric. To compare with existing popular methods, we conduct different evaluation metrics on different datasets. For 300W dataset, We follow the protocol in <ref type="bibr" target="#b20">[20]</ref> and use Normalized mean errors (NME) which normalizes the error by the inter-pupil distance. For AFLW, we follow <ref type="bibr" target="#b34">[32]</ref> to use face size as the normalizing factor. For 300-VW dataset, we employed the standard normalized root mean squared error (RMSE) <ref type="bibr" target="#b22">[22]</ref> which normalizes the error by the outer eye corner distance.</p><p>Implementation Details. In our experiments, all the training and testing images are cropped and resized to 256×256 according to the provided bounding boxes. To perform data augmentation, we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution. We use a four-stage stacked hourglass network <ref type="bibr" target="#b17">[17]</ref> as our backbone which is trained by the optimizer RMSprop. As described in Section 4, our algorithm comprises two parts: network training and real groundtruth searching, which are alternatively optimized. Specifically, at each epoch, we first search the real ground-trutĥ y and then useŷ to supervise the network training. When training the roughly converged model with human annotations, the initial learning rate is 2.5 × 10 −4 which is decayed to 2.5 × 10 −6 after 120 epochs. When training with Semantic Alignment from the beginning of the aforementioned roughly converged model, the initial learning rate is 2.5 × 10 −6 and is divided by 5, 2 and 2 at epoch 30, 60 and 90 respectively. During semantic alignment, we search the latent variableŷ from a 17×17 region centered at the current observation point o, and we crop a no larger than 25×25 patch from the predicted heatmap around current position for Pearson Chi-square test in Eq. (4). We set batch size to 10 for network training. For GHCU, the network architecture is shown in Tab. 1. All our models are trained with PyTorch [18] on 2 Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison experiment</head><p>300W. We compare our approach against the state-ofthe-art methods on 300W in Tab. 2. The baseline (HGs in Tab. 2) uses the hourglass architecture with human annotations, which is actually the traditional landmark detector training. From Tab. 2, we can see that HGs with our Semantic Alignment (HGs + SA) greatly outperform hourglass (HGs) only, 4.37% vs 5.04% in terms of NME on Full set, showing the great effectiveness of our Semantic Alignment (SA). By adding GHCU, we can see that HGs+SA+GHCU slightly outperforms the HGs+SA. The improvement is not significant because the images of 300W are of high resolution, while GHCU works particularly well for images of low resolution and occlusions verified in the following evaluations. Following <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b31">[29]</ref> which normalize the in-plane-rotation by training a preprocessing network, we conduct this normalization (HGs+SA+GHCU+Norm) and achieve state of the art performance on Challenge set and Full set: 6.38% and 4.02%. In particular, on Challenge set, we significantly outperform the state of the art method: 6.38% (HGs+SA+GHCU+Norm) vs 6.98% (LAB), meaning that our method is particularly effective on challenging scenarios. AFLW. 300W has 68 facial points which contain many weak semantic landmarks (e.g. those on face contours). In comparison, AFLW has only 19 points, most of which are strong semantic landmarks. Since our SA is particularly effective on weak semantic points, we conduct experiments on AFLW to verify whether SA generalizes well to the point set, most of which are strong semantic points. For fair comparison, we do not compare methods using additional outside training data, e.g. LAB <ref type="bibr" target="#b26">[26]</ref> used additional boundary information from outside database. As shown in Tab. 3, HGs+SA outperforms HGs, 1.62% vs 1.95%. It means that even though corner points are easily to be recognized, there is still random error in annotation, which can be corrected by SA. It is also observed that HGs+SA+GHCU works better than HGs+SA.</p><p>300-VW. Unlike the image-based databases 300W and AFLW, 300-VW is video-based database, which is more challenging because the frame is of low resolution and with strong occlusions. The subset Category 3 is the most challenging one. From Tab. 4, we can see that HGs + SA greatly outperforms HGs in each of these three test sets. Furthermore, compared with HGs + SA, HGs + SA + GHCU reduce the error rate (RMSE) by 18% on Category 3 test set, meaning that GHCU is very effective for video-based challenges such as low resolution and occlusions because <ref type="table">Table 3</ref>. Comparison with state of the art on AFLW dataset. The error (NME) is normalized by the face bounding box size. <ref type="bibr">Method</ref> AFLW-Full (%) LBF <ref type="bibr" target="#b20">[20]</ref> 4.25 CFSS <ref type="bibr" target="#b34">[32]</ref> 3.92 CCL (CVPR16) <ref type="bibr" target="#b35">[33]</ref> 2.72 TSR (CVPR17) <ref type="bibr" target="#b13">[13]</ref> 2.17 DCFE (ECCV18) <ref type="bibr" target="#b25">[25]</ref> 2.17 SBR (CVPR18) <ref type="bibr" target="#b5">[6]</ref> 2.14 DSRN (CVPR18) <ref type="bibr" target="#b16">[16]</ref> 1.86 Wing (CVPR18) <ref type="bibr" target="#b6">[7]</ref> 1.65 HGs 1.95 HGs + SA 1.62 HGs + SA + GHCU <ref type="bibr">1.60</ref> GHCU considers the global face shape as constraint, being robust to such challenging factors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Self evaluations</head><p>Balance of prior and likelihood As shown in Eq. (6), the 'real' ground-truth is optimized using two parts: prior and likelihood, where σ 1 and σ 2 determine the importance of these two parts. Thus, we can use one parameter σ 2 2 /σ 2 1 to estimate this importance weighting. We evaluate different values of σ 2 2 /σ 2 1 in Tab. 5. Clearly, the performance of σ 2 2 /σ 2 1 = 0 (removing Semantic Alignment and using human annotations only) is worst, showing the importance of the proposed Semantic Alignment. We find that σ 2 2 /σ 2 1 = 0.1 achieves the best performance, meaning that the model relies much more (10 times) on prior than likelihood to achieve the best trade-off. Template size. As discussed in the Section 3, for a position y, the similarity between the heatmap region around it and standard Gaussian template is closely related to the detection confidence. Therefore, the size of the Gaussian template, which is used to measure the network confidence in Eq. (5), can affect the final results. <ref type="table" target="#tab_4">Table 6</ref> reports the results under different template sizes using the model HGs+SA. Too small size (size=1) means that the heatmap value is directly used to model the likelihood instead of Chi-square test. Not surprisingly, the performance with size=1 is not promising. Large size (size=25) introduces more useless information, degrading the performance. In our experiment, we find size=15 for AFLW and size=19 for 300W can achieve the best result. Analysis of the training of semantic alignment. To verify the effectiveness of Semantic Alignment, we train a baseline network using hourglass under the supervision of human annotation to converge. Use this roughly converged baseline, we continue training using 3 strategies as shown in <ref type="figure">Fig. 6 and 7</ref>: baseline, SA w/o update (always using human annotation as the observation, see Eq. <ref type="formula" target="#formula_5">(6)</ref>) and SA (the observation is iteratively updated). <ref type="figure">Fig. 6 and 7</ref> visualize the changes of training loss and NME on test set against the training epochs, respectively. We can see that the baseline curve in <ref type="figure">Fig. 6 and 7</ref> do not decrease because of the 'semantic ambiguity'. By introducing SA, the training loss and test NME steadily drop. Obviously, SA reduces the random optimizing directions and helps the roughly converged network to further improve the detection accuracy.</p><p>We also evaluate the condition that uses semantic alignment without updating the observation o ('SA w/o update' in <ref type="figure">Fig. 6 and 7)</ref>. It means o is always set to the human annotations. We can see that the curve of 'SA w/o update' can be further optimized but quickly trapped into local optima, leading to worse performance than SA. We assume that the immutable observation o reduces the capacity of searching 'real' ground-truthŷ.  after each epoch. To explore the effects of the number of epochs on model convergence, we train different models by stopping semantic alignment at different epochs. In <ref type="figure">Fig 8,</ref> it is observed that the final performance keeps improving with the times of semantic alignment, which demonstrates that the improvement is highly positive related to the quality of the learnedŷ. From our experiment, 10 epochs of semantic alignment are enough for our data sets. Test NME (%) <ref type="figure">Figure 8</ref>. NME vs Semantic Alignment update epochs on 300W full test set Quality of the searched 'real' ground-truth. One important assumption of this work is that there exist 'real' ground-truths which are better than the human annotations. To verify this, we train two networks which are supervised by the human annotations provided by public database and the searched 'real' ground-truth, respectively. These two detectors are a Hourglass model (HGs) and a ResNet <ref type="bibr" target="#b7">[8]</ref> model which directly regresses the landmark coordinates as <ref type="bibr" target="#b6">[7]</ref>. As shown in Tab. 7, we can see that on both models the 'real' ground-truth (SA) outperforms the human annotations (HA). Clearly, our learned labels are better than the human annotations, verifying our assumption that the se-mantic alignment can find the semantic consistent groundtruths. Global heatmap correction unit. The 2D shape PCA can well keep the face constraint and can be conducted as a post-processing step to enhance the performance of heatmap based methods, like CLM <ref type="bibr" target="#b3">[4]</ref> and most recently LGCN <ref type="bibr" target="#b14">[14]</ref>. We apply the powerful PCA refinement method in LGCN and compare it with our GHCU. We evaluate on 300-VW where the occlusion and low-quality are particularly challenging. As shown in Tab. 8, our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency. Ablation study. To verify the effectiveness of different components in our framework, we conduct this ablation study on 300-VW. For a fair comparison, all the experiments use the same parameter settings. As shown in Tab. 9, Semantic alignment can consistently improve the performance on all subset sets, demonstrating the strong generalization capacity of SA. GHCU is more effective on the challenge data set (Category 3): 8.15% vs 9.91%; Combining SA and GHCU works better than single of them, showing the complementary of these two mechanisms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we first analyze the semantic ambiguity of facial landmarks and show that the potential random noises of landmark annotations can degrade the performance considerably. To address this issue, we propose a a novel latent variable optimization strategy to find the semantically consistent annotations and alleviate random noises during training stage. Extensive experiments demonstrated that our method effectively improves the landmark detection accuracy on different data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The effect of semantic ambiguity detector tends to output an unexpected random position on the 'flat hat'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The search of 'real' ground-truthŷ. Yellow and red boxes represent the searching space N defined in Eq.<ref type="bibr" target="#b6">(7)</ref> and the region corresponding to one candidateŷ, respectively. The weighted sum of likelihood and prior is computed as Eq.<ref type="bibr" target="#b7">(8)</ref>. The search target is to find a positionŷ with the maximum output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Gradual convergence (one image represents one iteration) from the observation o (i.e.ŷ of the last iteration, green dots) to the estimate of real ground-truthŷ (red dots). For last image, the optimization converges because red and green dots are completely overlapped. The use of GHCU for correcting some failed points. Correcting challenging points with GHCU on 300-VW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Global Heatmap Correction Unit (GHCU)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Training loss of the baseline, Semantic Alignment without updating observation (SA w/o update) and Semantic Alignment (SA). The training starts at a roughly converged model (trained using human annotations only) using 300W training set. The update of Semantic Alignment. Under Semantic Alignment framework, all the training labels are updated NME of the baseline, Semantic Alignment without updating observation (SA w/o update) and Semantic Alignment (SA). The training starts at a roughly converged model (trained using human annotations only) on 300W full test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>GHCU Architecture (N is the number of the landmarks)</figDesc><table><row><cell>Layers</cell><cell>Output size</cell><cell>GHCU</cell></row><row><cell>Conv1</cell><cell>128×128</cell><cell>[5×5, 64], stride 2</cell></row><row><cell>Conv2</cell><cell>64×64</cell><cell>[3×3, 64], stride 2</cell></row><row><cell>Conv3</cell><cell>32×32</cell><cell>[3×3, 32], stride 2</cell></row><row><cell>Conv4</cell><cell>16×16</cell><cell>[3×3, 32], stride 2</cell></row><row><cell>Conv5</cell><cell>8×8</cell><cell>[3×3, 16], stride 2</cell></row><row><cell>Conv6</cell><cell>4×4</cell><cell>[3×3, 16], stride 2</cell></row><row><cell>FC1</cell><cell>-</cell><cell>256</cell></row><row><cell>FC2</cell><cell>-</cell><cell>2N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with state of the art on 300W dataset. The error (NME) is normalized by the inter-pupil distance.</figDesc><table><row><cell>Method</cell><cell>subset</cell><cell cols="2">Com. Challenge</cell><cell>Full</cell></row><row><cell>SDM [28]</cell><cell></cell><cell>5.60</cell><cell>15.40</cell><cell>7.52</cell></row><row><cell>CFSS [32]</cell><cell></cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell cols="2">TCDCN [30]</cell><cell>4.80</cell><cell>8.60</cell><cell>5.54</cell></row><row><cell>LBF [20]</cell><cell></cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell cols="2">3DDFA (CVPR16) [35]</cell><cell>6.15</cell><cell>10.59</cell><cell>7.01</cell></row><row><cell cols="2">3DDFA + SDM</cell><cell>5.53</cell><cell>9.56</cell><cell>6.31</cell></row><row><cell cols="2">RAR (ECCV16) [27]</cell><cell>4.12</cell><cell>8.35</cell><cell>4.94</cell></row><row><cell cols="2">TR-DRN (CVPR17) [13]</cell><cell>4.36</cell><cell>7.56</cell><cell>4.99</cell></row><row><cell cols="2">Wing (CVPR18) [7]</cell><cell>3.27</cell><cell>7.18</cell><cell>4.04</cell></row><row><cell cols="2">LAB (CVPR18) [26]</cell><cell>3.42</cell><cell>6.98</cell><cell>4.12</cell></row><row><cell cols="2">SBR (CVPR18) [6]</cell><cell>3.28</cell><cell>7.58</cell><cell>4.10</cell></row><row><cell cols="2">PCD-CNN (CVPR18) [10]</cell><cell>3.67</cell><cell>7.62</cell><cell>4.44</cell></row><row><cell cols="2">DCFE (ECCV18) [25]</cell><cell>3.83</cell><cell>7.54</cell><cell>4.55</cell></row><row><cell>HGs</cell><cell></cell><cell>4.43</cell><cell>7.56</cell><cell>5.04</cell></row><row><cell>HGs + SA</cell><cell></cell><cell>3.75</cell><cell>6.90</cell><cell>4.37</cell></row><row><cell cols="2">HGs + SA + GHCU</cell><cell>3.74</cell><cell>6.87</cell><cell>4.35</cell></row><row><cell cols="2">HGs + Norm</cell><cell>3.95</cell><cell>6.51</cell><cell>4.45</cell></row><row><cell cols="2">HGs + SA + Norm</cell><cell>3.46</cell><cell>6.38</cell><cell>4.03</cell></row><row><cell cols="2">HGs + SA + Norm + GHCU</cell><cell>3.45</cell><cell>6.38</cell><cell>4.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state of the art on 300-VW dataset. The error (RMSE) is normalized by the inter-ocular distance.</figDesc><table><row><cell>Method</cell><cell cols="3">Category 1 Category 2 Category 3</cell></row><row><cell>SDM [28]</cell><cell>7.41</cell><cell>6.18</cell><cell></cell></row><row><cell>CFSS [32]</cell><cell>7.68</cell><cell>6.42</cell><cell>13.67</cell></row><row><cell>TCDCN [31]</cell><cell>7.66</cell><cell>6.77</cell><cell>14.98</cell></row><row><cell>TSTN [12]</cell><cell>5.36</cell><cell>4.51</cell><cell>12.84</cell></row><row><cell>DSRN (CVPR18) [16]</cell><cell>5.33</cell><cell>4.92</cell><cell>8.85</cell></row><row><cell>HGs</cell><cell>4.32</cell><cell>3.83</cell><cell>9.91</cell></row><row><cell>HGs + SA</cell><cell>4.06</cell><cell>3.58</cell><cell>9.19</cell></row><row><cell>HGs + SA + GHCU</cell><cell>3.85</cell><cell>3.46</cell><cell>7.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The effect of the ratio σ 2 2 /σ 2 1 in Eq. (8) on 300W.</figDesc><table><row><cell>σ 2 2 /σ 2 1</cell><cell>0</cell><cell>0.01 0.05</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell></row><row><cell>NME (%)</cell><cell cols="3">4.99 4.79 4.40 4.37</cell><cell cols="3">4.46 4.54 4.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The effects of template size on 300W and AFLW test sets.</figDesc><table><row><cell>template size</cell><cell>1</cell><cell>7</cell><cell>11</cell><cell>15</cell><cell>19</cell><cell>25</cell></row><row><cell>300W Full(%)</cell><cell cols="4">4.76 4.72 4.61 4.53</cell><cell>4.37</cell><cell>4.43</cell></row><row><cell>AFLW Full (%)</cell><cell cols="3">1.89 1.80 1.72</cell><cell>1.62</cell><cell cols="2">1.66 1.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The comparison of the labels searched by our Semantic Alignment (SA) and human annotations (HA) on 300w-full set</figDesc><table><row><cell>Method</cell><cell>HGs (HA)</cell><cell>HGs (SA)</cell><cell>Reg (HA)</cell><cell>Reg (SA)</cell></row><row><cell>NME (%)</cell><cell>5.04</cell><cell>4.37</cell><cell>5.49</cell><cell>5.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>The comparison of GHCU with traditional PCA-based refinement on 300-VW database.</figDesc><table><row><cell>Method</cell><cell cols="4">Category 1 Category 2 Category 3 CPU Time (ms)</cell></row><row><cell>Baseline</cell><cell>4.06</cell><cell>3.58</cell><cell>9.19</cell><cell>-</cell></row><row><cell>PCA [14]</cell><cell>3.99</cell><cell>3.26</cell><cell>7.69</cell><cell>1219</cell></row><row><cell>GHCU</cell><cell>3.85</cell><cell>3.46</cell><cell>7.51</cell><cell>149</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Effectiveness of SA and GHCU tested on 300-VW.</figDesc><table><row><cell>Semantic Alignment (SA)</cell><cell></cell><cell></cell></row><row><cell>GHCU</cell><cell></cell><cell></cell></row><row><cell>Category 1</cell><cell>3.85</cell><cell>4.03 4.06 4.32</cell></row><row><cell>Category 2</cell><cell>3.46</cell><cell>3.66 3.58 3.83</cell></row><row><cell>Category 3</cell><cell>7.51</cell><cell>8.15 9.19 9.91</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Offline deformable face tracking in arbitrary videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Grigoris G Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Vuong Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twostream transformer networks for video-based face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep regression architecture with twostage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via a fully-convolutional localglobal context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Merget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xm2vts: the extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>Second International Conference on Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5040" to="5049" />
		</imprint>
	</monogr>
	<note>Vassilis Athitsos, and Heng Huang</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A semi-automatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grigoris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deeply-initialized coarse-tofine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Robust facial landmark detection via recurrent attentive-refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando De La</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2025" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="918" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
